{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776254"
                        ],
                        "name": "Yedid Hoshen",
                        "slug": "Yedid-Hoshen",
                        "structuredName": {
                            "firstName": "Yedid",
                            "lastName": "Hoshen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yedid Hoshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39571582"
                        ],
                        "name": "Ron J. Weiss",
                        "slug": "Ron-J.-Weiss",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Weiss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron J. Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12812321"
                        ],
                        "name": "K. Wilson",
                        "slug": "K.-Wilson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Wilson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 190
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 231
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; Tu\u0308ske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7778523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58592e7acf612023406800794a8b86e926f20cba",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard deep neural network-based acoustic models for automatic speech recognition (ASR) rely on hand-engineered input features, typically log-mel filterbank magnitudes. In this paper, we describe a convolutional neural network - deep neural network (CNN-DNN) acoustic model which takes raw multichannel waveforms as input, i.e. without any preceding feature extraction, and learns a similar feature representation through supervised training. By operating directly in the time domain, the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs, where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival, i.e. a bank of bandpass beamformers with an auditory-like frequency scale. When trained on data corrupted with noise coming from different spatial locations, the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses log-mel filterbank magnitude features under noisy and reverberant conditions."
            },
            "slug": "Speech-acoustic-modeling-from-raw-multichannel-Hoshen-Weiss",
            "title": {
                "fragments": [],
                "text": "Speech acoustic modeling from raw multichannel waveforms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A convolutional neural network - deep neural network (CNN-DNN) acoustic model which takes raw multichannel waveforms as input, and learns a similar feature representation through supervised training and outperforms a DNN that uses log-mel filterbank magnitude features under noisy and reverberant conditions."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715204"
                        ],
                        "name": "P. Muthukumar",
                        "slug": "P.-Muthukumar",
                        "structuredName": {
                            "firstName": "Prasanna",
                            "lastName": "Muthukumar",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Muthukumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 155
                            }
                        ],
                        "text": "There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16268912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "595f25770afe27204bec92f4e65920407de50000",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis. We create an invertible, low-dimensional, noise-robust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is then unwrapped and used as the initialization for a Multi-Layer Perceptron (MLP). The MLP is fine-tuned by training it to reconstruct the input at the output layer. This MLP is then split down the middle to form encoding and decoding networks. These networks produce a parameterization of the Mel Log Spectrum that is intended to better fulfill the requirements of synthesis. Results are reported for experiments conducted using this resulting parameterization with the ClusterGen speech synthesizer."
            },
            "slug": "A-Deep-Learning-Approach-to-Data-driven-for-Speech-Muthukumar-Black",
            "title": {
                "fragments": [],
                "text": "A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper creates an invertible, low-dimensional, noise-robust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA), and investigates a data-driven parameterization technique that is designed for the specific requirements of synthesis."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39571582"
                        ],
                        "name": "Ron J. Weiss",
                        "slug": "Ron-J.-Weiss",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Weiss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron J. Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12812321"
                        ],
                        "name": "K. Wilson",
                        "slug": "K.-Wilson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Wilson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 190
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 252
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; Tu\u0308ske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 966801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd5474f21495989777cbff507ecf1b37b7091475",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an acoustic model directly from the raw waveform has been an active area of research. However, waveformbased models have not yet matched the performance of logmel trained neural networks. We will show that raw waveform features match the performance of log-mel filterbank energies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech. Specifically, we will show the benefit of the CLDNN, namely the time convolution layer in reducing temporal variations, the frequency convolution layer for preserving locality and reducing frequency variations, as well as the LSTM layers for temporal modeling. In addition, by stacking raw waveform features with log-mel features, we achieve a 3% relative reduction in word error rate."
            },
            "slug": "Learning-the-speech-front-end-with-raw-waveform-Sainath-Weiss",
            "title": {
                "fragments": [],
                "text": "Learning the speech front-end with raw waveform CLDNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that raw waveform features match the performance of log-mel filterbank energies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790221"
                        ],
                        "name": "Zolt\u00e1n T\u00fcske",
                        "slug": "Zolt\u00e1n-T\u00fcske",
                        "structuredName": {
                            "firstName": "Zolt\u00e1n",
                            "lastName": "T\u00fcske",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zolt\u00e1n T\u00fcske"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3207549"
                        ],
                        "name": "Pavel Golik",
                        "slug": "Pavel-Golik",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Golik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pavel Golik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144490010"
                        ],
                        "name": "R. Schl\u00fcter",
                        "slug": "R.-Schl\u00fcter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Schl\u00fcter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schl\u00fcter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 190
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 211
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; Tu\u0308ske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17323509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dae891c65f9c555cff233fe4bf6502d283c31ecd",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate how much feature extraction is required by a deep neural network (DNN) based acoustic model for automatic speech recognition (ASR). We decompose the feature extraction pipeline of a state-of-the-art ASR system step by step and evaluate acoustic models trained on standard MFCC features, critical band energies (CRBE), FFT magnitude spectrum and even on the raw time signal. The focus is put on raw time signal as input features, i.e. as much as zero feature extraction prior to DNN training. Noteworthy, the gap in recognition accuracy between MFCC and raw time signal decreases strongly once we switch from sigmoid activation function to rectified linear units, offering a real alternative to standard signal processing. The analysis of the first layer weights reveals that the DNN can discover multiple band pass filters in time domain. Therefore we try to improve the raw time signal based system by initializing the first hidden layer weights with impulse responses of an audiologically motivated filter bank. Inspired by the multi-resolutional analysis layer learned automatically from raw time signal input, we train the DNN on a combination of multiple short-term features. This illustrates how the DNN can learn from the little differences between MFCC, PLP and Gammatone features, suggesting that it is useful to present the DNN with different views on the underlying audio."
            },
            "slug": "Acoustic-modeling-with-deep-neural-networks-using-T\u00fcske-Golik",
            "title": {
                "fragments": [],
                "text": "Acoustic modeling with deep neural networks using raw time signal for LVCSR"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Inspired by the multi-resolutional analysis layer learned automatically from raw time signal input, the DNN is trained on a combination of multiple short-term features, illustrating how the Dnn can learn from the little differences between MFCC, PLP and Gammatone features."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16664621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20969e35837d93b6e4eb52d75c8713abc6069f4a",
            "isKey": false,
            "numCitedBy": 801,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Conventional approaches to statistical parametric speech synthesis typically use decision tree-clustered context-dependent hidden Markov models (HMMs) to represent probability densities of speech parameters given texts. Speech parameters are generated from the probability densities to maximize their output probabilities, then a speech waveform is reconstructed from the generated parameters. This approach is reasonably effective but has a couple of limitations, e.g. decision trees are inefficient to model complex context dependencies. This paper examines an alternative scheme that is based on a deep neural network (DNN). The relationship between input texts and their acoustic realizations is modeled by a DNN. The use of the DNN can address some limitations of the conventional approach. Experimental results show that the DNN-based systems outperformed the HMM-based systems with similar numbers of parameters."
            },
            "slug": "Statistical-parametric-speech-synthesis-using-deep-Zen-Senior",
            "title": {
                "fragments": [],
                "text": "Statistical parametric speech synthesis using deep neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper examines an alternative scheme that is based on a deep neural network (DNN), the relationship between input texts and their acoustic realizations is modeled by a DNN, and experimental results show that the DNN- based systems outperformed the HMM-based systems with similar numbers of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825051"
                        ],
                        "name": "B. Uria",
                        "slug": "B.-Uria",
                        "structuredName": {
                            "firstName": "Benigno",
                            "lastName": "Uria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Uria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401922561"
                        ],
                        "name": "Cassia Valentini-Botinhao",
                        "slug": "Cassia-Valentini-Botinhao",
                        "structuredName": {
                            "firstName": "Cassia",
                            "lastName": "Valentini-Botinhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cassia Valentini-Botinhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 191
                            }
                        ],
                        "text": "\u2026developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 402835,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "731f7224a9c968e5b544a6747df4e23fa4e76d88",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a transcription, sampling from a good model of acoustic feature trajectories should result in plausible realizations of an utterance. However, samples from current probabilistic speech synthesis systems result in low quality synthetic speech. Henter et al. have demonstrated the need to capture the dependencies between acoustic features conditioned on the phonetic labels in order to obtain high quality synthetic speech. These dependencies are often ignored in neural network based acoustic models. We tackle this deficiency by introducing a probabilistic neural network model of acoustic trajectories, trajectory RNADE, able to capture these dependencies."
            },
            "slug": "Modelling-acoustic-feature-dependencies-with-neural-Uria-Murray",
            "title": {
                "fragments": [],
                "text": "Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A probabilistic neural network model of acoustic trajectories, trajectory RNADE, is introduced, able to capture the dependencies between acoustic features conditioned on the phonetic labels in order to obtain high quality synthetic speech."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16805640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bf053b97e0cce85aa47f6597cd5ec116333c4f8",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel approach for directly-modeling speech at the waveform level using a neural network. This approach uses the neural network-based statistical parametric speech synthesis framework with a specially designed output layer. As acoustic feature extraction is integrated to acoustic model training, it can overcome the limitations of conventional approaches, such as two-step (feature extraction and acoustic modeling) optimization, use of spectra rather than waveforms as targets, use of overlapping and shifting frames as unit, and fixed decision tree structure. Experimental results show that the proposed approach can directly maximize the likelihood defined at the waveform domain."
            },
            "slug": "Directly-modeling-speech-waveforms-by-neural-for-Tokuda-Zen",
            "title": {
                "fragments": [],
                "text": "Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Experimental results show that the proposed approach can directly maximize the likelihood defined at the waveform domain."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2922874"
                        ],
                        "name": "Dimitri Palaz",
                        "slug": "Dimitri-Palaz",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Palaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitri Palaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398480065"
                        ],
                        "name": "M. Magimai.-Doss",
                        "slug": "M.-Magimai.-Doss",
                        "structuredName": {
                            "firstName": "Mathew",
                            "lastName": "Magimai.-Doss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Magimai.-Doss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 190
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 191
                            }
                        ],
                        "text": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; Tu\u0308ske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1845461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "155e7a8ebabd17726bfa1c05b4bd5939e6772a51",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine learning techniques, more specifically in the field of image processing and text processing, have shown that such divide and conquer strategy (i.e., separating feature extraction and modeling steps) may not be necessary. Motivated from these studies, in the framework of convolutional neural networks (CNNs), this paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probability estimates. On TIMIT phoneme recognition task, we study different ANN architectures to show the benefit of CNNs and compare the proposed approach against conventional approach where, spectral-based feature MFCC is extracted and modeled by a multilayer perceptron. Our studies show that the proposed approach can yield comparable or better phoneme recognition performance when compared to the conventional approach. It indicates that CNNs can learn features relevant for phoneme classification automatically from the raw speech signal."
            },
            "slug": "Estimating-phoneme-class-conditional-probabilities-Palaz-Collobert",
            "title": {
                "fragments": [],
                "text": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper investigates a novel approach, where the input to the ANN is raw speech signal and the output is phoneme class conditional probability estimates, and indicates that CNNs can learn features relevant for phoneme classification automatically from the rawspeech signal."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424104"
                        ],
                        "name": "Shinnosuke Takamichi",
                        "slug": "Shinnosuke-Takamichi",
                        "structuredName": {
                            "firstName": "Shinnosuke",
                            "lastName": "Takamichi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinnosuke Takamichi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726559"
                        ],
                        "name": "T. Toda",
                        "slug": "T.-Toda",
                        "structuredName": {
                            "firstName": "Tomoki",
                            "lastName": "Toda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Toda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700325"
                        ],
                        "name": "Graham Neubig",
                        "slug": "Graham-Neubig",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Neubig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham Neubig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783949"
                        ],
                        "name": "S. Sakti",
                        "slug": "S.-Sakti",
                        "structuredName": {
                            "firstName": "Sakriani",
                            "lastName": "Sakti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sakti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145223960"
                        ],
                        "name": "Satoshi Nakamura",
                        "slug": "Satoshi-Nakamura",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satoshi Nakamura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 275
                            }
                        ],
                        "text": "\u2026developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12597237,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "4b890b6ded71f005414e55adb87c23efd437ef95",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents novel approaches based on modulation spectrum (MS) for high-quality statistical parametric speech synthesis, including text-to-speech (TTS) and voice conversion (VC). Although statistical parametric speech synthesis offers various advantages over concatenative speech synthesis, the synthetic speech quality is still not as good as that of concatenative speech synthesis or the quality of natural speech. One of the biggest issues causing the quality degradation is the over-smoothing effect often observed in the generated speech parameter trajectories. Global variance (GV) is known as a feature well correlated with the over-smoothing effect, and the effectiveness of keeping the GV of the generated speech parameter trajectories similar to those of natural speech has been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we propose using the MS of the generated speech parameter trajectories as a new feature to effectively quantify the over-smoothing effect. Moreover, we propose postfilters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech. The proposed postfilters are applicable to various synthesizers based on statistical parametric speech synthesis. We first perform an evaluation of the proposed method in the framework of hidden Markov model (HMM)-based TTS, examining its properties from different perspectives. Furthermore, effectiveness of the proposed postfilters are also evaluated in Gaussian mixture model (GMM)-based VC and classification and regression trees (CART)-based TTS (a.k.a., CLUSTERGEN). The experimental results demonstrate that 1) the proposed utterance-level postfilter achieves quality comparable to the conventional generation algorithm considering the GV, and yields significant improvements by applying to the GV-based generation algorithm in HMM-based TTS, 2) the proposed segment-level postfilter capable of achieving low-delay synthesis also yields significant improvements in synthetic speech quality, and 3) the proposed postfilters are also effective in not only HMM-based TTS but also GMM-based VC and CLUSTERGEN."
            },
            "slug": "Postfilters-to-Modify-the-Modulation-Spectrum-for-Takamichi-Toda",
            "title": {
                "fragments": [],
                "text": "Postfilters to Modify the Modulation Spectrum for Statistical Parametric Speech Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper proposes postfilters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech, applicable to various synthesizers based on statistical parametric speech synthesis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 201
                            }
                        ],
                        "text": "\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 280
                            }
                        ],
                        "text": "\u2026Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 13
                            }
                        ],
                        "text": "For example, Tokuda & Zen (2016) integrated non-stationary, nonzero-mean Gaussian process generative model of speech signals and LSTM-RNN-based sequence generative model to a single one and jointly optimized them by back-propagation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 181
                            }
                        ],
                        "text": "There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6445111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "629380a24c3fc7c26e96415f5cf8d6496b2d1c38",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel acoustic model based on neural networks for statistical parametric speech synthesis. The neural network outputs parameters of a non-zero mean Gaussian process, which defines a probability density function of a speech waveform given linguistic features. The mean and covariance functions of the Gaussian process represent deterministic (voiced) and stochastic (unvoiced) components of a speech waveform, whereas the previous approach considered the unvoiced component only. Experimental results show that the proposed approach can generate speech waveforms approximating natural speech waveforms."
            },
            "slug": "Directly-modeling-voiced-and-unvoiced-components-in-Tokuda-Zen",
            "title": {
                "fragments": [],
                "text": "Directly modeling voiced and unvoiced components in speech waveforms by neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel acoustic model based on neural networks for statistical parametric speech synthesis that can generate speech waveforms approximating natural speechWaveforms."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 156
                            }
                        ],
                        "text": "\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 215
                            }
                        ],
                        "text": "\u2026Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 251
                            }
                        ],
                        "text": "\u2026signals have a number of assumptions which are inspired from the speech production, such as\n\u2022 Use of fixed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "\u2022 Linear filter; These generative models are typically realized as a linear time-invariant filter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15553242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "436f38dc28ca25af965b202ebe0e27c747888da6",
            "isKey": true,
            "numCitedBy": 332,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a signal modeling technique based upon finite mixture autoregressive probabilistic functions of Markov chains is developed and applied to the problem of speech recognition, particularly speaker-independent recognition of isolated digits. Two types of mixture probability densities are investigated: finite mixtures of Gaussian autoregressive densities (GAM) and nearest-neighbor partitioned finite mixtures of Gaussian autoregressive densities (PGAM). In the former (GAM), the observation density in each Markov state is simply a (stochastically constrained) weighted sum of Gaussian autoregressive densities, while in the latter (PGAM) it involves nearest-neighbor decoding which in effect, defines a set of partitions on the observation space. In this paper we discuss the signal modeling methodology and give experimental results on speaker independent recognition of isolated digits. We also discuss the potential use of the modeling technique for other applications."
            },
            "slug": "Mixture-autoregressive-hidden-Markov-models-for-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "Mixture autoregressive hidden Markov models for speech signals"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The signal modeling methodology is discussed and experimental results on speaker independent recognition of isolated digits are given and the potential use of the modeling technique for other applications are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48097064"
                        ],
                        "name": "Andrew J. Hunt",
                        "slug": "Andrew-J.-Hunt",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Hunt",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew J. Hunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 157
                            }
                        ],
                        "text": "\u2026speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis (Moulines & Charpentier, 1990; Sagisaka et al., 1992; Hunt & Black, 1996), and parametric, model-based approach known as statistical parametric speech synthesis (Yoshimura, 2002; Zen et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14621185,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dd0140d51e870a713340ae30734c8438b03d1a3",
            "isKey": false,
            "numCitedBy": 1380,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning."
            },
            "slug": "Unit-selection-in-a-concatenative-speech-synthesis-Hunt-Black",
            "title": {
                "fragments": [],
                "text": "Unit selection in a concatenative speech synthesis system using a large speech database"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is proposed that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102981024"
                        ],
                        "name": "A. Poritz",
                        "slug": "A.-Poritz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Poritz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Poritz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 201
                            }
                        ],
                        "text": "\u2026Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 237
                            }
                        ],
                        "text": "\u2026signals have a number of assumptions which are inspired from the speech production, such as\n\u2022 Use of fixed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2022 Linear filter; These generative models are typically realized as a linear time-invariant filter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32413326,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1e6f96bee0a7b78402866e1461d00a72612dcc69",
            "isKey": true,
            "numCitedBy": 252,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for modelling time series is presented and then applied to the analysis of the speech signal. A time series is represented as a sample sequence generated by a finite state hidden Markov model with output densities parameterized by linear prediction polynomials and error variances. These objects are defined and their properties developed. The theory culminates in a theorem that provides a computationally efficient iterative scheme to improve the model. The theorem has been used to create models from speech signals of considerable length. One such model is examined with emphasis on the relationship between states of the model and traditional classes of speech events. A use of the method is illustrated by an application to the talker verification problem."
            },
            "slug": "Linear-predictive-hidden-Markov-models-and-the-Poritz",
            "title": {
                "fragments": [],
                "text": "Linear predictive hidden Markov models and the speech signal"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method for modelling time series is presented and then applied to the analysis of the speech signal, resulting in a theorem that provides a computationally efficient iterative scheme to improve the model."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880708"
                        ],
                        "name": "Shinji Takaki",
                        "slug": "Shinji-Takaki",
                        "structuredName": {
                            "firstName": "Shinji",
                            "lastName": "Takaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinji Takaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716857"
                        ],
                        "name": "J. Yamagishi",
                        "slug": "J.-Yamagishi",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Yamagishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yamagishi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 207
                            }
                        ],
                        "text": "There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10893790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24d6f1bfb663a9d7a6aa53bd12c0c8014aa78a2f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In the state-of-the-art statistical parametric speech synthesis system, a speech analysis module, e.g. STRAIGHT spectral analysis, is generally used for obtaining accurate and stable spectral envelopes, and then low-dimensional acoustic features extracted from obtained spectral envelopes are used for training acoustic models. However, a spectral envelope estimation algorithm used in such a speech analysis module includes various processing derived from human knowledge. In this paper, we present our investigation of deep autoencoder based, non-linear, data-driven and unsupervised low-dimensional feature extraction using FFT spectral envelopes for statistical parametric speech synthesis. Experimental results showed that a text-to-speech synthesis system using deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes is indeed a promising approach."
            },
            "slug": "A-deep-auto-encoder-based-low-dimensional-feature-Takaki-Yamagishi",
            "title": {
                "fragments": [],
                "text": "A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Experimental results showed that a text-to-speech synthesis system using deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes is indeed a promising approach."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2313661"
                        ],
                        "name": "\u00c9. Moulines",
                        "slug": "\u00c9.-Moulines",
                        "structuredName": {
                            "firstName": "\u00c9ric",
                            "lastName": "Moulines",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9. Moulines"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15216995"
                        ],
                        "name": "F. Charpentier",
                        "slug": "F.-Charpentier",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Charpentier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Charpentier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 148
                            }
                        ],
                        "text": "There are two main approaches to realize the speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis (Moulines & Charpentier, 1990; Sagisaka et al., 1992; Hunt & Black, 1996), and parametric, model-based approach known as statistical parametric speech\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15384823,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "d31898b96c994fb8d661a8094344a379712b672b",
            "isKey": false,
            "numCitedBy": 1496,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pitch-synchronous-waveform-processing-techniques-Moulines-Charpentier",
            "title": {
                "fragments": [],
                "text": "Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259013"
                        ],
                        "name": "Yannis Agiomyrgiannakis",
                        "slug": "Yannis-Agiomyrgiannakis",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Agiomyrgiannakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yannis Agiomyrgiannakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422416"
                        ],
                        "name": "N. Egberts",
                        "slug": "N.-Egberts",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Egberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Egberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068528804"
                        ],
                        "name": "Fergus Henderson",
                        "slug": "Fergus-Henderson",
                        "structuredName": {
                            "firstName": "Fergus",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fergus Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256449"
                        ],
                        "name": "Przemyslaw Szczepaniak",
                        "slug": "Przemyslaw-Szczepaniak",
                        "structuredName": {
                            "firstName": "Przemyslaw",
                            "lastName": "Szczepaniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Przemyslaw Szczepaniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Zen et al. (2016) showed that state-of-the-art statistical parametric speech syntheziers matched state-of-the-art concatenative ones in some languages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 240
                            }
                        ],
                        "text": "As example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative (Gonzalvo et al., 2016) and long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric (Zen et al., 2016) speech synthesizers were built."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6445820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16fea85194803a3980bf0381fa5ec997d3de178c",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Acoustic models based on long short-term memory recurrent neural networks (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and showed significant improvements in naturalness and latency over those based on hidden Markov models (HMMs). This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an {\\epsilon}-contaminated Gaussian loss function. Experimental results in subjective listening tests show that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime speed while maintaining naturalness. Evaluations between LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also presented."
            },
            "slug": "Fast,-Compact,-and-High-Quality-LSTM-RNN-Based-for-Zen-Agiomyrgiannakis",
            "title": {
                "fragments": [],
                "text": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an {\\epsilon}-contaminated Gaussian loss function are described."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259013"
                        ],
                        "name": "Yannis Agiomyrgiannakis",
                        "slug": "Yannis-Agiomyrgiannakis",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Agiomyrgiannakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yannis Agiomyrgiannakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 140
                            }
                        ],
                        "text": "There have been a number of attempts to address these issues individually, such as developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 177
                            }
                        ],
                        "text": "Although LSTM-RNNs were trained from speech at 22.05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder (Agiomyrgiannakis, 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 218221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22c1c1dc95f043ea04aadffb21c748977718f58b",
            "isKey": true,
            "numCitedBy": 69,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Vocoders received renewed attention recently as basic components in speech synthesis applications such as voice transformation, voice conversion and statistical parametric speech synthesis. This paper presents a new vocoder synthesizer, referred to as Vocaine, that features a novel Amplitude Modulated-Frequency Modulated (AM-FM) speech model, a new way to synthesize non-stationary sinusoids using quadratic phase splines and a super fast cosine generator. Extensive evaluations are made against several state-of-the-art methods in Copy-Synthesis and Text-To-Speech synthesis experiments. Vocaine matches or outperforms STRAIGHT in Copy-Synthesis experiments and outperforms our baseline real-time optimized Mixed-Excitation vocoder with the same computational cost. We report that Vocaine considerably improves our statistical TTS synthesizers and that our new statistical parametric synthesizer [1] matched the quality of our mature production Unit-Selection system with uncompressed waveforms."
            },
            "slug": "Vocaine-the-vocoder-and-applications-in-speech-Agiomyrgiannakis",
            "title": {
                "fragments": [],
                "text": "Vocaine the vocoder and applications in speech synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new vocoder synthesizer, referred to as Vocaine, that features a novel Amplitude Modulated-Frequency Modulated (AM-FM) speech model, a new way to synthesize non-stationary sinusoids using quadratic phase splines and a super fast cosine generator is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981206"
                        ],
                        "name": "R. Maia",
                        "slug": "R.-Maia",
                        "structuredName": {
                            "firstName": "Ranniery",
                            "lastName": "Maia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740397"
                        ],
                        "name": "M. Gales",
                        "slug": "M.-Gales",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gales",
                            "middleNames": [
                                "John",
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 113
                            }
                        ],
                        "text": "There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13382796,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "76093cb346dafdc36c33ea8aff8baaaa1ac80f4d",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel framework for statistical parametric speech synthesis in which statistical modeling of the speech waveform is performed through the joint estimation of acoustic and excitation model parameters. The proposed method combines extraction of spectral parameters, considered as hidden variables, and excitation signal modeling in a fashion similar to factor analyzed trajectory hidden Markov model. The resulting joint model can be interpreted as a waveform level closed-loop training, where the distance between natural and synthesized speech is minimized. An algorithm based on the maximum likelihood criterion is introduced to train the proposed joint model and some experiments are presented to show its effectiveness."
            },
            "slug": "Statistical-parametric-speech-synthesis-with-joint-Maia-Zen",
            "title": {
                "fragments": [],
                "text": "Statistical parametric speech synthesis with joint estimation of acoustic and excitation model parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A novel framework for statistical parametric speech synthesis in which statistical modeling of the speech waveform is performed through the joint estimation of acoustic and excitation model parameters, similar to factor analyzed trajectory hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "SSW"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7888497"
                        ],
                        "name": "Yuchen Fan",
                        "slug": "Yuchen-Fan",
                        "structuredName": {
                            "firstName": "Yuchen",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuchen Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48549092"
                        ],
                        "name": "Yao Qian",
                        "slug": "Yao-Qian",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34463646"
                        ],
                        "name": "Feng-Long Xie",
                        "slug": "Feng-Long-Xie",
                        "structuredName": {
                            "firstName": "Feng-Long",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng-Long Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705574"
                        ],
                        "name": "F. Soong",
                        "slug": "F.-Soong",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Soong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Soong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Yoshimura, 2002), feed-forward neural networks (Zen et al., 2013), and recurrent neural networks (Tuerk & Robinson, 1993; Karaali et al., 1997; Fan et al., 2014), is trained from the extracted vocoder parameters and linguistic features\nas \u039b\u0302 = arg max\n\u039b p (o | l,\u039b) , (4)\nwhere \u039b denotes the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 173
                            }
                        ],
                        "text": "\u2026developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18649557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c217905bc98f00af747e8e9d5f6b79fb89a90886",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Feed-forward, Deep neural networks (DNN)-based text-tospeech (TTS) systems have been recently shown to outperform decision-tree clustered context-dependent HMM TTS systems [1, 4]. However, the long time span contextual effect in a speech utterance is still not easy to accommodate, due to the intrinsic, feed-forward nature in DNN-based modeling. Also, to synthesize a smooth speech trajectory, the dynamic features are commonly used to constrain speech parameter trajectory generation in HMM-based TTS [2]. In this paper, Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis. Experimental results show that a hybrid system of DNN and BLSTM-RNN, i.e., lower hidden layers with a feed-forward structure which is cascaded with upper hidden layers with a bidirectional RNN structure of LSTM, can outperform either the conventional, decision tree-based HMM, or a DNN TTS system, both objectively and subjectively. The speech trajectory generated by the BLSTM-RNN TTS is fairly smooth and no dynamic constraints are needed."
            },
            "slug": "TTS-synthesis-with-bidirectional-LSTM-based-neural-Fan-Qian",
            "title": {
                "fragments": [],
                "text": "TTS synthesis with bidirectional LSTM based recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Recurrent Neural Networks (RNNs) with Bidirectional Long Short Term Memory (BLSTM) cells are adopted to capture the correlation or co-occurrence information between any two instants in a speech utterance for parametric TTS synthesis."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726559"
                        ],
                        "name": "T. Toda",
                        "slug": "T.-Toda",
                        "structuredName": {
                            "firstName": "Tomoki",
                            "lastName": "Toda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Toda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 254
                            }
                        ],
                        "text": "\u2026developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13526005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f9f4f197c79e6c41e4c6e7751b8a55bf0044fe0",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel parameter generation algorithm for an HMM-based speech synthesis technique. The conventional algorithm generates a parameter trajectory of static features that maximizes the likelihood of a given HMM for the parameter sequence consisting of the static and dynamic features under an explicit constraint between those two features. The generated trajectory is often excessively smoothed due to the statistical processing. Using the over-smoothed speech parameters usually causes muffled sounds. In order to alleviate the over-smoothing effect, we propose a generation algorithm considering not only the HMM likelihood maximized in the conventional algorithm but also a likelihood for a global variance (GV) of the generated trajectory. The latter likelihood works as a penalty for the over-smoothing, i.e., a reduction of the GV of the generated trajectory. The result of a perceptual evaluation demonstrates that the proposed algorithm causes considerably large improvements in the naturalness of synthetic speech."
            },
            "slug": "A-Speech-Parameter-Generation-Algorithm-Considering-Toda-Tokuda",
            "title": {
                "fragments": [],
                "text": "A Speech Parameter Generation Algorithm Considering Global Variance for HMM-Based Speech Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A generation algorithm considering not only the HMM likelihood maximized in the conventional algorithm but also a likelihood for a global variance of the generated trajectory works as a penalty for the over-smoothing."
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inf. Syst."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638666"
                        ],
                        "name": "O. Karaali",
                        "slug": "O.-Karaali",
                        "structuredName": {
                            "firstName": "Orhan",
                            "lastName": "Karaali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Karaali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37741621"
                        ],
                        "name": "Gerald Corrigan",
                        "slug": "Gerald-Corrigan",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Corrigan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald Corrigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36273679"
                        ],
                        "name": "I. Gerson",
                        "slug": "I.-Gerson",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Gerson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Gerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145678710"
                        ],
                        "name": "N. Massey",
                        "slug": "N.-Massey",
                        "structuredName": {
                            "firstName": "Noel",
                            "lastName": "Massey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Massey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Markov models (HMMs) (Yoshimura, 2002), feed-forward neural networks (Zen et al., 2013), and recurrent neural networks (Tuerk & Robinson, 1993; Karaali et al., 1997; Fan et al., 2014), is trained from the extracted vocoder parameters and linguistic features\nas \u039b\u0302 = arg max\n\u039b p (o | l,\u039b) ,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a069c56e05db4dcdbc8072d1795568443f3b439",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the design of a neural network that performs the phonetic-to-acoustic mapping in a speech synthesis system. The use of a time-domain neural network architecture limits discontinuities that occur at phone boundaries. Recurrent data input also helps smooth the output parameter tracks. Independent testing has demonstrated that the voice quality produced by this system compares favorably with speech from existing commercial text-to-speech systems."
            },
            "slug": "Text-to-speech-conversion-with-neural-networks:-a-Karaali-Corrigan",
            "title": {
                "fragments": [],
                "text": "Text-to-speech conversion with neural networks: a recurrent TDNN approach"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The design of a neural network that performs the phonetic-to-acoustic mapping in a speech synthesis system using a time-domain neural network architecture that limits discontinuities that occur at phone boundaries."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073063"
                        ],
                        "name": "Lucas Theis",
                        "slug": "Lucas-Theis",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Theis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucas Theis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731199"
                        ],
                        "name": "M. Bethge",
                        "slug": "M.-Bethge",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Bethge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bethge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 254
                            }
                        ],
                        "text": "One approach to modeling the conditional distributions p (xt | x1, . . . , xt\u22121) over the individual audio samples would be to use a mixture model such as a mixture density network (Bishop, 1994) or mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2865509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f267934e9de60c5badfa9d3f28918e67ae7a2bf4",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting."
            },
            "slug": "Generative-Image-Modeling-Using-Spatial-LSTMs-Theis-Bethge",
            "title": {
                "fragments": [],
                "text": "Generative Image Modeling Using Spatial LSTMs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces a recurrent image model based on multidimensional long short-term memory units which is particularly suited for image modeling due to their spatial structure and outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071514"
                        ],
                        "name": "Hideki Kawahara",
                        "slug": "Hideki-Kawahara",
                        "structuredName": {
                            "firstName": "Hideki",
                            "lastName": "Kawahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideki Kawahara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404618304"
                        ],
                        "name": "Ikuyo Masuda-Katsuse",
                        "slug": "Ikuyo-Masuda-Katsuse",
                        "structuredName": {
                            "firstName": "Ikuyo",
                            "lastName": "Masuda-Katsuse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ikuyo Masuda-Katsuse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7659608"
                        ],
                        "name": "A. Cheveign\u00e9",
                        "slug": "A.-Cheveign\u00e9",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Cheveign\u00e9",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cheveign\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 117
                            }
                        ],
                        "text": "There have been a number of attempts to address these issues individually, such as developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18151124,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "518c3867f4272fd46fc5e8ecf1eb4f6af517c698",
            "isKey": false,
            "numCitedBy": 2012,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Restructuring-speech-representations-using-a-and-an-Kawahara-Masuda-Katsuse",
            "title": {
                "fragments": [],
                "text": "Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110773766"
                        ],
                        "name": "Kazuhiro Nakamura",
                        "slug": "Kazuhiro-Nakamura",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuhiro Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700540"
                        ],
                        "name": "Kei Hashimoto",
                        "slug": "Kei-Hashimoto",
                        "structuredName": {
                            "firstName": "Kei",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kei Hashimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714830"
                        ],
                        "name": "Yoshihiko Nankaku",
                        "slug": "Yoshihiko-Nankaku",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Nankaku",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiko Nankaku"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35127305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8517b1a43a74fca82b549bcc9bd907bea2b1e4e4",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel approach for integrating spectral feature extraction and acoustic modeling in hidden Markov model (HMM) based speech synthesis. The statistical modeling process of speech waveforms is typically divided into two component modules: the frame-byframe feature extraction module and the acoustic modeling module. In the feature extraction module, the statistical mel-cepstral analysis technique has been used and the objective function is the likelihood of mel-cepstral coefficients for given speech waveforms. In the acoustic modeling module, the objective function is the likelihood of model parameters for given melcepstral coefficients. It is important to improve the performance of each component module for achieving higher quality synthesized speech. However, the final objective of speech synthesis systems is to generate natural speech waveforms from given texts, and the improvement of each component module does not always lead to the improvement of the quality of synthesized speech. Therefore, ideally all objective functions should be optimized based on an integrated criterion which well represents subjective speech quality of human perception. In this paper, we propose an approach to model speech waveforms directly and optimize the final objective function. Experimental results show that the proposed method outperformed the conventional methods in objective and subjective measures. key words: integrative model, HMM-based speech synthesis, acoustic modeling, mel-cepstral analysis, trajectory HMM"
            },
            "slug": "Integration-of-Spectral-Feature-Extraction-and-for-Nakamura-Hashimoto",
            "title": {
                "fragments": [],
                "text": "Integration of Spectral Feature Extraction and Modeling for HMM-Based Speech Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An approach to model speech waveforms directly and optimize the final objective function is proposed and Experimental results show that the proposed method outperformed the conventional methods in objective and subjective measures."
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inf. Syst."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422336"
                        ],
                        "name": "A\u00e4ron van den Oord",
                        "slug": "A\u00e4ron-van-den-Oord",
                        "structuredName": {
                            "firstName": "A\u00e4ron",
                            "lastName": "Oord",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A\u00e4ron van den Oord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311318"
                        ],
                        "name": "Lasse Espeholt",
                        "slug": "Lasse-Espeholt",
                        "structuredName": {
                            "firstName": "Lasse",
                            "lastName": "Espeholt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lasse Espeholt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 84
                            }
                        ],
                        "text": "For images, the equivalent of a causal convolution is a masked convolution (van den Oord et al., 2016a) which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "Remarkably, these architectures are able to model distributions over thousands of random variables (e.g. 64\u00d764 pixels as in PixelRNN (van den Oord et al., 2016a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 32
                            }
                        ],
                        "text": "Similarly to PixelCNNs (van den Oord et al., 2016a;b), the conditional probability distribution is modelled by a stack of convolutional layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 17
                            }
                        ],
                        "text": "However, van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 88
                            }
                        ],
                        "text": "This paper introduces WaveNet, an audio generative model based on the PixelCNN (van den Oord et al., 2016a;b) architecture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 180
                            }
                        ],
                        "text": "This work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images (van den Oord et al., 2016a;b) and text (Jo\u0301zefowicz et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 77
                            }
                        ],
                        "text": "We use the same gated activation unit as used in the gated PixelCNN (van den Oord et al., 2016b):\nz = tanh (Wf,k \u2217 x) \u03c3 (Wg,k \u2217 x) , (2)\nwhere \u2217 denotes a convolution operator, denotes an element-wise multiplication operator, \u03c3(\u00b7) is a sigmoid function, k is the layer index, f and g denote filter\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14989939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0936352b78a52bc5d2b5e3f04233efc56664af51",
            "isKey": false,
            "numCitedBy": 1607,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost."
            },
            "slug": "Conditional-Image-Generation-with-PixelCNN-Decoders-Oord-Kalchbrenner",
            "title": {
                "fragments": [],
                "text": "Conditional Image Generation with PixelCNN Decoders"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607963"
                        ],
                        "name": "Yonghui Wu",
                        "slug": "Yonghui-Wu",
                        "structuredName": {
                            "firstName": "Yonghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghui Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "plores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images (van den Oord et al., 2016a;b) and text (Jozefowicz et al., 2016). Modeling joint probabilities over pixels or words using\u00b4 neural architectures as products of conditional distributions yields state-of-the-art generation. Remarkably, these architectures are able to"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 260422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "isKey": false,
            "numCitedBy": 951,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon."
            },
            "slug": "Exploring-the-Limits-of-Language-Modeling-J\u00f3zefowicz-Vinyals",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This work explores recent advances in Recurrent Neural Networks for large scale Language Modeling, and extends current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422336"
                        ],
                        "name": "A\u00e4ron van den Oord",
                        "slug": "A\u00e4ron-van-den-Oord",
                        "structuredName": {
                            "firstName": "A\u00e4ron",
                            "lastName": "Oord",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A\u00e4ron van den Oord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 84
                            }
                        ],
                        "text": "For images, the equivalent of a causal convolution is a masked convolution (van den Oord et al., 2016a) which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "Remarkably, these architectures are able to model distributions over thousands of random variables (e.g. 64\u00d764 pixels as in PixelRNN (van den Oord et al., 2016a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 32
                            }
                        ],
                        "text": "Similarly to PixelCNNs (van den Oord et al., 2016a;b), the conditional probability distribution is modelled by a stack of convolutional layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 17
                            }
                        ],
                        "text": "However, van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 88
                            }
                        ],
                        "text": "This paper introduces WaveNet, an audio generative model based on the PixelCNN (van den Oord et al., 2016a;b) architecture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 180
                            }
                        ],
                        "text": "This work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images (van den Oord et al., 2016a;b) and text (Jo\u0301zefowicz et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 77
                            }
                        ],
                        "text": "We use the same gated activation unit as used in the gated PixelCNN (van den Oord et al., 2016b):\nz = tanh (Wf,k \u2217 x) \u03c3 (Wg,k \u2217 x) , (2)\nwhere \u2217 denotes a convolution operator, denotes an element-wise multiplication operator, \u03c3(\u00b7) is a sigmoid function, k is the layer index, f and g denote filter\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8142135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41f1d50c85d3180476c4c7b3eea121278b0d8474",
            "isKey": false,
            "numCitedBy": 1749,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent."
            },
            "slug": "Pixel-Recurrent-Neural-Networks-Oord-Kalchbrenner",
            "title": {
                "fragments": [],
                "text": "Pixel Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A deep neural network is presented that sequentially predicts the pixels in an image along the two spatial dimensions and encodes the complete set of dependencies in the image to achieve log-likelihood scores on natural images that are considerably better than the previous state of the art."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2959768"
                        ],
                        "name": "M. Morise",
                        "slug": "M.-Morise",
                        "structuredName": {
                            "firstName": "Masanori",
                            "lastName": "Morise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Morise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3435489"
                        ],
                        "name": "Fumiya Yokomori",
                        "slug": "Fumiya-Yokomori",
                        "structuredName": {
                            "firstName": "Fumiya",
                            "lastName": "Yokomori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fumiya Yokomori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974060"
                        ],
                        "name": "K. Ozawa",
                        "slug": "K.-Ozawa",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Ozawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ozawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026a number of attempts to address these issues individually, such as developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3570465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba91dabec842d507a647aab97ad224b4abdc1635",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of realtime applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing. key words: speech analysis, speech synthesis, vocoder, sound quality, realtime processing"
            },
            "slug": "WORLD:-A-Vocoder-Based-High-Quality-Speech-System-Morise-Yokomori",
            "title": {
                "fragments": [],
                "text": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of realtime applications using speech and showed that it was superior to the other systems in terms of both sound quality and processing speed."
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inf. Syst."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685310"
                        ],
                        "name": "F. Itakura",
                        "slug": "F.-Itakura",
                        "structuredName": {
                            "firstName": "Fumitada",
                            "lastName": "Itakura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Itakura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "It often includes cepstra (Imai & Furuichi, 1988) or line spectral pairs (Itakura, 1975), which represent vocal tract transfer function, and fundamental frequency (F0) and aperiodicity (Kawahara et al., 2001), which represent characteristics of vocal source excitation signals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121895817,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c06be257debf1a81633fb7916d72a8b2200c826d",
            "isKey": false,
            "numCitedBy": 635,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been known that the linear predictor coefficients (LPC) of speech signals can be transformed into a \u201cpseudo\u201d vocal\u2010tract area function whose boundary conditions are (a) a complete opening at the lips and (b) a matching resistance termination at the glottis. If the boundary condition at the glottis is replaced by a complete opening or a complete closure, all the poles of the resulting system function will move onto the unit circle in z plane. Using this fact it is possible to describe the original LPCs by two sets of pole frequencies corresponding to the two new boundary conditions at the glottis, or a set of frequency\u2010residue pairs corresponding to either set of poles. These representations have several important features: (1) If an original pole is narrow band, the new pole is close to the original pole; (2) the two sets of pole frequencies alternate and are ordered on the frequency axis; and (3) the problem of locating complex poles can be reduced to solving a polynomial with real roots whose ord..."
            },
            "slug": "Line-spectrum-representation-of-linear-predictor-of-Itakura",
            "title": {
                "fragments": [],
                "text": "Line spectrum representation of linear predictor coefficients of speech signals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360834"
                        ],
                        "name": "X. Gonzalvo",
                        "slug": "X.-Gonzalvo",
                        "structuredName": {
                            "firstName": "Xavi",
                            "lastName": "Gonzalvo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Gonzalvo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750627"
                        ],
                        "name": "Siamak Tazari",
                        "slug": "Siamak-Tazari",
                        "structuredName": {
                            "firstName": "Siamak",
                            "lastName": "Tazari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siamak Tazari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349900"
                        ],
                        "name": "Chun-an Chan",
                        "slug": "Chun-an-Chan",
                        "structuredName": {
                            "firstName": "Chun-an",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-an Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46450287"
                        ],
                        "name": "Markus Becker",
                        "slug": "Markus-Becker",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064159126"
                        ],
                        "name": "Alexander Gutkin",
                        "slug": "Alexander-Gutkin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gutkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Gutkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688107"
                        ],
                        "name": "Hanna Sil\u00e9n",
                        "slug": "Hanna-Sil\u00e9n",
                        "structuredName": {
                            "firstName": "Hanna",
                            "lastName": "Sil\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanna Sil\u00e9n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 123
                            }
                        ],
                        "text": "As example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative (Gonzalvo et al., 2016) and long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric (Zen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2139406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21c82f0bc54af14bc7c596a9512ec9e190dd936b",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents advances in Google\u2019s hidden Markov model (HMM)-driven unit selection speech synthesis system. We describe several improvements to the run-time system; these include minimal latency, high-quality and fast refresh cycle for new voices. Traditionally unit selection synthesizers are limited in terms of the amount of data they can handle and the real applications they are built for. That is even more critical for reallife large-scale applications where high-quality is expected and low latency is required given the available computational resources. In this paper we present an optimized engine to handle a large database at runtime, a composite unit search approach for combining diphones and phrase-based units. In addition a new voice building strategy for handling big databases and keeping the building times low is presented."
            },
            "slug": "Recent-Advances-in-Google-Real-Time-HMM-Driven-Unit-Gonzalvo-Tazari",
            "title": {
                "fragments": [],
                "text": "Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An optimized engine to handle a large database at runtime, a composite unit search approach for combining diphones and phrase-based units, and a new voice building strategy for handling big databases and keeping the building times low is presented."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " ReLU ReLU 1! 1 Dilated Conv tanh ! + ! 1! 1 + Softmax Residual Skip-connections k Layers Output Causal Conv Input Figure 4: Overview of the residual block and the entire architecture. Both residual (He et al., 2015) and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models. In Fig. 4 we show a residual block of our model, which is stacke"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 49
                            }
                        ],
                        "text": "2.4 RESIDUAL AND SKIP CONNECTIONS\nBoth residual (He et al., 2015) and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95371,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853283"
                        ],
                        "name": "Yi-Jian Wu",
                        "slug": "Yi-Jian-Wu",
                        "structuredName": {
                            "firstName": "Yi-Jian",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Jian Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2752824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "574e2b70f5999142c44b239798e4b32af7724894",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A minimum generation error (MGE) criterion had been proposed to solve the issues related to maximum likelihood (ML) based HMM training in HMM-based speech synthesis. In this paper, we improve the MGE criterion by imposing a log spectral distortion (LSD) instead of the Euclidean distance to define the generation error between the original and generated line spectral pair (LSP) coefficients. Moreover, we investigate the effect of different sampling strategies to calculate the integration of the LSD function. From the experimental results, using the LSDs calculated by sampling at LSPs achieved the best performance, and the quality of synthesized speech after the MGE-LSD training was improved over the original MGE training."
            },
            "slug": "Minimum-generation-error-training-with-direct-log-Wu-Tokuda",
            "title": {
                "fragments": [],
                "text": "Minimum generation error training with direct log spectral distortion on LSPs for HMM-based speech synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The MGE criterion is improved by imposing a log spectral distortion (LSD) instead of the Euclidean distance to define the generation error between the original and generated line spectral pair (LSP) coefficients."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 45
                            }
                        ],
                        "text": "Recurrent neural networks such as LSTM-RNNs (Hochreiter & Schmidhuber, 1997) have been a key component in these new speech classification pipelines, because they allow for building models with long range contexts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 9
                            }
                        ],
                        "text": "Although LSTM-RNNs were trained from speech at 22.05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder (Agiomyrgiannakis, 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51704,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726559"
                        ],
                        "name": "T. Toda",
                        "slug": "T.-Toda",
                        "structuredName": {
                            "firstName": "Tomoki",
                            "lastName": "Toda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Toda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12165238,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ca8ec76a2bca0bd60f9d28e3bb4724e4b480032b",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a novel statistical approach to the vocal tract transfer function (VTTF) estimation of a speech signal based on a factor analyzed trajectory hidden Markov model (HMM). Because speech is a quasi-periodic signal, there are many missing frequency components between adjacent Fo harmonics. The proposed method determines a time-varying VTTF sequence based on the maximum a posteriori (MAP) estimation considering not only harmonic components observed at each analyzed frame but also those at other frames for stochastically interpolating the missing frequency parts."
            },
            "slug": "Statistical-approach-to-vocal-tract-transfer-based-Toda-Tokuda",
            "title": {
                "fragments": [],
                "text": "Statistical approach to vocal tract transfer function estimation based on factor analyzed trajectory HMM"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A novel statistical approach to the vocal tract transfer function (VTTF) estimation of a speech signal based on a factor analyzed trajectory hidden Markov model (HMM) considering not only harmonic components observed at each analyzed frame but also those at other frames for stochastically interpolating the missing frequency parts."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3232238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "317d2b6e97b878e77f8aad964575bcaadddb83cf",
            "isKey": false,
            "numCitedBy": 895,
            "numCiting": 308,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-Parametric-Speech-Synthesis-Zen-Tokuda",
            "title": {
                "fragments": [],
                "text": "Statistical Parametric Speech Synthesis"
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145231047"
                        ],
                        "name": "V. Koltun",
                        "slug": "V.-Koltun",
                        "structuredName": {
                            "firstName": "Vladlen",
                            "lastName": "Koltun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17127188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
            "isKey": false,
            "numCitedBy": 5426,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
            },
            "slug": "Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Context Aggregation by Dilated Convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work develops a new convolutional network module that is specifically designed for dense prediction, and shows that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788590"
                        ],
                        "name": "T. Kitamura",
                        "slug": "T.-Kitamura",
                        "structuredName": {
                            "firstName": "Tadashi",
                            "lastName": "Kitamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kitamura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6464769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b690f6db23ea76c4b44d231b3c639da924c15cc5",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reformulating-the-HMM-as-a-trajectory-model-by-and-Zen-Tokuda",
            "title": {
                "fragments": [],
                "text": "Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic feature vector sequences"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071514"
                        ],
                        "name": "Hideki Kawahara",
                        "slug": "Hideki-Kawahara",
                        "structuredName": {
                            "firstName": "Hideki",
                            "lastName": "Kawahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideki Kawahara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49743381"
                        ],
                        "name": "J. Estill",
                        "slug": "J.-Estill",
                        "structuredName": {
                            "firstName": "Jo",
                            "lastName": "Estill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Estill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145453016"
                        ],
                        "name": "O. Fujimura",
                        "slug": "O.-Fujimura",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Fujimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Fujimura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 186
                            }
                        ],
                        "text": "It often includes cepstra (Imai & Furuichi, 1988) or line spectral pairs (Itakura, 1975), which represent vocal tract transfer function, and fundamental frequency (F0) and aperiodicity (Kawahara et al., 2001), which represent characteristics of vocal source excitation signals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10604954,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1c424745996fd31920cadeec62a5514f3ce1a8b8",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A new control paradigm of source signals for high quality speech synthesis is introduced to handle a variety of speech quality, based on timefrequency analyses by the use of an instantaneous frequency and group delay. The proposed signal representation consists of a frequency domain aperiodicity measure and a time domain energy concentration measure to represent source attributes, which supplement the conventional source information, such as F0 and power. The frequency domain aperiodicity measure is defined as a ratio between the lower and upper smoothed spectral envelopes to represent the relative energy distribution of aperiodic components. The time domain measure is defined as an effective duration of the aperiodic component. These aperiodicity parameters and F0 as time functions are used to generate the source signal for synthetic speech by controlling relative noise levels and the temporal envelope of the noise component of the mixed mode excitation signal, including fine timing and amplitude fluctuations. A series of preliminary simulation experiments was conducted to test and to demonstrate consistency of the proposed method. Examples sung in different voice qualities were also analyzed and resynthesized using the proposed method."
            },
            "slug": "Aperiodicity-extraction-and-control-using-mixed-and-Kawahara-Estill",
            "title": {
                "fragments": [],
                "text": "Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new control paradigm of source signals for high quality speech synthesis is introduced to handle a variety of speech quality, based on timefrequency analyses by the use of an instantaneous frequency and group delay, which consists of a frequency domain aperiodicity measure and a time domain energy concentration measure."
            },
            "venue": {
                "fragments": [],
                "text": "MAVEBA"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15114815"
                        ],
                        "name": "S. Peltonen",
                        "slug": "S.-Peltonen",
                        "structuredName": {
                            "firstName": "Sari",
                            "lastName": "Peltonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Peltonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9219875"
                        ],
                        "name": "M. Gabbouj",
                        "slug": "M.-Gabbouj",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Gabbouj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gabbouj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726534"
                        ],
                        "name": "J. Astola",
                        "slug": "J.-Astola",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "Astola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Astola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 141
                            }
                        ],
                        "text": "Although such non-linear filter can represent complicated signals while preserving the details, designing such filters is usually difficult (Peltonen et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7780185,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "defafc2bb5ce486b774bc523ee8624dcbd8fbef3",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear filtering techniques have serious limitations in dealing with signals that have been created or processed by a system exhibiting some degree of nonlinearity, or, in general, situations where the relevance of information cannot be specified in the frequency domain. In image processing, many of these characteristics are often present, and it is no wonder that image processing is the field where nonlinear filtering techniques have first shown clear superiority over linear filters. Since nonlinear filters are all of those filters that are not linear, there is a large variety of different filters in use, and no common theory can exist. This makes filter design challenging, and optimization is meaningful only after restricting the class. It can be done in several conceptually different ways and, in this paper, we consider these techniques and the optimization methods that go together with the particular restriction. We review polynomial and rational filter classes and the optimization of stack filters under structural constraints and statistical constraints."
            },
            "slug": "Nonlinear-filter-design:-methodologies-and-Peltonen-Gabbouj",
            "title": {
                "fragments": [],
                "text": "Nonlinear filter design: methodologies and challenges"
            },
            "venue": {
                "fragments": [],
                "text": "ISPA 2001. Proceedings of the 2nd International Symposium on Image and Signal Processing and Analysis. In conjunction with 23rd International Conference on Information Technology Interfaces (IEEE Cat."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 49
                            }
                        ],
                        "text": ", 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu & Koltun, 2016)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1996665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39ad6c911f3351a3b390130a6e4265355b4d593b",
            "isKey": false,
            "numCitedBy": 3347,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
            },
            "slug": "Semantic-Image-Segmentation-with-Deep-Convolutional-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF)."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 187
                            }
                        ],
                        "text": "We have already mentioned several different ways to increase the receptive field size of a WaveNet: increasing the number of dilation stages, using more layers, larger filters, greater dilation factors, or a combination thereof."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 181
                            }
                        ],
                        "text": "One approach to modeling the conditional distributions p (xt | x1, . . . , xt\u22121) over the individual audio samples would be to use a mixture model such as a mixture density network (Bishop, 1994) or mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14616022,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4ca18249446328c86d9da295a21c679aea1ed77",
            "isKey": false,
            "numCitedBy": 691,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "p(t | x) t x x x = 0.8 = 0.5 = 0.2 Figure 7: Plot of the conditional probability densities of the target data, for various values of x, obtained by taking vertical slices through the contours in Figure 6, for x = 0:2, x = 0:5 and x = 0:8. It is clear that the Mixture Density Network is able to capture correctly the multimodal nature of the target data density function at intermediate values of x. x Figure 8: Plot of the priors i (x) as a function of x for the 3 kernel functions from the same Mixture Density Network as was used to plot Figure 6. At both small and large values of x, where the conditional probability density of the target data is unimodal, only one of the kernels has a prior probability which diiers signiicantly from zero. At intermediate values of x, where the conditional density is tri-modal, the three kernels have comparable priors."
            },
            "slug": "Mixture-Density-Networks-Srihari",
            "title": {
                "fragments": [],
                "text": "Mixture Density Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7788300,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "df50c6e1903b1e2d657f78c28ab041756baca86a",
            "isKey": false,
            "numCitedBy": 8924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition."
            },
            "slug": "Fundamentals-of-speech-recognition-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "Fundamentals of speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book presents a meta-modelling framework for speech recognition that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually modeling speech."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall signal processing series"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236926"
                        ],
                        "name": "P. Dutilleux",
                        "slug": "P.-Dutilleux",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Dutilleux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dutilleux"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "lution. Fig. 3 depicts dilated causal convolutions for dilations 1, 2, 4, and 8. Dilated convolutions have previously been used in various contexts, e.g. signal processing (Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &amp; Koltun, 2016). Input Hidden Layer Dilation = 1 Hidden Layer Dilation = 2 Hidden Layer Dilation = 4 Output Dilation = 8 Figure 3: Visualization of "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118292458,
            "fieldsOfStudy": [
                "Computer Science",
                "Geology"
            ],
            "id": "f75b338c638edca496c99852b706524e3eef1e55",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The computation of the wavelet transform involves the computation of the convolution product of the signal to be analysed by the analysing wavelet. It will be shown that the computation load grows with the scale factor of the analysis. We are interested in musical sounds lasting a few seconds. Using a straightforward algorithm leads to a prohibitive computation time, so we need a more effective computation procedure."
            },
            "slug": "An-Implementation-of-the-\u201calgorithme-\u00e0-trous\u201d-to-Dutilleux",
            "title": {
                "fragments": [],
                "text": "An Implementation of the \u201calgorithme \u00e0 trous\u201d to Compute the Wavelet Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It will be shown that the computation load grows with the scale factor of the analysis, which leads to a prohibitive computation time, so a more effective computation procedure is needed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073603971"
                        ],
                        "name": "Vinod Nair",
                        "slug": "Vinod-Nair",
                        "structuredName": {
                            "firstName": "Vinod",
                            "lastName": "Nair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinod Nair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15539264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "isKey": false,
            "numCitedBy": 12809,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors."
            },
            "slug": "Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton",
            "title": {
                "fragments": [],
                "text": "Rectified Linear Units Improve Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units that learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064374874"
                        ],
                        "name": "Edith Law",
                        "slug": "Edith-Law",
                        "structuredName": {
                            "firstName": "Edith",
                            "lastName": "Law",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edith Law"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10402290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72dd231084d2fee5c31343b395ae372b68389ba9",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Since its introduction at CHI 2004, the ESP Game has inspired many similar games that share the goal of gathering data from players. This paper introduces a new mechanism for collecting labeled data using \"games with a purpose.\" In this mechanism, players are provided with either the same or a different object, and asked to describe that object to each other. Based on each other's descriptions, players must decide whether they have the same object or not. We explain why this new mechanism is superior for input data with certain characteristics, introduce an enjoyable new game called \"TagATune\" that collects tags for music clips via this mechanism, and present findings on the data that is collected by this game."
            },
            "slug": "Input-agreement:-a-new-mechanism-for-collecting-Law-Ahn",
            "title": {
                "fragments": [],
                "text": "Input-agreement: a new mechanism for collecting data using human computation games"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Why this new mechanism is superior for input data with certain characteristics, an enjoyable new game called \"TagATune\" that collects tags for music clips via this mechanism, and findings on the data that is collected by this game are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217430"
                        ],
                        "name": "M. Holschneider",
                        "slug": "M.-Holschneider",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Holschneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Holschneider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398530308"
                        ],
                        "name": "R. Kronland-Martinet",
                        "slug": "R.-Kronland-Martinet",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kronland-Martinet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kronland-Martinet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145515955"
                        ],
                        "name": "J. Morlet",
                        "slug": "J.-Morlet",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Morlet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Morlet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830702"
                        ],
                        "name": "P. Tchamitchian",
                        "slug": "P.-Tchamitchian",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Tchamitchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tchamitchian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 92
                            }
                        ],
                        "text": "Dilated convolutions have previously been used in various contexts, e.g. signal processing (Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu & Koltun, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We saw that this worked slightly worse in our experiments."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 60719890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef3ed634a28eee78a7e31fc525cf7083e6689345",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform. We shall briefly describe this transformation in the following. For more details, we refer to the literature [1]."
            },
            "slug": "A-real-time-algorithm-for-signal-analysis-with-the-Holschneider-Kronland-Martinet",
            "title": {
                "fragments": [],
                "text": "A real-time algorithm for signal analysis with the help of the wavelet transform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 84
                            }
                        ],
                        "text": "For images, the equivalent of a causal convolution is a masked convolution (van den Oord et al., 2016a) which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "Remarkably, these architectures are able to model distributions over thousands of random variables (e.g. 64\u00d764 pixels as in PixelRNN (van den Oord et al., 2016a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 32
                            }
                        ],
                        "text": "Similarly to PixelCNNs (van den Oord et al., 2016a;b), the conditional probability distribution is modelled by a stack of convolutional layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 17
                            }
                        ],
                        "text": "However, van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Abstract This demo presents WaveNet [1], a deep generative model of raw audio waveforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 88
                            }
                        ],
                        "text": "This paper introduces WaveNet, an audio generative model based on the PixelCNN (van den Oord et al., 2016a;b) architecture."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 180
                            }
                        ],
                        "text": "This work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images (van den Oord et al., 2016a;b) and text (Jo\u0301zefowicz et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 77
                            }
                        ],
                        "text": "We use the same gated activation unit as used in the gated PixelCNN (van den Oord et al., 2016b):\nz = tanh (Wf,k \u2217 x) \u03c3 (Wg,k \u2217 x) , (2)\nwhere \u2217 denotes a convolution operator, denotes an element-wise multiplication operator, \u03c3(\u00b7) is a sigmoid function, k is the layer index, f and g denote filter\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WaveNet: A generative model for raw audio"
            },
            "venue": {
                "fragments": [],
                "text": "https://drive.google.com/file/d/ 0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view, 2016."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685310"
                        ],
                        "name": "F. Itakura",
                        "slug": "F.-Itakura",
                        "structuredName": {
                            "firstName": "Fumitada",
                            "lastName": "Itakura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Itakura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 113
                            }
                        ],
                        "text": "Extracting vocoder parameters can be viewed as estimation of a generative model parameters given speech signals (Itakura & Saito, 1970; Imai & Furuichi, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 191
                            }
                        ],
                        "text": "\u2026signals have a number of assumptions which are inspired from the speech production, such as\n\u2022 Use of fixed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 155
                            }
                        ],
                        "text": "\u2026Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 96
                            }
                        ],
                        "text": "\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 41
                            }
                        ],
                        "text": "For example, linear predictive analysis (Itakura & Saito, 1970), which has been used in speech coding, assumes that the generative model of speech signals is a linear auto-regressive (AR) zero-mean Gaussian process;\nxt = P\u2211 p=1 apxt\u2212p + t (6)\nt \u223c N (0, G2) (7)\nwhere ap is a p-th order linear\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 98
                            }
                        ],
                        "text": "\u2022 Linear filter; These generative models are typically realized as a linear time-invariant filter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 231165709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3acdc93dab375c667480670cbd3c8e4d3f79f740",
            "isKey": true,
            "numCitedBy": 260,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-statistical-method-for-estimation-of-speech-and-Itakura",
            "title": {
                "fragments": [],
                "text": "A statistical method for estimation of speech spectral density and formant frequencies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2467151"
                        ],
                        "name": "John S. Garofolo",
                        "slug": "John-S.-Garofolo",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Garofolo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John S. Garofolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144982775"
                        ],
                        "name": "W. Fisher",
                        "slug": "W.-Fisher",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Fisher",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3241934"
                        ],
                        "name": "J. Fiscus",
                        "slug": "J.-Fiscus",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Fiscus",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fiscus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786370"
                        ],
                        "name": "D. Pallett",
                        "slug": "D.-Pallett",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pallett",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pallett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35669756"
                        ],
                        "name": "Nancy L. Dahlgren",
                        "slug": "Nancy-L.-Dahlgren",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Dahlgren",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nancy L. Dahlgren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 290
                            }
                        ],
                        "text": "We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 81
                            }
                        ],
                        "text": "As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60884624,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b3db94f62118e192ef0465ca9edafcd6c074c137",
            "isKey": true,
            "numCitedBy": 1001,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "DARPA-TIMIT::-acoustic-phonetic-continuous-speech-Garofolo-Lamel",
            "title": {
                "fragments": [],
                "text": "DARPA TIMIT:: acoustic-phonetic continuous speech corpus CD-ROM, NIST speech disc 1-1.1"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 258
                            }
                        ],
                        "text": "\u2026Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 274
                            }
                        ],
                        "text": "\u2026signals have a number of assumptions which are inspired from the speech production, such as\n\u2022 Use of fixed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 181
                            }
                        ],
                        "text": "\u2022 Linear filter; These generative models are typically realized as a linear time-invariant filter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech analysis with multi-kernel linear prediction"
            },
            "venue": {
                "fragments": [],
                "text": "Spring Conference of ASJ"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 136
                            }
                        ],
                        "text": "Extracting vocoder parameters can be viewed as estimation of a generative model parameters given speech signals (Itakura & Saito, 1970; Imai & Furuichi, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 214
                            }
                        ],
                        "text": "\u2026signals have a number of assumptions which are inspired from the speech production, such as\n\u2022 Use of fixed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 178
                            }
                        ],
                        "text": "\u2026Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 119
                            }
                        ],
                        "text": "\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 27
                            }
                        ],
                        "text": "It often includes cepstra (Imai & Furuichi, 1988) or line spectral pairs (Itakura, 1975), which represent vocal tract transfer function, and fundamental frequency (F0) and aperiodicity (Kawahara et al., 2001), which represent characteristics of vocal source excitation signals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 121
                            }
                        ],
                        "text": "\u2022 Linear filter; These generative models are typically realized as a linear time-invariant filter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unbiased estimation of log spectrum"
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118195613"
                        ],
                        "name": "T. Chiba",
                        "slug": "T.-Chiba",
                        "structuredName": {
                            "firstName": "Tsutomu",
                            "lastName": "Chiba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Chiba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 51
                            }
                        ],
                        "text": "From the source-filter model of speech production (Chiba & Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126372903,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6aa19e69d6b450a3010ac230d6cb8b811f2d0d36",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-vowel,-its-nature-and-structure-Chiba",
            "title": {
                "fragments": [],
                "text": "The vowel, its nature and structure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46230186"
                        ],
                        "name": "J. Combes",
                        "slug": "J.-Combes",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Combes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Combes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153244818"
                        ],
                        "name": "A. Grossmann",
                        "slug": "A.-Grossmann",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Grossmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Grossmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830702"
                        ],
                        "name": "P. Tchamitchian",
                        "slug": "P.-Tchamitchian",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Tchamitchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tchamitchian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143975931"
                        ],
                        "name": "A. Pierce",
                        "slug": "A.-Pierce",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Pierce",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pierce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121662452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8944a3b8b385df07fe7da2d5d08938e2afeeeee2",
            "isKey": false,
            "numCitedBy": 611,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wavelets:-Time-Frequency-Methods-and-Phase-Space-Combes-Grossmann",
            "title": {
                "fragments": [],
                "text": "Wavelets: Time-Frequency Methods and Phase Space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2648226"
                        ],
                        "name": "C. Veaux",
                        "slug": "C.-Veaux",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Veaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Veaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716857"
                        ],
                        "name": "J. Yamagishi",
                        "slug": "J.-Yamagishi",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Yamagishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yamagishi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066509436"
                        ],
                        "name": "Kirsten MacDonald",
                        "slug": "Kirsten-MacDonald",
                        "structuredName": {
                            "firstName": "Kirsten",
                            "lastName": "MacDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kirsten MacDonald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 81
                            }
                        ],
                        "text": "We used the English multi-speaker corpus from CSTR voice cloning toolkit (VCTK) (Yamagishi, 2012) and conditioned WaveNet only on the speaker."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 24
                            }
                        ],
                        "text": "Although it is difficult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64303572,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d4903c15a7aba8e2c2386b2fe95edf0905144d6a",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SUPERSEDED-CSTR-VCTK-Corpus:-English-Multi-speaker-Veaux-Yamagishi",
            "title": {
                "fragments": [],
                "text": "SUPERSEDED - CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099975631"
                        ],
                        "name": "\u5168 \u70b3\u6cb3",
                        "slug": "\u5168-\u70b3\u6cb3",
                        "structuredName": {
                            "firstName": "\u5168",
                            "lastName": "\u70b3\u6cb3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5168 \u70b3\u6cb3"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61596083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f09d02319bbab716cc5cb561f3f6175319d9332a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reformulating-HMM-as-a-trajectory-model-by-imposing-\u5168",
            "title": {
                "fragments": [],
                "text": "Reformulating HMM as a trajectory model by imposing explicit relationships between static and dynamic features"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105252246"
                        ],
                        "name": "\u5409\u6751 \u8cb4\u514b",
                        "slug": "\u5409\u6751-\u8cb4\u514b",
                        "structuredName": {
                            "firstName": "\u5409\u6751",
                            "lastName": "\u8cb4\u514b",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5409\u6751 \u8cb4\u514b"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57441287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c61926496fa04d04d937740aa31437485ed91b64",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Simultaneous-modeling-of-phonetic-and-prosodic-for-\u5409\u6751",
            "title": {
                "fragments": [],
                "text": "Simultaneous modeling of phonetic and prosodic parameters,and characteristic conversion for HMM-based text-to-speech systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678766"
                        ],
                        "name": "Y. Sagisaka",
                        "slug": "Y.-Sagisaka",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Sagisaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sagisaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705409"
                        ],
                        "name": "N. Kaiki",
                        "slug": "N.-Kaiki",
                        "structuredName": {
                            "firstName": "Nobuyoshi",
                            "lastName": "Kaiki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kaiki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728425"
                        ],
                        "name": "N. Iwahashi",
                        "slug": "N.-Iwahashi",
                        "structuredName": {
                            "firstName": "Naoto",
                            "lastName": "Iwahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Iwahashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50791651"
                        ],
                        "name": "Katsuhiko Mimura",
                        "slug": "Katsuhiko-Mimura",
                        "structuredName": {
                            "firstName": "Katsuhiko",
                            "lastName": "Mimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katsuhiko Mimura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026to realize the speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis (Moulines & Charpentier, 1990; Sagisaka et al., 1992; Hunt & Black, 1996), and parametric, model-based approach known as statistical parametric speech synthesis (Yoshimura, 2002;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41381561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce14e0a3c79e3c80bb64596a6aae3c4d7145dcc1",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ATR-\u03bc-talk-speech-synthesis-system-Sagisaka-Kaiki",
            "title": {
                "fragments": [],
                "text": "ATR \u03bc-talk speech synthesis system"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32601514"
                        ],
                        "name": "C. Tuerk",
                        "slug": "C.-Tuerk",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Tuerk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tuerk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "\u2026models, such as hidden Markov models (HMMs) (Yoshimura, 2002), feed-forward neural networks (Zen et al., 2013), and recurrent neural networks (Tuerk & Robinson, 1993; Karaali et al., 1997; Fan et al., 2014), is trained from the extracted vocoder parameters and linguistic features\nas \u039b\u0302 = arg\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31516151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "449dd056204e8f0ce6d21afa1d804eb61ce45b61",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speech-synthesis-using-artificial-neural-networks-Tuerk-Robinson",
            "title": {
                "fragments": [],
                "text": "Speech synthesis using artificial neural networks trained on cepstral coefficients"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913931"
                        ],
                        "name": "G. Fant",
                        "slug": "G.-Fant",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "Fant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 75
                            }
                        ],
                        "text": "From the source-filter model of speech production (Chiba & Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58020686,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5d79d86d4501b4a5381a7a8cc3562bf66cf975e6",
            "isKey": false,
            "numCitedBy": 3264,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Acoustic-Theory-Of-Speech-Production-Fant",
            "title": {
                "fragments": [],
                "text": "Acoustic Theory Of Speech Production"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 286
                            }
                        ],
                        "text": "\u2026part of the statistical parametric approach can be viewed as a two-step optimization and sub-optimal: extract vocoder parameters by fitting a generative model of speech signals then model trajectories of the extracted vocoder parameters by a separate generative model for time series (Tokuda, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech synthesis as a statistical machine learning problem"
            },
            "venue": {
                "fragments": [],
                "text": "Speech synthesis as a statistical machine learning problem"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English multi-speaker corpus for CSTR voice cloning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "These generative models are typically realized as a linear time-invariant filter (Itakura & Saito"
            },
            "venue": {
                "fragments": [],
                "text": "These generative models are typically realized as a linear time-invariant filter (Itakura & Saito"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pulse Code Modulation (PCM) of voice frequencies"
            },
            "venue": {
                "fragments": [],
                "text": "Pulse Code Modulation (PCM) of voice frequencies"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech synthesis as a statistical machine learning problem. http://www.sp"
            },
            "venue": {
                "fragments": [],
                "text": "nitech.ac.jp/ \u0303tokuda/tokuda_asru2011_for_pdf.pdf,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 93
                            }
                        ],
                        "text": "The linguistic features include phone, syllable, word, phrase, and utterance-level features (Zen, 2006) (e.g. phone identities, syllable stress, the number of syllables in a word, and position of the current syllable in a phrase) with additional frame position and phone duration features (Zen et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of context-dependent label format for HMM-based speech synthesis in English"
            },
            "venue": {
                "fragments": [],
                "text": "An example of context-dependent label format for HMM-based speech synthesis in English"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of context-dependent label format for HMM-based speech synthesis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kameoka et al., 2010) within a windowed frame. However, the relationship between successive audio samples can be highly non-linear"
            },
            "venue": {
                "fragments": [],
                "text": "Poritz,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "The statistical parametric approach first extracts a sequence of vocoder parameters (Dudley, 1939) o = {o1, . . . ,oN} from speech signals x = {x1, . . . , xT } and linguistic features l from the text W , where N and T correspond to the numbers of vocoder parameter vectors and speech signals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Remaking speech"
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 1939
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 34,
            "methodology": 20,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 68,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/WaveNet:-A-Generative-Model-for-Raw-Audio-Oord-Dieleman/df0402517a7338ae28bc54acaac400de6b456a46?sort=total-citations"
}