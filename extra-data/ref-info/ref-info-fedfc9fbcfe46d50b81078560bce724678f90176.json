{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 187
                            }
                        ],
                        "text": "\u2026is the Buyesiun notion of optimality (Berger, 1985; Kiefer, 1987), and has been used in several approaches to learning in neural nets based on statistical mechanics (Denker et al. 1987; Tishby et al., 1989; Gyorgyi and Tishby, 1990; Sompolinsky et al., 1990; Opper and Haussler, 1991a, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15012839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b10440620da8a43a1b97e3da4b1ff13746306475",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.<<ETX>>"
            },
            "slug": "Consistent-inference-of-probabilities-in-layered-Tishby-Levin",
            "title": {
                "fragments": [],
                "text": "Consistent inference of probabilities in layered networks: predictions and generalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework and the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30053069"
                        ],
                        "name": "Sompolinsky",
                        "slug": "Sompolinsky",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sompolinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30093773"
                        ],
                        "name": "Tishby",
                        "slug": "Tishby",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076815081"
                        ],
                        "name": "Seung",
                        "slug": "Seung",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Seung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17664980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c24568e36420c2ffe6a8483100430517a2bbc2b6",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A statistical mechanical theory of learning from examples in layered networks at finite temperature is studied. When the training error is a smooth function of continuously varying weights the generalization error falls off asymptotically as the inverse number of examples. By analytical and numerical studies of single-layer perceptrons we show that when the weights are discrete the generalization error can exhibit a discontinuous transition to perfect generalization. For intermediate sizes of the example set, the state of perfect generalization coexists with a metastable spin-glass state. Understanding how systems can be efficiently trained to perform tasks is of fundamental importance. A central issue in learning theory is the rate of improvement in the processing of novel data as a function of the number of examples presented during training, i.e. , the generalization curve. ' Numerical results on training in layered neural networks indicate that the generalization error improves gradually in some cases, and sharply in others. s In this work we use statistical mechanics to study generalization curves in large layered networks. We will first discuss the general theory and then present results for learning in a single-layer perceptron. The computational function of layered neural networks is described in terms of the input-output relations that they generate. We consider here a multilayer network with M input nodes, whose states are denoted by synaptic weights of the network. The network is trained by adjusting its weights to approximate or reproduce, if possible, a target function cto(S) on the input space."
            },
            "slug": "Learning-from-examples-in-large-neural-networks.-Sompolinsky-Tishby",
            "title": {
                "fragments": [],
                "text": "Learning from examples in large neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Numerical results on training in layered neural networks indicate that the generalization error improves gradually in some cases, and sharply in others, and statistical mechanics is used to study generalization curves in large layered networks."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "1 of Welzl (1988) (using the primal space instead of the dual)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 5
                            }
                        ],
                        "text": "1 of Welzl (1988) (using the primal space instead of the dual). This result also gives a stronger version of Theorem 4, part (3) of Benedek and Itai (1988). We give a still stronger version of this result in Theorem 6 below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 107
                            }
                        ],
                        "text": "One example of this in the computational learning theory literature is the recent investigation of Abe and Warmuth (1990) into the complexity of learning the parameters in a hidden Markov model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1522,
                                "start": 0
                            }
                        ],
                        "text": "We assume that the loss function is known to the learner. The learner tries to choose his actions so as to minimize his loss. Here we look at the case in which, based on the training examples, the learner develops a deterministic strategy that specifies what he believes is the appropriate action a for each instance x in X. He then uses this strategy on all future examples. Thus we look at \u201cbatch\u201d learning rather than \u201cincremental\u201d or \u201con-line\u201d learning (Littlestone, 1988). The learner\u2019s strategy, which is a function from the instance space X into the decision space A, is called a decision rule. We assume that the decision rule is chosen from a fixed decision rule space X of functions from X into A. For example, instances in X may be encoded as inputs to a neural network, and outputs of the network may be interpreted as actions in A. In this case the network represents a decision rule, and the decision rule space 2 may be all functions represented by networks obtained by varying the parameters of a fixed underlying network. The goal of learning is to find a decision rule in X that minimizes the expected loss, when examples are drawn at random from the unknown joint distribution on Xx Y. This learning framework can be applied in a variety of situations. We now give several illustrations. For further discussion, we refer the reader to the excellent surveys of White (1990b), Barron (1989), Devroye (1988), and Vapnik (1989), to which we are greatly indebted. We also recommend the text by Kiefer (1987), for a general introduction to statistical inference and decision theory."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 130
                            }
                        ],
                        "text": "This issue has been addressed extensively in the PAC literature, and is also addressed in Kearns and Schapire (1990), and Abe and Warmuth (1990). Of these three important issues, here we examine only the first."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1393,
                                "start": 0
                            }
                        ],
                        "text": "We assume that the loss function is known to the learner. The learner tries to choose his actions so as to minimize his loss. Here we look at the case in which, based on the training examples, the learner develops a deterministic strategy that specifies what he believes is the appropriate action a for each instance x in X. He then uses this strategy on all future examples. Thus we look at \u201cbatch\u201d learning rather than \u201cincremental\u201d or \u201con-line\u201d learning (Littlestone, 1988). The learner\u2019s strategy, which is a function from the instance space X into the decision space A, is called a decision rule. We assume that the decision rule is chosen from a fixed decision rule space X of functions from X into A. For example, instances in X may be encoded as inputs to a neural network, and outputs of the network may be interpreted as actions in A. In this case the network represents a decision rule, and the decision rule space 2 may be all functions represented by networks obtained by varying the parameters of a fixed underlying network. The goal of learning is to find a decision rule in X that minimizes the expected loss, when examples are drawn at random from the unknown joint distribution on Xx Y. This learning framework can be applied in a variety of situations. We now give several illustrations. For further discussion, we refer the reader to the excellent surveys of White (1990b), Barron (1989), Devroye (1988), and Vapnik (1989), to which we are greatly indebted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 98
                            }
                        ],
                        "text": "(For recent work in Bayesian approaches to neural network learning see MacKay (1992), Buntine and Weigend (1991), and for Bayesian versions of the PAC model see Haussler et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2201,
                                "start": 0
                            }
                        ],
                        "text": "We then provide a lemma (Lemma 1) that can be used to evaluate the performance of learning algorithms that work by minimizing empirical loss. To use this lemma, we need bounds on the rate of uniform convergence of empirical loss estimates to true expected losses. These are given in Section 3. The key bound is given in Theorem 2 in Section 3, and in a more general version in Theorem 3. To use the bound from Theorem 2 we need bounds on the \u201crandom covering numbers\u201d associated with the decision rule space X, the loss function 1, and the distribution P. These are related to the idea of an s-cover described above. In Section 4 we introduce Pollard\u2019s notion of the pseudo dimension as a means of bounding the random covering numbers. Applications of this method to several learning problems are described in Section 5. The techniques of Sections 4 and 5 only apply to the case when the action set A is real-valued. Tools for bounding the random covering numbers that apply in more general cases are developed in Section 6. Here we introduce the notion of the capacity of the decision rule space Y? (for a particular loss function I), and the related notion of the metric dimension of 2\u201d. In Section 7 we use these notions to obtain bounds on the performance (in terms of the number of training examples used) of learning algorithms that use multilayer feedforward neural networks, and work by minimizing empirical loss (Corollary 3). Finally, some further discussion of our results is given in the conclusion, Section 8. Many of the more technical proofs and definitions have been moved into the Appendix to make the paper more readable. The Appendix has several sections. Section 9.1 contains a brief overview of the theory of metric spaces, s-covers, and metric dimension. Notation from this section is used in several places in the paper. Section 9.2 deals with certain technical measurability requirements. Section 9.3 gives an analogue of Chernoff and Hoeffding bounds using the d,, metric. Section 9.4 contains the proof of Theorem 2. Finally, Section 9.5 contains a result on feedforward neural networks of linear threshold functions that is similar to that given in Baum and Haussler (1989), and provides a counterpart to Corollary 3 in Section 7."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60943338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f44fbc2fb3df4425654ae429c6cd1e175c3a522d",
            "isKey": true,
            "numCitedBy": 155,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contributions of this thesis are a Bayesian theory of learning classi cation rules, the uni cation and comparison of this theory with some previous theories of learning, and two extensive applications of the theory to the problems of learning class probability trees and bounding error when learning logical rules. The thesis is motivated by considering some current research issues in machine learning such as bias, over tting and search, and considering the requirements placed on a learning system when it is used for knowledge acquisition. Basic Bayesian decision theory relevant to the problem of learning classi cation rules is reviewed, then a Bayesian framework for such learning is presented. The framework has three components: the hypothesis space, the learning protocol, and criteria for successful learning. Several learning protocols are analysed in detail: queries, logical, noisy, uncertain and positive-only examples. The analysis is done by interpreting a protocol as a likelihood function and by performing conditional likelihood analysis. The aim of this framework and theoretical treatment is to consider how three important questions should be addressed: what actions a learner should take, whether the learner's initial subjective knowledge is appropriate, and how con dent the learner can be in results obtained. The resultant learning framework is compared and uni ed with the corresponding learning frameworks of Gold, Valiant, minimum encoding approaches such as MDL and MML, and classical methods from statistics and pattern recognition. Finally, two extensive case studies are reported. The rst case study works through conjunctive and complete hypothesis spaces to illustrate problems such as the inherent worst-case and sampleindependent analysis of the Valiantmodel and how this can be overcome in the Bayesian framework. Experiments reported indicate considerable improvements can be achieved. The second case study presents a Bayesian approach to learning decision trees based on the theory presented for the uncertain examples protocol. Experiments reported indicate the approach compares favourably with existing decision tree methods. The Bayesian method presented should readily transfer to the learning of disjunctive rules or other more exible representations, and to more exible search strategies. The Bayesian method should also provide much stronger feedback for an interactive style of learning."
            },
            "slug": "A-theory-of-learning-classification-rules-Buntine",
            "title": {
                "fragments": [],
                "text": "A theory of learning classification rules"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian theory of learning classi cation rules, the comparison and comparison of this theory with some previous theories of learning, and two extensive applications of the theory to the problems of learningclass probability trees and bounding error when learning logical rules are reported."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059884"
                        ],
                        "name": "V. Gullapalli",
                        "slug": "V.-Gullapalli",
                        "structuredName": {
                            "firstName": "Vijaykumar",
                            "lastName": "Gullapalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gullapalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "By weakening these assumptions, we can model other types of learning as well, including associative reinforcement learning (Barto and Anandan, 1985; Gullapalli, 1990) and the theory of learning automata (with static environment) (Narendra and Thathachar, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21486916,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e9dc8d71572719cec58ec815bbd331fbd07fa15",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-stochastic-reinforcement-learning-algorithm-for-Gullapalli",
            "title": {
                "fragments": [],
                "text": "A stochastic reinforcement learning algorithm for learning real-valued functions"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 99
                            }
                        ],
                        "text": "One example of this in the computational learning theory literature is the recent investigation of Abe and Warmuth (1990) into the complexity of learning the parameters in a hidden Markov model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 216
                            }
                        ],
                        "text": "The same method works for estimating distributions on discrete spaces: we restrict ourselves to a finite instance space X and demand that for all XE X and all probability distributions h E%, h(x) > b > 0 (see, e.g., Abe and Warmuth, 1990; Yamanishi, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 122
                            }
                        ],
                        "text": "This issue has been addressed extensively in the PAC literature, and is also addressed in Kearns and Schapire (1990), and Abe and Warmuth (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16163511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9286d708b6ed298f6645f1e5f3fbe5eedbd70fd1",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a rigorous performance criterion for training algorithms for probabilistic automata (PAs) and hidden Markov models (HMMs), used extensively for speech recognition, and analyze the complexity of the training problem as a computational problem. The PA training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by a PA. We investigate the following question about this important, well-studied problem: Does there exist an efficient training algorithm such that the trained PAs provably converge to a model close to an optimum one with high confidence, after only a feasibly small set of training data? We model this problem in the framework of computational learning theory and analyze the sample as well as computational complexity. We show that the number of examples required for training PAs is moderate\u2014except for some log factors the number of examples is linear in the number of transition probabilities to be trained and a low-degree polynomial in the example length and parameters quantifying the accuracy and confidence. Computationally, however, training PAs is quite demanding: Fixed state size PAs are trainable in time polynomial in the accuracy and confidence parameters and example length, but not in the alphabet size unless RP = NP. The latter result is shown via a strong non-approximability result for the single string maximum likelihood model probem for 2-state PAs, which is of independent interest."
            },
            "slug": "On-the-Computational-Complexity-of-Approximating-by-Abe-Warmuth",
            "title": {
                "fragments": [],
                "text": "On the Computational Complexity of Approximating Distributions by Probabilistic Automata"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A rigorous performance criterion for training algorithms for probabilistic automata (PAs) and hidden Markov models (HMMs), used extensively for speech recognition, is introduced and the complexity of the training problem as a computational problem is analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34755863"
                        ],
                        "name": "K. Yamanishi",
                        "slug": "K.-Yamanishi",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Yamanishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yamanishi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 165
                            }
                        ],
                        "text": "In the computational learning theory literature, a model of this type is investigated in Kearns and Schapire (1990) with Y = (0, 1 ), and for a more general case in Yamanishi (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 209
                            }
                        ],
                        "text": "For example, in density estimation it is possible to use other measures of the distance between two densities; e.g., the Hellinger distance or the total variational distance, as in Barron and Cover (1990) and Yamanishi (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 239
                            }
                        ],
                        "text": "The same method works for estimating distributions on discrete spaces: we restrict ourselves to a finite instance space X and demand that for all XE X and all probability distributions h E%, h(x) > b > 0 (see, e.g., Abe and Warmuth, 1990; Yamanishi, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22182777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2086eaeb1c2027a56384c1a75354a1a7bbc184d",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a learning criterion for stochastic rules. This criterion is developed by extending Valiant's PAC (Probably Approximately Correct) learning model, which is a learning criterion for deterministic rules. Stochastic rules here refer to those which probabilistically asign a number of classes, {Y}, to each attribute vector X. The proposed criterion is based on the idea that learning stochastic rules may be regarded as probably approximately correct identification of conditional probability distributions over classes for given input attribute vectors. An algorithm (an MDL algorithm) based on the MDL (Minimum Description Length) principle is used for learning stochastic rules. Specifically, for stochastic rules with finite partitioning (each of which is specified by a finite number of disjoint cells of the domain and a probability parameter vector associated with them), this paper derives target-dependent upper bounds and worst-case upper bounds on the sample size required by the MDL algorithm to learn stochastic rules with given accuracy and confidence. Based on these sample size bounds, this paper proves polynomial-sample-size learnability of stochastic decision lists (which are newly proposed in this paper as a stochastic analogue of Rivest's decision lists) with at mostk literals (k is fixed) in each decision, and polynomial-sample-size learnability of stochastic decision trees (a stochastic analogue of decision trees) with at mostk depth. Sufficient conditions for polynomial-sample-size learnability and polynomial-time learnability of any classes of stochastic rules with finite partitioning are also derived."
            },
            "slug": "A-learning-criterion-for-stochastic-rules-Yamanishi",
            "title": {
                "fragments": [],
                "text": "A learning criterion for stochastic rules"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper derives target-dependent upper bounds and worst-case upper bounds on the sample size required by the MDL algorithm to learn stochastic rules with given accuracy and confidence."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 89
                            }
                        ],
                        "text": "In the computational learning theory literature, a model of this type is investigated in Kearns and Schapire (1990) with Y = (0, 1 ), and for a more general case in Yamanishi (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 31
                            }
                        ],
                        "text": "Some applications are given in Kearns and Schapire (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 90
                            }
                        ],
                        "text": "This issue has been addressed extensively in the PAC literature, and is also addressed in Kearns and Schapire (1990), and Abe and Warmuth (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 19
                            }
                        ],
                        "text": "The criterion from Kearns and Schapire (1990) for inferring a good model ofprobabilit~ can also be defined using an appropriate regret function, without defining an underlying loss 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 37
                            }
                        ],
                        "text": "This case was examined with q = 2 by Kearns and Schapire (1990) in their investigation into the learnability of p-concepts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 54
                            }
                        ],
                        "text": "Some promising results along these lines are given in Kearns and Schapire (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": true,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1330691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "305c0eccac29c37da6ca9a33b247de27531e6ef5",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study a Bayesian or average-case model of concept learning with a twofold goal: to provide more precise characterizations of learning curve (sample complexity) behavior that depend on properties of both the prior distribution over concepts and the sequence of instances seen by the learner, and to smoothly unite in a common framework the popular statistical physics and VC dimension theories of learning curves. To achieve this, we undertake a systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain. This study leads to a new understanding of the sample complexity of learning in several existing models."
            },
            "slug": "Bounds-on-the-sample-complexity-of-Bayesian-using-Haussler-Kearns",
            "title": {
                "fragments": [],
                "text": "Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain is undertaken."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 270
                            }
                        ],
                        "text": "The latter is the Buyesiun notion of optimality (Berger, 1985; Kiefer, 1987), and has been used in several approaches to learning in neural nets based on statistical mechanics (Denker et al. 1987; Tishby et al., 1989; Gyorgyi and Tishby, 1990; Sompolinsky et al., 1990; Opper and Haussler, 1991a, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 242
                            }
                        ],
                        "text": "This \u201clinear bias\u201d in pattern recognition and statistics is in contrast to that in the PAC model and other AI areas, including work in neural networks, in which a rich variety of decision rule spaces are used (see, e.g., Touretsky, 1989, 1990; Haussler, 1988, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 224
                            }
                        ],
                        "text": "An alternate approach is to use the L, loss function 1( y, a) = Ia - yI, in which case the expected loss is minimized when a is the median of the conditional distribution of Y given the instance x. (See, e.g., White, 1990b; Haussler, 1990."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 280
                            }
                        ],
                        "text": "\u2026is the Buyesiun notion of optimality (Berger, 1985; Kiefer, 1987), and has been used in several approaches to learning in neural nets based on statistical mechanics (Denker et al. 1987; Tishby et al., 1989; Gyorgyi and Tishby, 1990; Sompolinsky et al., 1990; Opper and Haussler, 1991a, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 104
                            }
                        ],
                        "text": "Further work is needed to sharpen these relationships (see, e.g., the lower bounds obtained in Baum and Haussler, 1989))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "The term cupacify has also been used with many other related meanings (Mandelbrot, 1982; Vapnik, 1982; Kolmogorov and Tihomirov, 1961; Farmer et al., 1983; Baum and Haussler, 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2906216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fad05ed37159201af2212f239b3d2a2bfb262f75",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Calculation-of-the-learning-curve-of-Bayes-optimal-Opper-Haussler",
            "title": {
                "fragments": [],
                "text": "Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29996782"
                        ],
                        "name": "Opper",
                        "slug": "Opper",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084880812"
                        ],
                        "name": "Haussler",
                        "slug": "Haussler",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40821171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40ea198170b7e6af50b4e269b3a4d3cd86e2d1ad",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The generalization error of the Bayes optimal classification algorithm when learning a perceptron from noise-free random training examples is calculated exactly using methods of statistical mechanics. It is shown that if an assumption of replica symmetry is made, then, in the thermodynamic limit, the error of the Bayes optimal algorithm is less than the error of a canonical stochastic learning algorithm, by a factor approaching \\ensuremath{\\surd}2 as the ratio of the number of training examples to perceptron weights grows. In addition, it is shown that approximations to the generalization error of the Bayes optimal algorithm can be achieved by learning algorithms that use a two-layer neutral net to learn a perceptron."
            },
            "slug": "Generalization-performance-of-Bayes-optimal-for-a-Opper-Haussler",
            "title": {
                "fragments": [],
                "text": "Generalization performance of Bayes optimal classification algorithm for learning a perceptron."
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is shown that approximations to the generalization error of the Bayes optimal algorithm can be achieved by learning algorithms that use a two-layer neutral net to learn a perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dace286582d91916fe470d08f30381cf453f20",
            "isKey": false,
            "numCitedBy": 1612,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space."
            },
            "slug": "Learning-Quickly-When-Irrelevant-Attributes-Abound:-Littlestone",
            "title": {
                "fragments": [],
                "text": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177274"
                        ],
                        "name": "B. Clarke",
                        "slug": "B.-Clarke",
                        "structuredName": {
                            "firstName": "Bertrand",
                            "lastName": "Clarke",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 36
                            }
                        ],
                        "text": "Support for this belief is given in Clarke and Barron (1990), (manuscript), Haussler et al. (1991b), and Opper and Haussler (1991a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11635626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52af2a10ae1eb70222cd5d0a58d02594eaf9e311",
            "isKey": false,
            "numCitedBy": 461,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "In the absence of knowledge of the true density function, Bayesian models take the joint density function for a sequence of n random variables to be an average of densities with respect to a prior. The authors examine the relative entropy distance D/sub n/ between the true density and the Bayesian density and show that the asymptotic distance is (d/2)(log n)+c, where d is the dimension of the parameter vector. Therefore, the relative entropy rate D/sub n//n converges to zero at rate (log n)/n. The constant c, which the authors explicitly identify, depends only on the prior density function and the Fisher information matrix evaluated at the true parameter value. Consequences are given for density estimation, universal data compression, composite hypothesis testing, and stock-market portfolio selection. >"
            },
            "slug": "Information-theoretic-asymptotics-of-Bayes-methods-Clarke-Barron",
            "title": {
                "fragments": [],
                "text": "Information-theoretic asymptotics of Bayes methods"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The authors examine the relative entropy distance D/sub n/ between the true density and the Bayesian density and show that the asymptotic distance is (d/2)(log n)+c, where d is the dimension of the parameter vector."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 5119145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5609ee7a8c7432c0f502b2a6dcfe9c0039206ab",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of predicting {0, 1}-valued functions on Rn and smaller domains, based on their values on randomly drawn points. Our model is related to Valiant\u2032s PAC learning model, but does not require the hypotheses used for prediction to be represented in any specified form. In our main result we show how to construct prediction strategies that are optimal to within a constant factor for any reasonable class F of target functions. This result is based on new combinatorial results about classes of functions of finite VC dimension. We also discuss more computationally efficient algorithms for predicting indicator functions of axis-parallel rectangles, more general intersection closed concept classes, and halfspaces in Rn. These are also optimal to within a constant factor. Finally, we compare the general performance of prediction strategies derived by our method to that of those derived from methods in PAC learning theory."
            },
            "slug": "Predicting-{0,1}-functions-on-randomly-drawn-points-Haussler-Littlestone",
            "title": {
                "fragments": [],
                "text": "Predicting {0,1}-functions on randomly drawn points"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This model is related to Valiant\u2032s PAC learning model, but does not require the hypotheses used for prediction to be represented in any specified form and shows how to construct prediction strategies that are optimal to within a constant factor for any reasonable class F of target functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39419622"
                        ],
                        "name": "B. Natarajan",
                        "slug": "B.-Natarajan",
                        "structuredName": {
                            "firstName": "Balas",
                            "lastName": "Natarajan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Natarajan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32476742,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d50f10be1eb151bec79ee504518f1d5298129889",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents algorithms that construct approximations to sets and functions on the reals, from randomly chosen sample points. The model that is analyzed is a generalization of the paradigm of probably approximate learning proposed by Valiant. Previously, necessary and sufficient conditions for learning sets were established for the case when the class of sampling distributions is finite, and for the case when the class of sampling distributions is the set of all possible distributions. Here, sufficient conditions are obtained for learning sets and functions over general classes of sampling distributions. The results for functions are with respect to general metrics for measuring the distance between two functions."
            },
            "slug": "Probably-Approximate-Learning-Over-Classes-of-Natarajan",
            "title": {
                "fragments": [],
                "text": "Probably Approximate Learning Over Classes of Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Algorithm that construct approximations to sets and functions on the reals, from randomly chosen sample points, are presented, a generalization of the paradigm of probably approximate learning proposed by Valiant."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27204621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b814ad3055d6bfd7828effdbfbf1372646b7c22",
            "isKey": false,
            "numCitedBy": 551,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quantifying-Inductive-Bias:-AI-Learning-Algorithms-Haussler",
            "title": {
                "fragments": [],
                "text": "Quantifying Inductive Bias: AI Learning Algorithms and Valiant's Learning Framework"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 33
                            }
                        ],
                        "text": "6), Benedek and Itai (1988), and White (1990a))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 62
                            }
                        ],
                        "text": "7, and Quiroz, 1989), and non-i.i.d. sources of examples (see White, 1990a, and Nobel and Dembo, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "An alternate approach is to use the L, loss function 1( y, a) = Ia - yI, in which case the expected loss is minimized when a is the median of the conditional distribution of Y given the instance x. (See, e.g., White, 1990b; Haussler, 1990."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 221
                            }
                        ],
                        "text": "This means that the weights and biases associated with the hidden units do not need to be bounded in order to get the rates of uniform convergence given by Corollary 3, as they would, for example, if the methods given in White (1990a) were used to obtain a result of this type."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "For further discussion, we refer the reader to the excellent surveys of White (1990b), Barron (1989), Devroye (1988), and Vapnik (1989), to which we are greatly indebted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 254
                            }
                        ],
                        "text": "Cross validation techniques, in which some of the training examples are held in reserve and used instead to test the performance of the decision rules produced by the learning algorithm, are likely to perform better for this task in practice (see, e.g., White, 1990a; Weiss and Kulikowski, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "This works, and is a useful method of obtaining uniform convergence and learning results (see related techniques used in White, 1990a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 224
                            }
                        ],
                        "text": "\u2026on the number of training examples needed to avoid overfitting when learning with the decision rule space of feedforward neural nets (Rumelhart and McClelland, 1986), extending previous work in Baum and Haussler (1989) and White (1990a) (see also related work in Anthony and Shawe-Taylor, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 201
                            }
                        ],
                        "text": "For example, it is common to let the decision rule space depend on the number of training examples available, using richer and richer decision rule spaces as more examples become available (see, e.g., White, 1990a; Blumer et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205119351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fdd39ab366b65a617015a72fe8dc9d0b394d64",
            "isKey": true,
            "numCitedBy": 716,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-nonparametric-regression:-Multilayer-White",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37495267,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6af20fc8e3f848e8f4cd5c10dc04316400d6ca85",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inductive-principles-of-the-search-for-empirical-on-Vapnik",
            "title": {
                "fragments": [],
                "text": "Inductive principles of the search for empirical dependences (methods based on weak convergence of probability measures)"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145230652"
                        ],
                        "name": "D. B. Schwartz",
                        "slug": "D.-B.-Schwartz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726209"
                        ],
                        "name": "B. Wittner",
                        "slug": "B.-Wittner",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Wittner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wittner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7508740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33fdc91c520b54e097f5e09fae1cfc94793fbfcf",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Since an tiquity, man has dreamed of building a de vice that would \"learn from examples\" 1 \"form generalizations\", and \"discover t he rules\" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be \"curve fit\" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n \" regularized\", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he \"human \" solut ions stable against perturbations. Fortuna tely, the re are \u00a9 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of \"program ming\" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem (\"c l umps\") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that \"automat ic learn ing will always succeed, given t he right preprocessor,\" but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the \"programming\" or \"architecture\" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be \"programmed\" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a \"7\" or a \"Q\" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could \"discover t he rules\" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of \"test cases\" where t here was an alternativeth at is, where the \"correct\" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, \"moment um term s\" , \"weight decay te rms\" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing"
            },
            "slug": "Large-Automatic-Learning,-Rule-Extraction,-and-Denker-Schwartz",
            "title": {
                "fragments": [],
                "text": "Large Automatic Learning, Rule Extraction, and Generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 33
                            }
                        ],
                        "text": "6), Benedek and Itai (1988), and White (1990a))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 62
                            }
                        ],
                        "text": "7, and Quiroz, 1989), and non-i.i.d. sources of examples (see White, 1990a, and Nobel and Dembo, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "An alternate approach is to use the L, loss function 1( y, a) = Ia - yI, in which case the expected loss is minimized when a is the median of the conditional distribution of Y given the instance x. (See, e.g., White, 1990b; Haussler, 1990."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 221
                            }
                        ],
                        "text": "This means that the weights and biases associated with the hidden units do not need to be bounded in order to get the rates of uniform convergence given by Corollary 3, as they would, for example, if the methods given in White (1990a) were used to obtain a result of this type."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "For further discussion, we refer the reader to the excellent surveys of White (1990b), Barron (1989), Devroye (1988), and Vapnik (1989), to which we are greatly indebted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 254
                            }
                        ],
                        "text": "Cross validation techniques, in which some of the training examples are held in reserve and used instead to test the performance of the decision rules produced by the learning algorithm, are likely to perform better for this task in practice (see, e.g., White, 1990a; Weiss and Kulikowski, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "This works, and is a useful method of obtaining uniform convergence and learning results (see related techniques used in White, 1990a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 224
                            }
                        ],
                        "text": "\u2026on the number of training examples needed to avoid overfitting when learning with the decision rule space of feedforward neural nets (Rumelhart and McClelland, 1986), extending previous work in Baum and Haussler (1989) and White (1990a) (see also related work in Anthony and Shawe-Taylor, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 201
                            }
                        ],
                        "text": "For example, it is common to let the decision rule space depend on the number of training examples available, using richer and richer decision rule spaces as more examples become available (see, e.g., White, 1990a; Blumer et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": true,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697413"
                        ],
                        "name": "S. Kulkarni",
                        "slug": "S.-Kulkarni",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Kulkarni",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401659877"
                        ],
                        "name": "Decision Systems.",
                        "slug": "Decision-Systems.",
                        "structuredName": {
                            "firstName": "Decision",
                            "lastName": "Systems.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Decision Systems."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16304120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62b1f290b535f98b7798b2177175077dd7d4c2e0",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In [23], Valiant proposed a formal framework for distribution-free concept learning which has generated a great deal of interest. A fundamental result regarding this framework was proved by Blumer et al. [6] characterizing those concept classes which are learnable in terms of their Vapnik-Chervonenkis (VC) dimension. More recently, Benedek and Itai [4] studied learnability with respect to a fixed probability distribution (a variant of the original distribution-free framework) and proved an analogous result characterizing learnability in this case. They also stated a conjecture regarding learnability for a class of distributions. In this report, we first point out that the condition for learnability obtained in [4] is equivalent to the notion of finite metric entropy (which has been studied in other contexts). Some relationships, in addition to those shown in [4], between the VC dimension of a concept class and its metric entropy with respect to various distributions are then discussed. Finally, we prove some partial results regarding learnability for a class of distributions."
            },
            "slug": "On-metric-entropy,-Vapnik-Chervonenkis-dimension,-a-Kulkarni-Systems.",
            "title": {
                "fragments": [],
                "text": "On metric entropy, Vapnik-Chervonenkis dimension, and learnability for a class of distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is pointed out that the condition for learnability obtained in [4] is equivalent to the notion of finite metric entropy (which has been studied in other contexts) and some relationships, in addition to those shown in], between the VC dimension of a concept class and its metric entropy with respect to various distributions are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103188660"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "A.R.",
                            "lastName": "Barron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "This is known as the logistic loss (McCullagh and Nelder, 1989; Barron, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "For further discussion, we refer the reader to the excellent surveys of White (1990b), Barron (1989), Devroye (1988), and Vapnik (1989), to which we are greatly indebted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51276807,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "72d761afbe35634213849419ff63fad5bc9fabeb",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Convergence properties of empirically estimated neural networks are examined. In this theory, an appropriate size feedforward network is automatically determined from the data. The networks studied include two- and three-layer networks with an increasing number of simple sigmoidal nodes, multiple-layer polynomial networks, and networks with certain fixed structures but an increasing complexity in each unit. Each of these classes of networks is dense in the space of continuous functions on compact subsets of d-dimensional Euclidean space, with respect to the topology of uniform convergence. It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases. Bounds on the rate of convergence are given in terms of an index of the approximation capability of the class of networks.<<ETX>>"
            },
            "slug": "Statistical-properties-of-artificial-neural-Barron",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th IEEE Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150652510"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47494927"
                        ],
                        "name": "L. Pitt",
                        "slug": "L.-Pitt",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Pitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026decision rule space on an n-attribute domain X, (e.g., [0, 11\u201d). and to families of decision rule spaces of different \u201ccomplexities\u201d on a fixed domain (Kearns et al., 1987; Blumer et al., 1989; Haussler et al., 1991a), so that tradeoffs between decision rule complexity and empirical risk can be\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1053873,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3948494b79dcda6d53237397123efdbb7a9954b2",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the computational feasibility of learning boolean expressions from examples. Our goals are to prove results and develop general techniques that shed light on the boundary between the classes of expressions that are learnable in polynomial time and those that are apparently not. The elucidation of this boundary, for boolean expressions and possibly other knowledge representations, is an example of the potential contribution of complexity theory to artificial intelligence. We employ the distribution-free model of learning introduced in /lo]. A more complete discussion and justification of this model can be found in [4,10,11,12]. [4] includes some discussion that is relevant more particularly to infinite representations, such as geometric ones, rather than the finite case of boolean functions. For other recent related work see [1,2,7,&g]. The results of this paper fall into three categories: closure properties of learnable classes, negative results, and distribution-specific positive results. The closure properties are of two kinds. In section 3 we discuss closure under boolean operations on the members of the learnable classes. The assumption that the classes are learnable from positive or negative ex-"
            },
            "slug": "On-the-learnability-of-Boolean-formulae-Kearns-Li",
            "title": {
                "fragments": [],
                "text": "On the learnability of Boolean formulae"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The goals are to prove results and develop general techniques that shed light on the boundary between the classes of expressions that are learnable in polynomial time and those that are apparently not, and to employ the distribution-free model of learning."
            },
            "venue": {
                "fragments": [],
                "text": "STOC"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "898c01de58eb3b8e790b60e0fe0db2230d88f15b",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GRBF). GRBF networks are not only equivalent to generalized splines, but are also closely related to several pattern recognition methods and neural network algorithms. The paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data."
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Girosi-Poggio",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that is called Generalized Radial Basis Functions (GRBF), which is not only equivalent to generalized splines, but is closely related to several pattern recognition methods and neural network algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144537223"
                        ],
                        "name": "D. Lindley",
                        "slug": "D.-Lindley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lindley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lindley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117797898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d89e792f5cc698ca8049a8b0bd7a4362b595e618",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The first five sections of the paper describe the Bayesian paradigm for statistics and its relationship with other attitudes towards inference. Section 1 outlines Wald's major contributions and explains how they omit the vital consideration of coherence. When this point is included the Bayesian view results, with the main difference that Waldean ideas require the concept of the sample space, whereas the Bayesian approach may dispense with it, using a probability distribution over parameter space instead. Section 2 relates statistical ideas to the problem of inference in science. Scientific inference is essentially the passage from observed, past data to unobserved, future data. The roles of models and theories in doing this are explored. The Bayesian view is that all this should be accomplished entirely within the calculus of probability and Section 3 justifies this choice by various axiom systems. The claim is made that this leads to a quite different paradigm from that of classical statistics and, in particular, prob- lems in the latter paradigm cease to have importance within the other. Point estimation provides an illustration. Some counter-examples to the Bayesian view are discussed. It is important that statistical conclusions should be usable in making decisions. Section 4 explains how the Bayesian view achieves this practi- cality by introducing utilities and the principle of maximizing expected utility. Practitioners are often unhappy with the ideas of basing inferences on one number, probability, or action on another, an expectation, so these points are considered and the methods justified. Section 5 discusses why the Bayesian viewpoint has not achieved the success that its logic suggests. Points discussed include the relationship between the inferences and the practical situation, for example with multiple comparisons; and the lack of the need to confine attention to normality or the exponential family. Its extensive use by nonstatisticians is documented. The most important objection to the Bayesian view is that which rightly says that probabilities are hard to assess. Consequently Section 6 considers how this might be done and an attempt is made to appreciate how accurate formulae like the extension of the conversation, the product law and Bayes rule are in evaluating probabilities."
            },
            "slug": "The-1988-Wald-Memorial-Lectures:-The-Present-in-Lindley",
            "title": {
                "fragments": [],
                "text": "The 1988 Wald Memorial Lectures: The Present Position in Bayesian Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The first five sections of the paper describe the Bayesian paradigm for statistics and its relationship with other attitudes towards inference and an attempt is made to appreciate how accurate formulae like the extension of the conversation, the product law and Bayes rule are in evaluating probabilities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 45
                            }
                        ],
                        "text": "In addition to using an additive model as in Lineal et al. (1988), we also define \u201cclose to\u201d using a measure of relative difference (the d, metric) similar to the standard multiplicative measure of approximation used in combinatorial optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12405514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "907f58a77ef4909d0e96c352cd5c7379eb22d5f5",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a concept from examples in a distribution-free model is considered. The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension. An important variation on the problem of learning from examples, called approximating from examples, is also discussed. The problem of computing the VC dimension of a finite concept set defined on a finite domain is considered.<<ETX>>"
            },
            "slug": "Results-on-learnability-and-the-Vapnik-Chervonenkis-Linial-Mansour",
            "title": {
                "fragments": [],
                "text": "Results on learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced and is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1925579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b83396caf4762c906530c9219a9e4dd0658232b0",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-general-lower-bound-on-the-number-of-examples-for-Ehrenfeucht-Haussler",
            "title": {
                "fragments": [],
                "text": "A general lower bound on the number of examples needed for learning"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2270210"
                        ],
                        "name": "R. S. Wenocur",
                        "slug": "R.-S.-Wenocur",
                        "structuredName": {
                            "firstName": "Roberta",
                            "lastName": "Wenocur",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S. Wenocur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 8
                            }
                        ],
                        "text": "Then\n1 (Wenocur and Dudley, 1981). dim,(G) = dim,(F) and\n2 (Nolan and Pollard, 1987; Dudley, 1987). dim,(H) <dim,(F), with equality if h is continuous and strictly increasing (resp. continuous and strictly decreasing)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 204985985,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e6dfb46ed298ff037e166291c128a465f90bfc0",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-special-vapnik-chervonenkis-classes-Wenocur-Dudley",
            "title": {
                "fragments": [],
                "text": "Some special vapnik-chervonenkis classes"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Math."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 85
                            }
                        ],
                        "text": "An example is the recent work using weight penalty functions in neural net training (Weigend et al., 1990; LeCun et al., 1990; Nowlan and Hinton, 1991; MacKay, 1992; Buntine and Weigend, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36452235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cee043045b529fceda7964a70e626d45657245a",
            "isKey": false,
            "numCitedBy": 842,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the effectiveness of connectionist architectures for predicting the future behavior of nonlinear dynamical systems. We focus on real-world time series of limited record length. Two examples are analyzed: the benchmark sunspot series and chaotic data from a computational ecosystem. The problem of overfitting, particularly serious for short records of noisy data, is addressed both by using the statistical method of validation and by adding a complexity term to the cost function (\"back-propagation with weight-elimination\"). The dimension of the dynamics underlying the time series, its Liapunov coefficient, and its nonlinearity can be determined via the network. We also show why sigmoid units are superior in performance to radial basis functions for high-dimensional input spaces. Furthermore, since the ultimate goal is accuracy in the prediction, we find that sigmoid networks trained with the weight-elimination algorithm outperform traditional nonlinear statistical approaches."
            },
            "slug": "Predicting-the-Future:-a-Connectionist-Approach-Weigend-Huberman",
            "title": {
                "fragments": [],
                "text": "Predicting the Future: a Connectionist Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Since the ultimate goal is accuracy in the prediction, it is found that sigmoid networks trained with the weight-elimination algorithm outperform traditional nonlinear statistical approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559845"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 124
                            }
                        ],
                        "text": "By weakening these assumptions, we can model other types of learning as well, including associative reinforcement learning (Barto and Anandan, 1985; Gullapalli, 1990) and the theory of learning automata (with static environment) (Narendra and Thathachar, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5915714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0120eefaf05bfad5293e87f56d2e787c05f78cf7",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks. These tasks are called associative reinforcement learning tasks. An algorithm is presented, called the associative reward-penalty, or AR-P algorithm for which a form of optimal performance is proved. This algorithm simultaneously generalizes a class of stochastic learning automata and a class of supervised learning pattern-classification methods related to the Robbins-Monro stochastic approximation procedure. The relevance of this hybrid algorithm is discussed with respect to the collective behaviour of learning automata and the behaviour of networks of pattern-classifying adaptive elements. Simulation results are presented that illustrate the associative reinforcement learning task and the performance of the AR-P algorithm as compared with that of several existing algorithms."
            },
            "slug": "Pattern-recognizing-stochastic-learning-automata-Barto-Anandan",
            "title": {
                "fragments": [],
                "text": "Pattern-recognizing stochastic learning automata"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks, called associative reinforcement learning tasks, and an algorithm is presented, called the associative reward-penalty, or AR-P algorithm, for which a form of optimal performance is proved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 86
                            }
                        ],
                        "text": "(For recent work in Bayesian approaches to neural network learning see MacKay (1992), Buntine and Weigend (1991), and for Bayesian versions of the PAC model see Haussler et al. (1991b) and Buntine (1990).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 142
                            }
                        ],
                        "text": "Other approaches, e.g., the method of structuraf risk minimization introduced by Vapnik (1982), and the Bayesiun (Berger, 1985; Mackay, 1992; Buntine and Weigend, 1991) and minimum description length (MDL) approaches (Barron and Cover, 1990; Rissanen, 1986), try to find a decision rule that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 166
                            }
                        ],
                        "text": "An example is the recent work using weight penalty functions in neural net training (Weigend et al., 1990; LeCun et al., 1990; Nowlan and Hinton, 1991; MacKay, 1992; Buntine and Weigend, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6686370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >"
            },
            "slug": "Automatic-Pattern-Recognition:-A-Study-of-the-of-Devroye",
            "title": {
                "fragments": [],
                "text": "Automatic Pattern Recognition: A Study of the Probability of Error"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Vapnik-Chervonenkis method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17487287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57c4baa5528ba805fc27eee86613c99503978fed",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "One popular class of unsupervised algorithms are competitive algorithms. In the traditional view of competition, only one competitor, the winner, adapts for any given case. I propose to view competitive adaptation as attempting to fit a blend of simple probability generators (such as gaussians) to a set of data-points. The maximum likelihood fit of a model of this type suggests a \"softer\" form of competition, in which all competitors adapt in proportion to the relative probability that the input came from each competitor. I investigate one application of the soft competitive model, placement of radial basis function centers for function interpolation, and show that the soft model can give better performance with little additional computational cost."
            },
            "slug": "Maximum-Likelihood-Competitive-Learning-Nowlan",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Competitive Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes to view competitive adaptation as attempting to fit a blend of simple probability generators to a set of data-points, and investigates one application of the soft competitive model, placement of radial basis function centers for function interpolation, and shows that the soft model can give better performance with little additional computational cost."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4191,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39419622"
                        ],
                        "name": "B. Natarajan",
                        "slug": "B.-Natarajan",
                        "structuredName": {
                            "firstName": "Balas",
                            "lastName": "Natarajan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Natarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729906"
                        ],
                        "name": "Prasad Tadepalli",
                        "slug": "Prasad-Tadepalli",
                        "structuredName": {
                            "firstName": "Prasad",
                            "lastName": "Tadepalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasad Tadepalli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 151
                            }
                        ],
                        "text": "\u2026specific probability distributions, and is related to the work of Natarajan and Tadepalli on extensions of the VC dimension to multivalued functions (Natarajan and Tadepalli, 1988; Natarajan, 1989b) and PAC learnability with respect to classes of probability distributions (Natarajan, 1988, 1989a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17243777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6c3e9e9e10a1bf803de2e161bd91c7831ecb1a5",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Two-New-Frameworks-for-Learning-Natarajan-Tadepalli",
            "title": {
                "fragments": [],
                "text": "Two New Frameworks for Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7165930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e02d1ffa45de336af35ae6fde2e8f6f19d5e50ff",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Equivalence-of-models-for-polynomial-learnability-Haussler-Kearns",
            "title": {
                "fragments": [],
                "text": "Equivalence of models for polynomial learnability"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "The pseudo dimension also generalizes the algebraic notion of the dimension of a vector space of real-valued functions (Dudley, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 33
                            }
                        ],
                        "text": "Using techniques that go back to Dudley (1978), Pollard has obtained a beautiful theorem bounding the metric dimension of (F, dLlcpj) by dim,(F) for any probability measure P on 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "This technique, due to Pollard (1984), who extended methods from Dudley (1978) is based on certain"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "The learning framework we described above in Section 1.1 is defined more formally in\n5 Specifically, Lemma 7.13 of Dudley (1978) is nearly equivalent to Lemma4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "Here we build further on the beautiful results of Vapnik and Chervonenkis (Vapnik, 1982), Dudley (1978), and Pollard (1984), which relate a type of generalized VC dimension for a decision rule space to the number of balls of  radius E required to cover the space, with respect to certain metrics."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 51
                            }
                        ],
                        "text": "We give an elementary proof using a technique from Dudley (1978)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121416923,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "10fd7180b2c0f14e5575b4892e74932b983af822",
            "isKey": true,
            "numCitedBy": 570,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $(X, \\mathscr{A}, P)$ be a probability space. Let $X_1, X_2,\\cdots,$ be independent $X$-valued random variables with distribution $P$. Let $P_n := n^{-1}(\\delta_{X_1} + \\cdots + \\delta_{X_n})$ be the empirical measure and let $\\nu_n := n^\\frac{1}{2}(P_n - P)$. Given a class $\\mathscr{C} \\subset \\mathscr{a}$, we study the convergence in law of $\\nu_n$, as a stochastic process indexed by $\\mathscr{C}$, to a certain Gaussian process indexed by $\\mathscr{C}$. If convergence holds with respect to the supremum norm $\\sup_{C \\in \\mathscr{C}}|f(C)|$, in a suitable (usually nonseparable) function space, we call $\\mathscr{C}$ a Donsker class. For measurability, $X$ may be a complete separable metric space, $\\mathscr{a} =$ Borel sets, and $\\mathscr{C}$ a suitable collection of closed sets or open sets. Then for the Donsker property it suffices that for some $m$, and every set $F \\subset X$ with $m$ elements, $\\mathscr{C}$ does not cut all subsets of $F$ (Vapnik-Cervonenkis classes). Another sufficient condition is based on metric entropy with inclusion. If $\\mathscr{C}$ is a sequence $\\{C_m\\}$ independent for $P$, then $\\mathscr{C}$ is a Donsker class if and only if for some $r, \\sigma_m(P(C_m)(1 - P(C_m)))^r < \\infty$."
            },
            "slug": "Central-Limit-Theorems-for-Empirical-Measures-Dudley",
            "title": {
                "fragments": [],
                "text": "Central Limit Theorems for Empirical Measures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 109
                            }
                        ],
                        "text": "This is similar to what is called a radial basis unit in the neural net literature (Poggio and Girosi, 1989; Moody and Darken, 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": false,
            "numCitedBy": 3493,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115302"
                        ],
                        "name": "N. Sauer",
                        "slug": "N.-Sauer",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Sauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 35
                            }
                        ],
                        "text": "Since dim,(F) = d, it follows from Sauer\u2019s lemma that )sign(F,,-7))) ~(em/d)~ for all mad, 2~2\u201d and 7~[0,M]\u2018\u201c."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 191
                            }
                        ],
                        "text": "The first, which we give without proof, was discovered independently by a number of people (see Assouad, 1983) including Vapnik and Chervonenkis (1971), but is most often attributed to Sauer (1972) in the computer science literature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "LEMMA 3 (Sauer)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7231983,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Density-of-Families-of-Sets-Sauer",
            "title": {
                "fragments": [],
                "text": "On the Density of Families of Sets"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Theory, Ser. A"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143689789"
                        ],
                        "name": "D. Pollard",
                        "slug": "D.-Pollard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "These we have indicated by saying that F must be a permissible class (Pollard, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 105
                            }
                        ],
                        "text": "The generalization we have proposed here, the pseudo dimension, is a minor variant of the notion used by Pollard (1984) to define classes of real-valued functions of polynomial discrimination, called VC-subgraph classes in Dudley (1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 26
                            }
                        ],
                        "text": "See Exercise 10, p. 39 of Pollard (1984) for an indication of the kind of problems that can come up with non-permissible classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 155
                            }
                        ],
                        "text": "In the simplest version of this technique, the choice of F, depends only on F and the distribution P, as in the \u201cdirect method\u201d discussed in Section II.2 of Pollard (1984) (see also Vapnik (1982, Sect."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 34
                            }
                        ],
                        "text": "For a more general treatment, see Pollard (1984) and Dudley (1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 101
                            }
                        ],
                        "text": "The proof of the first part of the lemma is similar, except we use Hoeffding\u2019s inequality (see, e.g., Pollard, 1984), which implies that for any singlef,\nPr(lE,(f)-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 150
                            }
                        ],
                        "text": "\u2026uniformly good estimate of the expectation of every function in a class F of real-valued functions has been widely studied (see, e.g., Vapnik, 1982, Pollard, 1984, Dudley, 1984, and their references), If no assumptions at all are made about the functions in F, we immediately run into the problem\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "Proof: If Z and F are uncountable, the assumption of permissibility guarantees that these probabilities are well-defined (see Section 9.2 and Pollard (1984))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 27
                            }
                        ],
                        "text": "E(f)\\ >E)<c?\nfor E, 6 > 0 (Pollard, 1984; Dudley, 1984; Vapnik, 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 23
                            }
                        ],
                        "text": "This technique, due to Pollard (1984), who extended methods from Dudley (1978) is based on certain"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "We now state and prove a version of Pollard\u2019s result (Pollard, 1984, Lemma 25, p. 27) for the special case when F is a class of functions taking values in the interval CO, M] with somewhat better bounds on the packing numbers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 100
                            }
                        ],
                        "text": "Since E( Y,) = 0 and - IXi - x,, iI   Yi   Ixi - x, + J, we can apply Hoeffding\u2019s inequality (see, e.g., Pollard, 1984) to bound the latter probability by\n2e2m a*c,rn+x ,=,- r,)*/2~~,I(X,--x,+,)*\nLet fi=Cf\u201d, xi."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "One of the key ideas we use is the notion of an a-cover of a metric space (Dudley, 1989; Pollard, 1984; Benedek and Itai, 1988; Natarajan, 1989a, Quiroz, 1989) and the associated idea of metric dimension (Kolmogorov and Tihomirov, 1961) (also called the fractal dimension (Farmer, 1982))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "First, we propose an extension of the PAC model, based on the work of Vapnik and Chervonenkis (Vapnik, 1989) and Pollard (1984, 1990), that addresses these and other issues."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 46
                            }
                        ],
                        "text": "\u201d This is a measurability condition defined in Pollard (1984) which need not concern us in practice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "ProoJ For the second part of the theorem, using Bernstein\u2019s inequality (see, e.g., Pollard, 1984) it is easy to show that for any single function f with O<f GM,\nPr(d,(E,(f), E(f)) > CI)   2e~\u201c2\u201d\u201c\u2018M.\nDetails are given in Lemma 9, part (2) in the Appendix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 194
                            }
                        ],
                        "text": "\u2026that the maximum value of this expression occurs at p = (vM)/(2v + M), and that this gives an upper bound of\nM\u2019 4&w(M + v)\u2019\nTo obtain the second bound, we apply Bernstein\u2019s inequality (see, e.g., Pollard, 1984, p. 192), which states that\nfor any zero mean i.i.d. random variables Y,, . . . ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 169
                            }
                        ],
                        "text": "Hence Z is shattered, implying that dim,(F) 2 d. m\nThere are many other ways that the VC dimension can be generalized to real-valued functions (Natarajan, 1989a, 1989b; Pollard, 1984; Vapnik, 1989; Dudley, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "By setting v and c( appropriately, we obtain results similar to those of Pollard (1984) and Vapnik (1982) as special cases of our main theorem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 271
                            }
                        ],
                        "text": "\u2026examples needed so that with high probability, any decision rule in 2 that has small empirical loss on the training examples will have small actual expected loss; i.e. we get uniform convergence results for empirical estimates like those in Vapnik (1982) Dudley (1984), and Pollard (1984, 1990)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "Here we build further on the beautiful results of Vapnik and Chervonenkis (Vapnik, 1982), Dudley (1978), and Pollard (1984), which relate a type of generalized VC dimension for a decision rule space to the number of balls of  radius E required to cover the space, with respect to certain metrics."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 74
                            }
                        ],
                        "text": "This definition will be suitable for our purposes; we refer the reader to Pollard (1984) and Dudley (1984) for a more general treatment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 150
                            }
                        ],
                        "text": "Our work builds directly on the work of Vapnik and Chervonenkis, Pollard, and Dudley on the uniform convergence of empirical estimates (Vapnik, 1982; Pollard, 1984; Dudley, 1984) and its application to pattern recognition (Vapnik, 1982, 1989; Devroye?"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 100
                            }
                        ],
                        "text": "GENERALIZATIONSOFTHE PAC MODEL 107\nThe proof of Theorem 2 follows the proof of Pollard\u2019s Theorem 24 (Pollard, 1984, p. 25) in general outline."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 36
                            }
                        ],
                        "text": "Thenforanyv>OandO<r<l,\nCOROLLARY 1 (Pollard, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 261
                            }
                        ],
                        "text": "\u2026by making assumptions about the moments of the functions in F, as in Vapnik (1982), or by assuming that there exists a single non-negative function with a finite expectation (called an envelope) that lies above the absolute value of every function in F, as in Pollard (1984) and Dudley (1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "they can be annoying (see Vapnik (1989) and Pollard (1984, 1990) for alternative approaches for unbounded loss functions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122104450,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0fca96e6ad7adf6b79ef5eb60e0360c20f8b3311",
            "isKey": true,
            "numCitedBy": 1852,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I Functional on Stochastic Processes.- 1. Stochastic Processes as Random Functions.- Notes.- Problems.- II Uniform Convergence of Empirical Measures.- 1. Uniformity and Consistency.- 2. Direct Approximation.- 3. The Combinatorial Method.- 4. Classes of Sets with Polynomial Discrimination.- 5. Classes of Functions.- 6. Rates of Convergence.- Notes.- Problems.- III Convergence in Distribution in Euclidean Spaces.- 1. The Definition.- 2. The Continuous Mapping Theorem.- 3. Expectations of Smooth Functions.- 4. The Central Limit Theorem.- 5. Characteristic Functions.- 6. Quantile Transformations and Almost Sure Representations.- Notes.- Problems.- IV Convergence in Distribution in Metric Spaces.- 1. Measurability.- 2. The Continuous Mapping Theorem.- 3. Representation by Almost Surely Convergent Sequences.- 4. Coupling.- 5. Weakly Convergent Subsequences.- Notes.- Problems.- V The Uniform Metric on Spaces of Cadlag Functions.- 1. Approximation of Stochastic Processes.- 2. Empirical Processes.- 3. Existence of Brownian Bridge and Brownian Motion.- 4. Processes with Independent Increments.- 5. Infinite Time Scales.- 6. Functional of Brownian Motion and Brownian Bridge.- Notes.- Problems.- VI The Skorohod Metric on D(0, ?).- 1. Properties of the Metric.- 2. Convergence in Distribution.- Notes.- Problems.- VII Central Limit Theorems.- 1. Stochastic Equicontinuity.- 2. Chaining.- 3. Gaussian Processes.- 4. Random Covering Numbers.- 5. Empirical Central Limit Theorems.- 6. Restricted Chaining.- Notes.- Problems.- VIII Martingales.- 1. A Central Limit Theorem for Martingale-Difference Arrays.- 2. Continuous Time Martingales.- 3. Estimation from Censored Data.- Notes.- Problems.- Appendix A Stochastic-Order Symbols.- Appendix B Exponential Inequalities.- Notes.- Problems.- Appendix C Measurability.- Notes.- Problems.- References.- Author Index."
            },
            "slug": "Convergence-of-stochastic-processes-Pollard",
            "title": {
                "fragments": [],
                "text": "Convergence of stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394227"
                        ],
                        "name": "J. D. Farmer",
                        "slug": "J.-D.-Farmer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Farmer",
                            "middleNames": [
                                "Doyne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Farmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144596753"
                        ],
                        "name": "E. Ott",
                        "slug": "E.-Ott",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9861772"
                        ],
                        "name": "J. Yorke",
                        "slug": "J.-Yorke",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Yorke",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yorke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 135
                            }
                        ],
                        "text": "The term cupacify has also been used with many other related meanings (Mandelbrot, 1982; Vapnik, 1982; Kolmogorov and Tihomirov, 1961; Farmer et al., 1983; Baum and Haussler, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "This quantity has also been called the fractal dimension (Farmer, 1982) and the capacity dimension (Farmer et al., 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "This notion of dimension has played an important role in the now very active study of fractals in nature (Mandelbrot, 1982) especially in connection with chaos in dynamical systems (Farmer, 1982; Farmer et al., 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118704682,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd7e97a73660b52a99d9e189869f4cebbeaddec3",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dimension-of-chaotic-attractors-Farmer-Ott",
            "title": {
                "fragments": [],
                "text": "Dimension of chaotic attractors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187514"
                        ],
                        "name": "R. Durbin",
                        "slug": "R.-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 234
                            }
                        ],
                        "text": "Our model for feedforward neural nets is quite general in that it allows many types of units in the nets, including quasi-linear units (Rumelhart and McClelland, 1986), radial basis units (Poggio and Girosi, 1989), and product units (Durbin and Rumelhart, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 90
                            }
                        ],
                        "text": "Then\nf(P)=d(e+ i wjlogxj)=~(~~~lx:,. /=I\ngiving what is commonly known as a product unit (Durbin and Rumelhart, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30181318,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a2f69bd15a2353920bddc878e8f9db5abf17e9c7",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new form of computational unit for feedforward learning networks of the backpropagation type. Instead of calculating a weighted sum this unit calculates a weighted product, where each input is raised to a power determined by a variable weight. Such a unit can learn an arbitrary polynomial term, which would then feed into higher level standard summing units. We show how learning operates with product units, provide examples to show their efficiency for various types of problems, and argue that they naturally extend the family of theoretical feedforward net structures. There is a plausible neurobiological interpretation for one interesting configuration of product and summing units."
            },
            "slug": "Product-Units:-A-Computationally-Powerful-and-to-Durbin-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Product Units: A Computationally Powerful and Biologically Plausible Extension to Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new form of computational unit for feedforward learning networks of the backpropagation type that calculates a weighted product, where each input is raised to a power determined by a variable weight."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70988756"
                        ],
                        "name": "G. S. Rogers",
                        "slug": "G.-S.-Rogers",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Rogers",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Rogers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 145
                            }
                        ],
                        "text": "Overview of the Proposed Framework\nTo extend the PAC model, we propose a more general framework based on statistical decision theory (see, e.g., Ferguson (1967), Kiefer (1987), or Berger (1985))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121859210,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c191a25ddbdc47a7b596b34bd756f4540f7a81f",
            "isKey": false,
            "numCitedBy": 1469,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mathematical-Statistics:-A-Decision-Theoretic-Rogers",
            "title": {
                "fragments": [],
                "text": "Mathematical Statistics: A Decision Theoretic Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135764"
                        ],
                        "name": "A. Kolmogorov",
                        "slug": "A.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Kolmogorov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100859010"
                        ],
                        "name": "V. Tikhomirov",
                        "slug": "V.-Tikhomirov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Tikhomirov",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Tikhomirov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 10
                            }
                        ],
                        "text": "Following Kolmogorov and Tihomirov (1961) we define the upper metric dimension of the set T of points by\ndim(T) = limsup 1% NE, T)\nE\u2019O l%(l/&) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 0
                            }
                        ],
                        "text": "Kolmogorov and Tihomirov (1961) we define the upper metric dimension of a (pseudo) metric space (S, p) by\ndim(S) = liy:;p log J-(-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 205
                            }
                        ],
                        "text": "One of the key ideas we use is the notion of an a-cover of a metric space (Dudley, 1989; Pollard, 1984; Benedek and Itai, 1988; Natarajan, 1989a, Quiroz, 1989) and the associated idea of metric dimension (Kolmogorov and Tihomirov, 1961) (also called the fractal dimension (Farmer, 1982))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 103
                            }
                        ],
                        "text": "The term cupacify has also been used with many other related meanings (Mandelbrot, 1982; Vapnik, 1982; Kolmogorov and Tihomirov, 1961; Farmer et al., 1983; Baum and Haussler, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 59
                            }
                        ],
                        "text": "The following inequalities are easily verified (see, e.g., Kolmogorov and Tihomirov, 1961):\nTHEOREM 12."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121117171,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b21b75140076a263f25d017fe24dd13f5a6a131e",
            "isKey": true,
            "numCitedBy": 756,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The article is mainly devoted to the systematic exposition of results that were published in the years 1954\u20131958 by K. I. Babenko [1], A. G. Vitushkin [2,3], V. D. Yerokhin [4], A. N. Kolmogorov [5,6] and V. M. Tikhomirov [7]. It is natural that when these materials were systematically rewritten, several new theorems were proved and certain examples were computed in more detail. This review also incorporates results not published previously which go beyond the framework of such a systematization, and belong to V. I. Arnold (\u00a76) and V. M. Tikhomirov (\u00a7\u00a74,7 and \u00a78)."
            },
            "slug": "Entropy-and-\"-capacity-of-sets-in-func-tional-Kolmogorov-Tikhomirov",
            "title": {
                "fragments": [],
                "text": "Entropy and \"-capacity of sets in func-tional spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394227"
                        ],
                        "name": "J. D. Farmer",
                        "slug": "J.-D.-Farmer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Farmer",
                            "middleNames": [
                                "Doyne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Farmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3502678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6247c7766e9f4951b4fcc0d49714d205236d996",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The concepts of entropy and dimension as applied to dynamical systems are reviewed from a physical point of view. The information dimension, which measures the rate at which the information contained in a probability density scales with resolution, fills a logical gap in the classification of attractors in terms of metric entropy, fractal dimension, and topological entropy. Several examples are presented of chaotic attractors that have a self similar, geometrically scaling structure in their probability distribution; for these attractors the information dimension and fractal dimension are different. Just as the metric (Kolmogorov-Sinai) entropy places an upper bound on the information gained in a sequence of measurements, the information dimension can be used to estimate the information obtained in an isolated measurement. The metric entropy can be expressed in terms of the information dimension of a probability distribution constructed from a sequence of measurements. An algorithm is presented that allows the experimental determination of the information dimension and metric entropy."
            },
            "slug": "Information-Dimension-and-the-Probabilistic-of-Farmer",
            "title": {
                "fragments": [],
                "text": "Information Dimension and the Probabilistic Structure of Chaos"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The concepts of entropy and dimension as applied to dynamical systems are reviewed from a physical point of view and an algorithm is presented that allows the experimental determination of the information dimension and metric entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 95
                            }
                        ],
                        "text": "This allows us to state the relevant uniform convergence bounds as generalized \u201cChernoff-style\u201d (Angluin and Valiant, 1979) bounds, as in Pollard (1986) and Breiman et al. (1984, Chap."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1279641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c92795deeea3f6765e7b5e593f4f31ffb634ed76",
            "isKey": false,
            "numCitedBy": 666,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The main purpose of this paper is to give techniques for analysing the probabilistic performance of certain kinds of algorithms, and hence to suggest some fast algorithms with provably desirable probabilistic behaviour. The particular problems we consider are: finding Hamiltonian circuits in directed graphs (DHC), finding Hamiltonian circuits in undirected graphs (UHC), and finding perfect matchings in undirected graphs (PM). We show that for each problem there is an algorithm that is extremely fast (0(n(log n)2) for DHC and UHC, and 0(nlog n) for PM), and which with probability tending to one finds a solution in randomly chosen graphs of sufficient density. These results contrast with the known NP-completeness of the first two problems [2,12] and the best worst-case upper bound known of 0(n2.5) for the last [9]."
            },
            "slug": "Fast-probabilistic-algorithms-for-hamiltonian-and-Angluin-Valiant",
            "title": {
                "fragments": [],
                "text": "Fast probabilistic algorithms for hamiltonian circuits and matchings"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that for each problem there is an algorithm that is extremely fast, and which with probability tending to one finds a solution in randomly chosen graphs of sufficient density, and the results contrast with the known NP-completeness of the first two problems."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '77"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 136
                            }
                        ],
                        "text": "Our model for feedforward neural nets is quite general in that it allows many types of units in the nets, including quasi-linear units (Rumelhart and McClelland, 1986), radial basis units (Poggio and Girosi, 1989), and product units (Durbin and Rumelhart, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 142
                            }
                        ],
                        "text": "\u2026bounds on the number of training examples needed to avoid overfitting when learning with the decision rule space of feedforward neural nets (Rumelhart and McClelland, 1986), extending previous work in Baum and Haussler (1989) and White (1990a) (see also related work in Anthony and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 125
                            }
                        ],
                        "text": "We now present some applications of the results of the previous section to learning with feedforward neural nets (see, e.g., Rumelhart and McClelland, 1986; Poggio and Girosi, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 171
                            }
                        ],
                        "text": "\u2026use this extension to obtain distribution-independent upper bounds on the size of the training set needed for learning with various kinds of feedforward neural networks (Rumelhart and McClelland, 1986; Poggio and Girosi, 1989), a popular learning method that is not covered by the basic PAC model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 97
                            }
                        ],
                        "text": "In this standard case, if the global modifier p = 0 we get what is known as a quasi-linear unit (Rumelhart and McClelland, 1986):\nIn the standard case, if ~(2) = C:=, xy we get a unit that computes a function of the form\nf(n\u2018)=O(@+ i (X,-L7j)2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": true,
            "numCitedBy": 15710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153023666"
                        ],
                        "name": "B. K. Natarajan",
                        "slug": "B.-K.-Natarajan",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Natarajan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. K. Natarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 437,
                                "start": 49
                            }
                        ],
                        "text": "Here a decision rule is a diagnostic method, and Bayes optimal decision rule is the one that minimizes the expected loss from misdiagnosis when examples (x, y) of test results and associated disease states occur randomly according to some unknown \u201cnatural\u201d joint distribution. This medical diagnosis situation is a typical example of a classification learning problem in the field of pattern recognition (see, e.g., Duda and Hart (1973))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 66
                            }
                        ],
                        "text": ", the Hellinger distance or the total variational distance, as in Barron and Cover (1990) and Yamanishi (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 66
                            }
                        ],
                        "text": ", the Hellinger distance or the total variational distance, as in Barron and Cover (1990) and Yamanishi (1990). The criterion from Kearns and Schapire (1990) for inferring a good model ofprobabilit~ can also be defined using an appropriate regret function, without defining an underlying loss 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 2
                            }
                        ],
                        "text": "\u201d By setting v and c( appropriately, we obtain results similar to those of Pollard (1984) and Vapnik (1982) as special cases of our main theorem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 5
                            }
                        ],
                        "text": "1 of Blumer et al. (1989). A number of applications of this result are outlined in Blumer ef al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": ", Ferguson (1967), Kiefer (1987), or Berger (1985))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 6
                            }
                        ],
                        "text": "9) or Blumer et al. (1989, Appendix), for a proof > 9 of the second inequality. Note also that Vapnik and Chervonenkis (1971) actually contains a slightly weaker result."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 185
                            }
                        ],
                        "text": "(Vapnik\u2019s (1982) use of the relative difference between empirical estimates and true expectations also has this advantage; see Anthony and Shawe-Taylor ( 1990) and also the Appendix of Blumer et al. (1989).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 582267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82933fe53b38ccd8847d91cf41371bbd44fb7f62",
            "isKey": true,
            "numCitedBy": 7,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents some formal results on learning. In particular, it concerns algorithms that learn sets and functions from examples. We seek conditions necessary and sufficient for learning over a range of probabilistic models for such algorithms."
            },
            "slug": "Some-results-on-learning-Natarajan",
            "title": {
                "fragments": [],
                "text": "Some results on learning"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents some formal results on learning for algorithms that learn sets and functions from examples over a range of probabilistic models for such algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10570532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306c40863d0c9d57b238b980bb288b008425b475",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Result-of-Vapnik-with-Applications-Anthony-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "A Result of Vapnik with Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31443676"
                        ],
                        "name": "K. Alexander",
                        "slug": "K.-Alexander",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Alexander",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Alexander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 128
                            }
                        ],
                        "text": "Still different, and more involved, techniques are used in the more general theory of weighted empirical processes developed by Alexander (1985, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 390086,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0dda3293914d863f2c57668f79e7ad4dc1fff1db",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryProbability inequalities are obtained for the supremum of a weighted empirical process indexed by a Vapnik-\u010cervonenkis class C of sets. These inequalities are particularly useful under the assumption P(\u222a{C\u2208C:P(C)<t})\u00bb0 as t\u00bb0. They are used to obtain almost sure bounds on the rate of growth of the process as the sample size approaches infinity, to find an asymptotic sample modulus for the unweighted empirical process, and to study the ratio Pn/P of the empirical measure to the actual measure."
            },
            "slug": "Rates-of-growth-and-sample-moduli-for-weighted-by-Alexander",
            "title": {
                "fragments": [],
                "text": "Rates of growth and sample moduli for weighted empirical processes indexed by sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 181
                            }
                        ],
                        "text": "For example, in density estimation it is possible to use other measures of the distance between two densities; e.g., the Hellinger distance or the total variational distance, as in Barron and Cover (1990) and Yamanishi (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 152
                            }
                        ],
                        "text": "\u2026introduced by Vapnik (1982), and the Bayesiun (Berger, 1985; Mackay, 1992; Buntine and Weigend, 1991) and minimum description length (MDL) approaches (Barron and Cover, 1990; Rissanen, 1986), try to find a decision rule that minimizes some function of empirical loss and decision rule complexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 45
                            }
                        ],
                        "text": "Other approaches include the MDL (see, e.g., Barron and Cover, 1990), regularization (see, e.g., Poggio and Girosi, 1989), and more general Bayesian methods (see, e.g., Berger, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14460436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fca98082fa9ff8e9dbae9922491ae54976a0ccef",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an index of resolvability that is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length. The results on the index of resolvability demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference. The minimum complexity estimator converges to true density nearly as fast as an estimator based on prior knowledge of the true subclass of densities. Interpretations and basic properties of minimum complexity estimators are discussed. Some regression and classification problems that can be examined from the minimum description-length framework are considered. >"
            },
            "slug": "Minimum-complexity-density-estimation-Barron-Cover",
            "title": {
                "fragments": [],
                "text": "Minimum complexity density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An index of resolvability is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length to demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42185255,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "44eeb93197dcf2e7bf4ed9172a82f81de9c05365",
            "isKey": false,
            "numCitedBy": 1597,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability. It will lead them to live and work much better. This is why, the students, workers, or even employers should have reading habit for books. Any book will give certain knowledge to take all benefits. This is what this learning automata an introduction tells you. It will add more knowledge of you to life and work better. Try it and prove it."
            },
            "slug": "Learning-automata-an-introduction-Narendra-Thathachar",
            "title": {
                "fragments": [],
                "text": "Learning automata - an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability and this learning automata an introduction tells you that any book will give certain knowledge to take all benefits."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122230395,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c59250736174b8d80c40de6e1d5e1641366ef416",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When \\(\\mathfrak{F}\\) is a universal Donsker class, then for independent, indetically distributed (i.i.d) observation \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\) with an unknown law P, for any \\(\\mathfrak{f}_i\\)in \\(\\mathfrak{F},\\) \\(i=1,\\ldots,m,\\quad n^{-1/2}\\left\\{ \\mathfrak{f}_1\\left(\\mathbf{X}_1\\right)+\\ldots+\\mathfrak{f}_i\\left(\\mathbf{X}_n\\right)\\right\\}_{1\\leq i\\leq m}\\) is asymptotically normal with mean Vector \\(n^{1/2}\\left\\{\\int\\mathfrak{f}_i\\left(\\mathbf{X}_n\\right)d\\mathbf{P}\\left(x\\right)\\right\\}_{1_\\leq i\\leq m}\\) and covariance matrix \\(\\int\\mathfrak{f}_i\\mathfrak{f}_j d\\mathbf{P}-\\int\\mathfrak{f}_id\\mathbf{P}\\int\\mathfrak{f}_jd\\mathbf{P},\\) uniformly for \\({\\mathfrak{f}_i}\\in \\mathfrak{F}.\\) Then, for certain Statistics formed frome the \\(\\mathfrak{f}_i\\left(\\mathbf{X}_k\\right),\\) even where \\(\\mathfrak{f}_i\\) may be chosen depending on the \\(\\mathbf{X}_k\\) there will be asymptotic distribution as \\(n \\rightarrow \\infty.\\) For example, for \\(\\mathbf{X}^2\\) statistics, where \\(f_i\\) are indicators of disjoint intervals, depending suitably on \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\), whose union is the real line, \\(\\mathbf{X}^2\\) quadratic forms have limiting distributions [Roy (1956) and Watson (1958)] which may, however, not be \\(\\mathbf{X}^2\\) distributions and may depend on P [Chernoff and Lehmann (1954)]. Universal Donsker classes of sets are, up to mild measurability conditions, just classes satisfying the Vapnik\u2013Cervonenkis comdinatorial conditions defined later in this section Donsker the Vapnik-Cervonenkis combinatorial conditions defined later in this section [Durst and Dudley (1981) and Dudley (1984) Chapter 11]. The use of such classes allows a variety of extensions of the Roy\u2013Watson results to general (multidimensional) sample spaces [Pollard (1979) and Moore and Subblebine (1981)]. Vapnik and Cervonenkis (1974) indicated application of their families of sets to classification (pattern recognition) problems. More recently, the classes have been applied to tree-structured classifiacation [Breiman, Friedman, Olshen and Stone (1984), Chapter 12]."
            },
            "slug": "Universal-Donsker-Classes-and-Metric-Entropy-Dudley",
            "title": {
                "fragments": [],
                "text": "Universal Donsker Classes and Metric Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5222015"
                        ],
                        "name": "R. Sloan",
                        "slug": "R.-Sloan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Sloan",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sloan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 240
                            }
                        ],
                        "text": "Some practitioners are wary of the assumption that the examples are generated from an underlying \u201ctarget function,\u201d and are not satisfied with the noise models that have been proposed to weaken this assumption (e.g., Angluin and Laird, 1988; Sloan, 1988; Shackelford and Volper, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3610395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b9357d47d3f4da2b886baae05afdd8a35b70bee",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The algorithm for pac learning <italic>k</italic>-DNF or <italic>k</italic>-CNF in the presence of malicious attribute noise in polynomial time claimed by Sloan [Slo88] does not work. It is currently open whether such an algorithm exists."
            },
            "slug": "Corrigendum-to-types-of-noise-in-data-for-concept-Sloan",
            "title": {
                "fragments": [],
                "text": "Corrigendum to types of noise in data for concept learning"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The algorithm for pac learning k-DNF or k-CNF in the presence of malicious attribute noise in polynomial time claimed by Sloan [Slo88] does not work."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 15
                            }
                        ],
                        "text": "Note also that Vapnik and Chervonenkis (1971) actually contains a slightly weaker result."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 121
                            }
                        ],
                        "text": "The first, which we give without proof, was discovered independently by a number of people (see Assouad, 1983) including Vapnik and Chervonenkis (1971), but is most often attributed to Sauer (1972) in the computer science literature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717081"
                        ],
                        "name": "E. Welzl",
                        "slug": "E.-Welzl",
                        "structuredName": {
                            "firstName": "Emo",
                            "lastName": "Welzl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Welzl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12479197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1398eb165420574c286dc7f762864191b1ebea1e",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The range searching problems which allow partition trees where every query enters only a sublinear number of nodes are characterized as those with finite Vapnik - Chervonenk is dimension.\nThe concrete combinatorial bounds obtained imply\u2014among others \u2014 that every set of n points in the plane<list><item>can be stored in an &Ogr;(n) space data structure which allows triangle counting queries in &Ogr;(\u221an\u00b7 <italic>log</italic><supscrpt>3</supscrpt>n) time, and\n</item><item>can be stored in an &Ogr;(n \u00b7 <italic>log</italic> n) space data structure which allows disk counting queries in &Ogr;(\u221an\u00b7 <italic>log</italic><supscrpt>3</supscrpt>n) time;\n</item></list> the preprocessing time for the data structures is polynomial. Recent results by Chazelle entail that these bounds for space and query time are optimal up to polylog \u2014 factors."
            },
            "slug": "Partition-trees-for-triangle-counting-and-other-Welzl",
            "title": {
                "fragments": [],
                "text": "Partition trees for triangle counting and other range searching problems"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Bounds for space and query time are optimal up to polylog \u2014 factors and the preprocessing time for the data structures is polynomial according to recent results by Chazelle."
            },
            "venue": {
                "fragments": [],
                "text": "SCG '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60565534,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69d7086300e7f5322c06f2f242a565b3a182efb5",
            "isKey": false,
            "numCitedBy": 4652,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Bill Baird { Publications References 1] B. Baird. Bifurcation analysis of oscillating neural network model of pattern recognition in the rabbit olfactory bulb. In D. 3] B. Baird. Bifurcation analysis of a network model of the rabbit olfactory bulb with periodic attractors stored by a sequence learning algorithm. 5] B. Baird. Bifurcation theory methods for programming static or periodic attractors and their bifurcations in dynamic neural networks."
            },
            "slug": "In-Advances-in-Neural-Information-Processing-Hanson",
            "title": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717081"
                        ],
                        "name": "E. Welzl",
                        "slug": "E.-Welzl",
                        "structuredName": {
                            "firstName": "Emo",
                            "lastName": "Welzl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Welzl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 116
                            }
                        ],
                        "text": "This is the definition of the Vapnik-Chervonenkis dimension of a class F of (0, 1 }-valued functions (Vapnik, 1982; Haussler and Welzl, 1987; Blumer et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8530681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f52e66066c129bb87f6a4f12f1c17fd4a2b440b3",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new technique for half-space and simplex range query using <italic>&Ogr;</italic>(<italic>n</italic>) space and <italic>&Ogr;</italic>(<italic>n</italic><supscrpt><italic>a</italic></supscrpt>) query time, where <italic>a</italic> < <italic>d</italic>(d-1)/<italic>d</italic>(<italic>d</italic>-1) + 1 + \u03b3 for all dimensions <italic>d</italic> \u2265 2 and <italic>\u03b3</italic> > 0. These bounds are better than those previously published for all <italic>d</italic> \u2265 2. The technique uses random sampling to build a partition-tree structure. We introduce the concept of an <italic>\u03b5</italic>-net for an abstract set of ranges to describe the desired result of this random sampling and give necessary and sufficient conditions that a random sample is an <italic>\u03b5</italic>-net with high probability. We illustrate the application of these ideas to other range query problems."
            },
            "slug": "Epsilon-nets-and-simplex-range-queries-Haussler-Welzl",
            "title": {
                "fragments": [],
                "text": "Epsilon-nets and simplex range queries"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new technique for half-space and simplex range query using random sampling to build a partition-tree structure and introduces the concept of an\u03b5-net for an abstract set of ranges to describe the desired result of this random sampling."
            },
            "venue": {
                "fragments": [],
                "text": "SCG '86"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "The latter is the Buyesiun notion of optimality (Berger, 1985; Kiefer, 1987), and has been used in several approaches to learning in neural nets based on statistical mechanics (Denker et al. 1987; Tishby et al., 1989; Gyorgyi and Tishby, 1990; Sompolinsky et al., 1990; Opper and Haussler, 1991a,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "Other approaches, e.g., the method of structuraf risk minimization introduced by Vapnik (1982), and the Bayesiun (Berger, 1985; Mackay, 1992; Buntine and Weigend, 1991) and minimum description length (MDL) approaches (Barron and Cover, 1990; Rissanen, 1986), try to find a decision rule that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "Other approaches include the MDL (see, e.g., Barron and Cover, 1990), regularization (see, e.g., Poggio and Girosi, 1989), and more general Bayesian methods (see, e.g., Berger, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 104
                            }
                        ],
                        "text": "Other approaches, e.g., the method of structuraf risk minimization introduced by Vapnik (1982), and the Bayesiun (Berger, 1985; Mackay, 1992; Buntine and Weigend, 1991) and minimum description length (MDL) approaches (Barron and Cover, 1990; Rissanen, 1986), try to find a decision rule that minimizes some function of empirical loss and decision rule complexity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 180
                            }
                        ],
                        "text": "Overview of the Proposed Framework\nTo extend the PAC model, we propose a more general framework based on statistical decision theory (see, e.g., Ferguson (1967), Kiefer (1987), or Berger (1985))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 272
                            }
                        ],
                        "text": "\u2026find the linear function h that minimizes the average of I( y, h(.u)) = (h(x) - .v)~ over all examples\n4 For general regression with the negative log likelihood loss function, the principle of minimizing empirical loss is the same as the principle of maximum likelihood (Berger, 1985; Kiefer, 1987)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": true,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 176
                            }
                        ],
                        "text": "\u2026introduced by Vapnik (1982), and the Bayesiun (Berger, 1985; Mackay, 1992; Buntine and Weigend, 1991) and minimum description length (MDL) approaches (Barron and Cover, 1990; Rissanen, 1986), try to find a decision rule that minimizes some function of empirical loss and decision rule complexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120741100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On demontre un theoreme fondamental qui donne une borne inferieure pour la longueur de code et donc, pour les erreurs de prediction. On definit les notions \u00abd'information a priori\u00bb et \u00abd'information utile\u00bb dans les donnees"
            },
            "slug": "Stochastic-Complexity-and-Modeling-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity and Modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717081"
                        ],
                        "name": "E. Welzl",
                        "slug": "E.-Welzl",
                        "structuredName": {
                            "firstName": "Emo",
                            "lastName": "Welzl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Welzl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 116
                            }
                        ],
                        "text": "This is the definition of the Vapnik-Chervonenkis dimension of a class F of (0, 1 }-valued functions (Vapnik, 1982; Haussler and Welzl, 1987; Blumer et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27638326,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dbdfc10b6af746580bd48f2f3757e9060326e5ab",
            "isKey": false,
            "numCitedBy": 674,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractWe demonstrate the existence of data structures for half-space and simplex range queries on finite point sets ind-dimensional space,d\u22652, with linear storage andO(n\u03b1) query time,\n% MathType!MTEF!2!1!+-% feaafiart1ev1aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr% 4rNCHbGeaGqiVC0xe9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Gqpi0x% c9q8qqaqFj0df9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x% fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaeqySdeMaey% ypa0ZaaSaaaeaacaWGKbGaaiikaiaadsgacqGHsislcaaIXaGaaiyk% aaqaaiaadsgacaGGOaGaamizaiabgkHiTiaaigdacaGGPaGaey4kaS% IaaGymaaaacqGHRaWkcqaHZoWzieaacaWFGaGaa8hiaiaa-bcacaWF% GaGaa8hiaiaa-bcacaWFGaGaa8Nzaiaa-9gacaWFYbGaa8hiaiaa-f% gacaWFSbGaa8hBaiaa-bcacaWFGaGaa8hiaiabeo7aNjaa-5dacaWF% Waaaaa!574F!\n\n$$\\alpha = \\frac{{d(d - 1)}}{{d(d - 1) + 1}} + \\gamma for all \\gamma > 0$$\n.These bounds are better than those previously published for alld\u22652. Based on ideas due to Vapnik and Chervonenkis, we introduce the concept of an \u025b-net of a set of points for an abstract set of ranges and give sufficient conditions that a random sample is an \u025b-net with any desired probability. Using these results, we demonstrate how random samples can be used to build a partition-tree structure that achieves the above query time."
            },
            "slug": "\u025b-nets-and-simplex-range-queries-Haussler-Welzl",
            "title": {
                "fragments": [],
                "text": "\u025b-nets and simplex range queries"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The concept of an \u025b-net of a set of points for an abstract set of ranges is introduced and sufficient conditions that a random sample is an \u00c3\u201a-net with any desired probability are given."
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Comput. Geom."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 139
                            }
                        ],
                        "text": "This medical diagnosis situation is a typical example of a classification learning problem in the field of pattern recognition (see, e.g., Duda and Hart (1973))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 60
                            }
                        ],
                        "text": "One common choice is a mixture of Gaussian densities (e.g., Duda and Hart, 1973; Nowlan, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "Such parameters would be needed to do any reasonable type of kernel-based density estimation (see, e.g., Duda and Hart (1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "P(xl y) is a multi-variate Gaussian distribution on X (Duda and Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 267
                            }
                        ],
                        "text": "Similarly, in binary classification, where there are only two possible outcomes in Y as in the PAC model, linear threshold functions are most often used as decision rules, and there are straightforward generalizations for the case of k-ary classification (see, e.g., Duda and Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": true,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144336939"
                        ],
                        "name": "D. Nolan",
                        "slug": "D.-Nolan",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Nolan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143689789"
                        ],
                        "name": "D. Pollard",
                        "slug": "D.-Pollard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 178
                            }
                        ],
                        "text": "Although uniform convergence results such as those we develop here are also used in the analysis of such methods (Vapnik, 1982) (and in the analysis of cross-validation methods (Nolan and Pollard, 1987)), the full treatment of such approaches is beyond the scope of the present paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 60
                            }
                        ],
                        "text": "Then\n1 (Wenocur and Dudley, 1981). dim,(G) = dim,(F) and\n2 (Nolan and Pollard, 1987; Dudley, 1987). dim,(H) <dim,(F), with equality if h is continuous and strictly increasing (resp. continuous and strictly decreasing)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120790247,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c200a40faccb0d6e4557b8e93ba6cf020b5cc702",
            "isKey": false,
            "numCitedBy": 405,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On introduit un nouveau processus stochastique, une collection de statistiques U indicees par une famille de noyaux symetriques. On obtient des conditions pour la convergence presque sure uniforme d'une suite de tels processus"
            },
            "slug": "$U$-Processes:-Rates-of-Convergence-Nolan-Pollard",
            "title": {
                "fragments": [],
                "text": "$U$-Processes: Rates of Convergence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103073712"
                        ],
                        "name": "P. Assouad",
                        "slug": "P.-Assouad",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Assouad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Assouad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "The first, which we give without proof, was discovered independently by a number of people (see Assouad, 1983) including Vapnik and Chervonenkis (1971), but is most often attributed to Sauer (1972) in the computer science literature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123874242,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c537dca516ed9500b525b18ebeb494997b4b8eef",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 Annales de l\u2019institut Fourier, 1983, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives de la revue \u00ab Annales de l\u2019institut Fourier \u00bb (http://annalif.ujf-grenoble.fr/) implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/conditions). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright."
            },
            "slug": "Densit\u00e9-et-dimension-Assouad",
            "title": {
                "fragments": [],
                "text": "Densit\u00e9 et dimension"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88082960"
                        ],
                        "name": "G. F. Simmons",
                        "slug": "G.-F.-Simmons",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Simmons",
                            "middleNames": [
                                "Finlay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. F. Simmons"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 168
                            }
                        ],
                        "text": "Condition (1) is satisfied as well in this case, since we can take T to be the one-point compactification of T, obtained by adding a point at infinity to T (see, e.g., Simmons, 1963)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119761258,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "f162c9bb7921e7cda3545049f630fa85ff16c31f",
            "isKey": false,
            "numCitedBy": 658,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This material is intended to contribute to a wider appreciation of the mathematical words \"continuity and linearity\". The book's purpose is to illuminate the meanings of these words and their relation to each other."
            },
            "slug": "Introduction-to-Topology-and-Modern-Analysis-Simmons",
            "title": {
                "fragments": [],
                "text": "Introduction to Topology and Modern Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The purpose is to illuminate the meanings of these words \"continuity and linearity\" and their relation to each other."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721914"
                        ],
                        "name": "H. Edelsbrunner",
                        "slug": "H.-Edelsbrunner",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Edelsbrunner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Edelsbrunner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6323368,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "777d4efa4622137282b469e12a06aa2a765cea7c",
            "isKey": false,
            "numCitedBy": 2281,
            "numCiting": 188,
            "paperAbstract": {
                "fragments": [],
                "text": "This book offers a modern approach to computational geo- metry, an area thatstudies the computational complexity of geometric problems. Combinatorial investigations play an important role in this study."
            },
            "slug": "Algorithms-in-Combinatorial-Geometry-Edelsbrunner",
            "title": {
                "fragments": [],
                "text": "Algorithms in Combinatorial Geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This book offers a modern approach to computational geo- metry, an area thatstudies the computational complexity of geometric problems with an important role in this study."
            },
            "venue": {
                "fragments": [],
                "text": "EATCS Monographs in Theoretical Computer Science"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 53
                            }
                        ],
                        "text": "Since the pseudo dimension is the same as the Vapnik-Chervonenkis dimension for classes of indicator functions, this implies that the class has Vapnik-Chervonenkis dimension at most k + 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "Although uniform convergence results such as those we develop here are also used in the analysis of such methods (Vapnik, 1982) (and in the analysis of cross-validation methods (Nolan and Pollard, 1987)), the full treatment of such approaches is beyond the scope of the present paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 15
                            }
                        ],
                        "text": "Note also that Vapnik and Chervonenkis (1971) actually contains a slightly weaker result."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "E(f)\\ >E)<c?\nfor E, 6 > 0 (Pollard, 1984; Dudley, 1984; Vapnik, 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 11
                            }
                        ],
                        "text": "Results in Vapnik (1982), (Theorem A.2, p. 220) indicate that uniform convergence does not take place in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "This problem can be avoided by making assumptions about the moments of the functions in F, as in Vapnik (1982), or by assuming that there exists a single non-negative function with a finite expectation (called an envelope) that lies above the absolute value of every function in F, as in Pollard\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 38
                            }
                        ],
                        "text": "Now let d be the sum of all the VapnikChervonenkis dimensions of all the classes of functions associated with the computation units of the architecture &."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "These two types of bounds are analogous to the two types of bounds that Vapnik (1982) gives in his book, in that one uses a measure of absolute difference and the other a measure of relative difference."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 121
                            }
                        ],
                        "text": "The first, which we give without proof, was discovered independently by a number of people (see Assouad, 1983) including Vapnik and Chervonenkis (1971), but is most often attributed to Sauer (1972) in the computer science literature."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": "Other approaches, e.g., the method of structuraf risk minimization introduced by Vapnik (1982), and the Bayesiun (Berger, 1985; Mackay, 1992; Buntine and Weigend, 1991) and minimum description length (MDL) approaches (Barron and Cover, 1990; Rissanen, 1986), try to find a decision rule that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "First, we propose an extension of the PAC model, based on the work of Vapnik and Chervonenkis (Vapnik, 1989) and Pollard (1984, 1990), that addresses these and other issues."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Thus the pseudo dimension generalizes the Vapnik-Chervonenkis dimension to arbitrary classes of real-valued functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "By setting v and c( appropriately, we obtain results similar to those of Pollard (1984) and Vapnik (1982) as special cases of our main theorem."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 238
                            }
                        ],
                        "text": "\u2026examples needed so that with high probability, any decision rule in 2 that has small empirical loss on the training examples will have small actual expected loss; i.e. we get uniform convergence results for empirical estimates like those in Vapnik (1982) Dudley (1984), and Pollard (1984, 1990)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "\u2026obtaining a uniformly good estimate of the expectation of every function in a class F of real-valued functions has been widely studied (see, e.g., Vapnik, 1982, Pollard, 1984, Dudley, 1984, and their references), If no assumptions at all are made about the functions in F, we immediately run into\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "Here we build further on the beautiful results of Vapnik and Chervonenkis (Vapnik, 1982), Dudley (1978), and Pollard (1984), which relate a type of generalized VC dimension for a decision rule space to the number of balls of  radius E required to cover the space, with respect to certain metrics."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "As mentioned in the previous section, dim,(X) is the same as the Vapnik-Chervonenkis dimension of H in this case; hence this bound is, up to constants, the same as that given in Theorem 2.1 of Blumer et al. (1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "The term cupacify has also been used with many other related meanings (Mandelbrot, 1982; Vapnik, 1982; Kolmogorov and Tihomirov, 1961; Farmer et al., 1983; Baum and Haussler, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "Our work builds directly on the work of Vapnik and Chervonenkis, Pollard, and Dudley on the uniform convergence of empirical estimates (Vapnik, 1982; Pollard, 1984; Dudley, 1984) and its application to pattern recognition (Vapnik, 1982, 1989; Devroye?"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 102
                            }
                        ],
                        "text": "This is the definition of the Vapnik-Chervonenkis dimension of a class F of (0, 1 }-valued functions (Vapnik, 1982; Haussler and Welzl, 1987; Blumer et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "It generalizes the techniques based on the Vapnik-Chervonenkis dimension used in Blumer et al. (1989), which apply only to (0, 1 }-valued functions."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data. SpringerVerlag, New York,"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 53
                            }
                        ],
                        "text": "For a more general treatment, see Pollard (1984) and Dudley (1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "E(f)\\ >E)<c?\nfor E, 6 > 0 (Pollard, 1984; Dudley, 1984; Vapnik, 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 277
                            }
                        ],
                        "text": "\u2026numbers are distribution specific, yet we only apply it here in a distribution independent setting), decision rule spaces\nGENERALIZATIONSOFTHE PAC MODEL 135\nwith infinite pseudo and metric dimensions (these include various classes of \u201csmooth\u201d functions and their relatives; see Dudley, 1984, Chap."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026estimate of the expectation of every function in a class F of real-valued functions has been widely studied (see, e.g., Vapnik, 1982, Pollard, 1984, Dudley, 1984, and their references), If no assumptions at all are made about the functions in F, we immediately run into the problem that some\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 29
                            }
                        ],
                        "text": "Our usage here is taken from Dudley (1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 252
                            }
                        ],
                        "text": "\u2026examples needed so that with high probability, any decision rule in 2 that has small empirical loss on the training examples will have small actual expected loss; i.e. we get uniform convergence results for empirical estimates like those in Vapnik (1982) Dudley (1984), and Pollard (1984, 1990)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 93
                            }
                        ],
                        "text": "This definition will be suitable for our purposes; we refer the reader to Pollard (1984) and Dudley (1984) for a more general treatment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 165
                            }
                        ],
                        "text": "Our work builds directly on the work of Vapnik and Chervonenkis, Pollard, and Dudley on the uniform convergence of empirical estimates (Vapnik, 1982; Pollard, 1984; Dudley, 1984) and its application to pattern recognition (Vapnik, 1982, 1989; Devroye?"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 280
                            }
                        ],
                        "text": "\u2026by making assumptions about the moments of the functions in F, as in Vapnik (1982), or by assuming that there exists a single non-negative function with a finite expectation (called an envelope) that lies above the absolute value of every function in F, as in Pollard (1984) and Dudley (1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120994616,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3e211ab5f8fe33612149727fb3e5b1ad4af65d34",
            "isKey": true,
            "numCitedBy": 396,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-course-on-empirical-processes-Dudley",
            "title": {
                "fragments": [],
                "text": "A course on empirical processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145599528"
                        ],
                        "name": "D. Pollard",
                        "slug": "D.-Pollard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pollard",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "Our treatment closely parallels the approach given in Pollard (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "First, we propose an extension of the PAC model, based on the work of Vapnik and Chervonenkis (Vapnik, 1989) and Pollard (1984, 1990), that addresses these and other issues."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 271
                            }
                        ],
                        "text": "\u2026examples needed so that with high probability, any decision rule in 2 that has small empirical loss on the training examples will have small actual expected loss; i.e. we get uniform convergence results for empirical estimates like those in Vapnik (1982) Dudley (1984), and Pollard (1984, 1990)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "The pseudo dimension has a few invariance properties that are useful (see Pollard, 1990, for further results of this type)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "The pseudo dimension will be used in the form defined above in Pollard\u2019s new book (Pollard, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "they can be annoying (see Vapnik (1989) and Pollard (1984, 1990) for alternative approaches for unbounded loss functions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117788733,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d298ce81fa8f6d73689f84535efce90dfcb4bf78",
            "isKey": true,
            "numCitedBy": 883,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-Processes:-Theory-and-Applications-Pollard",
            "title": {
                "fragments": [],
                "text": "Empirical Processes: Theory and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821511"
                        ],
                        "name": "Gyora M. Benedek",
                        "slug": "Gyora-M.-Benedek",
                        "structuredName": {
                            "firstName": "Gyora",
                            "lastName": "Benedek",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyora M. Benedek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736744"
                        ],
                        "name": "A. Itai",
                        "slug": "A.-Itai",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Itai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Itai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "6), Benedek and Itai (1988), and White (1990a))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 30
                            }
                        ],
                        "text": "It also builds on the work of Benedek and Itai (1988) on PAC learnability with respect to specific probability distributions, and is related to the work of Natarajan and Tadepalli on extensions of the VC dimension to multivalued functions (Natarajan and Tadepalli, 1988; Natarajan, 1989b) and PAC\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "One of the key ideas we use is the notion of an a-cover of a metric space (Dudley, 1989; Pollard, 1984; Benedek and Itai, 1988; Natarajan, 1989a, Quiroz, 1989) and the associated idea of metric dimension (Kolmogorov and Tihomirov, 1961) (also called the fractal dimension (Farmer, 1982))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 131
                            }
                        ],
                        "text": "It is interesting to note that related results connecting e-covers with the VC dimension have also been independently developed in Benedek and Itai (1988) and in recent computational geometry work (Welzl, 1988)\u2019 This work seems to lead to a potentially rich area of investigation that combines\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 68
                            }
                        ],
                        "text": "This result also gives a stronger version of Theorem 4, part (3) of Benedek and Itai (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5260687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c23ed6b3bf16778c9f2a6a5b6b0b629e9b1b0af1",
            "isKey": true,
            "numCitedBy": 72,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learnability-by-fixed-distributions-Benedek-Itai",
            "title": {
                "fragments": [],
                "text": "Learnability by fixed distributions"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "6), Benedek and Itai (1988), and White (1990a))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 30
                            }
                        ],
                        "text": "It also builds on the work of Benedek and Itai (1988) on PAC learnability with respect to specific probability distributions, and is related to the work of Natarajan and Tadepalli on extensions of the VC dimension to multivalued functions (Natarajan and Tadepalli, 1988; Natarajan, 1989b) and PAC\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "One of the key ideas we use is the notion of an a-cover of a metric space (Dudley, 1989; Pollard, 1984; Benedek and Itai, 1988; Natarajan, 1989a, Quiroz, 1989) and the associated idea of metric dimension (Kolmogorov and Tihomirov, 1961) (also called the fractal dimension (Farmer, 1982))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 131
                            }
                        ],
                        "text": "It is interesting to note that related results connecting e-covers with the VC dimension have also been independently developed in Benedek and Itai (1988) and in recent computational geometry work (Welzl, 1988)\u2019 This work seems to lead to a potentially rich area of investigation that combines\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 68
                            }
                        ],
                        "text": "This result also gives a stronger version of Theorem 4, part (3) of Benedek and Itai (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learnability by xed distributions"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 1988 Workshop on Comp. Learning Theory,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "This cleaner approach is detailed in Pollard (1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 136
                            }
                        ],
                        "text": "This allows us to state the relevant uniform convergence bounds as generalized \u201cChernoff-style\u201d (Angluin and Valiant, 1979) bounds, as in Pollard (1986) and Breiman et al. (1984, Chap."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 112
                            }
                        ],
                        "text": "However, our results are restricted to the case where all functions in F are positive and uniformly bounded.\nlo Pollard (1986) also gives results that can be used to bound the sample size needed so that\nPr 3f~F: (\nl&(f) -"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 204
                            }
                        ],
                        "text": "The main result about uniform empirical estimates for infinite classes of functions is similar to Theorem 1 except that the random covering numbers are used in place of (FJ.\n106 DAVID HAUSSLER\nTHEOREM 2 (Pollard, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 96
                            }
                        ],
                        "text": "The approach taken here is different from that taken (independently, but prior to this work) in Pollard (1986)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 6
                            }
                        ],
                        "text": "As in Pollard (1986), we give bounds on the sample size needed so that\nPr(3feF: d,(&(f), E(f))>cc)\n=Pr 3fEF: i\nl@,(f)-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of uniform almost-sure convergence for empirical processes indexed by unbounded classes of functions, manuscript"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 7
                            }
                        ],
                        "text": "7, and Quiroz, 1989), and non-i.i.d. sources of examples (see White, 1990a, and Nobel and Dembo, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 25
                            }
                        ],
                        "text": "(E, P, p) (Dudley, 1987; Quiroz, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "One of the key ideas we use is the notion of an a-cover of a metric space (Dudley, 1989; Pollard, 1984; Benedek and Itai, 1988; Natarajan, 1989a, Quiroz, 1989) and the associated idea of metric dimension (Kolmogorov and Tihomirov, 1961) (also called the fractal dimension (Farmer, 1982))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "In addition, Quiroz (1989) and Kulkarni (1989) have each independently generalized the PAC model in a related manner."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Metric entropy and learnability, unpublished mannuscript"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "This cleaner approach is detailed in Pollard (1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 136
                            }
                        ],
                        "text": "This allows us to state the relevant uniform convergence bounds as generalized \u201cChernoff-style\u201d (Angluin and Valiant, 1979) bounds, as in Pollard (1986) and Breiman et al. (1984, Chap."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 112
                            }
                        ],
                        "text": "However, our results are restricted to the case where all functions in F are positive and uniformly bounded.\nlo Pollard (1986) also gives results that can be used to bound the sample size needed so that\nPr 3f~F: (\nl&(f) -"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 204
                            }
                        ],
                        "text": "The main result about uniform empirical estimates for infinite classes of functions is similar to Theorem 1 except that the random covering numbers are used in place of (FJ.\n106 DAVID HAUSSLER\nTHEOREM 2 (Pollard, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 96
                            }
                        ],
                        "text": "The approach taken here is different from that taken (independently, but prior to this work) in Pollard (1986)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 6
                            }
                        ],
                        "text": "As in Pollard (1986), we give bounds on the sample size needed so that\nPr(3feF: d,(&(f), E(f))>cc)\n=Pr 3fEF: i\nl@,(f)-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of uniform almost-sure convergence for empirical processes indexed by unbounded classes of functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 136
                            }
                        ],
                        "text": "Our model for feedforward neural nets is quite general in that it allows many types of units in the nets, including quasi-linear units (Rumelhart and McClelland, 1986), radial basis units (Poggio and Girosi, 1989), and product units (Durbin and Rumelhart, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 142
                            }
                        ],
                        "text": "\u2026bounds on the number of training examples needed to avoid overfitting when learning with the decision rule space of feedforward neural nets (Rumelhart and McClelland, 1986), extending previous work in Baum and Haussler (1989) and White (1990a) (see also related work in Anthony and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 125
                            }
                        ],
                        "text": "We now present some applications of the results of the previous section to learning with feedforward neural nets (see, e.g., Rumelhart and McClelland, 1986; Poggio and Girosi, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 171
                            }
                        ],
                        "text": "\u2026use this extension to obtain distribution-independent upper bounds on the size of the training set needed for learning with various kinds of feedforward neural networks (Rumelhart and McClelland, 1986; Poggio and Girosi, 1989), a popular learning method that is not covered by the basic PAC model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 97
                            }
                        ],
                        "text": "In this standard case, if the global modifier p = 0 we get what is known as a quasi-linear unit (Rumelhart and McClelland, 1986):\nIn the standard case, if ~(2) = C:=, xy we get a unit that computes a function of the form\nf(n\u2018)=O(@+ i (X,-L7j)2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed P r ocessing: Explorations in the Microstructure o f C o gnition. Volume 1: Foundations"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Distributed P r ocessing: Explorations in the Microstructure o f C o gnition. Volume 1: Foundations"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "Our treatment closely parallels the approach given in Pollard (1990)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "First, we propose an extension of the PAC model, based on the work of Vapnik and Chervonenkis (Vapnik, 1989) and Pollard (1984, 1990), that addresses these and other issues."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 271
                            }
                        ],
                        "text": "\u2026examples needed so that with high probability, any decision rule in 2 that has small empirical loss on the training examples will have small actual expected loss; i.e. we get uniform convergence results for empirical estimates like those in Vapnik (1982) Dudley (1984), and Pollard (1984, 1990)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "The pseudo dimension has a few invariance properties that are useful (see Pollard, 1990, for further results of this type)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "The pseudo dimension will be used in the form defined above in Pollard\u2019s new book (Pollard, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "they can be annoying (see Vapnik (1989) and Pollard (1984, 1990) for alternative approaches for unbounded loss functions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical Processes: Theory and Applications, volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Institute of Math. Stat. and Am. Stat. Assoc.,"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607995"
                        ],
                        "name": "C. Sparrow",
                        "slug": "C.-Sparrow",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Sparrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sparrow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 71
                            }
                        ],
                        "text": "The term cupacify has also been used with many other related meanings (Mandelbrot, 1982; Vapnik, 1982; Kolmogorov and Tihomirov, 1961; Farmer et al., 1983; Baum and Haussler, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 106
                            }
                        ],
                        "text": "This notion of dimension has played an important role in the now very active study of fractals in nature (Mandelbrot, 1982) especially in connection with chaos in dynamical systems (Farmer, 1982; Farmer et al., 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 49
                            }
                        ],
                        "text": "A very lucid and intuitive treatment is given in Mandelbrot (1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126043587,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "ddf2e2abdca1c49f75091d81e8e6e047fe450cb6",
            "isKey": false,
            "numCitedBy": 4694,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Fractal-Geometry-of-Nature-Sparrow",
            "title": {
                "fragments": [],
                "text": "The Fractal Geometry of Nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794435"
                        ],
                        "name": "A. Nobel",
                        "slug": "A.-Nobel",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Nobel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nobel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768934"
                        ],
                        "name": "A. Dembo",
                        "slug": "A.-Dembo",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Dembo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dembo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118520606,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cef1a2aedad3225b171375552fdc2aa6847286c6",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-Uniform-Convergence-for-Dependent-Processes-Nobel-Dembo",
            "title": {
                "fragments": [],
                "text": "On Uniform Convergence for Dependent Processes"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Symposium on Information Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700185"
                        ],
                        "name": "S. Weiss",
                        "slug": "S.-Weiss",
                        "structuredName": {
                            "firstName": "Sholom",
                            "lastName": "Weiss",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 278
                            }
                        ],
                        "text": "Cross validation techniques, in which some of the training examples are held in reserve and used instead to test the performance of the decision rules produced by the learning algorithm, are likely to perform better for this task in practice (see, e.g., White, 1990a; Weiss and Kulikowski, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57100530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e70b4af0c13eac9bbb4445b9822350a60aad15b3",
            "isKey": false,
            "numCitedBy": 1244,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Systems-That-Learn-Weiss",
            "title": {
                "fragments": [],
                "text": "Computer Systems That Learn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 583,
                                "start": 34
                            }
                        ],
                        "text": ", 1990; Opper and Haussler, 1991a, 1991b). Unfortunately this last question has no clear cut answer, and leads us directly into a longstanding unresolved debate in statistics (see, e.g., Lindley (1990) and following discussion.). Since we have set out to generalize the PAC model, and since our results are best illustrated in the minimax setting, we formalize the notion of a basic learning problem using the minimax criterion. In subsequent work we hope to further explore this Bayesian setting. (For recent work in Bayesian approaches to neural network learning see MacKay (1992), Buntine and Weigend (1991), and for Bayesian versions of the PAC model see Haussler et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 34
                            }
                        ],
                        "text": ", 1990; Opper and Haussler, 1991a, 1991b). Unfortunately this last question has no clear cut answer, and leads us directly into a longstanding unresolved debate in statistics (see, e.g., Lindley (1990) and following discussion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 224
                            }
                        ],
                        "text": "An alternate approach is to use the L, loss function 1( y, a) = Ia - yI, in which case the expected loss is minimized when a is the median of the conditional distribution of Y given the instance x. (See, e.g., White, 1990b; Haussler, 1990."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35773435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf8124552f41c27338a6dfc7482a83af4faf1dd7",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Learning Model"
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143931389"
                        ],
                        "name": "George Shackelford",
                        "slug": "George-Shackelford",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Shackelford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Shackelford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2306485"
                        ],
                        "name": "D. Volper",
                        "slug": "D.-Volper",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Volper",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Volper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 269
                            }
                        ],
                        "text": "Some practitioners are wary of the assumption that the examples are generated from an underlying \u201ctarget function,\u201d and are not satisfied with the noise models that have been proposed to weaken this assumption (e.g., Angluin and Laird, 1988; Sloan, 1988; Shackelford and Volper, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11551810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80701ffa3e41bf410d3e4bad3a3510ba476bd8bb",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-k-DNF-with-noise-in-the-attributes-Shackelford-Volper",
            "title": {
                "fragments": [],
                "text": "Learning k-DNF with noise in the attributes"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic learning. rule extraction and generalization, Complex"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic complexity and modeling. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic complexity and modeling. The Annals of Statistics"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic complexity and modeling, Ann. Sfotisf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 189
                            }
                        ],
                        "text": "(For recent work in Bayesian approaches to neural network learning see MacKay (1992), Buntine and Weigend (1991), and for Bayesian versions of the PAC model see Haussler et al. (1991b) and Buntine (1990).)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Theory of Learning Classi cation Rules"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of uniform almost-sure convergence for empirical processes indexed by u n bounded classes of functions. manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "Rates of uniform almost-sure convergence for empirical processes indexed by u n bounded classes of functions. manuscript"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 234
                            }
                        ],
                        "text": "Our model for feedforward neural nets is quite general in that it allows many types of units in the nets, including quasi-linear units (Rumelhart and McClelland, 1986), radial basis units (Poggio and Girosi, 1989), and product units (Durbin and Rumelhart, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 90
                            }
                        ],
                        "text": "Then\nf(P)=d(e+ i wjlogxj)=~(~~~lx:,. /=I\ngiving what is commonly known as a product unit (Durbin and Rumelhart, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Product units: A computationaly powerful and biologically plausible extension to backporpogation networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 76
                            }
                        ],
                        "text": "Thus we look at \u201cbatch\u201d learning rather than \u201cincremental\u201d or \u201con-line\u201d learning (Littlestone, 1988)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning quickly when irrelevant attributes abound: A new linearthreshold"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "By weakening these assumptions, we can model other types of learning as well, including associative reinforcement learning (Barto and Anandan, 1985; Gullapalli, 1990) and the theory of learning automata (with static environment) (Narendra and Thathachar, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic reinforcement algortihm for learning real-valued functions"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of growth for weighted empirical processes"
            },
            "venue": {
                "fragments": [],
                "text": "Rates of growth for weighted empirical processes"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 234
                            }
                        ],
                        "text": "Our model for feedforward neural nets is quite general in that it allows many types of units in the nets, including quasi-linear units (Rumelhart and McClelland, 1986), radial basis units (Poggio and Girosi, 1989), and product units (Durbin and Rumelhart, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 90
                            }
                        ],
                        "text": "Then\nf(P)=d(e+ i wjlogxj)=~(~~~lx:,. /=I\ngiving what is commonly known as a product unit (Durbin and Rumelhart, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Product units: A computationaly powerful and biologically plausible extension to backporpogation networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "It is shown in Baum and Haussler (1989, Theorem 1 ), (and is also implied directly from\nGENERALIZATIONS OF THE PAC MODEL 145\nresults in Cover (1968)) that for any class X as above, IJQ   (Nem/d)d for all x\u2019= (x1, . . . . x,), where d and N are as above."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Capacity problems for linear machines"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition,"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 208
                            }
                        ],
                        "text": "\u2026is the Buyesiun notion of optimality (Berger, 1985; Kiefer, 1987), and has been used in several approaches to learning in neural nets based on statistical mechanics (Denker et al. 1987; Tishby et al., 1989; Gyorgyi and Tishby, 1990; Sompolinsky et al., 1990; Opper and Haussler, 1991a, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical theory of learning a rule"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks and Spin Glasses. World Scienti c,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft Weight-Sharing,"
            },
            "venue": {
                "fragments": [],
                "text": "Information Processing Systems\u201d (D. Touretsky, Ed.)"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classi cation and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 275
                            }
                        ],
                        "text": "\u2026specific probability distributions, and is related to the work of Natarajan and Tadepalli on extensions of the VC dimension to multivalued functions (Natarajan and Tadepalli, 1988; Natarajan, 1989b) and PAC learnability with respect to classes of probability distributions (Natarajan, 1988, 1989a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning over classes of distributions"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the 1988 Workshop on Computational Learning Theory,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OPPER, M., AND HAUSLER, D. (1991b), Generalization performance of Bayes optimal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learnability b y xed distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1988 Workshop on Comp. Learning Theory"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 141
                            }
                        ],
                        "text": "\u2026e-covers with the VC dimension have also been independently developed in Benedek and Itai (1988) and in recent computational geometry work (Welzl, 1988)\u2019 This work seems to lead to a potentially rich area of investigation that combines elements of combinatorics, topology and geometry, and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "1 of Welzl (1988) (using the primal space instead of the dual)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Partition trees for triangle counting and other range search problems, in \u201c\u20184th ACM Symposium on Computational Geometry."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees. W adsworth International Group"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees. W adsworth International Group"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 81
                            }
                        ],
                        "text": "It generalizes the techniques based on the Vapnik-Chervonenkis dimension used in Blumer et al. (1989), which apply only to (0, 1 }-valued functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 28
                            }
                        ],
                        "text": "similar to that obtained in Baum and Haussler (1989). Since W appears to be the dominant factor in these bounds, apart from the accuracy parameters a and v, these bounds support the conventional wisdom that the training set size should be primarily related to the number of adjustable parameters in the net."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning over classes of distributions, in \u201cProceedings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artiicial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "28th Conference o n D e cision and Control BC900 Andrew Barron and Thomas Cover. Minumum complexity density estimation. IEEE Transactions on Information Theory"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic reinforcement algortihm for learning real-valued functions"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "l987), \u201cIntroduction to Statistical Inference,"
            },
            "venue": {
                "fragments": [],
                "text": "Amer. Math. Sot. Trunsl. Ser"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 0
                            }
                        ],
                        "text": "We show how these give upper bounds on sufficient training sample size like those derived in Blumer et al. (1989) and elsewhere using the notion of the VC dimension, and generalize those results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ClassiIication and Regression Trees,"
            },
            "venue": {
                "fragments": [],
                "text": "AND STONE, C. J"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Capacity problems for linear machines"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Densit e e t dimension"
            },
            "venue": {
                "fragments": [],
                "text": "Densit e e t dimension"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian back propagation. Unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian back propagation. Unpublished manuscript"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tihomirov. -entropy and -capacity of sets in functional spaces"
            },
            "venue": {
                "fragments": [],
                "text": "Amer. Math. Soc. Translations (Ser"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 128
                            }
                        ],
                        "text": "Still different, and more involved, techniques are used in the more general theory of weighted empirical processes developed by Alexander (1985, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of growth for weighted empirical processes"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of Berkeley Conference in Honor of Jerzy Neyman and Jack Kiefer,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems, v olume 1"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems, v olume 1"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 127
                            }
                        ],
                        "text": "An example is the recent work using weight penalty functions in neural net training (Weigend et al., 1990; LeCun et al., 1990; Nowlan and Hinton, 1991; MacKay, 1992; Buntine and Weigend, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft weight-sharing"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Dept. of Comp. Sci., U. Toronto,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning over classes of distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1988 Workshop on Computational Learning Theory"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Theory of Learning Classiication Rules"
            },
            "venue": {
                "fragments": [],
                "text": "A Theory of Learning Classiication Rules"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "This is known as the logistic loss (McCullagh and Nelder, 1989; Barron, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "For further discussion, we refer the reader to the excellent surveys of White (1990b), Barron (1989), Devroye (1988), and Vapnik (1989), to which we are greatly indebted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical properties of arti cial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "In 28th Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications"
            },
            "venue": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Metric entropy and learnability. Unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "Universidad Sim on Bol ivar"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable, Contm"
            },
            "venue": {
                "fragments": [],
                "text": "VAPNIK, V. N"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 27
                            }
                        ],
                        "text": "(See the recent results of Talagrand (1991) for much better constants for Corollary 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sharper bounds for empirical processes, unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PITT, L"
            },
            "venue": {
                "fragments": [],
                "text": "AND VALIANT. L"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 128
                            }
                        ],
                        "text": "Still different, and more involved, techniques are used in the more general theory of weighted empirical processes developed by Alexander (1985, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rates of growth for weighted empirical processes, in \u201cProceedings of Berkeley"
            },
            "venue": {
                "fragments": [],
                "text": "Conference in Honor of Jerzy Neyman and Jack Kiefer,\u201d"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical Processes: Theory and Applications, v olume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Institute of Math. Stat. and Am. Stat. Assoc"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 51,
            "methodology": 37,
            "result": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 124,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler/fedfc9fbcfe46d50b81078560bce724678f90176?sort=total-citations"
}