{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119321034"
                        ],
                        "name": "Anand Kumar",
                        "slug": "Anand-Kumar",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "One option is to quantize and/or index image features and retrieve them whenever necessary [2], [9], [4], [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13506212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6aa6a4033db777551c625314d6e42af2a264900b",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an efficient indexing and retrieval scheme for searching in document image databases. In many non-European languages, optical character recognizers are not very accurate. Word spotting - word image matching - may instead be used to retrieve word images in response to a word image query. The approaches used for word spotting so far, dynamic time warping and/or nearest neighbor search, tend to be slow. Here indexing is done using locality sensitive hashing (LSH) - a technique which computes multiple hashes - using word image features computed at word level. Efficiency and scalability is achieved by content-sensitive hashing implemented through approximate nearest neighbor computation. We demonstrate that the technique achieves high precision and recall (in the 90% range), using a large image corpus consisting of seven Kalidasa's (a well known Indian poet of antiquity) books in the Telugu language. The accuracy is comparable to using dynamic time warping and nearest neighbor search while the speed is orders of magnitude better - 20000 word images can be searched in milliseconds."
            },
            "slug": "Efficient-Search-in-Document-Image-Collections-Kumar-Jawahar",
            "title": {
                "fragments": [],
                "text": "Efficient Search in Document Image Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents an efficient indexing and retrieval scheme for searching in document image databases that achieves high precision and recall, using a large image corpus consisting of seven Kalidasa's books in the Telugu language."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3059058"
                        ],
                        "name": "I. Z. Yalniz",
                        "slug": "I.-Z.-Yalniz",
                        "structuredName": {
                            "firstName": "Ismet",
                            "lastName": "Yalniz",
                            "middleNames": [
                                "Zeki"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Z. Yalniz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10374246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cd7660fd6be488024418c4e8ae0f3da07af6eb5",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper aims to evaluate the accuracy of optical character recognition (OCR) systems on real scanned books. The ground truth e-texts are obtained from the Project Gutenberg website and aligned with their corresponding OCR output using a fast recursive text alignment scheme (RETAS). First, unique words in the vocabulary of the book are aligned with unique words in the OCR output. This process is recursively applied to each text segment in between matching unique words until the text segments become very small. In the final stage, an edit distance based alignment algorithm is used to align these short chunks of texts to generate the final alignment. The proposed approach effectively segments the alignment problem into small sub problems which in turn yields dramatic time savings even when there are large pieces of inserted or deleted text and the OCR accuracy is poor. This approach is used to evaluate the OCR accuracy of real scanned books in English, French, German and Spanish."
            },
            "slug": "A-Fast-Alignment-Scheme-for-Automatic-OCR-of-Books-Yalniz-Manmatha",
            "title": {
                "fragments": [],
                "text": "A Fast Alignment Scheme for Automatic OCR Evaluation of Books"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The proposed approach effectively segments the alignment problem into small sub problems which in turn yields dramatic time savings even when there are large pieces of inserted or deleted text and the OCR accuracy is poor."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145537764"
                        ],
                        "name": "K. Sankar",
                        "slug": "K.-Sankar",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Sankar",
                            "middleNames": [
                                "Pramod"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Besides, there are a number of scripts such as Telugu and Ottoman for which no OCR engine is available [10], [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1508626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dac3cf11624be87c025087d8ba2e94e949f70e80",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conventional optical character recognition (OCR) systems operate on individual characters and words, and do not normally exploit document or collection context. We describe a Collection OCR which takes advantage of the fact that multiple examples of the same word (often in the same font) may occur in a document or collection. The idea here is that an OCR or a reCAPTCHA like process generates a partial set of recognized words. In the second stage, a nearest neighbor algorithm compares the remaining word-images to those already recognized and propagates labels from the nearest neighbors. It is shown that by using an approximate fast nearest neighbor algorithm based on Hierarchical K-Means (HKM), we can do this accurately and efficiently. It is also shown that profile based features perform much better than SIFT and Pyramid Histogram of Gradient (PHOG) features. We believe that this is because profile features are more robust to word degradations (common in our documents). This approach is applied to a collection of Telugu books - a language for which no commercial OCR exists. We show from a selection of 33 Telugu books that starting with OCR labels for only 30% of the collection we can recognize the remaining 70% of the words in the collection with 70% accuracy using this approach. Since the approach makes no language specific assumptions, it should be applicable to a large number of languages. In particular we are interested in its applicability to Indic languages and scripts."
            },
            "slug": "Nearest-neighbor-based-collection-OCR-Sankar-Jawahar",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor based collection OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Collection OCR which takes advantage of the fact that multiple examples of the same word may occur in a document or collection, and makes no language specific assumptions, and should be applicable to a large number of languages."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857558"
                        ],
                        "name": "Shaolei Feng",
                        "slug": "Shaolei-Feng",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "One option is to quantize and/or index image features and retrieve them whenever necessary [2], [9], [4], [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3873933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e21100feb2125b2075c9b0c93ac40b25745a1090",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a formal framework for image and video retrieval using discrete Markov random fields (MRF). The training dataset consists of images with keywords (regions are not labeled). The model is built using a discrete vocabulary of vector quantized region or point features generated from the training images. Since performance is dependent on the size of the vocabulary, a large vocabulary of a couple of million visterms is used. Such large vocabularies cannot be generated by conventional clustering algorithms so hierarchical k-means is used to generate it. Unlike many previous techniques, our MRF based model doesn't require an explicit annotation step for retrieval. The model directly ranks all test images according to the posterior probability of an image given a query. Traditionally, most models are trained by maximizing likelihood - instead this model is trained by maximizing average precision. Image and video retrieval experiments are performed on two standard datasets (a Corel dataset and a TRECVID3 dataset) which consist of 4,500 images and about 44,100 keyframes respectively. The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing (continuous feature) models."
            },
            "slug": "A-discrete-direct-retrieval-model-for-image-and-Feng-Manmatha",
            "title": {
                "fragments": [],
                "text": "A discrete direct retrieval model for image and video retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results show that based on a large visual vocabulary the model runs extremely fast on even very large datasets while having comparable retrieval performance to the best performing (continuous feature) models."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2352980"
                        ],
                        "name": "T. Rath",
                        "slug": "T.-Rath",
                        "structuredName": {
                            "firstName": "Toni",
                            "lastName": "Rath",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "One option is to quantize and/or index image features and retrieve them whenever necessary [2], [9], [4], [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14226850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8a66067ec01e486ea39c0d6bec42fee80a3dc0",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many museum and library archives are digitizing their large collections of handwritten historical manuscripts to enable public access to them. These collections are only available in image formats and require expensive manual annotation work for access to them. Current handwriting recognizers have word error rates in excess of 50% and therefore cannot be used for such material. We describe two statistical models for retrieval in large collections of handwritten manuscripts given a text query. Both use a set of transcribed page images to learn a joint probability distribution between features computed from word images and their transcriptions. The models can then be used to retrieve unlabeled images of handwritten documents given a text query. We show experiments with a training set of 100 transcribed pages and a test set of 987 handwritten page images from the George Washington collection. Experiments show that the precision at 20 documents is about 0.4 to 0.5 depending on the model. To the best of our knowledge, this is the first automatic retrieval system for historical manuscripts using text queries, without manual transcription of the original corpus."
            },
            "slug": "A-search-engine-for-historical-manuscript-images-Rath-Manmatha",
            "title": {
                "fragments": [],
                "text": "A search engine for historical manuscript images"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work describes two statistical models for retrieval in large collections of handwritten manuscripts given a text query, which is the first automatic retrieval system for historical manuscripts using text queries, without manual transcription of the original corpus."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3083483"
                        ],
                        "name": "D. Nist\u00e9r",
                        "slug": "D.-Nist\u00e9r",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nist\u00e9r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nist\u00e9r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3086037"
                        ],
                        "name": "Henrik Stew\u00e9nius",
                        "slug": "Henrik-Stew\u00e9nius",
                        "structuredName": {
                            "firstName": "Henrik",
                            "lastName": "Stew\u00e9nius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henrik Stew\u00e9nius"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "One well-known practice is to map feature vectors to discrete values using clustering techniques [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1654266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3e7d3e37e67af7f4546b46051063bea1b62dbae",
            "isKey": false,
            "numCitedBy": 3890,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u2019s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images."
            },
            "slug": "Scalable-Recognition-with-a-Vocabulary-Tree-Nist\u00e9r-Stew\u00e9nius",
            "title": {
                "fragments": [],
                "text": "Scalable Recognition with a Vocabulary Tree"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A recognition scheme that scales efficiently to a large number of objects and allows a larger and more discriminatory vocabulary to be used efficiently is presented, which it is shown experimentally leads to a dramatic improvement in retrieval quality."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145537764"
                        ],
                        "name": "K. Sankar",
                        "slug": "K.-Sankar",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Sankar",
                            "middleNames": [
                                "Pramod"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "One option is to quantize and/or index image features and retrieve them whenever necessary [2], [9], [4], [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7494371,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science",
                "Education"
            ],
            "id": "98045997e87241bbab51d0e7e4b44d4132abdda4",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic annotation is an elegant alternative to explicit recognition in images. In annotation, the image is matched with keyword models, and the most relevant keywords are assigned to the image. Using existing techniques, the annotation time for large collections is very high, while the annotation performance degrades with increase in number of keywords. Towards the goal of large scale annotation, we present an approach called \"Reverse Annotation \". Unlike traditional annotation where keywords are identified for a given image, in Reverse Annotation, the relevant images are identified for each keyword. With this seemingly simple shift in perspective, the annotation time is reduced significantly. To be able to rank relevant images, the approach is extended to Probabilistic Reverse Annotation. Our framework is applicable to a wide variety of multimedia documents, and scalable to large collections. Here, we demonstrate the framework over a large collection of 75,000 document images, containing 21 million word segments, annotated by 35000 keywords. Our image retrieval system replicates text-based search engines, in response time."
            },
            "slug": "Probabilistic-Reverse-Annotation-for-Large-Scale-Sankar-Jawahar",
            "title": {
                "fragments": [],
                "text": "Probabilistic Reverse Annotation for Large Scale Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents an approach called \"Reverse Annotation\", where unlike traditional annotation where keywords are identified for a given image, in Reverse Annotation, the relevant images are identify for each keyword."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066819269"
                        ],
                        "name": "James Philbin",
                        "slug": "James-Philbin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Philbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Philbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "For matching natural images, use of larger vocabularies is shown to perform better [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12203312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28e4b8ebbdb0e80f03b6f0578deeb38694af081e",
            "isKey": false,
            "numCitedBy": 2923,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, \"web-scale \" image corpora."
            },
            "slug": "Object-retrieval-with-large-vocabularies-and-fast-Philbin-Chum",
            "title": {
                "fragments": [],
                "text": "Object retrieval with large vocabularies and fast spatial matching"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "To improve query performance, this work adds an efficient spatial verification stage to re-rank the results returned from the bag-of-words model and shows that this consistently improves search quality, though by less of a margin when the visual vocabulary is large."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721991"
                        ],
                        "name": "E. Rosten",
                        "slug": "E.-Rosten",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Rosten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rosten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418842"
                        ],
                        "name": "T. Drummond",
                        "slug": "T.-Drummond",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Drummond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Drummond"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1388140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
            "isKey": false,
            "numCitedBy": 4030,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time. By comparison neither the Harris detector (120%) nor the detection stage of SIFT (300%) can operate at full frame rate. \n \nClearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion."
            },
            "slug": "Machine-Learning-for-High-Speed-Corner-Detection-Rosten-Drummond",
            "title": {
                "fragments": [],
                "text": "Machine Learning for High-Speed Corner Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3059058"
                        ],
                        "name": "I. Z. Yalniz",
                        "slug": "I.-Z.-Yalniz",
                        "structuredName": {
                            "firstName": "Ismet",
                            "lastName": "Yalniz",
                            "middleNames": [
                                "Zeki"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Z. Yalniz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757482"
                        ],
                        "name": "Ismail Seng\u00f6r Alting\u00f6vde",
                        "slug": "Ismail-Seng\u00f6r-Alting\u00f6vde",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Alting\u00f6vde",
                            "middleNames": [
                                "Seng\u00f6r"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ismail Seng\u00f6r Alting\u00f6vde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746035"
                        ],
                        "name": "Ugur G\u00fcd\u00fckbay",
                        "slug": "Ugur-G\u00fcd\u00fckbay",
                        "structuredName": {
                            "firstName": "Ugur",
                            "lastName": "G\u00fcd\u00fckbay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ugur G\u00fcd\u00fckbay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801322"
                        ],
                        "name": "\u00d6. Ulusoy",
                        "slug": "\u00d6.-Ulusoy",
                        "structuredName": {
                            "firstName": "\u00d6zg\u00fcr",
                            "lastName": "Ulusoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d6. Ulusoy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Besides, there are a number of scripts such as Telugu and Ottoman for which no OCR engine is available [10], [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2912811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b8942de869a2d9e92520fe3633657a819565b76",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents Ottoman Archives Explorer, a Content-Based Retrieval (CBR) system based on character recognition for printed and handwritten historical documents. Several methods for character segmentation and recognition stages are investigated. In particular, sliding-window and histogram segmentation methods are coupled with recognition approaches using spatial features, neural networks, and a graph-based model. The prototype system provides CBR of document images using both example-based queries and a virtual keyboard to construct query words."
            },
            "slug": "Ottoman-archives-explorer:-A-retrieval-system-for-Yalniz-Alting\u00f6vde",
            "title": {
                "fragments": [],
                "text": "Ottoman archives explorer: A retrieval system for digital Ottoman archives"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This article presents Ottoman Archives Explorer, a Content-Based Retrieval (CBR) system based on character recognition for printed and handwritten historical documents, and several methods for character segmentation and recognition stages are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "JOCCH"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487006"
                        ],
                        "name": "B. Fulkerson",
                        "slug": "B.-Fulkerson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Fulkerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fulkerson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "In this framework, hierarchical K-Means (HIKMEANS) is utilized for quantizing the SIFT descriptors [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "96% of the processing is the extraction of SIFT descriptors [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1458265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d720a95e1501922ea17ee31f299f43b2db5e15ef",
            "isKey": false,
            "numCitedBy": 3386,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research."
            },
            "slug": "Vlfeat:-an-open-and-portable-library-of-computer-Vedaldi-Fulkerson",
            "title": {
                "fragments": [],
                "text": "Vlfeat: an open and portable library of computer vision algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "VLFeat is an open and portable library of computer vision algorithms that includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747046"
                        ],
                        "name": "S. Deorowicz",
                        "slug": "S.-Deorowicz",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Deorowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deorowicz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11775874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "638e88dd36e284d775af5e99feea94a5fe002ec1",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern graphical processing units (GPUs) offer much more computational power than modern central processing units. Therefore, it is natural that GPUs are applied not only for their original purposes, but also for general processing (GPGPU). In the field of sequence processing, one of the most important problems is the measuring of sequence similarity. There are many sequence similarity measures, e.g. edit distance, longest common subsequence length, and their derivatives. We examine the possibility of speeding up the algorithms computing some of them. We chose three measures useful in different situations. The experimental results show that the GPU versions of the examined algorithms are faster than their serial counterparts by a factor between 4 and 65. Copyright \u00a9 2010 John Wiley & Sons, Ltd."
            },
            "slug": "Solving-longest-common-subsequence-and-related-on-Deorowicz",
            "title": {
                "fragments": [],
                "text": "Solving longest common subsequence and related problems on graphical processing units"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines the possibility of speeding up the algorithms computing some of the sequence similarity measures, and chooses three measures useful in different situations."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465976"
                        ],
                        "name": "M. Fischler",
                        "slug": "M.-Fischler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "A well-known approach is the RANSAC algorithm [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 972888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278c9a78d4505cfaf6b709df364dbd1206a017c1",
            "isKey": false,
            "numCitedBy": 15951,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing"
            },
            "slug": "Random-sample-consensus:-a-paradigm-for-model-with-Fischler-Bolles",
            "title": {
                "fragments": [],
                "text": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "New results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form that provide the basis for an automatic system that can solve the Location Determination Problem under difficult viewing."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The SIFT keypoint detector provides intrinsic scale and orientation for each keypoint automatically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "On the right column, it is seen that Fast-CornerDetector is much more repeatable than the SIFT and it is more likely to locate the same corner in spite of the noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Unlike Fast-Corner-Detector, SIFT is capable of extracting scale and rotation invariant keypoints which are shown to be distinctive especially in natural images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "SIFT descriptors are used in this study."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "Two different approaches were for detecting keypoints: Fast-Corner-Detector [8] and Scale Invariant Feature Transform (SIFT) [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The total number of keypoints are almost the same for the SIFT and Fast-Corner-Detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "More specifically, SIFT [5] descriptors are extracted for each corner point detected by the Fast-Corner-Detection algorithm [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "It is seen that the SIFT features are distorted heavily at the bottom of the word image around the noisy region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "In this framework, hierarchical K-Means (HIKMEANS) is utilized for quantizing the SIFT descriptors [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Fast-Corner-Points are claimed to be more repeatable than well-known SIFT keypoints [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": true,
            "numCitedBy": 25497,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 7
                            }
                        ],
                        "text": "Unlike Fast-Corner-Detector, SIFT is capable of extracting scale and rotation invariant keypoints which are shown to be distinctive especially in natural images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Fast-Corner-Detector finds corners points in images in a fraction of a second."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 10
                            }
                        ],
                        "text": "Therefore Fast-Corner-Detector is used for keypoint localization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 37
                            }
                        ],
                        "text": "However, it is not the case with the Fast-Corner-Detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "Two different approaches were for detecting keypoints: Fast-Corner-Detector [8] and Scale Invariant Feature Transform (SIFT) [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "The total number of keypoints are almost the same for the SIFT and Fast-Corner-Detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "More specifically, SIFT [5] descriptors are extracted for each corner point detected by the Fast-Corner-Detection algorithm [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Fast-Corner-Points are claimed to be more repeatable than well-known SIFT keypoints [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine learning for highspeed corner detection"
            },
            "venue": {
                "fragments": [],
                "text": "In European Conference on Computer Vision,"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38955885"
                        ],
                        "name": "Changchang Wu",
                        "slug": "Changchang-Wu",
                        "structuredName": {
                            "firstName": "Changchang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changchang Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The SIFT keypoint detector provides intrinsic scale and orientation for each keypoint automatically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "On the right column, it is seen that Fast-CornerDetector is much more repeatable than the SIFT and it is more likely to locate the same corner in spite of the noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Using a GPU implementation of SIFT [13], the offline processing would take less than 5 minutes for a book with 200 pages and 100MB of main memory is sufficient for online queries."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Unlike Fast-Corner-Detector, SIFT is capable of extracting scale and rotation invariant keypoints which are shown to be distinctive especially in natural images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "SIFT descriptors are used in this study."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Two different approaches were for detecting keypoints: Fast-Corner-Detector [8] and Scale Invariant Feature Transform (SIFT) [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The total number of keypoints are almost the same for the SIFT and Fast-Corner-Detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "More specifically, SIFT [5] descriptors are extracted for each corner point detected by the Fast-Corner-Detection algorithm [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "It is seen that the SIFT features are distorted heavily at the bottom of the word image around the noisy region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "In this framework, hierarchical K-Means (HIKMEANS) is utilized for quantizing the SIFT descriptors [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Fast-Corner-Points are claimed to be more repeatable than well-known SIFT keypoints [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 203702977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "288020dbbecf002fa15d3db254a39681e954515e",
            "isKey": true,
            "numCitedBy": 522,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SiftGPU-:-A-GPU-Implementation-of-Scale-Invariant-Wu",
            "title": {
                "fragments": [],
                "text": "SiftGPU : A GPU Implementation of Scale Invariant Feature Transform (SIFT)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Efficient-Framework-for-Searching-Text-in-Noisy-Yalniz-Manmatha/3b2f1e41c1eb329650259801eb6b77cbafce5a5f?sort=total-citations"
}