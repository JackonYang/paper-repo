{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 143
                            }
                        ],
                        "text": "\u2026tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by theteacher signal dk(t) in subsequent computation of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 414,
                                "start": 405
                            }
                        ],
                        "text": "Such an approximation algorithm would provide an interesting blend of aspects ofboth truncated BPTT and subgrouped RTRL.9 Teacher ForcingAn interesting strategy that has appeared implicitly or explicitly in the work of a number ofinvestigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by theteacher signal dk(t) in subsequent computation of the behavior of the network, whenever such atarget value exists."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 147
                            }
                        ],
                        "text": "\u2026by Robinson and Fallside (1987), Kuhn (1987),Bachrach (1988, [chapter ??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williamsand Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doyaand Yoshizawa (1989), and Sato (1990a; 1990b).5.1 The\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "While truncatedBPTT achieves a simpli cation by ignoring long-term temporal dependencies in the network'soperation, this modi cation to RTRL, proposed in (Zipser, 1989), achieves its simpli cation byignoring certain structural dependencies in the network's operation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 156
                            }
                        ],
                        "text": "While truncated BPTT achieves a simpli cation by ignoring long-term temporal dependencies in the network's operation, this modi cation to RTRL, proposed in (Zipser, 1989), achieves its simpli cation by ignoring certain structural dependencies in the network's operation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 196
                            }
                        ],
                        "text": "This algorithmhas been independently derived in various forms by Robinson and Fallside (1987), Kuhn (1987),Bachrach (1988, [chapter ??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williamsand Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doyaand Yoshizawa (1989), and Sato (1990a; 1990b).5.1 The AlgorithmFor each k 2 U , i 2 U , j 2 U [ I, and t0 t t1, we de nepkij(t) = @yk(t)@wij : (31)This quantity measures the sensitivity of the value of the output of the kth unit at time t to asmall increase in the value of wij, taking into account the e ect of such a change in the weightover the entire trajectory from t0 to t but assuming that the initial state of the network, the inputover [t0; t), and the remaining weights are not altered.From equations (7) and (8) and use of the chain rule, we nd that@J(t)@wij = Xk2U ek(t)pkij(t) (32)for each i 2 U and j 2 U [ I."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 278
                            }
                        ],
                        "text": "\u2026described here, both subgrouped RTRL andtruncated BPTT were tested for their ability to train fully recurrent networks to emulate the nitestate machine part of a Turing machine for balancing parentheses, a task that had previously beenshown to be learnable by RTRL (Williams & Zipser, 1989b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27529351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bda89a6b28f234e9159b4fc884980bdd6163819a",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm, called RTRL, for training fully recurrent neural networks has recently been studied by Williams and Zipser (1989a, b). Whereas RTRL has been shown to have great power and generality, it has the disadvantage of requiring a great deal of computation time. A technique is described here for reducing the amount of computation required by RTRL without changing the connectivity of the networks. This is accomplished by dividing the original network into subnets for the purpose of error propagation while leaving them undivided for activity propagation. An example is given of a 12-unit network that learns to be the finite-state part of a Turing machine and runs 10 times faster using the subgrouping strategy than the original algorithm."
            },
            "slug": "A-Subgrouping-Strategy-that-Reduces-Complexity-and-Zipser",
            "title": {
                "fragments": [],
                "text": "A Subgrouping Strategy that Reduces Complexity and Speeds Up Learning in Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A technique is described here for reducing the amount of computation required by RTRL without changing the connectivity of the networks by dividing the original network into subnets for the purpose of error propagation while leaving them undivided for activity propagation."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 320,
                                "start": 180
                            }
                        ],
                        "text": "9 Teacher Forcing An interesting strategy that has appeared implicitly or explicitly in the work of a number of investigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989; Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams & Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by the teacher signal dk(t) in subsequent computation of the behavior of the network, whenever such a target value exists."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "One example of this architecture is provided by the work of Jordan (1986), who has considered a version in which the hidden stage and the output stage are each one-layer networks, with feedback connections going from all units in the output stage to all units in the hidden stage."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 60
                            }
                        ],
                        "text": "One example of this architecture is provided by the work of Jordan(1986), who has considered a version in which the hidden stage and the output stage are eachone-layer networks, with feedback connections going from all units in the output stage to all unitsin the hidden stage."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2026implicitly or explicitly in the work of a number ofinvestigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026carry out various language processing or motor control tasksas a means of understanding the information processing strategies involved (Elman, 1988; Jordan,1986; Mozer, 1989, [chapter ??, this volume]; Cleeremans, Servan-Screiber, and McClelland, 1989,[chapter ??, this volume]; Smith & Zipser,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56723681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d76aafbeb54575859441a442376766c597f6bb52",
            "isKey": true,
            "numCitedBy": 1102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Attractor-dynamics-and-parallelism-in-a-sequential-Jordan",
            "title": {
                "fragments": [],
                "text": "Attractor dynamics and parallelism in a connectionist sequential machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "This algorithm, which is a discrete-time version of the algorithm described by Almeida (1987) and Pineda (1987; [chapter ??, this volume]) is obtained as follows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 46
                            }
                        ],
                        "text": "The recurrent backpropagation (RBP) algorithm (Almeida, 1987; Pineda, 1987) for training settling networks having constant input consists of applying the following steps: 1) the network is allowed to settle (with the time at which settling has completed regarded as t1); 2) the BPTT computation given by equations (28), (29), and (30) is performed for as long as needed until the i values converge; 3) all the @J total(t0; t1)=@wij values are computed using equation (27); and 4) weights are changed accordingly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 47
                            }
                        ],
                        "text": "The recurrent backpropagation (RBP) algorithm (Almeida, 1987; Pineda, 1987) for trainingsettling networks having constant input consists of applying the following steps: 1) the networkis allowed to settle (with the time at which settling has completed regarded as t1); 2) the BPTTcomputation given\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "Thisalgorithm, which is a discrete-time version of the algorithm described by Almeida (1987) andPineda (1987; [chapter ??, this volume]) is obtained as follows."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58820035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be3f21ab796bd9811382b560507c1c679fae37f",
            "isKey": true,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-rule-for-asynchronous-perceptrons-with-a-Almeida",
            "title": {
                "fragments": [],
                "text": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2003303"
                        ],
                        "name": "M. Gherrity",
                        "slug": "M.-Gherrity",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gherrity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gherrity"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 144
                            }
                        ],
                        "text": "\u2026??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williamsand Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doyaand Yoshizawa (1989), and Sato (1990a; 1990b).5.1 The AlgorithmFor each k 2 U , i 2 U , j 2 U [ I, and t0 t t1, we de nepkij(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 65
                            }
                        ],
                        "text": "Such information is also present in RTRL, albeit implicitly, and Gherrity (1989) has speci callyaddressed this issue by incorporating into his continuous-time version of RTRL an exponentialdecay on the contributions from past times."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28051649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006c42929dcd480490fdb367fd7478b2956dbc99",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm for recurrent neural networks is derived. This algorithm allows a network to learn specified trajectories in state space in response to various input sequences. The network dynamics are described by a system of coupled differential equations that specify the continuous change of the unit activities and weights over time. The algorithm is nonlocal, in that a change in the connection weight between two units may depend on the values for some of the weights between different units. However, the operation of a learned network (fixed weights) is local. If the network units are specified to behave like electronic amplifiers, then an analog implementation of the learned network is straightforward. An example demonstrates the use of the algorithm in a completely connected network of four units. The network creates a limit cycle attractor in order to perform the specified task.<<ETX>>"
            },
            "slug": "A-learning-algorithm-for-analog,-fully-recurrent-Gherrity",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for analog, fully recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A learning algorithm for recurrent neural networks is derived that allows a network to learn specified trajectories in state space in response to various input sequences."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48462607"
                        ],
                        "name": "L. Shastri",
                        "slug": "L.-Shastri",
                        "structuredName": {
                            "firstName": "Lokendra",
                            "lastName": "Shastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 25
                            }
                        ],
                        "text": "A number of researchers (Watrous & Shastri, 1986; Elman, 1988; Cleeremans, Servan-Schreiber,& McClelland, 1989, [chapter ??, this volume]) have performed experimental studies of learningalgorithms based on this approximate gradient computation algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 25
                            }
                        ],
                        "text": "|||||||||||||||||||||||| Watrous and Shastri (1986) have derived a generalization of BPTT to this more general case, and it is straightforward to extend the RTRL approach as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 24
                            }
                        ],
                        "text": "||||||||||||||||||||||||Watrous and Shastri (1986) have derived a generalization of BPTT to this more general case,and it is straightforward to extend the RTRL approach as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12357500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1e40283ecd4633c36c70fbc8dbb14e9a4afb37f",
            "isKey": true,
            "numCitedBy": 81,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectral/temporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. \n \nA simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated 98% of 25 other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items."
            },
            "slug": "Learning-Phonetic-Features-Using-Connectionist-Watrous-Shastri",
            "title": {
                "fragments": [],
                "text": "Learning Phonetic Features Using Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method for learning phonetic features from speech data using connectionist networks is described and a supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 914,
                                "start": 0
                            }
                        ],
                        "text": "Pineda (1989, [chapter ??, this volume]) has pointed out some other potential problems with teacher forcing. One of these is that it may create trajectories which are not attractors but repellers. One potential way around this and other di culties with teacher forcing is to consider a slight generalization in which xk(t) is set equal to yk(t) + ek(t) for k 2 U , where 2 [0; 1] is a constant. Teacher forcing uses = 1 while = 0 represents its absence. But other values of represent a mix of the two strategies. For this generalization, the correct gradient computation involves attenuating the virtual error backpropagated from later times by the factor 1 in BPTT or multiplying plij(t) by 1 before propagating the activity gradient forward in RTRL. A related strategy is to use teacher forcing intermittently rather than on every time step when target values are available. This has been tested by Tsung (1990) and found useful for dealing with the somewhat di erent but related problem of training network trajectories that vary extremely slowly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 46
                            }
                        ],
                        "text": "The recurrent backpropagation (RBP) algorithm (Almeida, 1987; Pineda, 1987) for training settling networks having constant input consists of applying the following steps: 1) the network is allowed to settle (with the time at which settling has completed regarded as t1); 2) the BPTT computation given by equations (28), (29), and (30) is performed for as long as needed until the i values converge; 3) all the @J total(t0; t1)=@wij values are computed using equation (27); and 4) weights are changed accordingly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 62
                            }
                        ],
                        "text": "The recurrent backpropagation (RBP) algorithm (Almeida, 1987; Pineda, 1987) for trainingsettling networks having constant input consists of applying the following steps: 1) the networkis allowed to settle (with the time at which settling has completed regarded as t1); 2) the BPTTcomputation given\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1087,
                                "start": 0
                            }
                        ],
                        "text": "Pineda (1989, [chapter ??, this volume]) has pointed out some other potential problems with teacher forcing. One of these is that it may create trajectories which are not attractors but repellers. One potential way around this and other di culties with teacher forcing is to consider a slight generalization in which xk(t) is set equal to yk(t) + ek(t) for k 2 U , where 2 [0; 1] is a constant. Teacher forcing uses = 1 while = 0 represents its absence. But other values of represent a mix of the two strategies. For this generalization, the correct gradient computation involves attenuating the virtual error backpropagated from later times by the factor 1 in BPTT or multiplying plij(t) by 1 before propagating the activity gradient forward in RTRL. A related strategy is to use teacher forcing intermittently rather than on every time step when target values are available. This has been tested by Tsung (1990) and found useful for dealing with the somewhat di erent but related problem of training network trajectories that vary extremely slowly. Finally, we note that Rohwer (1990) has expanded on this idea of teacher forcing to develop an interesting new epochwise learning algorithm based on computation of the gradient of performance with respect to unit activities rather than network weights."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation to recurrent neural networks, Physical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102393714"
                        ],
                        "name": "L. Mcbride",
                        "slug": "L.-Mcbride",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Mcbride",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mcbride"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091875864"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Narendra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121410355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b6957da5a80f33c03e980c9d11d37a527dc118a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of self-optimization using system models to compute error-criterion gradients in a parameter space is extended to time-varying systems. When the parameters are permitted to vary only slowly, the gradient computer is similar to that used for stationary systems of fixed configuration. When the parameters vary more rapidly, it is found that only the gradient with respect to the plant input function is meaningful. This influence function is obtained as the output from a model which can be defined whether or not a state-variable representation for the plant is known; a procedure for computing optimal control functions in a variety of linear and nonlinear systems is thus obtained."
            },
            "slug": "Optimization-of-time-varying-systems-Mcbride-Narendra",
            "title": {
                "fragments": [],
                "text": "Optimization of time-varying systems"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A method of self-optimization using system models to compute error-criterion gradients in a parameter space is extended to time-varying systems and a procedure for computing optimal control functions in a variety of linear and nonlinear systems is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3286081"
                        ],
                        "name": "S. Lockery",
                        "slug": "S.-Lockery",
                        "structuredName": {
                            "firstName": "Shawn",
                            "lastName": "Lockery",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lockery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840458369"
                        ],
                        "name": "Yan Fang",
                        "slug": "Yan-Fang",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62615343,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e273e23196d0e235a34257b40015b19af6dbb7fa",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons. These \"hidden\" units coordinate several different behaviors. We used physiological and anatomical constraints to construct a model of the local bending reflex. Dynamical networks were trained on experimentally derived input-output patterns using recurrent back-propagation. Units in the model were modified to include electrical synapses and multiple synaptic time constants. The properties of the hidden units that emerged in the simulations matched those in the leech. The model and data support distributed rather than localist representations in the local bending reflex. These results also explain counterintuitive aspects of the local bending circuitry."
            },
            "slug": "Neural-Network-Analysis-of-Distributed-of-Dynamical-Lockery-Fang",
            "title": {
                "fragments": [],
                "text": "Neural Network Analysis of Distributed Representations of Dynamical Sensory-Motor Transformations in the Leech"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A model of the local bending reflex was constructed using physiological and anatomical constraints and units in the model were modified to include electrical synapses and multiple synaptic time constants that matched those in the leech."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1989"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 267
                            }
                        ],
                        "text": "\u2026described here, both subgrouped RTRL andtruncated BPTT were tested for their ability to train fully recurrent networks to emulate the nitestate machine part of a Turing machine for balancing parentheses, a task that had previously beenshown to be learnable by RTRL (Williams & Zipser, 1989b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60666828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424710825d726e10b016204ed2bc979e2a342d10",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the thi..."
            },
            "slug": "Experimental-Analysis-of-the-Real-time-Recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Experimental Analysis of the Real-time Recurrent Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A series of simulation experiments are used to investigate the power and properties of the real-time recurrent learning algorithm, a gradient-following learning algorithm for completely recurrent networks running in continually sampled time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Werbos (1974; 1988), in addressing this same problem,uses the standard partial derivative notation to refer to explicit dependencies only, introducing the term orderedderivative, denoted in a di erent fashion, for a partial derivative which takes into account all in uences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 53
                            }
                        ],
                        "text": "Various forms of this algorithm have been derived by Werbos (1974), Rumelhart, Hinton, and Williams 9"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "Variousforms of this algorithm have been derived by Werbos (1974), Rumelhart, Hinton, and Williams9\n(1986), and Robinson and Fallside (1987), and continuous-time versions have been derived byPearlmutter (1989) and by Sato (1990a; 1990b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689675"
                        ],
                        "name": "J. Meditch",
                        "slug": "J.-Meditch",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Meditch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Meditch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 51664181,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "ec0ed6c4fba767af67e7e156faffe7ddc990a264",
            "isKey": false,
            "numCitedBy": 1241,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applied-optimal-control-Meditch",
            "title": {
                "fragments": [],
                "text": "Applied optimal control"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50278650"
                        ],
                        "name": "S. Piche",
                        "slug": "S.-Piche",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Piche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Piche"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "In work very similar in spirit to that we have presented here, Piche (1994) has shown howvarious forms of backpropagation through time and forward gradient computation may be de-rived in a uni ed manner from a standard Euler-Lagrange optimal-control-theoretic formulation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10138712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b71dcfdfcf23658d57b32ce5af6cfdc0425c982",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of steepest descent algorithms have been developed for adapting discrete-time dynamical systems, including the backpropagation through time and recursive backpropagation algorithms. In this paper, a tutorial on the use of these algorithms for adapting neural network controllers and filters is presented. In order to effectively compare and contrast the algorithms, a unified framework for the algorithms is developed. This framework is based upon a standard representation of a discrete-time dynamical system. Using this framework, the computational and storage requirements of the algorithms are derived. These requirements are used to select the appropriate algorithm for training a neural network controller or filter. Finally, to illustrate the usefulness of the techniques presented in this paper, a neural network control example and a neural network filtering example are presented."
            },
            "slug": "Steepest-descent-algorithms-for-neural-network-and-Piche",
            "title": {
                "fragments": [],
                "text": "Steepest descent algorithms for neural network controllers and filters"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A tutorial on the use of these algorithms for adapting neural network controllers and filters is presented and a unified framework for the algorithms is developed based upon a standard representation of a discrete-time dynamical system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39671808"
                        ],
                        "name": "Fu-Sheng Tsung",
                        "slug": "Fu-Sheng-Tsung",
                        "structuredName": {
                            "firstName": "Fu-Sheng",
                            "lastName": "Tsung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fu-Sheng Tsung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734225"
                        ],
                        "name": "A. Selverston",
                        "slug": "A.-Selverston",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Selverston",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Selverston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46607341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03ac166407838478d33a4603f6f27e047a513d64",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors focus on limit cycle experiments with neural nets, showing that it is possible for standard sigmoidal unit networks to learn stable, collective oscillations involving tens of units. The authors also model a biological network oscillator, showing that recurrent networks can help gain useful insights into the biological system. The R. Williams and D. Zipser (1989) learning algorithm was used with the teacher-forcing technique during the learning phase"
            },
            "slug": "Some-experiments-on-learning-stable-network-Tsung-Cottrell",
            "title": {
                "fragments": [],
                "text": "Some experiments on learning stable network oscillations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "It is shown that it is possible for standard sigmoidal unit networks to learn stable, collective oscillations involving tens of units and a biological network oscillator is model, showing that recurrent networks can help gain useful insights into the biological system."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116584040"
                        ],
                        "name": "Anthony V. W. Smith",
                        "slug": "Anthony-V.-W.-Smith",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Smith",
                            "middleNames": [
                                "V.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony V. W. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 274
                            }
                        ],
                        "text": "\u2026out various language processing or motor control tasksas a means of understanding the information processing strategies involved (Elman, 1988; Jordan,1986; Mozer, 1989, [chapter ??, this volume]; Cleeremans, Servan-Screiber, and McClelland, 1989,[chapter ??, this volume]; Smith & Zipser, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207107675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent connections in neural networks potentially allow information about events occurring in the past to be preserved and used in current computations. How effectively this potential is realized depends on the power of the learning algorithm used. As an example of a task requiring recurrency, Servan-Schreiber, Cleeremans, and McClelland1 have applied a simple recurrent learning algorithm to the task of recognizing finite-state grammars of increasing difficulty. These nets showed considerable power and were able to learn fairly complex grammars by emulating the state machines that produced them. However, there was a limit to the difficulty of the grammars that could be learned. We have applied a more powerful recurrent learning procedure, called real-time recurrent learning2,6 (RTRL), to some of the same problems studied by Servan-Schreiber, Cleeremans, and McClelland. The RTRL algorithm solved more difficult forms of the task than the simple recurrent networks. The internal representations developed by RTRL networks revealed that they learn a rich set of internal states that represent more about the past than is required by the underlying grammar. The dynamics of the networks are determined by the state structure and are not chaotic."
            },
            "slug": "Learning-Sequential-Structure-with-the-Real-Time-Smith-Zipser",
            "title": {
                "fragments": [],
                "text": "Learning Sequential Structure with the Real-Time Recurrent Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more powerful recurrent learning procedure, called real-time recurrent learning2,6 (RTRL), is applied to some of the same problems studied by Servan-Schreiber, Cleeremans, and McClelland and revealed that the internal representations developed by RTRL networks revealed that they learn a rich set of internal states that represent more about the past than is required by the underlying grammar."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921528"
                        ],
                        "name": "S. Yoshizawa",
                        "slug": "S.-Yoshizawa",
                        "structuredName": {
                            "firstName": "Shuji",
                            "lastName": "Yoshizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yoshizawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "\u2026strategy that has appeared implicitly or explicitly in the work of a number ofinvestigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27248882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd0da2f1d2b95e5b62221a00ff132219d0c853b7",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-neural-oscillator-using-continuous-time-Doya-Yoshizawa",
            "title": {
                "fragments": [],
                "text": "Adaptive neural oscillator using continuous-time back-propagation learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 320,
                                "start": 180
                            }
                        ],
                        "text": "9 Teacher Forcing An interesting strategy that has appeared implicitly or explicitly in the work of a number of investigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989; Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams & Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by the teacher signal dk(t) in subsequent computation of the behavior of the network, whenever such a target value exists."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026a number ofinvestigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by theteacher\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27867182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5146d7902132bcb0b2e6fe5f607358768fc47323",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-and-architecture-for-neural-computation-Pineda",
            "title": {
                "fragments": [],
                "text": "Dynamics and architecture for neural computation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27092071"
                        ],
                        "name": "B. Baird",
                        "slug": "B.-Baird",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Baird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 55
                            }
                        ],
                        "text": "Also, another body of techniques has been developed by Baird(1989) for synthesizing networks having prescribed dynamical properties."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14557759,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78f5003f19c3e724c0d35bcf0d0287977e315265",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Analytic methods of bifurcation theory are used to design algorithms for determining synaptic weights in recurrent analog neural network architectures. Nonnumerical formulas, as well as numerical learning algorithms using hidden units for the storage of static and periodic attractors, are introduced. These algorithms allow programming of the network vector field regardless of the patterns to be stored. Stability of patterns and cycles, basin geometry, and rates of convergence may be controlled. For a network of n nodes, n static of n/2 periodic attractors can be stored by a projection algorithm. For orthogonal patterns, this learning operation reduces to a kind of periodic outer product rule that allows local, additive, and commutative incremental learning. Standing- or traveling-wave cycles may be stored to mimic the kind of oscillating spatial patterns that appear in the neural activity of the olfactory bulb and prepyriform cortex during inspiration, and suffice, in the bulb, to predict the pattern recognition behavior of rabbits in classical conditioning experiments. These attractors arise, during simulated inspiration, through a multiple Hopf bifurcation, which can act as a critical decision point for their selection by very small input patterns.<<ETX>>"
            },
            "slug": "A-bifurcation-theory-approach-to-vector-field-for-Baird",
            "title": {
                "fragments": [],
                "text": "A bifurcation theory approach to vector field programming for periodic attractors"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Analytic methods of bifurcation theory are used to design algorithms for determining synaptic weights in recurrent analog neural network architectures and nonnumerical formulas, as well as numerical learning algorithms using hidden units for the storage of static and periodic attractors, are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323963"
                        ],
                        "name": "S. Alexander",
                        "slug": "S.-Alexander",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Alexander",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Alexander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 166
                            }
                        ],
                        "text": "For example, it appears in the adaptive signal processing literatureas an \\equation error\" technique for synthesizing linear lters having an in nite impulse response(Widrow & Stearns, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12374910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d78b005150b873a1b72423cdc045267e03daa7",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Signal-Processing-Alexander",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Werbos (1974; 1988), in addressing this same problem,uses the standard partial derivative notation to refer to explicit dependencies only, introducing the term orderedderivative, denoted in a di erent fashion, for a partial derivative which takes into account all in uences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 120
                            }
                        ],
                        "text": "While limited forms of time-varying behaviors can be handled by using feedforwardnetworks and tapped delay lines (e.g., Waibel et al., 1987), recurrent networks o er a muchricher set of possibilities for representing the necessary internal state."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 122
                            }
                        ],
                        "text": "Another example is sequence production, as studied by Jordan1\n(1986), in which the input is a constant pattern and the corresponding desired output is a time-varying sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2786,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 186
                            }
                        ],
                        "text": "32\nmethods to develop networks which carry out various language processing or motor control tasksas a means of understanding the information processing strategies involved (Elman, 1988; Jordan,1986; Mozer, 1989, [chapter ??, this volume]; Cleeremans, Servan-Screiber, and McClelland, 1989,[chapter ??, this volume]; Smith & Zipser, 1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 21
                            }
                        ],
                        "text": "Here we consider algorithms for training recurrent networks to perform temporal supervisedlearning tasks, in which the speci cation of desired behavior is in the form of speci c examplesof input and desired output trajectories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 157
                            }
                        ],
                        "text": "\u2026out various language processing or motor control tasksas a means of understanding the information processing strategies involved (Elman, 1988; Jordan,1986; Mozer, 1989, [chapter ??, this volume]; Cleeremans, Servan-Screiber, and McClelland, 1989,[chapter ??, this volume]; Smith & Zipser, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18036435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fb4d10f6d2ee3133135958aefd50bf22dcced9d",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Time is at the heart of many pattern recognition tasks (e.g., speech recognition). However, connectionist learning algorithms to date are not well-suited for dealing with time-varying input patterns. This chapter introduces a specialized connectionist architecture and corresponding specialization of the back-propagation learning algorithm that operates efficiently, both in computational time and space requirements, on temporal sequences. The key feature of the architecture is a layer of selfconnected hidden units that integrate their current value with the new input at each time step to construct a static representation of the temporal input sequence. This architecture avoids two deficiencies found in the back-propagation unfolding-intime procedure (Rumelhart, Hinton, & Williams, 1986) for handing sequence recognition tasks: first, it reduces the difficulty of temporal credit assignment by focusing the back-propagated error signal; second, it eliminates the need for a buffer to hold the input sequence and/or intermediate activity levels. The latter property is due to the fact that during the forward (activation) phase, incremental activity traces can be locally computed that hold all information necessary for back propagation in time. It is argued that this architecture should scale better than conventional recurrent architectures with respect to sequence length. The architecture has been used to implement a temporal version of Rumelhart and McClelland's (1986) verb past-tense model. The hidden units learn to behave something like Rumelhart and McClelland's \"Wickelphones,\" a rich and flexible representation of temporal information."
            },
            "slug": "A-Focused-Backpropagation-Algorithm-for-Temporal-Mozer",
            "title": {
                "fragments": [],
                "text": "A Focused Backpropagation Algorithm for Temporal Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A specialized connectionist architecture and corre sponding specialization of the backpropagation learnin g algori thm th at opera tes efficiently on temporal sequences is introduced and should scale better than conventional recurrent architectures wit h respect to sequenc e length."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 12
                            }
                        ],
                        "text": "(1986), and Robinson and Fallside (1987), and continuous-time versions have been derived by Pearlmutter (1989) and by Sato (1990a; 1990b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14595475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0a945cf025236264d3f8cf20c7851c1cedbb217",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Error backpropagation in feedforward neural network models is a popular learning algorithm that has its roots in nonlinear estimation and optimization. It is being used routinely to calculate error gradients in nonlinear systems with hundreds of thousands of parameters. However, the classical architecture for backpropagation has severe restrictions. The extension of backpropagation to networks with recurrent connections will be reviewed. It is now possible to efficiently compute the error gradients for networks that have temporal dynamics, which opens applications to a host of problems in systems identification and control."
            },
            "slug": "Recurrent-Backpropagation-and-the-Dynamical-to-Pineda",
            "title": {
                "fragments": [],
                "text": "Recurrent Backpropagation and the Dynamical Approach to Adaptive Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is now possible to efficiently compute the error gradients for networks that have temporal dynamics, which opens applications to a host of problems in systems identification and control."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34915378"
                        ],
                        "name": "R. Rohwer",
                        "slug": "R.-Rohwer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rohwer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rohwer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "Asdescribed earlier in the discussion of the teacher forcing technique, Rohwer (1990) has proposedan epochwise approach based on computation of the error gradient with respect to unit activitiesrather than network weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "Finally, we note that Rohwer (1990) has expanded on this idea of teacher forcing to develop aninteresting new epochwise learning algorithm based on computation of the gradient of performancewith respect to unit activities rather than network weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 736183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af7802a50a8c294ebfd539ad72158475e5ecd9f2",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past."
            },
            "slug": "The-\"Moving-Targets\"-Training-Algorithm-Rohwer",
            "title": {
                "fragments": [],
                "text": "The \"Moving Targets\" Training Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A simple method for training the dynamical behavior of a neural network using a gradient-based method and the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9326933"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "Cun",
                            "middleNames": [
                                "le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 13
                            }
                        ],
                        "text": "For example, leCun (1988) haspointed to the work of Bryson and Ho (1969) in optimal control theory as containing a descriptionof what can now be recognized as error backpropagation when applied to multilayer networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16775098,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d1f4d800a3a3bd0bf10839f9869f533e0d41c23",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theoretical-Framework-for-Back-Propagation-Cun",
            "title": {
                "fragments": [],
                "text": "A Theoretical Framework for Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537431"
                        ],
                        "name": "Axel Cleeremans",
                        "slug": "Axel-Cleeremans",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Cleeremans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Cleeremans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403615640"
                        ],
                        "name": "D. Servan-Schreiber",
                        "slug": "D.-Servan-Schreiber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Servan-Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Servan-Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7741931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd46c1b5948abe04e565a8bae6454da63a1b021e",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "slug": "Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber",
            "title": {
                "fragments": [],
                "text": "Finite State Automata and Simple Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A network architecture introduced by Elman (1988) for predicting successive elements of a sequence and shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6211302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a24508e65e599b5b20c33af96dbe7017d5caca37",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\\em automatically} learning or biasing the learner's hypothesis space is introduced. It works by first learning an appropriate {\\em internal representation} for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment. \nAn internal representation must be learnt by sampling from {\\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used). \nIt is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "slug": "Learning-internal-representations-Baxter",
            "title": {
                "fragments": [],
                "text": "Learning internal representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is proved that the number of examples required to ensure good generalisation from a representation learner obeys and that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026faster than RTRL.11For these studies the variant in which past weight values are stored in the history bu er was used.31\nIn another set of studies (Williams & Peng, 1990), BPTT(16;8) was found to succeed as oftenas BPTT(9) on this task, while running twice as fast.12 Note that BPTT(16;8) is thus\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bc0449360d7016f684eafae5b5d2feded32041",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time."
            },
            "slug": "An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng",
            "title": {
                "fragments": [],
                "text": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described, intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 72
                            }
                        ],
                        "text": "This algorithm, rst proposed in (Williams,1989), and later described by Schmidhuber (1992), is interesting both because it helps shed lighton the relationship between BPTT and RTRL and because it can yield exact error gradientinformation for a continually running network more e ciently than any\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11761172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b9a181801f32bf62c4237c4265ba036a79f9dc",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units. I describe a method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "slug": "A-Fixed-Size-Storage-O(n3)-Time-Complexity-Learning-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122191359"
                        ],
                        "name": "M. Pedersen",
                        "slug": "M.-Pedersen",
                        "structuredName": {
                            "firstName": "M.W.",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "\u2026studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by theteacher signal dk(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4999850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b22ba3742650d27652a038d063b2745d67d11da3",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Training recurrent networks is generally believed to be a difficult task. Excessive training times and lack of convergence to an acceptable solution are frequently reported. In this paper we seek to explain the reason for this from a numerical point of view and show how to avoid problems when training. In particular we investigate ill-conditioning, the need for and effect of regularization and illustrate the superiority of second-order methods for training."
            },
            "slug": "Training-recurrent-networks-Pedersen",
            "title": {
                "fragments": [],
                "text": "Training recurrent networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In particular, ill-conditioning, the need for and effect of regularization and the superiority of second-order methods for training are investigated and illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 34
                            }
                        ],
                        "text": "This point has been emphasized in (Williams, 1990) and similar arguments have been made by Mozer (1989; [chapter ??, this volume])."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 35
                            }
                        ],
                        "text": "This point has been emphasized in (Williams, 1990)and similar arguments have been made by Mozer (1989; [chapter ??, this volume])."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17962678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25d9530f5a1ae0630d451d754407fc9ab7ceb4b2",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-state-representation-and-estimation-using-Williams",
            "title": {
                "fragments": [],
                "text": "Adaptive state representation and estimation using recurrent connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 267
                            }
                        ],
                        "text": "\u2026described here, both subgrouped RTRL andtruncated BPTT were tested for their ability to train fully recurrent networks to emulate the nitestate machine part of a Turing machine for balancing parentheses, a task that had previously beenshown to be learnable by RTRL (Williams & Zipser, 1989b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 143
                            }
                        ],
                        "text": "\u2026to data on the temporal patternsthey generate (Arnold & Robinson, 1989; Lockery, Fang, & Sejnowski, 1990; Tsung, Cottrell, &Selverston, 1990; Anastasio, 1991) and a number of studies have been undertaken to apply these12Careful analysis of the computational requirements of BPTT(9) and of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network models of velocity storage in the horizontal vestibuloocular reeex"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "This algorithmhas been independently derived in various forms by Robinson and Fallside (1987), Kuhn (1987),Bachrach (1988, [chapter ??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williamsand Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doyaand\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A rst look at phonetic discrimination using a connectionist network with recurrent links (SCIMP Working Paper No"
            },
            "venue": {
                "fragments": [],
                "text": "Communications Research Division"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2026advantage of using such dynamics where k 1 is that certain classes of task may be more readily learned by such systems, as hasbeen observed by Tsung (1990).4 The particular advantage possessed by such systems is that thegradient computation used in the learning algorithms to be described\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 24
                            }
                        ],
                        "text": "This has been tested by Tsung (1990) and found useful for dealing withthe somewhat di erent but related problem of training network trajectories that vary extremelyslowly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning in recurrent nite diierence networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1990 Connectionist Models Summer School"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "\u2026studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual output yk(t) of a unit by theteacher signal dk(t)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training recurrent networks. In L"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026faster than RTRL.11For these studies the variant in which past weight values are stored in the history bu er was used.31\nIn another set of studies (Williams & Peng, 1990), BPTT(16;8) was found to succeed as oftenas BPTT(9) on this task, while running twice as fast.12 Note that BPTT(16;8) is thus\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An e cient gradient-based algorithm for on-line training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 112
                            }
                        ],
                        "text": "Variousforms of this algorithm have been derived by Werbos (1974), Rumelhart, Hinton, and Williams9\n(1986), and Robinson and Fallside (1987), and continuous-time versions have been derived byPearlmutter (1989) and by Sato (1990a; 1990b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 65
                            }
                        ],
                        "text": "This algorithmhas been independently derived in various forms by Robinson and Fallside (1987), Kuhn (1987),Bachrach (1988, [chapter ??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williamsand Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doyaand\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 72
                            }
                        ],
                        "text": "This algorithm, rst proposed in (Williams,1989), and later described by Schmidhuber (1992), is interesting both because it helps shed lighton the relationship between BPTT and RTRL and because it can yield exact error gradientinformation for a continually running network more e ciently than any\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "This algorithm, rst proposed in (Williams, 1989), and later described by Schmidhuber (1992), is interesting both because it helps shed light on the relationship between BPTT and RTRL and because it can yield exact error gradient information for a continually running network more e ciently than any other method we know."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A xed size storage O(n3) time complexity learning algorithm for fully"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 97
                            }
                        ],
                        "text": "This may be viewed as a generalization of the single self-recurrent unit architecture studied by Bachrach (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 109
                            }
                        ],
                        "text": "This algorithm has been independently derived in various forms by Robinson and Fallside (1987), Kuhn (1987), Bachrach (1988, [chapter ??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williams and Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doya and Yoshizawa (1989), and Sato (1990a; 1990b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "This may be viewed as a generalization of the single selfrecurrent unit architecture studied by Bachrach (1988). One of the algorithms he investigated coincides with that described here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to represent state"
            },
            "venue": {
                "fragments": [],
                "text": "Unpublished master's thesis. University of"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning neural-network model of the oculomotor integrator"
            },
            "venue": {
                "fragments": [],
                "text": "Society of Neuroscience Abstracts"
            },
            "year": 1049
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 97
                            }
                        ],
                        "text": "This may be viewed as a generalization of the single self-recurrent unit architecture studied by Bachrach (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to represent state. Unpublished master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Learning to represent state. Unpublished master's thesis"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 152
                            }
                        ],
                        "text": "Some of these techniques have already been used successfully to t models of biological neural subsystems to data on the temporal patterns they generate (Arnold & Robinson, 1989; Lockery, Fang, & Sejnowski, 1990; Tsung, Cottrell, & Selverston, 1990; Anastasio, 1991) and a number of studies have been undertaken to apply these 12Careful analysis of the computational requirements of BPTT(9) and of BPTT(16;8), taking into account the xed overhead of running the network in the forward direction that must be borne by any algorithm, would suggest that one should expect about a factor of 4 speedup when using BPTT(16;8)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 143
                            }
                        ],
                        "text": "\u2026to data on the temporal patternsthey generate (Arnold & Robinson, 1989; Lockery, Fang, & Sejnowski, 1990; Tsung, Cottrell, &Selverston, 1990; Anastasio, 1991) and a number of studies have been undertaken to apply these12Careful analysis of the computational requirements of BPTT(9) and of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network models of velocity storage in the horizontal vestibulo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 147
                            }
                        ],
                        "text": "\u2026or explicitly in the work of a number ofinvestigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identi cation and control of dynamic systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 147
                            }
                        ],
                        "text": "\u2026of these techniques have already beenused successfully to t models of biological neural subsystems to data on the temporal patternsthey generate (Arnold & Robinson, 1989; Lockery, Fang, & Sejnowski, 1990; Tsung, Cottrell, &Selverston, 1990; Anastasio, 1991) and a number of studies have been\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A learning neural-network model of the oculomotor"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "This algorithmhas been independently derived in various forms by Robinson and Fallside (1987), Kuhn (1987),Bachrach (1988, [chapter ??, this volume]), Mozer (1989, [chapter ??, this volume]), and Williamsand Zipser (1989a), and continuous-time versions have been proposed by Gherrity (1989), Doyaand\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A rst look at phonetic discrimination using a connectionist network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 147
                            }
                        ],
                        "text": "\u2026or explicitly in the work of a number ofinvestigators studying supervised learning tasks for recurrent nets (Doya & Yoshizawa, 1989;Jordan, 1986; Narendra & Parthasarathy, 1990; Pineda, 1988; Rohwer & Renals, 1989; Williams& Zipser, 1989a; 1989b) is to replace, during training, the actual\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identiication and control of dynamic systems using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "Asdescribed earlier in the discussion of the teacher forcing technique, Rohwer (1990) has proposedan epochwise approach based on computation of the error gradient with respect to unit activitiesrather than network weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 22
                            }
                        ],
                        "text": "Finally, we note that Rohwer (1990) has expanded on this idea of teacher forcing to develop aninteresting new epochwise learning algorithm based on computation of the gradient of performancewith respect to unit activities rather than network weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The \\moving targets\" training algorithm Portugal, 15-17 Feb"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the EURASIP Workshop on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 120
                            }
                        ],
                        "text": "While limited forms of time-varying behaviors can be handled by using feedforwardnetworks and tapped delay lines (e.g., Waibel et al., 1987), recurrent networks o er a muchricher set of possibilities for representing the necessary internal state."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finite-state automata and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network analysis of distributed rep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2026advantage of using such dynamics where k 1 is that certain classes of task may be more readily learned by such systems, as hasbeen observed by Tsung (1990).4 The particular advantage possessed by such systems is that thegradient computation used in the learning algorithms to be described\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 24
                            }
                        ],
                        "text": "This has been tested by Tsung (1990) and found useful for dealing withthe somewhat di erent but related problem of training network trajectories that vary extremelyslowly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning in recurrent nite di erence networks"
            },
            "venue": {
                "fragments": [],
                "text": "Touretzky, D. S., Elman,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation to recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Physical Review Letters"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding structure in time (CRL Technical Report 8801)"
            },
            "venue": {
                "fragments": [],
                "text": "Finding structure in time (CRL Technical Report 8801)"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 30,
            "methodology": 23,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 57,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser/10dae7fca6b65b61d155a622f0c6ca2bc3922251?sort=total-citations"
}