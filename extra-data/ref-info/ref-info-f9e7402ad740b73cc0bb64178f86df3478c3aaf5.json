{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2178614"
                        ],
                        "name": "N. Ashish",
                        "slug": "N.-Ashish",
                        "structuredName": {
                            "firstName": "Naveen",
                            "lastName": "Ashish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ashish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8200914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15b3cd323910de0551c680e8dae27d7413494a46",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "To simplify the task of obtaining information from the vast number of information sources that are available on the World Wide Web (WWW), the authors are building information mediators for extracting and integrating data from multiple Web sources. In a mediator based approach, wrappers are built around individual information sources to translate between the mediator query language and the individual sources. They present an approach for semi-automatically generating wrappers for structured Internet sources. The key idea is to exploit formatting information in Web pages to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that facilitates querying of a source and possibly integrating it with other sources. They demonstrate the ease with which they are able to build wrappers for a number of Web sources using their implemented wrapper generation toolkit."
            },
            "slug": "Semi-automatic-wrapper-generation-for-Internet-Ashish-Knoblock",
            "title": {
                "fragments": [],
                "text": "Semi-automatic wrapper generation for Internet information sources"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The key idea is to exploit formatting information in Web pages to hypothesize the underlying structure of a page and generate a wrapper that facilitates querying of a source and possibly integrating it with other sources."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of CoopIS 97: 2nd IFCIS Conference on Cooperative Information Systems"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10566644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "130cbc5e907cccbd0fcd4f9138bc9886dc3217d7",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a wealth of information to be mined from narrative text on the World Wide Web. Unfortunately, standard natural language processing (NLP) extraction techniques expect full, grammatical sentences, and perform poorly on the choppy sentence fragments that are often found on web pages. \n \nThis paper1 introduces Webfoot, a preprocessor that parses web pages into logically coherent segments based on page layout cues. Output from Webfoot is then passed on to CRYSTAL, an NLP system that learns text extraction rules from example. Webfoot and CRYSTAL transform the text into a formal representation that is equivalent to relational database entries. This is a necessary first step for knowledge discovery and other automated analysis of free text."
            },
            "slug": "Learning-to-Extract-Text-Based-Information-from-the-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Text-Based Information from the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Webfoot, a preprocessor that parses web pages into logically coherent segments based on page layout cues, is introduced and passed on to CRYSTAL, an NLP system that learns text extraction rules from example."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729294"
                        ],
                        "name": "Boris Chidlovskii",
                        "slug": "Boris-Chidlovskii",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Chidlovskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Chidlovskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067217778"
                        ],
                        "name": "Uwe M. Borghoff",
                        "slug": "Uwe-M.-Borghoff",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Borghoff",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe M. Borghoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144474509"
                        ],
                        "name": "P. Chevalier",
                        "slug": "P.-Chevalier",
                        "structuredName": {
                            "firstName": "Pierre-Yves",
                            "lastName": "Chevalier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Chevalier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13386925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bcf687a7fe2908ccea673ecfd316090e5ff164",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Access to on-line information via the Web is exploding. Index and retrieval engines already start to integrate a huge variety of heterogeneous repositories. However, the heterogeneity issue remains, both in terms of the search formats and the formats of the result pages. \n \nIn this paper we focus on html-based search and result presentations. We discuss our experience in the design, the development and the maintenance of wrappers (in the context of the Knowledge Broker project). We outline different ways to write wrappers, illustrate some of the lessons learned, and conclude by describing a semi-automatic approach for an efficient wrapping of Web-based information repositories. Throughout the paper, we give illustrating examples for hands-on readers."
            },
            "slug": "Towards-Sophisticated-Wrapping-of-Web-based-Chidlovskii-Borghoff",
            "title": {
                "fragments": [],
                "text": "Towards Sophisticated Wrapping of Web-based information Repositories"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper outlines different ways to write wrappers, illustrates some of the lessons learned, and concludes by describing a semi-automatic approach for an efficient wrapping of Web-based information repositories."
            },
            "venue": {
                "fragments": [],
                "text": "RIAO"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3017541"
                        ],
                        "name": "M. Perkowitz",
                        "slug": "M.-Perkowitz",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Perkowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perkowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5903740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dabaac2a302bf2c99cd21341802fbe885398408",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the problem of automatically learning declarative models of information sources available on the Internet. We report on ILA, a domain-independent program that learns the meaning of external information by explaining it in terms of internal categories. In our experiments, ILA starts with knowledge of local faculty members, and is able to learn models of the Internet service whois and of the personnel directories available at Berkeley, Brown, Caltech, Cornell, Rice, Rutgers, and UCI, averaging fewer than 40 queries per information source. ILA\u2019s hypothesis language is first-order conjunctions, and its bias is compactly encoded as a determination. We analyze ILA\u2019s sample complexity within the Valiant model, and using a probabilistic model specifically tailored to ILA."
            },
            "slug": "Category-Translation:-Learning-to-Understand-on-the-Perkowitz-Etzioni",
            "title": {
                "fragments": [],
                "text": "Category Translation: Learning to Understand Information on the Internet"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "IA is a domain-independent program that learns the meaning of external information by explaining it in terms of internal categories, and its bias is compactly encoded as a determination."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46987823"
                        ],
                        "name": "Chung T. Kwok",
                        "slug": "Chung-T.-Kwok",
                        "structuredName": {
                            "firstName": "Chung",
                            "lastName": "Kwok",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chung T. Kwok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8883690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b748701a24cb1ae63466891565b2158f51db10f2",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe Occam, a query planning algorithm that determines the best way to integrate data from di erent sources. As input, Occam takes a library of site descriptions and a user query. As output, Occam automatically generates one or more plans that encode alternative ways to gather the requested information. Occam has several important features: (1) it integrates both legacy systems and full relational databases with an e cient, domain-independent, query-planning algorithm, (2) it reasons about the capabilities of di erent information sources, (3) it handles partial goal satisfaction i.e., gathers as much data as possible when it can't gather exactly all that the user requested, (4) it is both sound and complete, (5) it is e cient. We present empirical results demonstrating Occam's performance on a variety of information gathering tasks."
            },
            "slug": "Planning-to-Gather-Information-Kwok-Weld",
            "title": {
                "fragments": [],
                "text": "Planning to Gather Information"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Occam, a query planning algorithm that determines the best way to integrate data from data from di erent sources, is described and empirical results demonstrating Occam's performance on a variety of information gathering tasks are presented."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145944286"
                        ],
                        "name": "D. Rus",
                        "slug": "D.-Rus",
                        "structuredName": {
                            "firstName": "Daniela",
                            "lastName": "Rus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144922029"
                        ],
                        "name": "D. Subramanian",
                        "slug": "D.-Subramanian",
                        "structuredName": {
                            "firstName": "Devika",
                            "lastName": "Subramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Subramanian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13156464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9c6887986159246f98648b5a9b432b41f5e17a3",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a customizable architecture for software agents that capture and access information in large, heterogeneous, distributed electronic repositories. The key idea is to exploit underlying structure at various levels of granularity to build high-level indices with task-specific interpretations. Information agents construct such indices and are configured as a network of reusable modules called structure detectors and segmenters. We illustrate our architecture with the design and implementation of smart information filters in two contexts: retrieving stock market data from Internet newsgroups and retrieving technical reports from Internet FTP sites."
            },
            "slug": "Customizing-information-capture-and-access-Rus-Subramanian",
            "title": {
                "fragments": [],
                "text": "Customizing information capture and access"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This article presents a customizable architecture for software agents that capture and access information in large, heterogeneous, distributed electronic repositories to exploit underlying structure at various levels of granularity to build high-level indices with task-specific interpretations."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157101"
                        ],
                        "name": "L. Shklar",
                        "slug": "L.-Shklar",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Shklar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shklar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692403"
                        ],
                        "name": "S. Thatte",
                        "slug": "S.-Thatte",
                        "structuredName": {
                            "firstName": "Satish",
                            "lastName": "Thatte",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thatte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49766501"
                        ],
                        "name": "H. Marcus",
                        "slug": "H.-Marcus",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Marcus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144463965"
                        ],
                        "name": "A. Sheth",
                        "slug": "A.-Sheth",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Sheth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sheth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17720959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3455504d5fbdc720a260c2b9427285e78ef6b983",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The \u201cInfoHarness\u201d information integration platform, tools, and services being developed at Bellcore are aimed atproviding integrated and rapid access to huge amounts of heterogeneous information independent of the type, rep-resentation, and location of information. InfoHarness provides advanced search and browsing capabilities withoutimposing the burden of restructuring, reformatting or relocating information on information suppliers or creators.This is achieved through object-oriented encapsulation of information and the associated meta-information (e.g.,type, location, access rights, owner, creation date, etc.). The meta-information extraction methods ensure rapidand largely automatic creation of information repositories. A gateway that supports access to InfoHarness reposi-tories from Mosaic and other HyperText Transfer Protocol (HTTP) compliant browsers is currently available. AnHTTP compliant InfoHarness server is under construction."
            },
            "slug": "The-\"-InfoHarness\"-Information-Integration-Platform-Shklar-Thatte",
            "title": {
                "fragments": [],
                "text": "The \" InfoHarness\" Information Integration Platform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The \u201cInfoHarness\u201d information integration platform, tools, and services being developed at Bellcore are aimed at providing integrated and rapid access to huge amounts of heterogeneous information independent of the type, rep-resentation, and location of information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69484854"
                        ],
                        "name": "A. Rajaraman",
                        "slug": "A.-Rajaraman",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rajaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rajaraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2193834"
                        ],
                        "name": "J. Ordille",
                        "slug": "J.-Ordille",
                        "structuredName": {
                            "firstName": "Joann",
                            "lastName": "Ordille",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ordille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14649913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e93b228ffa2c27e96ba7d0158cc6ba5b1cdcb5d",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the architecture and query-answering algorithms used in the Information Manifold, an implemented information gathering system that provides uniform access to structured information sources on the World-Wide Web. Our architecture provides an expressive language for describing information sources, which makes it easy to add new sources and to model the fine-grained distinctions between their contents. The query-answering algorithm guarantees that the descriptions of the sources are exploited to access only sources that are relevant to a given query. Accessing only relevant sources is crucial to scale up such a system to large numbers of sources. In addition, our algorithm can exploit run-time information to further prune information sources and to reduce the cost of query planning."
            },
            "slug": "Query-Answering-Algorithms-for-Information-Agents-Halevy-Rajaraman",
            "title": {
                "fragments": [],
                "text": "Query-Answering Algorithms for Information Agents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The architecture provides an expressive language for describing information sources, which makes it easy to add new sources and to model the fine-grained distinctions between their contents, and the query-answering algorithm guarantees that the descriptions of the sources are exploited to access only sources that are relevant to a given query."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16747313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f80e289b2f52b558d388aba7df2d1689513a928b",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The eld of information extraction (IE) is concerned with applying natural language processing (NLP) and information retrieval (IR) techniques to the automatic extraction of essential details from text documents. We are exploring the use of machine learning methods for IE. While the most promising methods we have developed perform well for problems deened over a collection of electronic seminar announcements, they are imprecise in their identiication of the boundaries of relevant text fragments (elds). Here, we entertain the idea of using grammatical inference (GI) methods to learn the appropriate form of a eld. We describe one method for translating raw text into an abstract alphabet suitable for GI, and show that, by combining one IE learning method with the resulting inferred grammars, large improvements in precision can be realized for some elds."
            },
            "slug": "Using-grammatical-inference-to-improve-precision-in-Freitag",
            "title": {
                "fragments": [],
                "text": "Using grammatical inference to improve precision in information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "One method for translating raw text into an abstract alphabet suitable for GI is described, and it is shown that, by combining one IE learning method with the resulting inferred grammars, large improvements in precision can be realized for some elds."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15645051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "810420af4fa5f3ed932724aea5f7b66d3bd592b2",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The World Wide Web (WWW) is filled with \"resource directories\"--i.e., documents that collect together links to all known documents on a specific topic. Keeping resource directories up-to-date is difficult because of the rapid growth in online documents. We propose using machine learning methods to address this problem. In particular, we propose to treat a resource directory as a list of positive examples of an unknown concept, and then use machine learning methods to construct from these examples a definition of the unknown concept. If the learned definition is in the appropriate form, it can be translated into a query, or series of queries, for a WWW search engine. This query can be used at a later date to detect any new instances of the concept. We present experimental results with two implemented systems, and two learning methods. One system is interactive, and is implemented as an augmented WWW browser; the other is a batch system, which can collect and label documents without any human intervention. The learning methods are the RIPPER rule learning system, and a rule-learning version of a new online weight allocation algorithm called the sleeping experts prediction algorithm. The experiments are performed on real data obtained from the WWW."
            },
            "slug": "Learning-to-Query-the-Web-Cohen-Singer",
            "title": {
                "fragments": [],
                "text": "Learning to Query the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes to treat a resource directory as a list of positive examples of an unknown concept, and then use machine learning methods to construct from these examples a definition of the unknown concept if the learned definition is in the appropriate form."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105119697"
                        ],
                        "name": "Jonathan Shakes",
                        "slug": "Jonathan-Shakes",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Shakes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Shakes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47085377"
                        ],
                        "name": "Marc Langheinrich",
                        "slug": "Marc-Langheinrich",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Langheinrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Langheinrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40413824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32c4fd87ccccf21880ff70f46875788942a66311",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-Reference-Sifting:-A-Case-Study-in-the-Shakes-Langheinrich",
            "title": {
                "fragments": [],
                "text": "Dynamic Reference Sifting: A Case Study in the Homepage Domain"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "The inductive algorithm requires an oracle to label examples; we solve this labeling problem [ Etzioni, 1996 ] by composing oracles from heuristic knowledge, and we demonstrate that our system degrades gracefully with imperfect heuristics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 2
                            }
                        ],
                        "text": "[\nEtzioni, 1996\n]\nO. Etzioni."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3353849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f9b342307b1149615a49486ff7d2abf783314ff",
            "isKey": false,
            "numCitedBy": 706,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Skeptics believe the Web is too unstructured for Web mining to succeed. Indeed, data mining has been applied traditionally to databases, yet much of the information on the Web lies buried in documents designed for human consumption such as home pages or product catalogs. Furthermore, much of the information on the Web is presented in natural-language text with no machine-readable semantics; HTML annotations structure the display of Web pages, but provide little insight into their content. Some have advocated transforming the Web into a massive layered database to facilitate data mining [12], but the Web is too dynamic and chaotic to be tamed in this manner. Others have attempted to hand code site-specific \u201cwrappers\u201d that facilitate the extraction of information from individual Web resources (e.g., [8]). Hand coding is convenient but cannot keep up with the explosive growth of the Web. As an alternative, this article argues for the structured Web hypothesis: Information on the Web is sufficiently structured to facilitate effective Web mining. Examples of Web structure include linguistic and typographic conventions, HTML annotations (e.g., <title>), classes of semi-structured documents (e.g., product catalogs), Web indices and directories, and much more. To support the structured Web hypothesis, this article will survey preliminary Web mining successes and suggest directions for future work. Web mining may be organized into the following subtasks:"
            },
            "slug": "The-World-Wide-Web:-quagmire-or-gold-mine-Etzioni",
            "title": {
                "fragments": [],
                "text": "The World-Wide Web: quagmire or gold mine?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Information on the Web is sufficiently structured to facilitate effective Web mining, according to the structured Web hypothesis, and preliminary Web mining successes are surveyed and directions for future work are suggested."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236427"
                        ],
                        "name": "E. Selberg",
                        "slug": "E.-Selberg",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Selberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Selberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2056547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05c92c1593707c60d96275645ee15e6adfc79a71",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard Web search services, though useful, are far from ideal. There are over a dozen di erent search services currently in existence, each with a unique interface and a database covering a di erent portion of the Web. As a result, users are forced to repeatedly try and retry their queries across di erent services. Furthermore, the services return many responses that are irrelevant, outdated, or unavailable, forcing the user to manually sift through the responses searching for useful information. This paper presents the MetaCrawler, a elded Web service that represents the next level up in the information \\food chain.\" The MetaCrawler provides a single, central interface for Web document searching. Upon receiving a query, the MetaCrawler posts the query to multiple search services in parallel, collates the returned references, and loads those references to verify their existence and to ensure that they contain relevant information. The MetaCrawler is su ciently lightweight to reside on a user's machine, which facilitates customization, privacy, sophisticated ltering of references, and more. The MetaCrawler also serves as a tool for comparison of diverse search services. Using the MetaCrawler's data, we present a \\Consumer Reports\" evaluation of six Web search services: Galaxy[5], InfoSeek[1], Lycos[15], Open Text[20], WebCrawler[22], and Yahoo[9]. In addition, we also report on the most commonly submitted queries to the MetaCrawler."
            },
            "slug": "Multi-Service-Search-and-Comparison-Using-the-Selberg-Etzioni",
            "title": {
                "fragments": [],
                "text": "Multi-Service Search and Comparison Using the MetaCrawler"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The MetaCrawler provides a single, central interface for Web document searching that facilitates customization, privacy, sophisticated ltering of references, and more and serves as a tool for comparison of diverse search services."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145565197"
                        ],
                        "name": "S. Tejada",
                        "slug": "S.-Tejada",
                        "structuredName": {
                            "firstName": "Sheila",
                            "lastName": "Tejada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tejada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10937189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37af3ea61235845e53957a558eda547f829abbed",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the growing number of information sources available through the internet there are many cases in which information needed to solve a problem or answer a question is spread across several information sources. For example, when given two sources, one about comic books and the other about super heroes, you might want to ask the question {open_quotes}Is Spiderman a Marvel Super Hero?{close_quotes} This query accesses both sources; therefore, it is necessary to have information about the relationships of the data within each source and between sources to properly access and integrate the data retrieved. The SIMS information broker captures this type of information in the form of a model. All the information sources map into the model providing the user a single interface to multiple sources."
            },
            "slug": "Learning-Models-for-Multi-Source-Integration-Tejada-Knoblock",
            "title": {
                "fragments": [],
                "text": "Learning Models for Multi-Source Integration"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Because of the growing number of information sources available through the internet there are many cases in which information needed to solve a problem or answer a question is spread across several information sources, it is necessary to have information about the relationships of the data within each source and between sources to properly access and integrate the data retrieved."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2257053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdc08721414c972ab451f8ef3ef39d63c741b324",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary."
            },
            "slug": "Automatically-Constructing-a-Dictionary-for-Tasks-Riloff",
            "title": {
                "fragments": [],
                "text": "Automatically Constructing a Dictionary for Information Extraction Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using AutoSlog, a system that automatically builds a domain-specific dictionary of concepts for extracting information from text, a dictionary for the domain of terrorist event descriptions was constructed in only 5 person-hours and the overall scores were virtually indistinguishable."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 2
                            }
                        ],
                        "text": "[\nDoorenbos et al., 1997\n]\nR. Doorenbos, O. Etzioni, &\nD. Weld."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 42
                            }
                        ],
                        "text": "A second related application is shopbot [ Doorenbos et al., 1997 ] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3531043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b3b14828a71c3bd4e56fda87a8c89a72d358c4e",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The WorldWideWeb is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following questions: (1) To what extent can an agent understand information published at Web sites? (2) Is the agent's understanding sufficient to provide genuinely useful assistance to users? (3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites? (4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using ShopBot, a fully-implemented, domainindependent comparison-shopping agent. Given the home pages of several online stores, ShopBot autonomously learns how to shop at those vendors. After learning, it is able to speedily visit over a dozen software and CD vendors, extract product information, and summarize the results for the user. Preliminary studies show that ShopBot enables users to both find superior prices and substantially reduce Web shopping time. Remarkably, ShopBot achieves this performance without sophisticated natural language processing, and requires only minimal knowledge about different product domains. Instead, ShopBot relies on a combination of heuristic search, pattern matching, and inductive learning techniques. PERMISSION TO COPY WITHOUT FEE ALL OR OR PART OF THIS MATERIAL IS GRANTED PROVIDED THAT THE COPIES ARE NOT MADE OR DISTRIBUTED FOR DIRECT COMMERCIAL ADVANTAGE, THE ACM copyRIGHT NOTICE AND THE TITLE OF THE PUBLICATION AND ITS DATE APPEAR, AND NOTICE IS GIVEN THAT COPYING IS BY PERMISSION OF ACM. To COPY OTHERWISE, OR TO REPUBLISH, REQUIRES A FEE AND/OR SPECIFIC PERMISSION. AGENTS '97 CONFERENCE PROCEEDINGS, COPYRIGHT 1997 ACM."
            },
            "slug": "A-scalable-comparison-shopping-agent-for-the-Web-Doorenbos-Etzioni",
            "title": {
                "fragments": [],
                "text": "A scalable comparison-shopping agent for the World-Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "ShopBot, a fully-implemented, domainindependent comparison-shopping agent that relies on a combination of heuristic search, pattern matching, and inductive learning techniques, enables users to both find superior prices and substantially reduce Web shopping time."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2685671"
                        ],
                        "name": "L. F. Rau",
                        "slug": "L.-F.-Rau",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Rau",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. F. Rau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54084367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c6028ca44a5dac9078a9bed9d8b55612a0051e3",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A detailed description is given of an implemented algorithm that extracts company names automatically from financial news. Extracting company names from text is one problem; recognizing subsequent references to a company is another. The author addresses both problems in an implemented, well-tested module that operates as a detachable process from a set of natural language processing tools. She implements a good algorithm by combining heuristics, exception lists and extensive corpus analysis. The algorithm generates the most likely variations that those names may go by, for use in subsequent retrieval. Tested on over one million words of naturally occurring financial news, the system has extracted thousands of company names with over 95% accuracy (precision) compared to a human, and succeeded in extracting 25% more companies than were indexed by a human.<<ETX>>"
            },
            "slug": "Extracting-company-names-from-text-Rau",
            "title": {
                "fragments": [],
                "text": "Extracting company names from text"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A detailed description is given of an implemented algorithm that extracts company names automatically from financial news by combining heuristics, exception lists and extensive corpus analysis."
            },
            "venue": {
                "fragments": [],
                "text": "[1991] Proceedings. The Seventh IEEE Conference on Artificial Intelligence Application"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11571384"
                        ],
                        "name": "M. Roth",
                        "slug": "M.-Roth",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39055806"
                        ],
                        "name": "P. Schwarz",
                        "slug": "P.-Schwarz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Schwarz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Schwarz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9200728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f7ea8be40125cc90f7813d88df6079d5ab42a89",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Garlic is a middleware system that provides an integrated view of a variety of legacy data sources, without changing how or where data is stored. In this paper, we describe our architecture for wrappers, key components of Garlic that encapsulate data sources and mediate between them and the middleware. Garlic wrappers model legacy data as objects, participate in query planning, and provide standard interfaces for method invocation and query execution. To date, we have built wrappers for 10 data sources. Our experience shows that Garlic wrappers can be written quickly and that our architecture is flexible enough to accommodate data sources with a variety of data models and a broad range of traditional and non-traditional query processing capabilities."
            },
            "slug": "Don't-Scrap-It,-Wrap-It!-A-Wrapper-Architecture-for-Roth-Schwarz",
            "title": {
                "fragments": [],
                "text": "Don't Scrap It, Wrap It! A Wrapper Architecture for Legacy Data Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The architecture for wrappers, key components of Garlic that encapsulate data sources and mediate between them and the middleware are described, which shows that Garlic wrappers can be written quickly and that the architecture is flexible enough to accommodate data sources with a variety of data models and a broad range of traditional and non-traditional query processing capabilities."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1901883"
                        ],
                        "name": "Shona Douglas",
                        "slug": "Shona-Douglas",
                        "structuredName": {
                            "firstName": "Shona",
                            "lastName": "Douglas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shona Douglas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145501780"
                        ],
                        "name": "Matthew F. Hurst",
                        "slug": "Matthew-F.-Hurst",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hurst",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew F. Hurst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15236531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2052918dc135e2fbc4679438a218978eaa3f8e14",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe some of the interactions between layout and language we have been dealing with in recent applied NLP projects. We present two complementary views of lists and tables, intended to bridge the gap between considering them as a type of running text (which linguistics knows how to deal with) and as a multi-dimensional relation represented in two dimensions, which may have many reading-paths (which linguistics doesn't know how to deal with). Stated or inferred linguistic and world knowledge in the text surrounding tables and lists provides a context for the interpretation of a set of tuples extracted from tables or lists together with heuristics about how multi-dimensional information is projected on to two dimensions."
            },
            "slug": "Layout-and-Language:-lists-and-tables-in-technical-Douglas-Hurst",
            "title": {
                "fragments": [],
                "text": "Layout and Language: lists and tables in technical documents"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two complementary views of lists and tables are presented, intended to bridge the gap between considering them as a type of running text and as a multi-dimensional relation represented in two dimensions, which may have many reading-paths."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706276"
                        ],
                        "name": "S. Luke",
                        "slug": "S.-Luke",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Luke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144665700"
                        ],
                        "name": "L. Spector",
                        "slug": "L.-Spector",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Spector",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Spector"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850356"
                        ],
                        "name": "D. Rager",
                        "slug": "D.-Rager",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rager",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701341"
                        ],
                        "name": "J. Hendler",
                        "slug": "J.-Hendler",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hendler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hendler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7695193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98e857491b7774ea6c7e4d176d1ef8f636e087d8",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes SHOE, a set of Simple HTML Ontology Extensions which allow World-Wide Web authors to annotate their pages with semantic knowledge such as \u201cI am a graduate student\u201d or \u201cThis person is my graduate advisor\u201d. These annotations are expressed in terms of ontological knowledge which can be generated by using or extending standard ontologies available on the Web. This makes it possible to ask Web agent queries such as \u201cFind me all graduate students in Maryland who are working on a project funded by DoD initiative 123-4567\u201d, instead of simplistic keyword searches enabled by current search engines. We have also developed a web-crawling agent, Expos\u00b4 e, which interns SHOE knowledge from web documents, making these kinds queries a reality."
            },
            "slug": "Ontology-based-Web-agents-Luke-Spector",
            "title": {
                "fragments": [],
                "text": "Ontology-based Web agents"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "SHOE, a set of Simple HTML Ontology Extensions which allow World-Wide Web authors to annotate their pages with semantic knowledge such as \u201cI am a graduate student\u201d or \u201cThis person is my graduate advisor\u201d, is described."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786049"
                        ],
                        "name": "Y. Papakonstantinou",
                        "slug": "Y.-Papakonstantinou",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Papakonstantinou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Papakonstantinou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737896"
                        ],
                        "name": "J. Widom",
                        "slug": "J.-Widom",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Widom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Widom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b878dfbf8a3deb493cffba506883a9a95cce8216",
            "isKey": false,
            "numCitedBy": 996,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.<<ETX>>"
            },
            "slug": "Object-exchange-across-heterogeneous-information-Papakonstantinou-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Object exchange across heterogeneous information sources"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An object-based information exchange model and a corresponding query language are defined that are well suited for integration of diverse information sources and used to integrate heterogeneous bibliographic information sources."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eleventh International Conference on Data Engineering"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120279604"
                        ],
                        "name": "E. Monge",
                        "slug": "E.-Monge",
                        "structuredName": {
                            "firstName": "Ernesto",
                            "lastName": "Monge",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Monge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080911709"
                        ],
                        "name": "Charles P. ElkanDepartment",
                        "slug": "Charles-P.-ElkanDepartment",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "ElkanDepartment",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles P. ElkanDepartment"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14881094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "975fce5257f2da5fa7f5557967bd7c37ed4ef3a4",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "To combine information from heterogeneous sources, equivalent data in the multiple sources must be identi-ed. This task is the eld matching problem. Specii-cally, the task is to determine whether or not two syntactic values are alternative designations of the same semantic entity. For example the addresses Dept. of do designate the same department. This paper describes three eld matching algorithms, and evaluates their performance on real-world datasets. One proposed method is the well-known Smith-Waterman algorithm for comparing DNA and protein sequences. Several applications of eld matching in knowledge discovery are described brieey, including webfind, which is a new software tool that discovers scientiic papers published on the worldwide web. webfind uses external information sources to guide its search for authors and papers. Like many other worldwide web tools, webfind needs to solve the eld matching problem in order to navigate between information sources."
            },
            "slug": "The-Eld-Matching-Problem:-Algorithms-and-Monge-ElkanDepartment",
            "title": {
                "fragments": [],
                "text": "The Eld Matching Problem: Algorithms and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Three eld matching algorithms are described, one of which is the well-known Smith-Waterman algorithm for comparing DNA and protein sequences, and their performance on real-world datasets is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717530"
                        ],
                        "name": "Y. Arens",
                        "slug": "Y.-Arens",
                        "structuredName": {
                            "firstName": "Yigal",
                            "lastName": "Arens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Arens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081090860"
                        ],
                        "name": "C. Knobloch",
                        "slug": "C.-Knobloch",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Knobloch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Knobloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295983"
                        ],
                        "name": "C. Chee",
                        "slug": "C.-Chee",
                        "structuredName": {
                            "firstName": "Chin",
                            "lastName": "Chee",
                            "middleNames": [
                                "Yi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34607455"
                        ],
                        "name": "Chun-Nan Hsu",
                        "slug": "Chun-Nan-Hsu",
                        "structuredName": {
                            "firstName": "Chun-Nan",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Nan Hsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 112
                            }
                        ],
                        "text": "Therefore, software systems using such resources (e.g., heterogeneous database systems [\nChawathe et al., 1994; Arens et al., 1996\n]\nor software\nagents\n[\nEtzioni & Weld, 1994; Kirk et al., 1995\n]\n) must\ntranslate query responses to relational form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 247
                            }
                        ],
                        "text": "As suggested at the outset, wrapper construction is motivated by the software engineering issues involved with deploying software systems that rely on external information resources; examples include [ Chawathe et al., 1994; Etzioni & Weld, 1994; Arens et al., 1996; Kirk et al., 1995 ] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "[\nArens et al., 1996\n]\nY. Arens, C. Knoblock, C. Chee, &\nC. Hsu."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59996964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1424a313382df0baa0ff9307268664bdef3f00c4",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : With the current explosion of data, retrieving and integrating information from various sources is a critical problem. This report describes work performed at USC/ISI, aimed at developing a general and extensible approach to this problem. The SIMS approach exploits a semantic model of a problem domain to integrate the information from various sources, i.e., databases and knowledge bases. The domain and the information sources are modeled. Queries submitted to SIMS are mapped into a set of queries to individual information sources. The set of queries is then further optimized, using knowledge, about the domain and the information sources. The data obtained is then returned to the user. SIMS utilizes techniques from the areas of knowledge representation, planning, and learning"
            },
            "slug": "SIMS:-Single-Interface-to-Multiple-Sources-Arens-Knobloch",
            "title": {
                "fragments": [],
                "text": "SIMS: Single Interface to Multiple Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The SIMS approach exploits a semantic model of a problem domain to integrate the information from various sources, i.e., databases and knowledge bases, and utilizes techniques from the areas of knowledge representation, planning, and learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094423"
                        ],
                        "name": "Amar Gupta",
                        "slug": "Amar-Gupta",
                        "structuredName": {
                            "firstName": "Amar",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amar Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62144236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97061091c6f1afcb020f8c6c899a7ee6a3591f7a",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Where you can find the integration of information systems bridging heterogeneous databases easily? Is it in the book store? On-line book store? are you sure? Keep in mind that you will find the book in this site. This book is very referred for you because it gives not only the experience but also lesson. The lessons are very valuable to serve for you, that's not about who are reading this integration of information systems bridging heterogeneous databases book. It is about this book that will give wellness for all people from many societies."
            },
            "slug": "Integration-of-Information-Systems:-Bridging-Gupta",
            "title": {
                "fragments": [],
                "text": "Integration of Information Systems: Bridging Heterogeneous Databases"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is very referred for you because it gives not only the experience but also lesson, it is about this book that will give wellness for all people from many societies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683380"
                        ],
                        "name": "Osmar R Zaiane",
                        "slug": "Osmar-R-Zaiane",
                        "structuredName": {
                            "firstName": "Osmar",
                            "lastName": "Zaiane",
                            "middleNames": [
                                "R"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Osmar R Zaiane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145325584"
                        ],
                        "name": "Jiawei Han",
                        "slug": "Jiawei-Han",
                        "structuredName": {
                            "firstName": "Jiawei",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiawei Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17826303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d96e02757cec37daeef6af05e067d1da5c8e9db0",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient and effective discovery of resource and knowledge from the Internet has become an imminent research issue, especially with the advent of the Information Super-Highway. A multiple layered database (MLDB) approach is proposed to handle the resource and knowledge discovery in global information base. A preliminary experiment shows the advantages of such an approach. Information retrieval, data mining, and data analysis techniques can be used to extract and transform information from a lower layer database to a higher one. Resources can be found by controlled search through different layers of the database, and knowledge discovery can be performed efficiently in such a layered database."
            },
            "slug": "Resource-and-Knowledge-Discovery-in-Global-Systems:-Zaiane-Han",
            "title": {
                "fragments": [],
                "text": "Resource and Knowledge Discovery in Global Information Systems: A Preliminary Design and Experiment"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A preliminary experiment shows the advantages of a multiple layered database approach, which can be used to extract and transform information from a lower layer database to a higher one."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060202"
                        ],
                        "name": "F. Crimmins",
                        "slug": "F.-Crimmins",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Crimmins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crimmins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10529293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e448016b37b03635f39967514d6bd3b03e005d3a",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A fully operational large scale digital library is likely to be based on a distributed architecture and because of this it is likely that a number of independent search engines may be used to index different overlapping portions of the entire contents of the library. In any case, different media, text, audio, image, etc., will be indexed for retrieval by different search engines so techniques which provide a coherent and unified search over a suite of underlying independent search engines are thus likely to be an important part of navigating in a digital library. In this paper we present an architecture and a system for searching the world's largest DL, the world wide web. What makes our system novel is that we use a suite of underlying web search engines to do the bulk of the work while our system orchestrates them in a parallel fashion to provide a higher level of information retrieval functionality. Thus it is our meta search engine and not the underlying direct search engines that provide the relevance feedback and query expansion options for the user. The paper presents the design and architecture of the system which has been implemented, describes an initial version which has been operational for almost a year, and outlines the operation of the advanced version."
            },
            "slug": "Relevance-Feedback-and-Query-Expansion-for-the-Web:-Smeaton-Crimmins",
            "title": {
                "fragments": [],
                "text": "Relevance Feedback and Query Expansion for Searching the Web: A Model for Searching a Digital Library"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents an architecture and a system for searching the world's largest DL, the world wide web, which uses a suite of underlying web search engines to do the bulk of the work while the system orchestrates them in a parallel fashion to provide a higher level of information retrieval functionality."
            },
            "venue": {
                "fragments": [],
                "text": "ECDL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681227"
                        ],
                        "name": "C. Collet",
                        "slug": "C.-Collet",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Collet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Collet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744170"
                        ],
                        "name": "M. Huhns",
                        "slug": "M.-Huhns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Huhns",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Huhns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2628069"
                        ],
                        "name": "Wei-Min Shen",
                        "slug": "Wei-Min-Shen",
                        "structuredName": {
                            "firstName": "Wei-Min",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Min Shen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 14082993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495168474b5f963810cb1f6da3db0b239af1d655",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for integrating separately developed information resources that overcomes incompatibilities in syntax and semantics and permits the resources to be accessed and modified coherently is described. The method provides logical connectivity among the information resources via a semantic service layer that automates the maintenance of data integrity and provides an approximation of global data integration across systems. This layer is a fundamental part of the Carnot architecture, which provides tools for interoperability across global enterprises.<<ETX>>"
            },
            "slug": "Resource-integration-using-a-large-knowledge-base-Collet-Huhns",
            "title": {
                "fragments": [],
                "text": "Resource integration using a large knowledge base in Carnot"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The method provides logical connectivity among the information resources via a semantic service layer that automates the maintenance of data integrity and provides an approximation of global data integration across systems."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60585486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca198cc81878fd036c7b97ee10441f1d09839f65",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "An enormous amount of knowledge is needed to infer the meaning of unrestricted natural language. The problem can be reduced to a manageable size by restricting attention to a specific {\\em domain}, which is a corpus of texts together with a predefined set of {\\em concepts} that are of interest to that domain. Two widely different domains are used to illustrate this domain-specific approach. One domain is a collection of Wall Street Journal articles in which the target concept is management succession events: identifying persons moving into corporate management positions or moving out. A second domain is a collection of hospital discharge summaries in which the target concepts are various classes of diagnosis or symptom. The goal of an information extraction system is to identify references to the concept of interest for a particular domain. A key knowledge source for this purpose is a set of text analysis rules based on the vocabulary, semantic classes, and writing style peculiar to the domain. This thesis presents CRYSTAL, an implemented system that automatically induces domain-specific text analysis rules from training examples. CRYSTAL learns rules that approach the performance of hand-coded rules, are robust in the face of noise and inadequate features, and require only a modest amount of training data. CRYSTAL belongs to a class of machine learning algorithms called covering algorithms, and presents a novel control strategy with time and space complexities that are independent of the number of features. CRYSTAL navigates efficiently through an extremely large space of possible rules. CRYSTAL also demonstrates that expressive rule representation is essential for high performance, robust text analysis rules. While simple rules are adequate to capture the most salient regularities in the training data, high performance can only be achieved when rules are expressive enough to reflect the subtlety and variability of unrestricted natural language."
            },
            "slug": "Learning-text-analysis-rules-for-domain-specific-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning text analysis rules for domain-specific natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis presents CRYSTAL, an implemented system that automatically induces domain-specific text analysis rules from training examples that approach the performance of hand-coded rules, are robust in the face of noise and inadequate features, and require only a modest amount of training data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9086294"
                        ],
                        "name": "S. Chawathe",
                        "slug": "S.-Chawathe",
                        "structuredName": {
                            "firstName": "Sudarshan",
                            "lastName": "Chawathe",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chawathe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144477659"
                        ],
                        "name": "J. Hammer",
                        "slug": "J.-Hammer",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144185412"
                        ],
                        "name": "K. Ireland",
                        "slug": "K.-Ireland",
                        "structuredName": {
                            "firstName": "Kelly",
                            "lastName": "Ireland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ireland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786049"
                        ],
                        "name": "Y. Papakonstantinou",
                        "slug": "Y.-Papakonstantinou",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Papakonstantinou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Papakonstantinou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737896"
                        ],
                        "name": "J. Widom",
                        "slug": "J.-Widom",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Widom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Widom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 89
                            }
                        ],
                        "text": "Therefore, software systems using such resources (e.g., heterogeneous database systems [\nChawathe et al., 1994; Arens et al., 1996\n]\nor software\nagents\n[\nEtzioni & Weld, 1994; Kirk et al., 1995\n]\n) must\ntranslate query responses to relational form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 202
                            }
                        ],
                        "text": "As suggested at the outset, wrapper construction is motivated by the software engineering issues involved with deploying software systems that rely on external information resources; examples include [ Chawathe et al., 1994; Etzioni & Weld, 1994; Arens et al., 1996; Kirk et al., 1995 ] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 2
                            }
                        ],
                        "text": "[\nChawathe et al., 1994\n]\nS. Chawathe, H. Garcia-Molina,\nJ. Hammer, K. Ireland, Y. Papakonstantinou, J. Ullman, & J. Widom."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2113876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14348170a14b4e2edca01521184cb2cd60b83200",
            "isKey": false,
            "numCitedBy": 1264,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center. 1 Overview A common problem facing many organizations today is that of multiple, disparate information sources and repositories, including databases, object stores, knowledge bases, file systems, digital libraries, information retrieval systems, and electronic mail systems. Decision makers often need information from multiple sources, but are unable to get and fuse the required information in a timely fashion due to the diffculties of accessing the different systems, and due to the fact that the information obtained can be inconsistent and contradictory. Research sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Material Command, USAF, under Grant Number F33615-93-1-1339. The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the offcial policies or endorsements, either express or implied, of Wright Laboratory or the US Government. This work was also supported by the Reid and Polly Anderson Faculty Scholar Fund, the Center for Integrated Systems at Stanford University, and by Equipment Grants from Digital Equipment Corporation and IBM Corporation. The goal of the TSIMMIS 1 project is to provide tools for accessing, in an integrated fashion, multiple informati"
            },
            "slug": "The-TSIMMIS-Project:-Integration-of-Heterogeneous-Chawathe-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "The TSIMMIS Project: Integration of Heterogeneous Information Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An overview of the Tsimmis Project is given, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites."
            },
            "venue": {
                "fragments": [],
                "text": "IPSJ"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076231991"
                        ],
                        "name": "T. Kirk",
                        "slug": "T.-Kirk",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50874323"
                        ],
                        "name": "Alon Y. Levy",
                        "slug": "Alon-Y.-Levy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Levy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alon Y. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714472"
                        ],
                        "name": "Y. Sagiv",
                        "slug": "Y.-Sagiv",
                        "structuredName": {
                            "firstName": "Yehoshua",
                            "lastName": "Sagiv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sagiv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145860176"
                        ],
                        "name": "D. Srivastava",
                        "slug": "D.-Srivastava",
                        "structuredName": {
                            "firstName": "Divesh",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Srivastava"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63440469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8450a2666891b0fa6be16d0fa320db25abe9bbb",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to completely exploit knowledge about local closed world information (Etzioni et al. 1994)."
            },
            "slug": "The-Information-Manifold-Kirk-Levy",
            "title": {
                "fragments": [],
                "text": "The Information Manifold"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The Information Manifold is described, a system for browsing and querying of multiple networked information sources that demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate information sources."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236427"
                        ],
                        "name": "E. Selberg",
                        "slug": "E.-Selberg",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Selberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Selberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14643096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77e2e742d53ef382df9f896abadf291646eed3cc",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper discusses the MetaCrawler Softbot parallel Web search service that has been available at the University of Washington since June 1995. It provides users with a single interface for querying popular general-purpose Web search services, such as Lycos and AltaVista, and has some sophisticated features that allow it to obtain results of much higher quality than simply regurgitating the output from each search service."
            },
            "slug": "The-MetaCrawler-architecture-for-resource-on-the-Selberg-Etzioni",
            "title": {
                "fragments": [],
                "text": "The MetaCrawler architecture for resource aggregation on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The paper discusses the MetaCrawler Softbot parallel Web search service that has been available at the University of Washington since June 1995 and has some sophisticated features that allow it to obtain results of much higher quality than simply regurgitating the output from each search service."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456394"
                        ],
                        "name": "D. Florescu",
                        "slug": "D.-Florescu",
                        "structuredName": {
                            "firstName": "Daniela",
                            "lastName": "Florescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Florescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733877"
                        ],
                        "name": "L. Raschid",
                        "slug": "L.-Raschid",
                        "structuredName": {
                            "firstName": "Louiqa",
                            "lastName": "Raschid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Raschid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144255847"
                        ],
                        "name": "P. Valduriez",
                        "slug": "P.-Valduriez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Valduriez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Valduriez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17464597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1608965184be3efeef0b4334ac92c56a79c5829e",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to have signiicant practical impact on future information systems, multi-database management systems (MDBMS) must be both exible and eecient. We consider a MDBMS with a common object-oriented model, based on the ODMG standard, and local databases that may be relational or object-oriented. In this context , query rewriting (for optimization) is made diicult by schematic discrepancy, and the need to model mapping information between the multidatabase and local schemas. We address the exibility issue by representing the mappings from a local schema to the multidatabase schema, as a set of heterogeneous object equivalences, in a declarative language. EEciency is obtained by exploiting these equivalences to rewrite multidatabase OQL queries into equivalent, simpliied queries on the local schemas."
            },
            "slug": "Using-Heterogeneous-Equivalences-for-Query-in-Florescu-Raschid",
            "title": {
                "fragments": [],
                "text": "Using Heterogeneous Equivalences for Query Rewriting in Multidatabase Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work addresses the exibility issue by representing the mappings from a local schema to the multidatabase schema, as a set of heterogeneous object equivalences, in a declarative language, and exploiting these equivalences to rewrite multid atabase OQL queries into equivalent, simpliied queries on the local schemas."
            },
            "venue": {
                "fragments": [],
                "text": "CoopIS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Tim Finin",
                        "slug": "Tim-Finin",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Finin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Finin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1910723"
                        ],
                        "name": "R. Fritzson",
                        "slug": "R.-Fritzson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Fritzson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fritzson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39552279"
                        ],
                        "name": "D. McKay",
                        "slug": "D.-McKay",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "McKay",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McKay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568707"
                        ],
                        "name": "R. McEntire",
                        "slug": "R.-McEntire",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "McEntire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEntire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 46
                            }
                        ],
                        "text": "While data interchange protocols (e.g. kqml [ Finin et al., 1994 ] ) have been proposed to address these issues, they require cooperation on the part of information providers, and such cooperation is rare."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "[\nFinin et al., 1994\n]\nT. Finin, R. Fritzson, D. McKay, &\nR. McEntire."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15636884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0da82b917832ecb3dffc4dd16b0e1e1dbdf153b",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language (KQML), a new language and protocol for exchanging information and knowledge. This work is part a larger effort, the ARPA Knowledge Sharing Effort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML can be used as a language for an application program to interact with an intelligent system or for two or more intelligent systems to share knowledge in support of cooperative problem solving. KQML focuses on an extensible set of performatives, which defines the permissible operations that agents may attempt on each other\u2019s knowledge and goal stores. The performatives comprise a substrate on which to develop higher-level models of inter-agent interaction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called communication facilitators which coordinate the interactions of other agents The ideas which underlie the evolving design of KQML are currently being explored through experimental prototype systems which are being used to support several testbeds in such areas as concurrent engineering, intelligent design and intelligent planning and scheduling."
            },
            "slug": "KQML-A-Language-and-Protocol-for-Knowledge-and-Finin-Fritzson",
            "title": {
                "fragments": [],
                "text": "KQML - A Language and Protocol for Knowledge and Information Exchange"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The design of and experimentation with the Knowledge Query and Manipulation Language (KQML) are described, a new language and protocol for exchanging information and knowledge aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3187096"
                        ],
                        "name": "Jerry R. Hobbs",
                        "slug": "Jerry-R.-Hobbs",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Hobbs",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry R. Hobbs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 639439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6a439ec029efaa370ac287f4ff9fe576ef0e925",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An information extraction system is a cascade of transducers or modules that at each step add structure and often lose information, hopefully irrelevant, by applying rules that are acquired manually and/or automatically."
            },
            "slug": "The-generic-information-extraction-system-Hobbs",
            "title": {
                "fragments": [],
                "text": "The generic information extraction system"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An information extraction system is a cascade of transducers or modules that at each step add structure and often lose information, hopefully irrelevant, by applying rules that are acquired manually and/or automatically."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2361723"
                        ],
                        "name": "Scott E. Decatur",
                        "slug": "Scott-E.-Decatur",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Decatur",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Decatur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144724945"
                        ],
                        "name": "R. Gennaro",
                        "slug": "R.-Gennaro",
                        "structuredName": {
                            "firstName": "Rosario",
                            "lastName": "Gennaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gennaro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 201081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81608cdee514d24536ed646fa3c9a3b71da7b36a",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate learnability in the PAC model when the data used for learning, attributes and labels, is either corrupted or incomplete. In order to prove our main results, we define a new complexity measure on statistical query (SQ) learning algorithms. The view of an SQ algorithm is the maximum over all queries in the algorithm, of the number of input bits on which the query depends. We show that a restricted view SQ algorithm for a class is a general sufficient condition for learnability in both the models of attribute noise and covered (or missing) attributes, We further show that since the algorithms in question are statistical, they can also simultaneously tolerate classification noise. Classes for which these results hold, and can therefore be learned with simultaneous attribute noise and classification noise, include k-DNF, k-term-DNF by DNF representations, conjunctions with few relevant variables, and over the uniform distribution, decision lists. These noise models are the first PAC models in which all training data, attributes and labels, may be corrupted by a random process. Previous researchers had shown that the class of k-DNF is learnable with attribute noise if the attribute noise rate is known exactly. We show that all of our attribute noise learnability results, either with or without classification noise, also hold when the exact noise rate is not *Author was supported by an NDSEG Fellowship and by NSF Grant CCR-92-00884. Author\u2019s email address: sed@das. hsrvard. edu \u2018Author was supported by NSF Grant 9121466 CCR and a graduate fellowship from the Consiglio Nazionale delle Ricerche, Italy. Author\u2019s email address: rosario@theory. lcs .mit . edu Permission to make digital/hard copies of all or part of this material withmt fee is grantc.d provided that the copies are not made or distributed for profit or commercial advantage, the ACM copyright(scrvcr notice. the title of the publication and its date appear, and notice is given known, provided that the learner instead has a polynomially good approximation of the noise rate. In addition, we show that the results also hold when there is not one single noise rate, but a distinct noise rate for each attribute. Our results for learning with random covering do not require the learner to be told even an approximation of the covering rate and in addition hold in the setting with distinct covering rates for each attribute. Finally, we give lower bounds on the number of examples required for learning in the presence of attribute noise or covering."
            },
            "slug": "On-learning-from-noisy-and-incomplete-examples-Decatur-Gennaro",
            "title": {
                "fragments": [],
                "text": "On learning from noisy and incomplete examples"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new complexity measure on statistical query (SQ) learning algorithms is defined and it is shown that a restricted view SQ algorithm for a class is a general sufficient condition for learnability in both the models of attribute noise and covered (or missing) attributes."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144881104"
                        ],
                        "name": "E. A. Green",
                        "slug": "E.-A.-Green",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Green",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. A. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751773"
                        ],
                        "name": "M. Krishnamoorthy",
                        "slug": "M.-Krishnamoorthy",
                        "structuredName": {
                            "firstName": "Mukkai",
                            "lastName": "Krishnamoorthy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Krishnamoorthy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 866386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecb5fdcdeff0d56ebf61b151ac8617ae3b30881e",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a strategy for extracting the underlying relational information from the images of printed tables. Visual clues that exist in the image are used for extracting first the physical, and then the logical structure of the table. Since these visual clues generally have a logical meaning, there must be some association made between the graphical attributes extracted and their function of reflecting the logic expressed by the table; this knowledge is coordinated in a model. This approach, therefore, can be adapted to all tables which have graphical attributes discernible to the image analysis being used."
            },
            "slug": "Model-based-analysis-of-printed-tables-Green-Krishnamoorthy",
            "title": {
                "fragments": [],
                "text": "Model-based analysis of printed tables"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This strategy for extracting the underlying relational information from the images of printed tables is developed and can be adapted to all tables which have graphical attributes discernible to the image analysis being used."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139418"
                        ],
                        "name": "Sibel Adali",
                        "slug": "Sibel-Adali",
                        "structuredName": {
                            "firstName": "Sibel",
                            "lastName": "Adali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sibel Adali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720972"
                        ],
                        "name": "K. Candan",
                        "slug": "K.-Candan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Candan",
                            "middleNames": [
                                "Sel\u00e7uk"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Candan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786049"
                        ],
                        "name": "Y. Papakonstantinou",
                        "slug": "Y.-Papakonstantinou",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Papakonstantinou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Papakonstantinou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728462"
                        ],
                        "name": "V. S. Subrahmanian",
                        "slug": "V.-S.-Subrahmanian",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Subrahmanian",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. S. Subrahmanian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 629579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3b35d849389be508572b6c2d798e9cd4d0b2d76",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source's performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a cost-based optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants mechanism, which shows how semantic information about data sources may be used to discover cached query results of interest."
            },
            "slug": "Query-caching-and-optimization-in-distributed-Adali-Candan",
            "title": {
                "fragments": [],
                "text": "Query caching and optimization in distributed mediator systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A cost-based optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 240
                            }
                        ],
                        "text": "Though our focus is on Internet resources, these learned delimiters need not be html tags, but can be arbitrary text.\nhlrt corresponds essentially to a class of nite-state automata, so wrapper induction is similar to FSA induction (e.g., [ Angluin, 1982 ] )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "References\n[\nAngluin, 1982\n]\nD. Angluin."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18300595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fa2c3cc94a8d9de9b30a5488e67a5007e4856e",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "A famdy of efficient algorithms for referring certain subclasses of the regular languages from fmtte posttwe samples is presented These subclasses are the k-reversible languages, for k = 0, 1, 2, . . . . For each k there is an algorithm for finding the smallest k-reversible language containing any fimte posluve sample. It ts shown how to use this algorithm to do correct identification m the ILmlt of the kreversible languages from posmve data A reversible language is one that Is k-reverstble for some k __ 0. An efficient algonthrn is presented for mfernng reversible languages from posmve and negative examples, and it is shown that it leads to correct identification m the hmlt of the class of reversible languages. Numerous examples are gtven to dlustrate the algorithms and their behawor"
            },
            "slug": "Inference-of-Reversible-Languages-Angluin",
            "title": {
                "fragments": [],
                "text": "Inference of Reversible Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An efficient algonthrn is presented for mfernng reversible languages from posmve and negative examples, and it is shown that it leads to correct identification m the hmlt of the class of reversible languages."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696082"
                        ],
                        "name": "C. Borgman",
                        "slug": "C.-Borgman",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Borgman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Borgman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2261571"
                        ],
                        "name": "Susan L. Siegfried",
                        "slug": "Susan-L.-Siegfried",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Siegfried",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan L. Siegfried"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45482843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe44accbd175cfceaba3430a0ccc5949bc23af6c",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The study reported in this article was commissioned by the Getty Art History Information Program (AHIP) as a background investigation of personal name-matching programs in fields other than art history, for purposes of comparing them and their approaches with AHIP\u2019s SynonameTM project. We review techniques employed in a variety of applications, including art history, bibliography, genealogy, commerce, and government, providing a framework of personal name characteristics, factors in selecting matching techniques, and types of applications. Personal names, as data elements in information systems, vary for a wide range of legitimate reasons, including cultural and historical traditions, translation and transliteration, reporting and recording variations, as well as typographical and phonetic errors. Some matching applications seek to link variants, while others seek to correct errors. The choice of matching techniques will vary in the amount of domain knowledge about the names that is incorporated, the sources of data, and the human and computing resources required. Personal name-matching techniques may be included in name authority work, information retrieval, or duplicate detection, with some applications matching on name only, and others combining personal names with other data elements in record linkage techniques. We discuss both phoneticand pattern-matching techniques, reviewing a range of implemented and proposed namematching techniques in the context of these factors."
            },
            "slug": "Getty's-Synoname-and-Its-Cousins:-A-Survey-of-of-Borgman-Siegfried",
            "title": {
                "fragments": [],
                "text": "Getty's Synoname and Its Cousins: A Survey of Applications of Personal Name-Matching Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Techniques employed in personal name-matching programs in fields other than art history are reviewed, for purposes of comparing them and their approaches with AHIP\u2019s SynonameTM project."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11873053,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "225331d1700a9544545cc7c54a63c1b485269ce7",
            "isKey": false,
            "numCitedBy": 2037,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Regular-Sets-from-Queries-and-Angluin",
            "title": {
                "fragments": [],
                "text": "Learning Regular Sets from Queries and Counterexamples"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35204413"
                        ],
                        "name": "Keith S. Decker",
                        "slug": "Keith-S.-Decker",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Decker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keith S. Decker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9076478"
                        ],
                        "name": "K. Sycara",
                        "slug": "K.-Sycara",
                        "structuredName": {
                            "firstName": "Katia",
                            "lastName": "Sycara",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sycara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144149089"
                        ],
                        "name": "Mike Williamson",
                        "slug": "Mike-Williamson",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Williamson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6088203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0e5476537650b5f71ddac104c616196f6f4fe60",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Like middle-men in physical commerce, middleagents support the flow of information in electronic commerce, assisting in locating and connecting the ultimate information provider with the ultimate information requester. Many different types of middleagents will be useful in realistic, large, distributed, open multi-agent problem solving systems. These include matchmakers or yellow page agents that process advertisements, blackboard agents that collect requests, and brokers that process both. The behaviors of each type of middle-agent have certain performance characteristics\u2014privacy, robustness, and adaptiveness qualities\u2014that are related to characteristics of the external environment and of the agents themselves. For example, while brokered systems are more vulnerable to certain failures, they are also able to cope more quickly with a rapidly fluctuating agent workforce and meet certain privacy considerations. This paper identifies a spectrum of middle-agents, characterizes the behavior of three different types, and reports on initial experiments that focus on evaluating performance tradeoffs between matchmaking and brokering middle-agents, according to criteria such as load balancing, robustness, dynamic preferences or capabilities, and privacy."
            },
            "slug": "Middle-Agents-for-the-Internet-Decker-Sycara",
            "title": {
                "fragments": [],
                "text": "Middle-Agents for the Internet"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A spectrum of middle-agents is identified, characterizes the behavior of three different types, and reports on initial experiments that focus on evaluating performance tradeoffs between matchmaking and brokering middle- agents, according to criteria such as load balancing, robustness, dynamic preferences or capabilities, and privacy."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717349"
                        ],
                        "name": "D. Knuth",
                        "slug": "D.-Knuth",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Knuth",
                            "middleNames": [
                                "Ervin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Knuth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118077243"
                        ],
                        "name": "James H. Morris",
                        "slug": "James-H.-Morris",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Morris",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691192"
                        ],
                        "name": "V. Pratt",
                        "slug": "V.-Pratt",
                        "structuredName": {
                            "firstName": "Vaughan",
                            "lastName": "Pratt",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pratt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11697579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5253fead88bfeaaa2930daccb7324a264cb681a9",
            "isKey": false,
            "numCitedBy": 2974,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm is presented which finds all occurrences of one given string within another, in running time proportional to the sum of the lengths of the strings. The constant of proportionality is low enough to make this algorithm of practical use, and the procedure can also be extended to deal with some more general pattern-matching problems. A theoretical application of the algorithm shows that the set of concatenations of even palindromes, i.e., the language $\\{\\alpha \\alpha ^R\\}^*$, can be recognized in linear time. Other algorithms which run even faster on the average are also considered."
            },
            "slug": "Fast-Pattern-Matching-in-Strings-Knuth-Morris",
            "title": {
                "fragments": [],
                "text": "Fast Pattern Matching in Strings"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "An algorithm is presented which finds all occurrences of one given string within another, in running time proportional to the sum of the lengths of the strings, showing that the set of concatenations of even palindromes, i.e., the language $\\{\\alpha \\alpha ^R\\}^*$, can be recognized in linear time."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Comput."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2447472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "050584e3b74d1d7f5396d379510ca5204f02bfc1",
            "isKey": false,
            "numCitedBy": 602,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Internet Softbot (software robot) is a fullyimplemented AI agent developed at the University of Washington (Etzioni, Lcsh, & Segal 1993). The softbot uses a UNIX shell and the World-Wide Web to interact with a wide range of internet resources. The softbot\u2019s effectors include ftp, telnet, mail, and numerous file manipulation commaslds. Its sensors include internet facilities such as archie, gopher, netfind, and many more. The softbot is designed to incorporate new facilities into its repertoirc as they become available. The softbot\u2019s \"added value\" is three-fold. First, it provides an integrated and expressive interface to the internet. Second, the softbot dynamically chooses which facilities to invoke, and in what sequence. For example, the softbot might use netfind to determine David McAllester\u2019s e-mail address. Since it knows that netfind requires a person\u2019s institution as input, the softbot would first search bibliographic databases for a technical report by McAllester which would reveal his institutkm, and then feed that information to netfind. Third, the softbot fluidly backtracks from one facility to another based on information collected at run time. As a result., the softbot\u2019s behavior changes in response to transient system conditions (e.g., the UUCP gateway is down). In this article, we focus on the ideas underlying the softbot-based interface."
            },
            "slug": "A-softbot-based-interface-to-the-Internet-Etzioni-Weld",
            "title": {
                "fragments": [],
                "text": "A softbot-based interface to the Internet"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The Internet Softbot (software robot) is a fullyimplemented AI agent developed at the University of Washington that uses a UNIX shell and the World-Wide Web to interact with a wide range of internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 2
                            }
                        ],
                        "text": ", [Valiant, 1984; Blumer et al., 1987]), in which sample complexity grows with the number of hypotheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "Thus clearly the stated bound is tighter than obtainable under simple PAC models (e.g., [\nValiant, 1984; Blumer et al., 1987\n]\n), in which sam-\nple complexity grows with the number of hypotheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 2
                            }
                        ],
                        "text": "[\nValiant, 1984\n]\nL. Valiant."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4190,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686063"
                        ],
                        "name": "R. Greiner",
                        "slug": "R.-Greiner",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Greiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Greiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13241050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7ed8c715fe1a5a041faff5ff6fd7dded68f1b1f",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new strategies for \"probably approximately correct\" (par) learning that use fewer training examples than previous approaches. The idea is to observe training examples one-at-a-time and decide \"on-line\" when to return a hypothesis, rather than collect a large fixed-size training sample. This yields sequential learning procedures that par-learn by observing a small random number of examples. We provide theoretical bounds on the expected training sample size of our procedure -- but establish its efficiency primarily by a scries of experiments which show sequential learning actually uses many times fewer training examples in practice. These results demonstrate that pac-learning can be far more efficiently achieved in practice than previously thought."
            },
            "slug": "Practical-PAC-Learning-Schuurmans-Greiner",
            "title": {
                "fragments": [],
                "text": "Practical PAC Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "New strategies for \"probably approximately correct\" (par) learning that use fewer training examples than previous approaches are presented, demonstrating that pac-learning can be far more efficiently achieved in practice than previously thought."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7923260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9899c9d7f3a5b5d9fc918675c0843754d160c986",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Permission to copy without fee all or part of this material ie granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notica is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 24th ANNUAL ACM STOC 5/92/VICTORiA, B. C., CANADA G 1992 ACM ()-89791.51 2-7/92/0004/03~j -..$1 .~() the only textbook in the field is Natarajan\u2019s [101]. Surveys by Laird [83] and Valiant [129] are valuable. Somewhat more peripheral are the European meetings on Analogical and Inductive Inference, AH, and the AI machine learning communit y\u2019s annual International Conference on Machine Learning. In addition, the general AI meetings, AAAI and IJCAI, currently have a large number of papers devoted to learning, as do the neural net meetings."
            },
            "slug": "Computational-learning-theory:-survey-and-selected-Angluin",
            "title": {
                "fragments": [],
                "text": "Computational learning theory: survey and selected bibliography"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The general AI meetings, AAAI and IJCAI, currently have a large number of papers devoted to learning, as do the neural net meetings, and the European meetings on Analogical and Inductive Inference and the AI machine learning communit y\u2019s annual International Conference on Machine Learning."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3012739"
                        ],
                        "name": "N. Lesh",
                        "slug": "N.-Lesh",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Lesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2229222"
                        ],
                        "name": "Richard B. Segal",
                        "slug": "Richard-B.-Segal",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Segal",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard B. Segal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14413334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "649c119f36c90c64a5a9d641a3b29995941c397b",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "AI is moving away from \"toy tasks\" such as block stacking towards real-world problems. This trend is positive, but the amount of preliminary groundwork required to tackle a real-world task can be staggering, particularly when developing an integrated agent architecture. To address this problem, we advocate real-world software environments, such as operating systems or databases, as domains for agent research. The cost, effort, and expertise required to develop and experiment with software agents is relatively low. Furthermore, software environments circumvent many thorny, but peripheral, research issues that are inescapable in other environments. Thus, software environments enable us to test agents in a real world yet focus on core AI research issues. To support this claim, we describe our project to develop UNIX 1 softbots (software robots)--complete intelligent agents that interact with UNIX. Our fully-implemented softbot is able to accept a diverse set of high-level goals, generate and execute plans to achieve these goals in real time, and recover from errors when necessary."
            },
            "slug": "Building-Softbots-for-UNIX-(Preliminary-Report)-Etzioni-Lesh",
            "title": {
                "fragments": [],
                "text": "Building Softbots for UNIX (Preliminary Report)"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The project to develop UNIX 1 softbots (software robots)--complete intelligent agents that interact with UNIX that are able to accept a diverse set of high-level goals, generate and execute plans to achieve these goals in real time, and recover from errors when necessary is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912032"
                        ],
                        "name": "Peter H. Aiken",
                        "slug": "Peter-H.-Aiken",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Aiken",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter H. Aiken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109735847,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "5c9ad411178ff92b12979e2d519a5168dcb9a6e3",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 DRE context: DRE defined DRE's role in enterprise integration data architectures in organizational strategy developing a DRE framework. Part 2 Investing in DRE projects: understanding the challenges and critical success factors DRE performance teams estimating DRE projects identifying and evaluating DRE opportunities. Part 3 DRE analysis and outputs: project initiation and management analyzing the target system tool support primary project outputs secondary outputs (data integration, business rules, data evolution, data exchange) trends and research in DRE."
            },
            "slug": "Data-Reverse-Engineering-:-Slaying-the-Legacy-Aiken",
            "title": {
                "fragments": [],
                "text": "Data Reverse Engineering : Slaying the Legacy Dragon"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6943123"
                        ],
                        "name": "Michael L. Brodie",
                        "slug": "Michael-L.-Brodie",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brodie",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael L. Brodie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145345023"
                        ],
                        "name": "M. Stonebraker",
                        "slug": "M.-Stonebraker",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stonebraker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stonebraker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39448410,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "36b7b4d6c932ad56dbd6f908e14cc405ef528bb4",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Many businesses are burdened by legacy ISs--existing systems that contain valuable data but lack the power or the agility to meet current organizational requirements. Since such systems are by nature resistant to modification, the only way an enterprise can gain full use of the data within one is to move to a completely new IS--ideally, one that won't become a legacy itself in a few years. This book discusses the challenges and provides a framework for a workable system migration, based on incremental modifications that allow the manager to control cost and risk. Focusing on data migration, the authors outline key techniques for transforming a legacy IS into a flexible system that meets current needs and is adaptable to unpredictable future changes. Because designing, developing, testing, and deploying a new system all at once would be extremely risky, the authors advocate a step-by-step strategy that migrates the system to a decomposable target environment while the current IS remains completely operational. Their flexible 11-step method allows any IS to be migrated incrementally, using gateways to access data reliably and accurately without any downtime. Because each company's IS is as unique as its business strategy, the authors focus on the choices and pitfalls that may be encountered in an IS migration. Drawing on over 45 years of collective IS experience, they illustrate their work with two complete case studies of migration projects they worked on themselves. Their versatile solution is effective for any migration, including those to a client/server environment using relational DBMSs and 4GLs. Features a special survey of migration tools by Jennifer Schmidt. Note: the authors would be very interested in receiving a brief report on any successful legacy IS migration project using their methodology or any other, or of significant problems encountered in a migration attempt. As such projects take years, success can apply to the first significant increment."
            },
            "slug": "Migrating-Legacy-Systems:-Gateways,-Interfaces,-and-Brodie-Stonebraker",
            "title": {
                "fragments": [],
                "text": "Migrating Legacy Systems: Gateways, Interfaces, and the Incremental Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Focusing on data migration, the authors outline key techniques for transforming a legacy IS into a flexible system that meets current needs and is adaptable to unpredictable future changes and advocate a step-by-step strategy that migrates the system to a decomposable target environment while the current IS remains completely operational."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152471016"
                        ],
                        "name": "Carl H. Smith",
                        "slug": "Carl-H.-Smith",
                        "structuredName": {
                            "firstName": "Carl H.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl H. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3209224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6524a6b8e6091c0a11d665180a5ad94bbf1d3b4",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a great deal of theoretical and experimental work in computer science on inductive inference systems, that is, systems that try to infer general rules from examples. However, a complete and applicable theory of such systems is still a distant goal. This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations. 154 references."
            },
            "slug": "Inductive-Inference:-Theory-and-Methods-Angluin-Smith",
            "title": {
                "fragments": [],
                "text": "Inductive Inference: Theory and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34808786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8a25cfaa3a8da901962b06af6d460689b40b1bf",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Investigating-the-distribution-assumptions-in-the-Bartlett-Williamson",
            "title": {
                "fragments": [],
                "text": "Investigating the distribution assumptions in the Pac learning model"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11738477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5af977ce4242cc0c4e249d7dac0c6c2ef58bdd4",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In his recent papers, entitled \u201cIntelligence without Representation\u201d and \u201cIntelligence without Reason,\u201d Brooks argues for mobile robots as the foundation of AI research. This article argues that even if we seek to investigate complete agents in real-world environments, robotics is neither necessary nor sufficient as a basis for AI research. The article proposes real-world software environments, such as operating systems or databases, as a complementary substrate for intelligent-agent research and considers the relative advantages of software environments as test beds for AI. First, the cost, effort, and expertise necessary to develop and systematically experiment with software artifacts are relatively low. Second, software environments circumvent many thorny but peripheral research issues that are inescapable in physical environments. Brooks\u2019s mobile robots tug AI toward a bottom-up focus in which the mechanics of perception and mobility mingle inextricably with or even supersede core AI research. In contrast, the softbots (software robots) I advocate facilitate the study of classical AI problems in real-world (albeit, software) domains. For example, the UNIX softbot under development at the University of Washington has led us to investigate planning with incomplete information, interleaving planning and execution, and a host of related high-level issues."
            },
            "slug": "Intelligence-without-Robots:-A-Reply-to-Brooks-Etzioni",
            "title": {
                "fragments": [],
                "text": "Intelligence without Robots: A Reply to Brooks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article proposes real-world software environments, such as operating systems or databases, as a complementary substrate for intelligent-agent research and considers the relative advantages of software environments as test beds for AI."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727680"
                        ],
                        "name": "A. Biermann",
                        "slug": "A.-Biermann",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Biermann",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Biermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165973"
                        ],
                        "name": "J. Feldman",
                        "slug": "J.-Feldman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Feldman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28452406,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "33d49e6fc8755a35ec909c884b8fe8c22b706a4c",
            "isKey": false,
            "numCitedBy": 523,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Nerode realization technique for synthesizing finite-state machines from their associated right-invariant equivalence relations is modified to give a method for synthesizing machines from finite subsets of their input-output behavior. The synthesis procedure includes a parameter that one may adjust to obtain machines that represent the desired behavior with varying degrees of accuracy and that consequently have varying complexities. We discuss some of the uses of the method, including an application to a sequential learning problem."
            },
            "slug": "On-the-Synthesis-of-Finite-State-Machines-from-of-Biermann-Feldman",
            "title": {
                "fragments": [],
                "text": "On the Synthesis of Finite-State Machines from Samples of Their Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The Nerode realization technique for synthesizing finite-state machines from their associated right-invariant equivalence relations is modified to give a method for synthesizer machines from finite subsets of their input-output behavior."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706352"
                        ],
                        "name": "N. Biggs",
                        "slug": "N.-Biggs",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Biggs",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Biggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60867168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f0e7c2b9f9899031a7bde1915be293141870b3d",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational learning theory is a subject which has been advancing rapidly in the last few years. The authors concentrate on the probably approximately correct model of learning, and gradually develop the ideas of efficiency considerations. Finally, applications of the theory to artificial neural networks are considered. Many exercises are included throughout, and the list of references is extensive. This volume is relatively self contained as the necessary background material from logic, probability and complexity theory is included. It will therefore form an introduction to the theory of computational learning, suitable for a broad spectrum of graduate students from theoretical computer science and mathematics."
            },
            "slug": "Computational-learning-theory:-an-introduction-Anthony-Biggs",
            "title": {
                "fragments": [],
                "text": "Computational learning theory: an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This volume is relatively self contained as the necessary background material from logic, probability and complexity theory is included, and will form an introduction to the theory of computational learning, suitable for a broad spectrum of graduate students from theoretical computer science and mathematics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46753437"
                        ],
                        "name": "U. Vazirani",
                        "slug": "U.-Vazirani",
                        "structuredName": {
                            "firstName": "Umesh",
                            "lastName": "Vazirani",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Vazirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 9
                            }
                        ],
                        "text": "'; see [ Kearns & Vazirani, 1994 ] for an introduction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 2
                            }
                        ],
                        "text": "[\nKearns & Vazirani, 1994\n]\nM. Kearns & U. Vazirani."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44944785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e14147f2e61456bba016f720488410393f9e48",
            "isKey": false,
            "numCitedBy": 1786,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata by experimentation appendix - some tools for probabilistic analysis."
            },
            "slug": "An-Introduction-to-Computational-Learning-Theory-Kearns-Vazirani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Computational Learning Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "In [Kushmerick, 1997], we prove the following theorem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "In [Kushmerick, 1997], we prove that: (1) BuildHLRT is sound (if BuildHLRT returns a wrapper, then it is consistent) and complete (if a consistent wrapper exists, BuildHLRT nds it); and (2) under reasonable assumptions, BuildHLRT runs in time O(KNMS(3)), where each tuple has K attributes, the shortest of the N example pages has length S, and M is maximum number of tuples in any single example."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 186
                            }
                        ],
                        "text": "To understand these results, recall that BuildHLRT is essentially computing common pre xes and su xes of sets of strings, which are highly constrained after relatively few examples; see [Kushmerick, 1997] for a detailed discussion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "In [Kushmerick, 1997], we describe Corrob in more detail and prove that it is correct."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "In [Kushmerick, 1997], we compare this noise model to others in the PAC literature."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "In [Kushmerick, 1997], we analyze this hierarchy in detail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Univ"
            },
            "venue": {
                "fragments": [],
                "text": "of Washington,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 194
                            }
                        ],
                        "text": "2 illustrates a partial hierarchy of wrapper classes. lr is less expressive than hlrt; for example, the country/code resource can be wrapped by hlrt but not lr. belr's b and e mark the beginning and end of each tuple, rather than page's body."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 188
                            }
                        ],
                        "text": "To understand these results, recall that BuildHLRT is essentially computing common pre xes and su xes of sets of strings, which are highly constrained after relatively few examples; see [ Kushmerick, 1997 ] for a detailed discussion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 5
                            }
                        ],
                        "text": "In [ Kushmerick, 1997 ] , we analyze this hierarchy in detail."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 5
                            }
                        ],
                        "text": "In [ Kushmerick, 1997 ] , we describe Corrob in more detail and prove that it is correct."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 2
                            }
                        ],
                        "text": "[\nKushmerick, 1997\n]\nN. Kushmerick."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 5
                            }
                        ],
                        "text": "In [ Kushmerick, 1997 ] , we compare this noise model to others in the PAC literature."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "For our problem:\nInstances correspond to pages|e.g. Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 5
                            }
                        ],
                        "text": "In [\nKushmerick, 1997\n]\n, we prove the following theorem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 5
                            }
                        ],
                        "text": "In\n[\nKushmerick, 1997\n]\n, we prove that: (1) BuildHLRT\nis sound (if BuildHLRT returns a wrapper, then it is consistent) and complete (if a consistent wrapper exists, BuildHLRT nds it); and (2) under reasonable assumptions, BuildHLRT runs in time O(KNMS 3 ), where each tuple has K attributes, the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrapper Construction for Information Extraction"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Univ. of Washington,"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148870"
                        ],
                        "name": "N. Tanida",
                        "slug": "N.-Tanida",
                        "structuredName": {
                            "firstName": "Noriyuki",
                            "lastName": "Tanida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Tanida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742006"
                        ],
                        "name": "T. Yokomori",
                        "slug": "T.-Yokomori",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Yokomori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yokomori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117966048,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "812ea65bfcb7e687fd6f80e7021d62d4164d3455",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Polynomial-Time-Identification-of-Strictly-Regular-Tanida-Yokomori",
            "title": {
                "fragments": [],
                "text": "Polynomial-Time Identification of Strictly Regular Languages in the Limit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109503620"
                        ],
                        "name": "Temple F. Smith",
                        "slug": "Temple-F.-Smith",
                        "structuredName": {
                            "firstName": "Temple",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Temple F. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": "[\nBlumer et al., 1987\n]\nA. Blumer, A. Ehrenfeucht, D. Haus-\nsler, & M. Warmuth."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "Thus clearly the stated bound is tighter than obtainable under simple PAC models (e.g., [\nValiant, 1984; Blumer et al., 1987\n]\n), in which sam-\nple complexity grows with the number of hypotheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4276691,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "0b4d43ef0051a225e07af8194e81007ebba8d787",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Occam's-razor-Smith",
            "title": {
                "fragments": [],
                "text": "Occam's razor"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16211998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61f61679c05641f9af596326628ab4107051e13e",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Learning-for-Information-Extraction-from-Freitag",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Information Extraction from Online Documents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 185
                            }
                        ],
                        "text": "\u2026its wrapper language is less expressive than hlrt.\nFinally, our recognition knowledge is similar to work on semantically labeling natural text, such as the MUC-6 \\Named Entity\" task [ DARPA, 1995 ] , though relatively little work has been done on corroborating multiple such knowledge sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 2
                            }
                        ],
                        "text": "[\nDARPA, 1995\n]\nDARPA."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. 6th Message Understanding Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 6th Message Understanding Conference"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": "[\nEtzioni & Weld, 1994\n]\nO. Etzioni & D. Weld."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 154
                            }
                        ],
                        "text": "Therefore, software systems using such resources (e.g., heterogeneous database systems [\nChawathe et al., 1994; Arens et al., 1996\n]\nor software\nagents\n[\nEtzioni & Weld, 1994; Kirk et al., 1995\n]\n) must\ntranslate query responses to relational form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 225
                            }
                        ],
                        "text": "As suggested at the outset, wrapper construction is motivated by the software engineering issues involved with deploying software systems that rely on external information resources; examples include [ Chawathe et al., 1994; Etzioni & Weld, 1994; Arens et al., 1996; Kirk et al., 1995 ] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A softbotbased interface to the Internet The World Wide Web: quagmire or gold mine? C"
            },
            "venue": {
                "fragments": [],
                "text": "C. ACM"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrapper Generation for Semi-structured Information Sources"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. ACM SIGMOD Workshop on Management of Semi-structured Data,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Valiant, 1984] L. Valiant. A theory of the learnable"
            },
            "venue": {
                "fragments": [],
                "text": "C. ACM"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learnability by xed distributions"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 1st Workshop Computational Learning Theory,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "The inductive algorithm requires an oracle to label examples; we solve this labeling problem [ Etzioni, 1996 ] by composing oracles from heuristic knowledge, and we demonstrate that our system degrades gracefully with imperfect heuristics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 93
                            }
                        ],
                        "text": "The inductive algorithm requires an oracle to label examples; we solve this labeling problem [Etzioni, 1996] by composing oracles from heuristic knowledge, and we demonstrate that our system degrades gracefully with imperfect heuristics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 2
                            }
                        ],
                        "text": "[\nEtzioni, 1996\n]\nO. Etzioni."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The World Wide Web: quagmire or gold mine? C"
            },
            "venue": {
                "fragments": [],
                "text": "ACM, 37(7):65{8"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Occam's razor. Information Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Occam's razor. Information Processing"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of automaton identi cation from given data"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Control,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 79
                            }
                        ],
                        "text": "The bound is also tighter than obtainable using VapnikCherv onenkis analysis [ Haussler, 1988 ] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 2
                            }
                        ],
                        "text": "[\nHaussler, 1988\n]\nD. Haussler."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quantifying inductive bias"
            },
            "venue": {
                "fragments": [],
                "text": "Arti cial Intelligence,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine Learning | An Arti cial Intelligence Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Categorizing and standardizing proper nouns for e cient retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. Assoc. for Computational Linguistics Workshop on the Aquisition of Lexical Knowledge from Text,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inducing a Conceptual Dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 14th Int. Joint Conf. AI,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Putting Legacy Data on the Web: A Repository De nition Language"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 3rd Int. WWW Conf.,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 2
                            }
                        ],
                        "text": "[\nKirk et al., 1995\n]\nT. Kirk, A. Levy, Y. Sagiv, & D. Srivas-\ntava."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 176
                            }
                        ],
                        "text": "Therefore, software systems using such resources (e.g., heterogeneous database systems [\nChawathe et al., 1994; Arens et al., 1996\n]\nor software\nagents\n[\nEtzioni & Weld, 1994; Kirk et al., 1995\n]\n) must\ntranslate query responses to relational form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 267
                            }
                        ],
                        "text": "As suggested at the outset, wrapper construction is motivated by the software engineering issues involved with deploying software systems that rely on external information resources; examples include [ Chawathe et al., 1994; Etzioni & Weld, 1994; Arens et al., 1996; Kirk et al., 1995 ] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Information Manifold. In AAAI Spring Symposium: Information Gathering from Heterogeneous"
            },
            "venue": {
                "fragments": [],
                "text": "Distributed Environments,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable Internet discovery: Research problems and approaches"
            },
            "venue": {
                "fragments": [],
                "text": "C. ACM,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The ConstraintBased Knowledge Broker Mode: Semantics, Implementation and Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "J. Symbolic Computation,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NameFinder: Softwre that nds names in text"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. Conf. Computer-Assisted Information Retrieval,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The BargainFinder agent: Comparison price shopping on the Internet"
            },
            "venue": {
                "fragments": [],
                "text": "Bots and Other Internet Beasties,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 131
                            }
                        ],
                        "text": "Finally, our recognition knowledge is similar to work on semantically labeling natural text, such as the MUC-6 \\Named Entity\" task [DARPA, 1995], though relatively little work has been done on corroborating multiple such knowledge sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "6th Message Understanding Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann, San Francisco"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": "[\nEtzioni & Weld, 1994\n]\nO. Etzioni & D. Weld."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 154
                            }
                        ],
                        "text": "Therefore, software systems using such resources (e.g., heterogeneous database systems [\nChawathe et al., 1994; Arens et al., 1996\n]\nor software\nagents\n[\nEtzioni & Weld, 1994; Kirk et al., 1995\n]\n) must\ntranslate query responses to relational form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 225
                            }
                        ],
                        "text": "As suggested at the outset, wrapper construction is motivated by the software engineering issues involved with deploying software systems that rely on external information resources; examples include [ Chawathe et al., 1994; Etzioni & Weld, 1994; Arens et al., 1996; Kirk et al., 1995 ] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A softbotbased interface to the Internet"
            },
            "venue": {
                "fragments": [],
                "text": "C. ACM,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "The inductive algorithm requires an oracle to label examples; we solve this labeling problem [ Etzioni, 1996 ] by composing oracles from heuristic knowledge, and we demonstrate that our system degrades gracefully with imperfect heuristics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 2
                            }
                        ],
                        "text": "[\nEtzioni, 1996\n]\nO. Etzioni."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Moving up the information food chain: softbots as information carnivores"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. 13th Nat. Conf. AI,"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 83,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld/f9e7402ad740b73cc0bb64178f86df3478c3aaf5?sort=total-citations"
}