{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 134
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 197
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18036435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fb4d10f6d2ee3133135958aefd50bf22dcced9d",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Time is at the heart of many pattern recognition tasks (e.g., speech recognition). However, connectionist learning algorithms to date are not well-suited for dealing with time-varying input patterns. This chapter introduces a specialized connectionist architecture and corresponding specialization of the back-propagation learning algorithm that operates efficiently, both in computational time and space requirements, on temporal sequences. The key feature of the architecture is a layer of selfconnected hidden units that integrate their current value with the new input at each time step to construct a static representation of the temporal input sequence. This architecture avoids two deficiencies found in the back-propagation unfolding-intime procedure (Rumelhart, Hinton, & Williams, 1986) for handing sequence recognition tasks: first, it reduces the difficulty of temporal credit assignment by focusing the back-propagated error signal; second, it eliminates the need for a buffer to hold the input sequence and/or intermediate activity levels. The latter property is due to the fact that during the forward (activation) phase, incremental activity traces can be locally computed that hold all information necessary for back propagation in time. It is argued that this architecture should scale better than conventional recurrent architectures with respect to sequence length. The architecture has been used to implement a temporal version of Rumelhart and McClelland's (1986) verb past-tense model. The hidden units learn to behave something like Rumelhart and McClelland's \"Wickelphones,\" a rich and flexible representation of temporal information."
            },
            "slug": "A-Focused-Backpropagation-Algorithm-for-Temporal-Mozer",
            "title": {
                "fragments": [],
                "text": "A Focused Backpropagation Algorithm for Temporal Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A specialized connectionist architecture and corre sponding specialization of the backpropagation learnin g algori thm th at opera tes efficiently on temporal sequences is introduced and should scale better than conventional recurrent architectures wit h respect to sequenc e length."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251336"
                        ],
                        "name": "D. Burr",
                        "slug": "D.-Burr",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Burr",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2845294"
                        ],
                        "name": "Y. Miyata",
                        "slug": "Y.-Miyata",
                        "structuredName": {
                            "firstName": "Yoshiro",
                            "lastName": "Miyata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Miyata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 91
                            }
                        ],
                        "text": "This di cult problem has been identi ed and studied byseveral other researchers, including Miyata and Burr (1990), Rohwer (1990), andSchmidhuber (1991).1 BUILDING A REDUCED DESCRIPTIONThe basic idea behind my work involves building a reduced description (Hinton,1988) of the sequence that makes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57578594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8183b1548750e5750d509af6a9fdc8f9a5a9252d",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Layered neural networks employing feedback links have been proposed for certain sequential pattern tasks in automatic music composition. A hierarchical version of this type of network is studied. The use of such a hierarchical neural network for modeling coarse and fine temporal structure in music is investigated. This network is trained on two classical waltzes and then used to generate novel waltzes. The generated waltzes contained both novel phrases and phrases from the original scores. They exhibit an overall structure which has been difficult to learn using conventional methods. It is argued that it is the synaptic links of artificial neural networks which allow them to learn the relationship between coarse and fine temporal structure.<<ETX>>"
            },
            "slug": "Hierarchical-recurrent-networks-for-learning-Burr-Miyata",
            "title": {
                "fragments": [],
                "text": "Hierarchical recurrent networks for learning musical structure"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that it is the synaptic links of artificial neural networks which allow them to learn the relationship between coarse and fine temporal structure."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing III - Proceedings of the 1993 IEEE-SP Workshop"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "In principle, variants of back propagation for recurrent networks (Rumelhart, Hin-ton, & Williams, 1986; Williams & Zipser, 1989) can discover an appropriate rep-resentation in the context layer for a particular task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 147
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34915378"
                        ],
                        "name": "R. Rohwer",
                        "slug": "R.-Rohwer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rohwer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rohwer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 736183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af7802a50a8c294ebfd539ad72158475e5ecd9f2",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past."
            },
            "slug": "The-\"Moving-Targets\"-Training-Algorithm-Rohwer",
            "title": {
                "fragments": [],
                "text": "The \"Moving Targets\" Training Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A simple method for training the dynamical behavior of a neural network using a gradient-based method and the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48429353"
                        ],
                        "name": "Pineda",
                        "slug": "Pineda",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Pineda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 165
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40994937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6602985bd326d9996c68627b56ed389e2c90fd08",
            "isKey": false,
            "numCitedBy": 905,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "slug": "Generalization-of-back-propagation-to-recurrent-Pineda",
            "title": {
                "fragments": [],
                "text": "Generalization of back-propagation to recurrent neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An adaptive neural network with asymmetric connections is introduced that bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 116
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15572239,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7a4038ec7f07b3d765260b9f17da386a85d4f7d1",
            "isKey": false,
            "numCitedBy": 1250,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This article examines the possibility that the components of an informationprocessing system all operate continuously, passing information from one to the next as it becomes available. A model called the cascade model is presented and it is shown to be compatible with the general form of the relation between time and accuracy in speed-accuracy trade-off experiments. In the model , experimentlLlmanipulations may have either or both of two effects on a processing level: They may alter the rate of response or the asymptotic quality pf the output. The effects of such manipulations on the output of a system of proessesare described. The model is then used to reexamine, the subtraction and additive factors methods for analyzing the composition of systems of processes. The examination of the additive factors method yields particularly interesting results. Among them is the finding that factors that affect the rates of two different processes would be expected to have additive effects on reaction times under the cascade model, whereas factors that both affect the rate of the same process would tend to interact, just as in the case in which the manipulations affect the durations of discrete stages. On the other hand, factors that affect asymptotic output tend to interact whether they affect the same or different processes. In light of this observation, the conclusions drawn from several studies about the locus of perceptual and attentional effects on processing are reexamined. Finally, an outline is presented of a new method for analyzing processes in cascade. The method extends the additive factors method to an analysis of the parameters of the function relating response time and accuracy."
            },
            "slug": "On-the-time-relations-of-mental-processes:-An-of-of-McClelland",
            "title": {
                "fragments": [],
                "text": "On the time relations of mental processes: An examination of systems of processes in cascade."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 41
                            }
                        ],
                        "text": "A variant of this task was rst studiedby Schmidhuber (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 26
                            }
                        ],
                        "text": "Figure 3: A sketch of the Schmidhuber (1991) architecture3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 192
                            }
                        ],
                        "text": "In practice, however, back propagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics (e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 224
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural sequence chunkers (Report FKI-148-91)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 41
                            }
                        ],
                        "text": "A variant of this task was rst studiedby Schmidhuber (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 26
                            }
                        ],
                        "text": "Figure 3: A sketch of the Schmidhuber (1991) architecture3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 224
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 148
                            }
                        ],
                        "text": "\u2026in designing a connectionist network for music composition, wehave encountered the problem that the net is able to learn musical struc-ture that occurs locally in time|e.g., relations among notes within a mu-sical phrase|but not structure that occurs over longer time periods|e.g.,relations among\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural ,equence chunker"
            },
            "venue": {
                "fragments": [],
                "text": "Neural ,equence chunker"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 102
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56723681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d76aafbeb54575859441a442376766c597f6bb52",
            "isKey": false,
            "numCitedBy": 1102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Attractor-dynamics-and-parallelism-in-a-sequential-Jordan",
            "title": {
                "fragments": [],
                "text": "Attractor dynamics and parallelism in a connectionist sequential machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "This di cult problem has been identi ed and studied byseveral other researchers, including Miyata and Burr (1990), Rohwer (1990), andSchmidhuber (1991).1 BUILDING A REDUCED DESCRIPTIONThe basic idea behind my work involves building a reduced description (Hinton,1988) of the sequence that makes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The 'moving targets' training algorithm Advances in neural information processing systems 2 (pp. 558{565)"
            },
            "venue": {
                "fragments": [],
                "text": "The 'moving targets' training algorithm Advances in neural information processing systems 2 (pp. 558{565)"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 194
                            }
                        ],
                        "text": "\u2026other researchers, including Miyata and Burr (1990), Rohwer (1990), andSchmidhuber (1991).1 BUILDING A REDUCED DESCRIPTIONThe basic idea behind my work involves building a reduced description (Hinton,1988) of the sequence that makes global aspects more explicit or more readily de-tectable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representing part-whole hierarchies in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding' of the Eighth Annual Conference of the Cognitive Science Society"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A focused back-propagation algorithm for temporal pattem recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Complez Syltem"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 165
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalisation of back propagation to recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Phy,ical Review Letter"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 115
                            }
                        ],
                        "text": "This di cult problem has been identi ed and studied byseveral other researchers, including Miyata and Burr (1990), Rohwer (1990), andSchmidhuber (1991).1 BUILDING A REDUCED DESCRIPTIONThe basic idea behind my work involves building a reduced description (Hinton,1988) of the sequence that makes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "Forexample, in designing a connectionist network for music composition, wehave encountered the problem that the net is able to learn musical struc-ture that occurs locally in time|e.g., relations among notes within a mu-sical phrase|but not structure that occurs over longer time\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The 'moving targets' training algorithm Advance, in neural information proce\"ing ,yltem, I (pp. 558-565)"
            },
            "venue": {
                "fragments": [],
                "text": "The 'moving targets' training algorithm Advance, in neural information proce\"ing ,yltem, I (pp. 558-565)"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 18
                            }
                        ],
                        "text": "In previous work (Mozer & Soukup, 1991), we designed a sequential prediction network, called concert, thatlearns to reproduce a set of pieces of a particular musical style. concert alsolearns structural regularities of the musical style, and can be used to compose newpieces in the same style.\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "See Mozer and Soukup (1991) for details of the networkarchitecture and note representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 62
                            }
                        ],
                        "text": "For instance,in earlier work on neural net music composition (Mozer & Soukup, 1991), we foundthat our network could master the rules of composition for notes within a musicalphrase, but not rules operating at a more global level|rules for how phrases areinterrelated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CONCERT: A connectionist composer of erudite"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 41
                            }
                        ],
                        "text": "A variant of this task was rst studiedby Schmidhuber (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 26
                            }
                        ],
                        "text": "Figure 3: A sketch of the Schmidhuber (1991) architecture3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 224
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural ,equence chunker, (Report FKI-148-91). Munich, Germany: Technische Universitaet Muenchen, Institut fuel Informatik"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning intemal representations by error propagation Parallel di,tributed proce\"ing: Ezploration, in the microltructure of cognition. Volume I: Foundation"
            },
            "venue": {
                "fragments": [],
                "text": "Learning intemal representations by error propagation Parallel di,tributed proce\"ing: Ezploration, in the microltructure of cognition. Volume I: Foundation"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CONCERT: A connectionist composer of erudite tunes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CONCERT: A connectionist composer of erudite tunes Advance, in neural information proce\"ing"
            },
            "venue": {
                "fragments": [],
                "text": "CONCERT: A connectionist composer of erudite tunes Advance, in neural information proce\"ing"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 134
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter,1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 101
                            }
                        ],
                        "text": "Time constants have been incorporated into the activation rules of other connectionist architectures (Jordan, 1987; McClelland, 1979; Mozer, 1989; Pearlmutter, 1989; Pineda, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 197
                            }
                        ],
                        "text": "In practice, however, backpropagation is not su ciently powerful to discover arbitrary contingencies, especially those that span long temporal intervals or that involve high order statistics(e.g., Mozer, 1989; Rohwer, 1990; Schmidhuber, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A focused back-propagation algorithm for temporal pattern"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 91
                            }
                        ],
                        "text": "This di cult problem has been identi ed and studied byseveral other researchers, including Miyata and Burr (1990), Rohwer (1990), andSchmidhuber (1991).1 BUILDING A REDUCED DESCRIPTIONThe basic idea behind my work involves building a reduced description (Hinton,1988) of the sequence that makes\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical recurrent networks for learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 18
                            }
                        ],
                        "text": "In previous work (Mozer & Soukup, 1991), we designed a sequential prediction network, called concert, thatlearns to reproduce a set of pieces of a particular musical style. concert alsolearns structural regularities of the musical style, and can be used to compose newpieces in the same style.\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "See Mozer and Soukup (1991) for details of the networkarchitecture and note representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 62
                            }
                        ],
                        "text": "For instance,in earlier work on neural net music composition (Mozer & Soukup, 1991), we foundthat our network could master the rules of composition for notes within a musicalphrase, but not rules operating at a more global level|rules for how phrases areinterrelated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CONCERT: A connectionist composer of erudite tunes"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in neural information processing systems 3"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Induction-of-Multiscale-Temporal-Structure-Mozer/e141d68065ce638f9fc4f006eab2f66711e89768?sort=total-citations"
}