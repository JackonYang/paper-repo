{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Approaches that determine the expression by matching stored image templates to the current image [ 5 ] use even less explicit spatial information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13535822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8e6c03bfcdf13afe3d484f32234488af573fc3c",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a computer system that allows real-time tracking of facial expressions. Sparse, fast visual measurements using 2D templates are used to observe the face of a subject. Rather than track features on the face, the distributed response of a set of templates is used to characterize a given facial region. These measurements ape coupled via a linear interpolation method to states in a physically-based model of facial animation, which includes both skin and muscle dynamics. By integrating real-time 2D image-processing with 3D models we obtain a system that is able to quickly track and interpret complex facial motions.<<ETX>>"
            },
            "slug": "Tracking-facial-motion-Essa-Darrell",
            "title": {
                "fragments": [],
                "text": "Tracking facial motion"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By integrating real-time 2D image-processing with 3D models, this work obtains a system that is able to quickly track and interpret complex facial motions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "\u20260:1 Mouth downward Div > 0:02 Mouth expansion< 0:02 Mouth contraction Def > 0:005 Mouth horizontal deformation< 0:005 Mouth vertical deformation Curl > 0:005 Mouth clockwise rotation< 0:005 Mouth counterclockwise rotation c < 0:0001 Mouth curving upward (\u2018U\u2019 like)> 0:0001 Mouth curving\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13200879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4eed9d20232953a1265ef35edeeb4843a519b319",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a computer vision system for observing the \"action units\" of a face using video sequences as input. The visual observation (sensing) is achieved by using an optimal estimation optical flow method coupled with a geometric and a physical (muscle) model describing the facial structure. This modeling results in a time-varying spatial patterning of facial shape and a parametric representation of the independent muscle action groups, responsible for the observed facial motions. These muscle action patterns may then be used for analysis, interpretation, and synthesis. Thus, by interpreting facial motions within a physics-based optimal estimation framework, a new control model of facial movement is developed. The newly extracted action units (which we name \"FACS+\") are both physics and geometry-based, and extend the well-known FACS parameters for facial expressions by adding temporal information and non-local spatial patterning of facial motion.<<ETX>>"
            },
            "slug": "A-vision-system-for-observing-and-extracting-facial-Essa-Pentland",
            "title": {
                "fragments": [],
                "text": "A vision system for observing and extracting facial action parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "By interpreting facial motions within a physics-based optimal estimation framework, a new control model of facial movement is developed and the newly extracted action units are both physics and geometry-based, and extend the well-known FACS parameters for facial expressions by adding temporal information and non-local spatial patterning of facial motion."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46398261"
                        ],
                        "name": "K. Waters",
                        "slug": "K.-Waters",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Waters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Waters"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work has typically focused on one part of the problem or the other: either rigid head tracking [1] with no facial expressions or expression recognition with either no motion at all [10] or a roughly stationary head with a changing expression [ 7 , 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "At one extreme are approaches which employ physically-based models of heads including skin and musculature [6,  7 ]. Slightly weaker models use deformable templates to represent feature shapes in the image [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Deformable models such as snakes provide good tracking of these regions [ 7 ] but their distributed nature does not admit simple, intuitive, characterizations of the motions as we saw above."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15057830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f77296f889b56d871b9ba9408338dbfd9a0cdeb3",
            "isKey": true,
            "numCitedBy": 614,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to the analysis of dynamic facial images for the purposes of estimating and resynthesizing dynamic facial expressions is presented. The approach exploits a sophisticated generative model of the human face originally developed for realistic facial animation. The face model which may be simulated and rendered at interactive rates on a graphics workstation, incorporates a physics-based synthetic facial tissue and a set of anatomically motivated facial muscle actuators. The estimation of dynamical facial muscle contractions from video sequences of expressive human faces is considered. An estimation technique that uses deformable contour models (snakes) to track the nonrigid motions of facial features in video images is developed. The technique estimates muscle actuator controls with sufficient accuracy to permit the face model to resynthesize transient expressions. >"
            },
            "slug": "Analysis-and-Synthesis-of-Facial-Image-Sequences-Terzopoulos-Waters",
            "title": {
                "fragments": [],
                "text": "Analysis and Synthesis of Facial Image Sequences Using Physical and Anatomical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An estimation technique that uses deformable contour models (snakes) to track the nonrigid motions of facial features in video images is developed and estimates muscle actuator controls with sufficient accuracy to permit the face model to resynthesize transient expressions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 235
                            }
                        ],
                        "text": "\u2026rotation c < 0:0001 Mouth curving upward (\u2018U\u2019 like)> 0:0001 Mouth curving downward\nTable 1: The mid-level predicates of the mouth derived from the motion parameter estimates.\nis primarily used to stabilize the head motion so that the relative motion of the features may be estimated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9794074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4302d843e008bdc4444e7fb161044a9c60b7c01d",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach for analysis and representation of facial dynamics for recognition of facial expressions from image sequences is proposed. The algorithms we develop utilize optical flow computation to identify the direction of rigid and non-rigid motions that are caused by human, facial expressions. A mid-level symbolic representation that is motivated by linguistic and psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, on a large set of image sequences is reported.<<ETX>>"
            },
            "slug": "Computing-spatio-temporal-representations-of-human-Yacoob-Davis",
            "title": {
                "fragments": [],
                "text": "Computing spatio-temporal representations of human faces"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An approach for analysis and representation of facial dynamics for recognition of facial expressions from image sequences is proposed and a mid-level symbolic representation that is motivated by linguistic and psychological considerations is developed."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981906"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 168
                            }
                        ],
                        "text": "Equation 12 is minimized using a simple gradient descent scheme with a continuation method that begins with a high value for and lowers it during the minimization (see [2, 3] for details)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14930294,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "65297c4e40fd9b8129fc593be1b8c8ef775c86ba",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors consider the problem of robustly estimating optical flow from a pair of images using a new framework based on robust estimation which addresses violations of the brightness constancy and spatial smoothness assumptions. They also show the relationship between the robust estimation framework and line-process approaches for coping with spatial discontinuities. In doing so, the notion of a line process is generalized to that of an outlier process that can account for violations in both the brightness and smoothness assumptions. A graduated non-convexity algorithm is presented for recovering optical flow and motion discontinuities. The performance of the robust formulation is demonstrated on both synthetic data and natural images.<<ETX>>"
            },
            "slug": "A-framework-for-the-robust-estimation-of-optical-Black-Anandan",
            "title": {
                "fragments": [],
                "text": "A framework for the robust estimation of optical flow"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The authors consider the problem of robustly estimating optical flow from a pair of images using a new framework based on robust estimation which addresses violations of the brightness constancy and spatial smoothness assumptions and presents a graduated non-convexity algorithm for recovering optical flow and motion discontinuities."
            },
            "venue": {
                "fragments": [],
                "text": "1993 (4th) International Conference on Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "The recovered image motion parameters correspond simply and intuitively to various facial expressions and are used to derive mid-level predicates describing the image motion of the facial features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1597358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5411297f6b703c6a48fafcc9066d725124061a15",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to labeling the components of human faces from range images is proposed. The components of interest are those humans usually find significant for recognition. To cope with the nonrigidity of faces, a qualitative approach is used. The preprocessing stage employs a multi-stage diffusion process to identify convexity and concavity points. These points are grouped into components and qualitative reasoning about possible interpretations of the components is performed. Consistency of hypothesized interpretations is carried out using context-based reasoning. Experimental results on real images of several faces are provided.<<ETX>>"
            },
            "slug": "Labeling-of-human-face-components-from-range-data-Yacoob-Davis",
            "title": {
                "fragments": [],
                "text": "Labeling of human face components from range data"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An approach to labeling the components of human faces from range images using a qualitative approach to cope with the nonrigidity of faces and consistency of hypothesized interpretations is carried out using context-based reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21451088"
                        ],
                        "name": "P. Ekman",
                        "slug": "P.-Ekman",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Ekman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ekman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Ground truth for the data set was determined by inspection using the cues proposed in the psychological literature [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "These rules are applied to the predicates of the mid-level representation and are similar to those proposed in the psychological literature [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "In developing the high-level models we rely on the classification of facial expressions described in the psychological literature [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 155567036,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3cc7e9d49d07b24fd50962f3100e2acc7a157b3e",
            "isKey": true,
            "numCitedBy": 1955,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unmasking-The-Face-Ekman",
            "title": {
                "fragments": [],
                "text": "Unmasking The Face"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145271456"
                        ],
                        "name": "A. Azarbayejani",
                        "slug": "A.-Azarbayejani",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Azarbayejani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Azarbayejani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144152544"
                        ],
                        "name": "B. Horowitz",
                        "slug": "B.-Horowitz",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For longer image sequences, more sophisticated tracking schemes could be used; for example, Kalman filtering [ 1 ], segmentation information, and spatial constraints on the feature locations might be added."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work has typically focused on one part of the problem or the other: either rigid head tracking [ 1 ] with no facial expressions or expression recognition with either no motion at all [10] or a roughly stationary head with a changing expression [7, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10074380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7bfa4190f8e4b746855adf8da214d5fdad8ede2",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Interactive graphics systems that are driven by visual input are discussed. The underlying computer vision techniques and a theoretical formulation that addresses issues of accuracy, computational efficiency, and compensation for display latency are presented. Experimental results quantitatively compare the accuracy of the visual technique with traditional sensing. An extension to the basic technique to include structure recovery is discussed. >"
            },
            "slug": "Visually-Controlled-Graphics-Azarbayejani-Starner",
            "title": {
                "fragments": [],
                "text": "Visually Controlled Graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Interactive graphics systems that are driven by visual input and an extension to the basic technique to include structure recovery is discussed, quantitatively comparing the accuracy of the visual technique with traditional sensing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981906"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "The motions of the face and facial features estimated between two frames are used to predict the locations of the features in the next frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "In this paper we do not address the problem of initially locating the various facial features; this topic has been addressed in [9, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118148710,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "12e52cf798e2273dccc6abafa9cae8640e8f3433",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-robust-estimation-of-multiple-motions:-Affine-Black-Anandan",
            "title": {
                "fragments": [],
                "text": "The robust estimation of multiple motions: Affine and piecewise smooth flow fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "For longer image sequences, more sophisticated tracking schemes could be used; for example, Kalman filtering [1], segmentation information, and spatial constraints on the feature locations might be added."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Previous work has typically focused on one part of the problem or the other: either rigid head tracking [1] with no facial expressions or expression recognition with either no motion at all [10] or a roughly stationary head with a changing expression [7, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visually controled graphics"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE PAMI,"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 10,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Tracking-and-recognizing-rigid-and-non-rigid-facial-Black-Yacoob/4ef915fa9e5b2260d4b45927c0033a7ba53bf66e?sort=total-citations"
}