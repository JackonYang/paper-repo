{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119287919"
                        ],
                        "name": "J. Littell",
                        "slug": "J.-Littell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Littell",
                            "middleNames": [
                                "Stockton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Littell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23073769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76f826f04895f2194e967586954d1f58bc980153",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "The hypothesis that word co-occurrence statistics extracted from text corpora can provide a basis for semantic representations has been gaining growing attention both in computational linguistics and in cognitive science. The terms distributional, context-theoretic, corpusbased or statistical can all be used (almost interchangeably) to qualify a rich family of approaches to semantics that share a \u201cusage-based\u201d perspective on meaning, and assume that the statistical distribution of words in context plays a key role in characterizing their semantic behavior. Besides this common core, many differences exist depending on the specific mathematical and computational techniques, the type of semantic properties associated with text distributions, the definition of the linguistic context used to determine the combinatorial spaces of lexical items, etc. Yet, at a closer look, we may discover that the commonalities are more than we could expect prima facie, and that a general model of meaning can indeed be discerned behind the differences, a model that formulates specific hypotheses on the format of semantic representations, and on the way they are built and processed by the human mind. Methods for computational analysis of word distributional properties have been developed both in computational linguistics and in psychology. Because of the different aims of each field, these lines of research have typically proceeded totally in a parallel fashion, often ignoring each other. The drawbacks of this situation are clear: many"
            },
            "slug": "Distributional-semantics-in-linguistic-and-research-Lenci-Littell",
            "title": {
                "fragments": [],
                "text": "Distributional semantics in linguistic and cognitive research"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work concludes that a general model of meaning can indeed be discerned behind the differences, a model that formulates specific hypotheses on the format of semantic representations, and on the way they are built and processed by the human mind."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840946"
                        ],
                        "name": "B. Murphy",
                        "slug": "B.-Murphy",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Murphy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34884385"
                        ],
                        "name": "E. Barbu",
                        "slug": "E.-Barbu",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Barbu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Barbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678591"
                        ],
                        "name": "Massimo Poesio",
                        "slug": "Massimo-Poesio",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Poesio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimo Poesio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": "The Battig test set introduced by Baroni et al. (2010) is based on the expanded Battig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 95
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "This model is based on the idea, motivated and tested by Baroni et al. (2010)\u2014 but see also Davidov and Rappoport (2008a, 2008b) for a related method\u2014that what matters is not so much the frequency of a link, but the variety of surface forms that express it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026word\u2013link\u2013word tuples from a dependency parse of a corpus, but this is not a requirement for DM: The links could for example be based on frequent n-grams as in Turney (2006b) and Baroni et al. (2010), or even on very different kinds of relation, such as co-occurring within the same document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 47
                            }
                        ],
                        "text": "Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 211
                            }
                        ],
                        "text": "Table 12 reports the percentage overlap with the gold standard properties (averaged across the 44 concepts) for our models as well as the only ESSLLI 2008 participant that tried this task, and for the models of Baroni et al. (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 275
                            }
                        ],
                        "text": "Recently, there has been some interest in the automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 4
                            }
                        ],
                        "text": "See Baroni et al. (2010) for the full list."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1656861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab582a92ab0a4e7ff68c7540ff3e7c7eb7c79441",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational models of meaning trained on naturally occurring text successfully model human performance on tasks involving simple similarity measures, but they characterize meaning in terms of undifferentiated bags of words or topical dimensions. This has led some to question their psychological plausibility (Murphy, 2002;Schunn, 1999). We present here a fully automatic method for extracting a structured and comprehensive set of concept descriptions directly from an English part-of-speech-tagged corpus. Concepts are characterized by weighted properties, enriched with concept-property types that approximate classical relations such as hypernymy and function. Our model outperforms comparable algorithms in cognitive tasks pertaining not only to concept-internal structures (discovering properties of concepts, grouping properties by property type) but also to inter-concept relations (clustering into superordinates), suggesting the empirical validity of the property-based approach."
            },
            "slug": "Strudel:-A-Corpus-Based-Semantic-Model-Based-on-and-Baroni-Murphy",
            "title": {
                "fragments": [],
                "text": "Strudel: A Corpus-Based Semantic Model Based on Properties and Types"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This model outperforms comparable algorithms in cognitive tasks pertaining not only to concept-internal structures but also to inter-concept relations (clustering into superordinates), suggesting the empirical validity of the property-based approach."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83d9fec6bee4cf6e01445d6ff51aae58b6d96d5a",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns (P1,..., Pm) is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X = ostrich and Y = bird, the two highest ranking output patterns are \"X is the largest Y\" and \"Y such as the X\". The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi. The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf."
            },
            "slug": "Expressing-Implicit-Semantic-Relations-without-Turney",
            "title": {
                "fragments": [],
                "text": "Expressing Implicit Semantic Relations without Supervision"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations, and achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9322367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e02837ac075543fe1b04f3003133a6015564d443",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning \u201cA is to B as C is to D\u201d; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \u201claser printer\u201d, according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations."
            },
            "slug": "Corpus-based-Learning-of-Analogies-and-Semantic-Turney-Littman",
            "title": {
                "fragments": [],
                "text": "Corpus-based Learning of Analogies and Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An algorithm for learning from unlabeled text that can solve verbal analogy questions of the kind found in the SAT college entrance exam and is state-of-the-art for both verbal analogies and noun-modifier relations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7747235,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art."
            },
            "slug": "Dependency-Based-Construction-of-Semantic-Space-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Construction of Semantic Space Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article presents a novel framework for constructing semantic spaces that takes syntactic relations into account, and introduces a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34926595"
                        ],
                        "name": "K. Heylen",
                        "slug": "K.-Heylen",
                        "structuredName": {
                            "firstName": "Kris",
                            "lastName": "Heylen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Heylen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349066"
                        ],
                        "name": "Yves Peirsman",
                        "slug": "Yves-Peirsman",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Peirsman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Peirsman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796288"
                        ],
                        "name": "D. Geeraerts",
                        "slug": "D.-Geeraerts",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Geeraerts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geeraerts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754574"
                        ],
                        "name": "D. Speelman",
                        "slug": "D.-Speelman",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Speelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Speelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 129
                            }
                        ],
                        "text": "tracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 187
                            }
                        ],
                        "text": "Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2351337,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "afd5da111e80fd0ee783b1ecf1bcb1be8cbff8f4",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts. Despite their increasing popularity, it is unclear which kind of semantic similarity they actually capture and for which kind of words. In this paper, we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic properties of the nouns influence the results. In particular, we compare results from a dependency-based model with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns\u0092 frequency, semantic speficity and semantic class. We find that all three models find more synonyms for high-frequency nouns and those belonging to abstract semantic classses. Semantic specificty does not have a clear influence."
            },
            "slug": "Modelling-Word-Similarity:-an-Evaluation-of-Heylen-Peirsman",
            "title": {
                "fragments": [],
                "text": "Modelling Word Similarity: an Evaluation of Automatic Synonymy Extraction Algorithms."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper uses three vector-based models to retrieve semantically related words for a set of Dutch nouns and examines the effect of the nouns\u0092 frequency, semantic speficity and semantic class, finding that all three models find more synonyms for high-frequency nounsand those belonging to abstract semantic classses."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 100
                            }
                        ],
                        "text": "Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Workshop shared tasks (Baroni, Evert, and Lenci 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 244
                            }
                        ],
                        "text": "Recently, there has been some interest in the automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 119
                            }
                        ],
                        "text": "We test this approach in the ESSLLI 2008 Distributional Semantic Workshop unconstrained property generation challenge (Baroni, Evert, and Lenci 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1187304,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2cda689911f9dcf7e1555e5cb0a25ae93b95ad0b",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties play a central role in most theories of conceptual knowledge. Since computational models derived from word co-occurrence statistics have been claimed to provide a natural basis for semantic representations, the question arises of whether such models are capable of producing reasonable property-based descriptions of concepts, and whether these descriptions are similar to those elicited from humans. This article presents a qualitative analysis of the properties generated by humans in two different settings, as well as those produced, for the same concepts, by two computational models. In order to find high-level generalizations, the analysis is conducted in terms of property types, i.e., categorizing properties into classes such as functional and taxonomic properties. We discover that differences and similarities among models cut across the human/computational distinction, suggesting on the one hand caution in making broad generalizations, e.g., about \u201cgrounded\u201d and \u201camodal\u201d approaches, and, on the other, that different models might reveal different facets of meaning, and thus they should rather be integrated than seen as rival ways to get at the same information."
            },
            "slug": "Concepts-and-properties-in-word-spaces-Baroni-Lenci",
            "title": {
                "fragments": [],
                "text": "Concepts and properties in word spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents a qualitative analysis of the properties generated by humans in two different settings, as well as those produced, for the same concepts, by two computational models, in terms of property types, finding that differences and similarities among models cut across the human/computational distinction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3339702"
                        ],
                        "name": "E. Joanis",
                        "slug": "E.-Joanis",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Joanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Joanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584212"
                        ],
                        "name": "S. Stevenson",
                        "slug": "S.-Stevenson",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054791766"
                        ],
                        "name": "D. James",
                        "slug": "D.-James",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "James",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. James"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 740196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca410234ba12ede041a91593be38c922c8919671",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Lexical semantic classes of verbs play an important role in structuring complex predicate information in a lexicon, thereby avoiding redundancy and enabling generalizations across semantically similar verbs with respect to their usage. Such classes, however, require many person-years of expert effort to create manually, and methods are needed for automatically assigning verbs to appropriate classes. In this work, we develop and evaluate a feature space to support the automatic assignment of verbs into a well-known lexical semantic classification that is frequently used in natural language processing. The feature space is general \u2013 applicable to any class distinctions within the target classification; broad \u2013 tapping into a variety of semantic features of the classes; and inexpensive \u2013 requiring no more than a POS tagger and chunker. We perform experiments using support vector machines (SVMs) with the proposed feature space, demonstrating a reduction in error rate ranging from 48% to 88% over a chance baseline accuracy, across classification tasks of varying difficulty. In particular, we attain performance comparable to or better than that of feature sets manually selected for the particular tasks. Our results show that the approach is generally applicable, and reduces the need for resource-intensive linguistic analysis for each new classification task. We also perform a wide range of experiments to determine the most informative features in the feature space, finding that simple, easily extractable features suffice for good verb classification performance."
            },
            "slug": "A-General-Feature-Space-for-Automatic-Verb-Joanis-Stevenson",
            "title": {
                "fragments": [],
                "text": "A General Feature Space for Automatic Verb Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work develops and evaluates a feature space to support the automatic assignment of verbs into a well-known lexical semantic classification that is frequently used in natural language processing, and shows that the approach is generally applicable, and reduces the need for resource-intensive linguistic analysis for each new classification task."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 107
                            }
                        ],
                        "text": "Other possibilities, such as graph-based algorithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), or deriving unstructured semantic spaces from the tensor by removing one of the indices, are left to future work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "The C/I data set was introduced by Baroni and Lenci (2009), but not tested in a classification task there."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 120
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6058012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf598f9a7763c7e07c1477bcc742f71ba9eb79e4",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks."
            },
            "slug": "One-Distributional-Memory,-Many-Semantic-Spaces-Baroni-Lenci",
            "title": {
                "fragments": [],
                "text": "One Distributional Memory, Many Semantic Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 171
                            }
                        ],
                        "text": "\u2026Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5715561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "509a2ca90a85c62d66a16b37e0de28715dd4e89f",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 140,
            "paperAbstract": {
                "fragments": [],
                "text": "Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language-processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure."
            },
            "slug": "Topics-in-semantic-representation.-Griffiths-Steyvers",
            "title": {
                "fragments": [],
                "text": "Topics in semantic representation."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference that leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939590"
                        ],
                        "name": "Paola Merlo",
                        "slug": "Paola-Merlo",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Merlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Merlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584212"
                        ],
                        "name": "S. Stevenson",
                        "slug": "S.-Stevenson",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9680240,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "2cafda848586a9a4061a11227adbaf70720644c8",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence-the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure-specifically, the thematic roles they assign to participants. We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8 accuracy for a task whose baseline is 34, and whose expert-based upper bound we calculate at 86.5. A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs. Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques."
            },
            "slug": "Automatic-Verb-Classification-Based-on-Statistical-Merlo-Stevenson",
            "title": {
                "fragments": [],
                "text": "Automatic Verb Classification Based on Statistical Distributions of Argument Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work reports on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure-specifically, the thematic roles they assign to participants, using linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 135
                            }
                        ],
                        "text": "Given the continuous growth in computational power and the fact that efficient tensor decomposition is a very active area of research (Turney 2007; Kolda and Sun 2008) full tensor decomposition is nevertheless a realistic near future task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 90
                            }
                        ],
                        "text": "Tensors are multi-way arrays, conventionally denoted by boldface Euler script letters: X (Turney 2007; Kolda and Bader 2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Turney (2007) explicitly formalizes the set of corpus-extracted word\u2013link\u2013word triples as a tensor, and was our primary source of inspiration in formalizing DM in these terms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 34
                            }
                        ],
                        "text": "The same conclusion is reached by Turney (2007), who uses essentially the same method (with some differences in implementation) to tackle the TOEFL task, and obtains more than 10% improvement in accuracy with respect to the corresponding raw tensor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "The higher-order tensor dimensionality reduction techniques tested on language data by Turney (2007) and Van de Cruys (2009) can be\nD ow nloaded from http://direct.m it.edu/coli/article-pdf/36/4/673/1810191/coli_a_00016.pdf by guest on 10 Septem ber 2021\napplied to the DM tensors before\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7898033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b88329392a75287942d85f42012f47f356a2714",
            "isKey": true,
            "numCitedBy": 215,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology."
            },
            "slug": "A-Uniform-Approach-to-Analogies,-Synonyms,-and-Turney",
            "title": {
                "fragments": [],
                "text": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A supervised corpus-based machine learning algorithm is introduced for classifying analogous word pairs and it is shown that it can solve multiple-choice SAT analogy questions, TOEFL synonyms questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153584694"
                        ],
                        "name": "R. Rapp",
                        "slug": "R.-Rapp",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Rapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 94
                            }
                        ],
                        "text": "\u2217\u2217 Department of Linguistics T. Bolelli, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1171753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b62fce4fc3476e36545acff0a4d0627326959a53",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In machine translation, information on word ambiguities is usually provided by the lexicographers who construct the lexicon. In this paper we propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of the distributional similarity between the words in a corpus. Our algorithm works as follows: The 20 strongest first-order associations to the ambiguous word are considered as sense descriptor candidates. All pairs of these candidates are ranked according to the following two criteria: First, the two words in a pair should be as dissimilar as possible. Second, although being dissimilar their co-occurrence vectors should add up to the co-occurrence vector of the ambiguous word scaled by two. Both conditions together have the effect that preference is given to pairs whose co-occurring words are complementary. For best results, our implementation uses singular value decomposition, entropy-based weights, and second-order similarity metrics."
            },
            "slug": "Word-sense-discovery-based-on-sense-descriptor-Rapp",
            "title": {
                "fragments": [],
                "text": "Word sense discovery based on sense descriptor dissimilarity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word, based on the statistics of the distributional similarity between the words in a corpus."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154735"
                        ],
                        "name": "A. Badulescu",
                        "slug": "A.-Badulescu",
                        "structuredName": {
                            "firstName": "Adriana",
                            "lastName": "Badulescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Badulescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 822,
                                "start": 91
                            }
                        ],
                        "text": "Our first test pertains to the seven relations between nominals in Task 4 of SEMEVAL 2007 (Girju et al. 2007): Cause\u2013Effect, Instrument\u2013Agency, Product\u2013 Producer, Origin\u2013Entity, Theme\u2013Tool, Part\u2013Whole, Content\u2013Container. For each relation, the data set includes 140 training and about 80 test items. Each item consists of a Web snippet, containing word pairs connected by a certain pattern (e.g., \u201c* causes *\u201d). The retrieved snippets are manually classified by the SEMEVAL organizers as positive or negative instances of a certain relation (see the earlier Cause\u2013Effect examples). About 50% training and test cases are positive instances. In our experiments we do not make use of the contexts of the target word pairs that are provided with the test set. The second data set (NS) comes from Nastase and Szpakowicz (2003). It pertains to"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6219536,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "966f28e435e6a5315e3d7918d3cdf2269f7fa3c8",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part-whole relations in text. First an algorithm is described that identifies lexico-syntactic patterns that encode part-whole relations. A difficulty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part-whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classification rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part-whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns."
            },
            "slug": "Automatic-Discovery-of-Part-Whole-Relations-Girju-Badulescu",
            "title": {
                "fragments": [],
                "text": "Automatic Discovery of Part-Whole Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part-whole relations in text and demonstrates the importance of word sense disambiguation for this task."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364466"
                        ],
                        "name": "B. Dorow",
                        "slug": "B.-Dorow",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Dorow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11473223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "477b4fa10a68e3eb203b412a64a815d108ade680",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word."
            },
            "slug": "A-Graph-Model-for-Unsupervised-Lexical-Acquisition-Widdows-Dorow",
            "title": {
                "fragments": [],
                "text": "A Graph Model for Unsupervised Lexical Acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms and focuses on the symmetric relationship between pairs of nouns which occur together in lists."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711215"
                        ],
                        "name": "U. Pad\u00f3",
                        "slug": "U.-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pad\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19255202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a508ee27bd5fbe8d8a91afdbf0561b30bc7b399",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "Models of human sentence processing have paid much attention to three key characteristics of the sentence processor: Its robust and accurate processing of unseen input (wide coverage), its immediate, incremental interpretation of partial input and its sensitivity to structural frequencies in previous language experience. In this thesis, we propose a model of human sentence processing that accounts for these three characteristics and also models a fourth key characteristic, namely the influence of semantic plausibility on sentence processing. The precondition for such a sentence processing model is a general model of human plausibility intuitions. We therefore begin by presenting a probabilistic model of the plausibility of verb-argument relations, which we estimate as the probability of encountering a verb-argument pair in the relation specified by a thematic role in a role-annotated training corpus. This model faces a significant sparse data problem, which we alleviate by combining two orthogonal smoothing methods. We show that the smoothed model\u2019s predictions are significantly correlated to human plausibility judgements for a range of test sets. We also demonstrate that our semantic plausibility model outperforms selectional preference models and a standard role labeller, which solve tasks from computational linguistics that are related to the prediction of human judgements. We then integrate this semantic plausibility model with an incremental, widecoverage, probabilistic model of syntactic processing to form the Syntax/Semantics (SynSem) Integration model of sentence processing. The SynSem-Integration model combines preferences for candidate syntactic structures from two sources: Syntactic probability estimates from a probabilistic parser and our semantic plausibility model\u2019s estimates of the verb-argument relations in each syntactic analysis. The model uses these preferences to determine a globally preferred structure and predicts difficulty in human sentence processing either if syntactic and semantic preferences conflict, or if the interpretation of the preferred analysis changes non-monotonically. In a thorough evaluation against the patterns of processing difficulty found for four ambiguity phenomena in eight reading-time studies, we demonstrate that the SynSem-Integration model reliably predicts human reading time behaviour."
            },
            "slug": "The-integration-of-syntax-and-semantic-plausibility-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "The integration of syntax and semantic plausibility in a wide-coverage model of human sentence processing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A semantic plausibility model is presented that outperforms selectional preference models and a standard role labeller, which solve tasks from computational linguistics that are related to the prediction of human judgements, and it is demonstrated that the SynSem-Integration model reliably predicts human reading time behaviour."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775745"
                        ],
                        "name": "A. Fazly",
                        "slug": "A.-Fazly",
                        "structuredName": {
                            "firstName": "Afsaneh",
                            "lastName": "Fazly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fazly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584212"
                        ],
                        "name": "S. Stevenson",
                        "slug": "S.-Stevenson",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10024028,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c750610ca9cb80765c4319a38964d54ab3dafa3f",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The lexical status of multiword expressions (MWEs), such as make a decision and shoot the breeze, has long been a matter of debate. Although MWEs behave much like phrases on the surface, it has been argued that they should be treated like words because their components together form a single unit of meaning. However, MWEs are not a homogeneous lexical category, but rather can have distinct semantic and syntactic properties. For example, the overall meaning of an MWE may vary in how much it diverges from the combined contribution of its constituent parts, with make a decision, e.g., having a strong relation to decide, while shoot the breeze is entirely idiomatic. In order to understand whether and how MWEs should be represented in a (computational) lexicon, it is necessary to look into the relationship between the underlying semantic properties of these expressions and their surface behaviour. We examine several properties of MWEs pertaining to their semantic idiosyncrasy, and relate them to the distributional behaviour of MWEs in their actual usages. Accordingly, we propose statistical measures for quantifying each property, which we then use for separating different types of MWEs that require different treatment within a lexicon."
            },
            "slug": "A-distributional-account-of-the-semantics-of-Fazly-Stevenson",
            "title": {
                "fragments": [],
                "text": "A distributional account of the semantics of multiword expressions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines several properties of MWEs pertaining to their semantic idiosyncrasy, and proposes statistical measures for quantifying each property, which are used for separating different types of Mwes that require different treatment within a lexicon."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707726"
                        ],
                        "name": "J. Pustejovsky",
                        "slug": "J.-Pustejovsky",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Pustejovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pustejovsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7129257,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "cae22af9bb9b7a2cebe2b4ee0a0364004ab73491",
            "isKey": false,
            "numCitedBy": 3805,
            "numCiting": 161,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole."
            },
            "slug": "The-Generative-Lexicon-Pustejovsky",
            "title": {
                "fragments": [],
                "text": "The Generative Lexicon"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that lexical decomposition is possible if it is performed generatively and a theory of lexical inheritance is outlined, which provides the necessary principles of global organization for the lexicon, enabling us to fully integrate the authors' natural language lexicon into a conceptual whole."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 62
                            }
                        ],
                        "text": "The latter can be either documents (Landauer and Dumais 1997; Griffiths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the target (Lund and Burgess 1996; Schu\u0308tze 1997; Rapp 2003; Bullinaria and Levy 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1588782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. \n \nWe present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "slug": "A-Structured-Vector-Space-Model-for-Word-Meaning-in-Erk-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "A Structured Vector Space Model for Word Meaning in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel structured vector space model is presented that makes it possible to integrate syntax into the computation of word meaning in context and performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256003"
                        ],
                        "name": "Vivi Nastase",
                        "slug": "Vivi-Nastase",
                        "structuredName": {
                            "firstName": "Vivi",
                            "lastName": "Nastase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivi Nastase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795595"
                        ],
                        "name": "S. Szpakowicz",
                        "slug": "S.-Szpakowicz",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szpakowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "Our first test pertains to the seven relations between nominals in Task 4 of SEMEVAL 2007 (Girju et al. 2007): Cause\u2013Effect, Instrument\u2013Agency, Product\u2013 Producer, Origin\u2013Entity, Theme\u2013Tool, Part\u2013"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 90
                            }
                        ],
                        "text": "Our first test pertains to the seven relations between nominals in Task 4 of SEMEVAL 2007 (Girju et al. 2007): Cause\u2013Effect, Instrument\u2013Agency, Product\u2013 Producer, Origin\u2013Entity, Theme\u2013Tool, Part\u2013Whole, Content\u2013Container."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14624577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1755f2ac7eb8a091f3613be47da844803df86fc1",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text. We present an evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nominals in a sentence. This is part of SemEval, the 4th edition of the semantic evaluation event previously known as SensEval. We define the task, describe the training/test data and their creation, list the participating systems and discuss their results. There were 14 teams who submitted 15 systems."
            },
            "slug": "SemEval-2007-Task-04:-Classification-of-Semantic-Girju-Nakov",
            "title": {
                "fragments": [],
                "text": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nominals in a sentence as part of SemEval, the 4th edition of the semantic evaluation event previously known as SensEval."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "The matricization of the tuple tensor produces both familiar spaces, similar to those commonly used for attributional or relational similarity, and other less known distributional spaces, which will yet prove useful for capturing some interesting semantic phenomena."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2280191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e897104540642698321c106cc9c35af369fe12",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed."
            },
            "slug": "Combining-Symbolic-and-Distributional-Models-of-Clark-Pulman",
            "title": {
                "fragments": [],
                "text": "Combining Symbolic and Distributional Models of Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method is to be adapted from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products, to adapt a method for language processing."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Spring Symposium: Quantum Interaction"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": false,
            "numCitedBy": 5789,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591835"
                        ],
                        "name": "K. Lund",
                        "slug": "K.-Lund",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Framework for Corpus-Based Semantics\nMarco Baroni\u2217\nUniversity of Trento\nAlessandro Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 57
                            }
                        ],
                        "text": "\u2217\u2217 Department of Linguistics T. Bolelli, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 234
                            }
                        ],
                        "text": "Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61090106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4",
            "isKey": false,
            "numCitedBy": 1721,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "slug": "Producing-high-dimensional-semantic-spaces-from-Lund-Burgess",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces from lexical co-occurrence"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word, which provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58810784,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "364d793a1d268e94f30d96a5da77d7ee49eb1d09",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The WordNet lexical database is now quite large and offers broad coverage of general lexical relations in English. As is evident in this volume, WordNet has been employed as a resource for many applications in natural language processing (NLP) and information retrieval (IR). However, many potentially useful lexical relations are currently missing from WordNet. Some of these relations, while useful for NLP and IR applications, are not necessarily appropriate for a general, domain-independent lexical database. For example, WordNet\u2019s coverage of proper nouns is rather sparse, but proper nouns are often very important in application tasks. The standard way lexicographers find new relations is to look through huge lists of concordance lines. However, culling through long lists of concordance lines can be a rather daunting task (Church and Hanks, 1990), so a method that picks out those lines that are very likely to hold relations of interest should be an improvement over more traditional techniques. This chapter describes a method for the automatic discovery of WordNetstyle lexico-semantic relations by searching for corresponding lexico-syntactic patterns in large text collections. Large text corpora are now widely available, and can be viewed as vast resources from which to mine lexical, syntactic, and semantic information. This idea is reminiscent of what is known as \u201cdata mining\u201d in the artificial intelligence literature (Fayyad and Uthurusamy, 1996), however, in this case the ore is raw text rather than tables of numerical data. The Lexico-Syntactic Pattern Extraction (LSPE) method is meant to be useful as an automated or semi-automated aid for lexicographers and builders of domain-dependent knowledge-bases. The LSPE technique is light-weight; it does not require a knowledge base or complex interpretation modules in order to suggest new WordNet relations."
            },
            "slug": "Automated-Discovery-of-WordNet-Relations-Hearst",
            "title": {
                "fragments": [],
                "text": "Automated Discovery of WordNet Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This chapter describes a method for the automatic discovery of WordNetstyle lexico-semantic relations by searching for correspondingLexico-syntactic patterns in large text collections using the Lexico-Syntactic Pattern Extraction (LSPE) method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15109128"
                        ],
                        "name": "D. Vinson",
                        "slug": "D.-Vinson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Vinson",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Vinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2517462"
                        ],
                        "name": "G. Vigliocco",
                        "slug": "G.-Vigliocco",
                        "structuredName": {
                            "firstName": "Gabriella",
                            "lastName": "Vigliocco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vigliocco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32436956,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "50a9fe366d64402ea2db8690564abf950f0c4a89",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic features produced by speakers of a language when given a word corresponding to a concept have provided insight into numerous behavioral phenomena concerning semantic representation in language-impaired and -unimpaired speakers. A number of theories concerning the organization of semantic memory have used features as their starting point. Here, we provide a set of feature norms collected from approximately 280 participants for a total of 456 words (169 nouns referring to objects, 71 nouns referring to events, and 216 verbs referring to events). Whereas a number of feature norms for object concepts already exist, we provide the first set of norms for event concepts. We have used these norms (for both objects and events) in research addressing questions concerning the similarities and differences between the semantic representation of objects and events and in research concerning the interface between semantics and syntax, given that events can be expressed in language as nouns or verbs. Some of this research is summarized here. These norms may be downloaded from www.psychonomic.org/archive."
            },
            "slug": "Semantic-feature-production-norms-for-a-large-set-Vinson-Vigliocco",
            "title": {
                "fragments": [],
                "text": "Semantic feature production norms for a large set of objects and events"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work provides the first set of norms for event concepts, a set of feature norms collected from approximately 280 participants for a total of 456 words, used in research addressing questions concerning the similarities and differences between the semantic representation of objects and events and in research concerning the interface between semantics and syntax."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 26
                            }
                        ],
                        "text": "The latter can be either documents (Landauer and Dumais 1997; Griffiths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the target (Lund and Burgess 1996; Schu\u0308tze 1997; Rapp 2003; Bullinaria and Levy 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2468783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "212d2715aee9fbefe140685b088b789d6c8277b0",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM."
            },
            "slug": "Similarity-of-Semantic-Relations-Turney",
            "title": {
                "fragments": [],
                "text": "Similarity of Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "LRA extends the VSM approach in three ways: the patterns are derived automatically from the corpus, the Singular Value Decomposition (SVD) is used to smooth the frequency data, and automatically generated synonyms are used to explore variations of the word pairs."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 2
                            }
                        ],
                        "text": "Structured DSMs extract a much richer array of distributional information from linguistic input, but they still represent it in the same way as unstructured models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 246
                            }
                        ],
                        "text": "\u2026Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "isKey": false,
            "numCitedBy": 2724,
            "numCiting": 208,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "slug": "From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel",
            "title": {
                "fragments": [],
                "text": "From Frequency to Meaning: Vector Space Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33406957"
                        ],
                        "name": "D. Davidov",
                        "slug": "D.-Davidov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Davidov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Davidov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145009917"
                        ],
                        "name": "A. Rappoport",
                        "slug": "A.-Rappoport",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Rappoport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rappoport"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7636604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa8c11c9e94f4ec55944d6b7c8b531167b4b884c",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many possible different semantic relationships between nominals. Classification of such relationships is an important and difficult task (for example, the well known noun compound classification task is a special case of this problem). We propose a novel pattern clusters method for nominal relationship (NR) classification. Pattern clusters are discovered in a large corpus independently of any particular training set, in an unsupervised manner. Each of the extracted clusters corresponds to some unspecified semantic relationship. The pattern clusters are then used to construct features for training and classification of specific inter-nominal relationships. Our NR classification evaluation strictly follows the ACL SemEval-07 Task 4 datasets and protocol, obtaining an f-score of 70.6, as opposed to 64.8 of the best previous work that did not use the manually provided WordNet sense disambiguation tags."
            },
            "slug": "Classification-of-Semantic-Relationships-between-Davidov-Rappoport",
            "title": {
                "fragments": [],
                "text": "Classification of Semantic Relationships between Nominals Using Pattern Clusters"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a novel pattern clusters method for nominal relationship (NR) classification that strictly follows the ACL SemEval-07 Task 4 datasets and protocol, obtaining an f-score of 70.6, as opposed to 64.8 of the best previous work."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8311581"
                        ],
                        "name": "Diarmuid \u00d3 S\u00e9aghdha",
                        "slug": "Diarmuid-\u00d3-S\u00e9aghdha",
                        "structuredName": {
                            "firstName": "Diarmuid",
                            "lastName": "S\u00e9aghdha",
                            "middleNames": [
                                "\u00d3"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diarmuid \u00d3 S\u00e9aghdha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15379653"
                        ],
                        "name": "Ann A. Copestake",
                        "slug": "Ann-A.-Copestake",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Copestake",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann A. Copestake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 36
                            }
                        ],
                        "text": "The second data set (NS) comes from Nastase and Szpakowicz (2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 958931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5d65d20b182400bbfd7dbbb11a2a714000096d7",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Many methods are available for computing semantic similarity between individual words, but certain NLP tasks require the comparison of word pairs. This paper presents a kernel-based framework for application to relational reasoning tasks of this kind. The model presented here combines information about two distinct types of word pair similarity: lexical similarity and relational similarity. We present an efficient and flexible technique for implementing relational similarity and show the effectiveness of combining lexical and relational models by demonstrating state-of-the-art results on a compound noun interpretation task."
            },
            "slug": "Using-Lexical-and-Relational-Similarity-to-Classify-S\u00e9aghdha-Copestake",
            "title": {
                "fragments": [],
                "text": "Using Lexical and Relational Similarity to Classify Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a kernel-based framework for application to relational reasoning tasks of this kind and shows the effectiveness of combining lexical and relational models by demonstrating state-of-the-art results on a compound noun interpretation task."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7965906"
                        ],
                        "name": "Sabine Schulte im Walde",
                        "slug": "Sabine-Schulte-im-Walde",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Schulte im Walde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Schulte im Walde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12997801,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9546f1131ee21086f7f4d222f03258c016401183",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 164,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents clustering experiments on German verbs: A statistical grammar model for German serves as the source for a distributional verb description at the lexical syntax-semantics interface, and the unsupervised clustering algorithm k-means uses the empirical verb properties to perform an automatic induction of verb classes. Various evaluation measures are applied to compare the clustering results to gold standard German semantic verb classes under different criteria. The primary goals of the experiments are (1) to empirically utilize and investigate the well-established relationship between verb meaning and verb behavior within a cluster analysis and (2) to investigate the required technical parameters of a cluster analysis with respect to this specific linguistic task. The clustering methodology is developed on a small-scale verb set and then applied to a larger-scale verb set including 883 German verbs."
            },
            "slug": "Experiments-on-the-Automatic-Induction-of-German-Walde",
            "title": {
                "fragments": [],
                "text": "Experiments on the Automatic Induction of German Semantic Verb Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This article presents clustering experiments on German verbs: a statistical grammar model for German serves as the source for a distributional verb description at the lexical syntax-semantics interface, and the unsupervised clustering algorithm k-means uses the empirical verb properties to perform an automatic induction of verb classes."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642122"
                        ],
                        "name": "Amac Herdagdelen",
                        "slug": "Amac-Herdagdelen",
                        "structuredName": {
                            "firstName": "Amac",
                            "lastName": "Herdagdelen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amac Herdagdelen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 262
                            }
                        ],
                        "text": "\u2026requires the links to extract typical subjects and objects, so we cannot use DV nor Win), results from Pado\u0301, Pado\u0301, and Erk (2007) (ParCos is the best among their purely corpus-based systems), and the performance on the Pado\u0301 data set of the supervised system of Herdag\u030cdelen and Baroni (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "A first step in this direction, within a framework similar to Turney\u2019s, was taken by Herdag\u030cdelen and Baroni (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 109
                            }
                        ],
                        "text": "Table 5 reports results (percentage accuracies) on the TOEFL set for our models as well as the best model of Herdag\u030cdelen and Baroni (2009) and the corpus-based models from the ACL Wiki TOEFL state-of-the-art table (we do not include those models from the Wiki that resort to other knowledge\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9612525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28a4aeca6b6fdc55eb3fab9ffe1a9e08d8dc9dff",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring."
            },
            "slug": "BagPack:-A-general-framework-to-represent-semantic-Herdagdelen-Baroni",
            "title": {
                "fragments": [],
                "text": "BagPack: A general framework to represent semantic relations"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33406957"
                        ],
                        "name": "D. Davidov",
                        "slug": "D.-Davidov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Davidov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Davidov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145009917"
                        ],
                        "name": "A. Rappoport",
                        "slug": "A.-Rappoport",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Rappoport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rappoport"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8078270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49fae1d85dc435e45dc118247cfa5d75eb197b30",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel framework for the discovery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identified with a cluster of patterns that captures this relationship. We give a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words. Pattern clusters can be used to extract instances of the corresponding relationships. To assess the quality of discovered relationships, we use the pattern clusters to automatically generate SAT analogy questions. We also compare to a set of known relationships, achieving very good results in both methods. The evaluation (done in both English and Russian) substantiates the premise that our pattern clusters indeed reflect relationships perceived by humans."
            },
            "slug": "Unsupervised-Discovery-of-Generic-Relationships-and-Davidov-Rappoport",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words, substantiates the premise that the pattern clusters indeed reflect relationships perceived by humans."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948254"
                        ],
                        "name": "J. Bullinaria",
                        "slug": "J.-Bullinaria",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bullinaria",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bullinaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053576"
                        ],
                        "name": "J. Levy",
                        "slug": "J.-Levy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 213
                            }
                        ],
                        "text": "Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1025306,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f76807536e7f6542a72609929a9630de802a597f",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures."
            },
            "slug": "Extracting-semantic-representations-from-word-A-Bullinaria-Levy",
            "title": {
                "fragments": [],
                "text": "Extracting semantic representations from word co-occurrence statistics: A computational study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics and finds that, once the best procedures are identified, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885890"
                        ],
                        "name": "T. V. D. Cruys",
                        "slug": "T.-V.-D.-Cruys",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Cruys",
                            "middleNames": [
                                "V.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. D. Cruys"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "The higher-order tensor dimensionality reduction techniques tested on language data by Turney (2007) and Van de Cruys (2009) can be\nD ow nloaded from http://direct.m it.edu/coli/article-pdf/36/4/673/1810191/coli_a_00016.pdf by guest on 10 Septem ber 2021\napplied to the DM tensors before\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Van de Cruys (2009) used tensor decomposition to find commonalities in latent dimensions across the fiber labels (in the DM formalism, this would amount to finding commonalities across w1, l, and w2 elements)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7771877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89645fff935867e3eb06b3899d6fc38688ced9ca",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity. Until now, most algorithms use two-way co-occurrence data to compute the meaning of words. Co-occurrence frequencies, however, need not be pairwise. One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond. This paper will investigate tensor factorization methods to build a model of three-way co-occurrences. The approach is applied to the problem of selectional preference induction, and automatically evaluated in a pseudo-disambiguation task. The results show that tensor factorization, and non-negative tensor factorization in particular, is a promising tool for Natural Language Processing (nlp)."
            },
            "slug": "A-non-negative-tensor-factorization-model-for-Cruys",
            "title": {
                "fragments": [],
                "text": "A non-negative tensor factorization model for selectional preference induction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Tensor factorization methods are investigated to build a model of three-way co-occurrence frequencies of three modes and beyond and are shown to be a promising tool for Natural Language Processing (nlp)."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465286"
                        ],
                        "name": "Timothy Baldwin",
                        "slug": "Timothy-Baldwin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Baldwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686223"
                        ],
                        "name": "Valia Kordoni",
                        "slug": "Valia-Kordoni",
                        "structuredName": {
                            "firstName": "Valia",
                            "lastName": "Kordoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valia Kordoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145585242"
                        ],
                        "name": "Aline Villavicencio",
                        "slug": "Aline-Villavicencio",
                        "structuredName": {
                            "firstName": "Aline",
                            "lastName": "Villavicencio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aline Villavicencio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 140
                            }
                        ],
                        "text": "The L\u00d7W1W2 space supports tasks where we are directly interested in the links as an object of study\u2014for example, characterizing prepositions (Baldwin, Kordoni, and Villavicencio 2009) or measuring the relative similarity of different kinds of verb\u2013noun relations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6602375,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "52341e9dc9d1b2f4ea967357e3c5ad9d320b2228",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 258,
            "paperAbstract": {
                "fragments": [],
                "text": "Prepositions1\u2014as well as prepositional phrases (PPs) and markers of various sorts\u2014 have a mixed history in computational linguistics (CL), as well as related fields such as artificial intelligence, information retrieval (IR), and computational psycholinguistics: On the one hand they have been championed as being vital to precise language understanding (e.g., in information extraction), and on the other they have been ignored on the grounds of being syntactically promiscuous and semantically vacuous, and relegated to the ignominious rank of \u201cstop word\u201d (e.g., in text classification and IR). Although NLP in general has benefitted from advances in those areas where prepositions have received attention, there are still many issues to be addressed. For example, in machine translation, generating a preposition (or \u201ccase marker\u201d in languages such as Japanese) incorrectly in the target language can lead to critical semantic divergences over the source language string. Equivalently in information retrieval and information extraction, it would seem desirable to be able to predict that book on NLP and book about NLPmean largely the same thing, but paranoid about drugs and paranoid on drugs suggest very different things. Prepositions are often among the most frequent words in a language. For example, based on the British National Corpus (BNC; Burnard 2000), four out of the top-ten most-frequent words in English are prepositions (of, to, in, and for). In terms of both parsing and generation, therefore, accurate models of preposition usage are essential to avoid repeatedly making errors. Despite their frequency, however, they are notoriously difficult to master, even for humans (Chodorow, Tetreault, and Han 2007). For example, Lindstromberg (2001) estimates that less than 10% of upper-level English as a Second"
            },
            "slug": "Prepositions-in-Applications:-A-Survey-and-to-the-Baldwin-Kordoni",
            "title": {
                "fragments": [],
                "text": "Prepositions in Applications: A Survey and Introduction to the Special Issue"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Although NLP in general has benefitted from advances in those areas where prepositions have received attention, there are still many issues to be addressed and accurate models of preposition usage are essential to avoid repeatedly making errors."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15763200,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "dbfd191afbbc8317577cbc44afe7156df546e143",
            "isKey": false,
            "numCitedBy": 3648,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested."
            },
            "slug": "Automatic-Acquisition-of-Hyponyms-from-Large-Text-Hearst",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Hyponyms from Large Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest are identified."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2173454"
                        ],
                        "name": "Alessandra Zarcone",
                        "slug": "Alessandra-Zarcone",
                        "structuredName": {
                            "firstName": "Alessandra",
                            "lastName": "Zarcone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandra Zarcone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15423966,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "cdbaefc0e56ab10bf5b60fd794ecbafa47681784",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Verb lexical semantic properties are only one of the factors that contribute to the determination of the event type expressed by a sentence, which is instead the result of a complex interplay between the verb meaning and its linguistic context. We report on two computational models for the automatic identification of event type in Italian. Both models use linguistically-motivated features extracted from Italian corpora. The main goal of our experiments is to evaluate the contribution of different types of linguistic indicators to identify the event type of a sentence, as well as to model various cases of context-driven event type shift. In the first model, event type identification has been modelled as a supervised classification task, performed with Maximum Entropy classifiers. In the second model, Self-Organizing Maps have been used to define and identify event types in an unsupervised way. The interaction of various contextual factors in determining the event type expressed by a sentence makes event type identification a highly challenging task. Computational models can help us to shed new light on the real structure of event type classes as well as to gain a better understanding of context-driven semantic shifts."
            },
            "slug": "Computational-Models-for-Event-Type-Classification-Zarcone-Lenci",
            "title": {
                "fragments": [],
                "text": "Computational Models for Event Type Classification in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two computational models for the automatic identification of event type in Italian are reported on, using linguistically-motivated features extracted from Italian corpora to shed new light on the real structure of event types and to gain a better understanding of context-driven semantic shifts."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5509836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e517e1645708e7b050787bb4734002ea194a1958",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing)."
            },
            "slug": "Mining-the-Web-for-Synonyms:-PMI-IR-versus-LSA-on-Turney",
            "title": {
                "fragments": [],
                "text": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL"
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111327888"
                        ],
                        "name": "Michael N. Jones",
                        "slug": "Michael-N.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael N. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821878"
                        ],
                        "name": "D. Mewhort",
                        "slug": "D.-Mewhort",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Mewhort",
                            "middleNames": [
                                "J.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mewhort"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7819391,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d3cf28ab36ff7f7601a55c1e832736b2473a07f0",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 164,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language. The model uses simple convolution and superposition mechanisms (cf. B. B. Murdock, 1982) to learn distributed holographic representations for words. The structure of the resulting lexicon can account for empirical data from classic experiments studying semantic typicality, categorization, priming, and semantic constraint in sentence completions. Furthermore, order information can be retrieved from the holographic representations, allowing the model to account for limited word transitions without the need for built-in transition rules. The model demonstrates that a broad range of psychological data can be accounted for directly from the structure of lexical representations learned in this way, without the need for complexity to be built into either the processing mechanisms or the representations. The holographic representations are an appropriate knowledge representation to be used by higher order models of language comprehension, relieving the complexity required at the higher level."
            },
            "slug": "Representing-word-meaning-and-order-information-in-Jones-Mewhort",
            "title": {
                "fragments": [],
                "text": "Representing word meaning and order information in a composite holographic lexicon."
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language demonstrates that a broad range of psychological data can be accounted for directly from the structure of lexical representations learned in this way, without the need for complexity to be built into either the processing mechanisms or the representations."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12044606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c69a90236f1c57348de858918c554a9420f1521",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic vector models have proven their worth in a number of natural language applications whose goals can be accomplished by modelling individual semantic concepts and measuring similarities between them. By comparison, the area of semantic compositionality in these models has so far remained underdeveloped. This will be a crucial hurdle for semantic vector models: in order to play a fuller part in the modelling of human language, these models will need some way of modelling the way in which single concepts are put together to form more complex conceptual structures. This paper explores some of the opportunities for using vector product operations to model compositional phenomena in natural language. These vector operations are all well-known and used in mathematics and physics, particularly in quantum mechanics. Instead of designing new vector composition operators, this paper gathers a list of existing operators, and a list of typical composition operations in natural language, and describes two small experiments that begin to investigate the use of certain vector operators to model certain"
            },
            "slug": "Semantic-Vector-Products:-Some-Initial-Widdows",
            "title": {
                "fragments": [],
                "text": "Semantic Vector Products: Some Initial Investigations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper gathers a list of existing operators, an list of typical composition operations in natural language, and describes two small experiments that begin to investigate the use of certain vector operators to model certain compositional phenomena innatural language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15089423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12afb4e89aefd688f8c73d52b44cebca49221f3e",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space Will Lowe (wlowe02@tufts.edu) Center for Cognitive Studies Tufts University; MA 21015 USA Abstract This paper adds some theory to the growing literature of semantic space models. We motivate semantic space models from the perspective of distributional linguistics and show how an explicit mathematical formulation can provide a better understanding of existing models and suggest changes and improvements. In addition to pro- viding a theoretical framework for current models, we consider the implications of statistical aspects of language data that have not been addressed in the psychological modeling literature. Statistical approaches to language must deal principally with count data, and this data will typically have a highly skewed frequency distribution due to Zipf\u2019s law. We consider the consequences of these facts for the construction of semantic space models, and present methods for removing frequency biases from se- mantic space models. Introduction There is a growing literature on the empirical adequacy of semantic space models across a wide range of sub- ject domains (Burgess et al., 1998; Landauer et al., 1998; Foltz et al., 1998; McDonald and Lowe, 1998; Lowe and McDonald, 2000). However, semantic space mod- els are typically structured and parameterized differently by each researcher. Levy and Bullinaria (2000) have ex- plored the implications of parameter changes empirically by running multiple simulations, but there has up until now been no work that places semantic space models in an overarching theoretical framework; consequently there there are few statements of how semantic spaces ought to be structured in the light of their intended pur- pose. In this paper we attempt to develop a theoretical framework for semantic space models by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search. The structure of the paper is as follows. The next sec- tion brie\u00a4y motivates semantic space models using ideas from distributional linguistics. We then review Zipf\u2019s law and its consequences the distributional character of linguistic data. The \u00a3nal section presents a formal de\u00a3- nition of semantic space models and considers what ef- fects different choices of component have on the result- ing models. Motivating Semantic Space Firth (1968) observed that \u201cyou shall know a word by the company it keeps\u201d. If we interpret company as lex- ical company, the words that occur near to it in text or speech, then two related claims are possible. The \u00a3rst is unexceptional: we come to know about the syntactic character of a word by examining the other words that may and may not occur around it in text. Syntactic theory then postulates latent variables e.g. parts of speech and branching structure, that control the distributional prop- erties of words and restrictions on their contexts of occur- rence. The second claim is that we come to know about the semantic character of a word by examining the other words that may and may not occur around it in text. The intuition for this distributional characterization of semantics is that whatever makes words similar or dis- similar in meaning, it must show up distributionally, in the lexical company of the word. Otherwise the suppos- edly semantic difference is not available to hearers and it is not easy to see how it may be learned. If words are similar to the extent that they occur in the similar contexts then we may de\u00a3ne a statistical re- placement test (Finch, 1993) which tests the meaning- fulness of the result of switching one word for another in a sentence. When a corpus of meaningful sentences is available the test may be reversed (Lowe, 2000a), and un- der a suitable representation of lexical context, we may hold each word constant and estimate its typical sur- rounding context. A semantic space model is a way of representing similarity of typical context in a Euclidean space with axes determined by local word co-occurrence counts. Counting the co-occurrence of a target word with a \u00a3xed set of D other words makes it possible to position the target in a space of dimension D. A target\u2019s position with respect to other words then expresses similarity of lexical context. Since the basic notion from distributional linguistics is \u2018intersubstitutability in context\u2019, a semantic space model is effective to the extent it realizes this idea accurately. Zipf\u2019s Law The frequency of a word is (approximately) proportional to the reciprocal of its rank in a frequency list (Zipf, 1949; Mandelbrot, 1954). This is Zipf\u2019s Law. Zipf\u2019s law ensures dramatically skewed distributions for almost"
            },
            "slug": "Towards-a-Theory-of-Semantic-Space-Lowe",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for semantic space models is developed by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748977"
                        ],
                        "name": "P. Cimiano",
                        "slug": "P.-Cimiano",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Cimiano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cimiano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39768781"
                        ],
                        "name": "Johanna Wenderoth",
                        "slug": "Johanna-Wenderoth",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Wenderoth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna Wenderoth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "D ow nloaded from http://direct.m it.edu/coli/article-pdf/36/4/673/1810191/coli_a_00016.pdf by guest on 10 Septem ber 2021"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3096172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bac0f1497d6f4e32aa4cc3252529fd0132613a0",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach for the automatic acquisition of qualia structures for nouns from the Web and thus opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale. The approach builds on earlier work based on the idea of matching specific lexico-syntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. In our approach, the qualia elements are actually ranked for each qualia role with respect to some measure. The specific contribution of the paper lies in the extensive analysis and quantitative comparison of different measures for ranking the qualia elements. Further, for the first time, we present a quantitative evaluation of such an approach for learning qualia structures with respect to a handcrafted gold standard."
            },
            "slug": "Automatic-Acquisition-of-Ranked-Qualia-Structures-Cimiano-Wenderoth",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Ranked Qualia Structures from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The approach builds on earlier work based on the idea of matching specific lexico-syntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines and opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349066"
                        ],
                        "name": "Yves Peirsman",
                        "slug": "Yves-Peirsman",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Peirsman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Peirsman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754574"
                        ],
                        "name": "D. Speelman",
                        "slug": "D.-Speelman",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Speelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Speelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7281230,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "398dca36536aaa1854bf8352b67c32b24bd2f3d4",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In the recognition of words that are typical of a specific language variety, the classic keyword approach performs rather poorly. We show how this keyword analysis can be complemented with a word space model constructed on the basis of two corpora: one representative of the language variety under investigation, and a reference corpus. This combined approach is able to recognize the markers of a language variety as words that not only have a significantly higher frequency as compared to the reference corpus, but also a different distribution. The application of word space models moreover makes it possible to automatically discover the lexical alternative to a specific marker in the reference corpus."
            },
            "slug": "Word-Space-Models-of-Lexical-Variation-Peirsman-Speelman",
            "title": {
                "fragments": [],
                "text": "Word Space Models of Lexical Variation"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work shows how this keyword analysis can be complemented with a word space model constructed on the basis of two corpora: one representative of the language variety under investigation, and a reference corpus, which makes it possible to automatically discover the lexical alternative to a specific marker in the reference corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18597583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5d67d1dc671bce42a9daac0c3605adb3fcfc697",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "slug": "Vector-based-Models-of-Semantic-Composition-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Vector-based Models of Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Under this framework, a wide range of composition models are introduced which are evaluated empirically on a sentence similarity task and demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17228581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ef1b0469b43acc8ede2e56d8f001ad090b04826",
            "isKey": false,
            "numCitedBy": 500,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Word space models enjoy considerable attention in current research on semantic indexing. Most notably, Latent Semantic Analysis/Indexing (LSA/LSI; Deerwester et al., 1990, Landauer & Dumais, 1997) has become a household name in information access research, and deservedly so; LSA has proven its mettle in numerous applications, and has more or less spawned an entire research field since its introduction around 1990. Today, there is a rich flora of word space models available, and there are numerous publications that report exceptional results in many different applications, including information retrieval (Dumais et al., 1988), word sense disambiguation (Schutze, 1993), various semantic knowledge tests (Lund et al., 1995, Karlgren & Sahlgren, 2001), and text categorization (Sahlgren & Karlgren, 2004). This paper introduces the Random Indexing word space approach, which presents an efficient, scalable and incremental alternative to standard word space methods. The paper is organized as follows: in the next section, we review the basic word space methodology. We then look at some of the problems that are inherent in the basic methodology, and also review some of the solutions that have been proposed in the literature. In the final section, we introduce the Random Indexing word space approach, and briefly review some of the experimental results that have been achieved with Random Indexing."
            },
            "slug": "An-Introduction-to-Random-Indexing-Sahlgren",
            "title": {
                "fragments": [],
                "text": "An Introduction to Random Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Random Indexing word space approach is introduced, which presents an efficient, scalable and incremental alternative to standard word space methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": false,
            "numCitedBy": 13578,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647414"
                        ],
                        "name": "T. Rogers",
                        "slug": "T.-Rogers",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Rogers",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 140
                            }
                        ],
                        "text": "However, we extend and generalize this assumption, by proposing that, once they are formalized as a threeway tensor, tuples can become the backbone of a unified model for distributional semantics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2979028,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c84f76770b820d69a6a1f3914a1c84e7c20a8271",
            "isKey": false,
            "numCitedBy": 1006,
            "numCiting": 357,
            "paperAbstract": {
                "fragments": [],
                "text": "This groundbreaking monograph offers a mechanistic theory of the representation and use of semantic knowledge, integrating the strengths and overcoming many of the weaknesses of hierarchical, categorization-based approaches, similarity-based approaches, and the approach often called \"theory theory.\" Building on earlier models by Geoffrey Hinton in the 1980s and David Rumelhart in the early 1990s, the authors propose that performance in semantic tasks arises through the propagation of graded signals in a system of interconnected processing units. The representations used in performing these tasks are patterns of activation across units, governed by weighted connections among them. Semantic knowledge is acquired through the gradual adjustment of the strengths of these connections in the course of day-to-day experience. The authors show how a simple computational model proposed by Rumelhart exhibits a progressive differentiation of conceptual knowledge, paralleling aspects of cognitive development seen in the work of Frank Keil and Jean Mandler. The authors extend the model to address aspects of conceptual knowledge acquisition in infancy, disintegration of conceptual knowledge in dementia, \"basic-level\" effects and their interaction with expertise, and many findings introduced to support the idea that semantic cognition is guided by naive, domain-specific theories."
            },
            "slug": "Semantic-Cognition:-A-Parallel-Distributed-Approach-Rogers-McClelland",
            "title": {
                "fragments": [],
                "text": "Semantic Cognition: A Parallel Distributed Processing Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors propose that performance in semantic tasks arises through the propagation of graded signals in a system of interconnected processing units, and show how a simple computational model proposed by Rumelhart exhibits a progressive differentiation of conceptual knowledge, paralleling aspects of cognitive development seen in the work of Frank Keil and Jean Mandler."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091160"
                        ],
                        "name": "B. Levin",
                        "slug": "B.-Levin",
                        "structuredName": {
                            "firstName": "Beth",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Levin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62585813,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6cbc1eb25f4ab29a613418b3b0740e74141a0f17",
            "isKey": false,
            "numCitedBy": 3246,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, \"English Verb Classes and Alternations\" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."
            },
            "slug": "English-Verb-Classes-and-Alternations:-A-Levin",
            "title": {
                "fragments": [],
                "text": "English Verb Classes and Alternations: A Preliminary Investigation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Beth Levin shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 86680084,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "decd9bc0385612bdf936928206d83730718e737e",
            "isKey": false,
            "numCitedBy": 2644,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "For the purposes of the present discussion, the term structure will be used in the following non-rigorous sense: A set of phonemes or a set of data is structured in respect to some feature, to the extent that we can form in terms of that feature some organized system of statements which describes the members of the set and their interrelations (at least up to some limit of complexity). In this sense, language can be structured in respect to various independent features. And whether it is structured (to more than a trivial extent) in respect to, say, regular historical change, social intercourse, meaning, or distribution - or to what extent it is structured in any of these respects - is a matter decidable by investigation. Here we will discuss how each language can be described in terms of a distributional structure, i.e. in terms of the occurrence of parts (ultimately sounds) relative to other parts, and how this description is complete without intrusion of other features such as history or meaning. It goes without saying that other studies of language - historical, psychological, etc.-are also possible, both in relation to distributional structure and independently of it."
            },
            "slug": "Distributional-Structure-Harris",
            "title": {
                "fragments": [],
                "text": "Distributional Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This discussion will discuss how each language can be described in terms of a distributional structure, i.e. in Terms of the occurrence of parts relative to other parts, and how this description is complete without intrusion of other features such as history or meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816914"
                        ],
                        "name": "Einat Minkov",
                        "slug": "Einat-Minkov",
                        "structuredName": {
                            "firstName": "Einat",
                            "lastName": "Minkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Einat Minkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7217671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2b9b0d7afd85c5d708b79a61d9a000c6c906d8c",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a parsed text corpus as an instance of a labelled directed graph, where nodes represent words and weighted directed edges represent the syntactic relations between them. We show that graph walks, combined with existing techniques of supervised learning, can be used to derive a task-specific word similarity measure in this graph. We also propose a new path-constrained graph walk method, in which the graph walk process is guided by high-level knowledge about meaningful edge sequences (paths). Empirical evaluation on the task of named entity coordinate term extraction shows that this framework is preferable to vector-based models for small-sized corpora. It is also shown that the path-constrained graph walk algorithm yields both performance and scalability gains."
            },
            "slug": "Learning-Graph-Walk-Based-Similarity-Measures-for-Minkov-Cohen",
            "title": {
                "fragments": [],
                "text": "Learning Graph Walk Based Similarity Measures for Parsed Text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new path-constrained graph walk method is proposed, in which the graph walk process is guided by high-level knowledge about meaningful edge sequences (paths), which shows that this framework is preferable to vector-based models for small-sized corpora."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2237884"
                        ],
                        "name": "Ergun Bi\u00e7ici",
                        "slug": "Ergun-Bi\u00e7ici",
                        "structuredName": {
                            "firstName": "Ergun",
                            "lastName": "Bi\u00e7ici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ergun Bi\u00e7ici"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14009721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d20611a45574733b6efc408757538bcff6818727",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We focus on answering word analogy questions by using clustering techniques. The increased performance in answering word similarity questions can have many possible applications, including question answering and information retrieval. We present an analysis of clustering algorithms\u2019 performance on answering word similarity questions. This paper\u2019s contributions can be summarized as: (i) casting the problem of solving word analogy questions as an instance of learning clusterings of data and measuring the effectiveness of prominent clustering techniques in learning semantic relations; (ii) devising a heuristic approach to combine the results of different clusterings for the purpose of distinctly separating word pair semantics; (iii) answering SAT-type word similarity questions using our technique."
            },
            "slug": "Clustering-Word-Pairs-to-Answer-Analogy-Questions-Bi\u00e7ici",
            "title": {
                "fragments": [],
                "text": "Clustering Word Pairs to Answer Analogy Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper presents an analysis of clustering algorithms\u2019 performance on answering word similarity questions and proposes a heuristic approach to combine the results of different clusterings for the purpose of distinctly separating word pair semantics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680410"
                        ],
                        "name": "E. Terra",
                        "slug": "E.-Terra",
                        "structuredName": {
                            "firstName": "Egidio",
                            "lastName": "Terra",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Terra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751287"
                        ],
                        "name": "C. Clarke",
                        "slug": "C.-Clarke",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Clarke",
                            "middleNames": [
                                "L.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Clarke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16497823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc74c850257ea516e3ac32ec30c72d56d80e53a6",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical measures of word similarity have application in many areas of natural language processing, such as language modeling and information retrieval. We report a comparative study of two methods for estimating word co-occurrence frequencies required by word similarity measures. Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures. We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word. For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context. Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets."
            },
            "slug": "Frequency-Estimates-for-Statistical-Word-Similarity-Terra-Clarke",
            "title": {
                "fragments": [],
                "text": "Frequency Estimates for Statistical Word Similarity Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A comparative study of two methods for estimating word co-occurrence frequencies required by word similarity measures, generated from a terabyte-sized corpus of Web data."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135004"
                        ],
                        "name": "K. Schuler",
                        "slug": "K.-Schuler",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Schuler",
                            "middleNames": [
                                "Kipper"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schuler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46420490,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2627f54745ed191ff921b9ddc818883a846d2994",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Argument Realization presents a thorough survey of current theories that deal with the relationship between verbs and their arguments. It addresses the problem of argument realization in its many aspects such as semantic roles, lexical semantic representation, conceptualizations of events, thematic hierarchies, and verb alternations. This is a linguistics book and therefore geared toward researchers in syntax and semantics. Nevertheless , this extensive and clearly presented body of work by Levin and Rappaport Hovav deserves a prominent place in computational linguistics libraries. The book is organized into three broad parts. The first part reviews current theories of semantic roles, lexical semantic representation, and event structures. The second part focuses on the mapping from lexical semantics to syntax and on thematic hierarchies. The last part examines perspectives on multiple argument realization. Chapter 1 presents a good survey of linguistic theories and their different ways of explaining syntactic similarities in language. It discusses proposed strategies of isolating semantic components of verbs and the fact that these meaning components cannot be taken in isolation to explain syntactic behavior. The chapter concentrates on the problems that must be taken into account when developing a theory of argument realization and a related theory of lexical semantic representation. It discusses the importance of viewing verb meanings as construals of happenings and explores the variety and uniformity in argument realization through cross-linguistic examples showing that argument realization involves more than just the agent and patient roles. Chapter 2 introduces work on semantic role lists and examines in detail their properties. This chapter includes a discussion on the limitations of semantic role list representations, in particular: (1) the difficulty of determining the right granularity for defining semantic roles; (2) the lack of internal structure of the typical semantic role inventory and the fact that cross-linguistic examples attest similarities between certain roles (e.g., patient and recipients, goals and benefactives); and (3) the problem of assigning a single instance of each role per clause and, if this constraint is relaxed, why only some pairings of roles are possible. Even with these limitations, Levin and Rappaport Hovav still maintain the notion that semantic roles are an appropriate basis for a lexical semantic representation. This chapter also reviews proposed efforts to solve these limitations through decomposition of semantic roles into binary features, and by allowing arguments to be assigned more than one semantic role. Chapter 3 reviews other current notions of semantic \u2026"
            },
            "slug": "Argument-Realization-Schuler",
            "title": {
                "fragments": [],
                "text": "Argument Realization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This is a linguistics book and therefore geared toward researchers in syntax and semantics, and this extensive and clearly presented body of work by Levin and Rappaport Hovav deserves a prominent place in computational linguistics libraries."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695574"
                        ],
                        "name": "T. Veale",
                        "slug": "T.-Veale",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Veale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Veale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058332"
                        ],
                        "name": "Yanfen Hao",
                        "slug": "Yanfen-Hao",
                        "structuredName": {
                            "firstName": "Yanfen",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanfen Hao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Many avenues are currently being explored in corpus-based semantics, and interesting synergies are emerging with research areas such as neural systems (Smolensky 1990; Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts and Czachor 2004; Widdows 2004;  Van Rijsbergen 2004;  Bruza and Cole 2005; Hou and Song 2009), holographic models of memory (Jones and Mewhort 2007), and so on. A core problem in dealing with ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202039,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "cefd62f862ddd5d44687c97ccc093cf1a5377885",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Many of the beliefs that one uses to reason about everyday entities and events are neither strictly true or even logically consistent. Rather, people appear to rely on a large body of folk knowledge in the form of stereotypical associations, cliches and other kinds of naturalistic descriptions, many of which express views of the world that are second-hand, overly-simplified and, in some cases, non-literal to the point of being poetic. These descriptions pervade our language yet one rarely finds them in authoritative linguistic resources like dictionaries and encyclopaedias. We describe here how such naturalistic descriptions can be harvested from the web in the guise of explicit similes and related text patterns, and empirically demonstrate that these descriptions do broadly capture the way people see the world, at least from the perspective of category organization in an ontology."
            },
            "slug": "Acquiring-Naturalistic-Concept-Descriptions-from-Veale-Hao",
            "title": {
                "fragments": [],
                "text": "Acquiring Naturalistic Concept Descriptions from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is described here how such naturalistic descriptions can be harvested from the web in the guise of explicit similes and related text patterns, and empirically demonstrate that these descriptions do broadly capture the way people see the world, at least from the perspective of category organization in an ontology."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2736974"
                        ],
                        "name": "Abdulrahman Almuhareb",
                        "slug": "Abdulrahman-Almuhareb",
                        "structuredName": {
                            "firstName": "Abdulrahman",
                            "lastName": "Almuhareb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdulrahman Almuhareb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678591"
                        ],
                        "name": "Massimo Poesio",
                        "slug": "Massimo-Poesio",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Poesio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimo Poesio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10857847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f7348c529944edff40be4a3858a4f0467dad0bc",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work, we found that a great deal of information about noun attributes can be extracted from the Web using simple text patterns, and that enriching vector-based models of concepts with this information about attributes led to drastic improvements in noun categorization. We extend this previous work in two ways: (i) by comparing concept descriptions extracted using patterns with descriptions extracted with a parser, and (ii) by developing an improved dataset balanced with respect to ambiguity, frequency, and WordNet unique beginners."
            },
            "slug": "Concept-Learning-and-Categorization-from-the-Web-Almuhareb-Poesio",
            "title": {
                "fragments": [],
                "text": "Concept Learning and Categorization from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work extends previous work by comparing concept descriptions extracted using patterns with descriptions extracted with a parser, and developing an improved dataset balanced with respect to ambiguity, frequency, and WordNet unique beginners."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711215"
                        ],
                        "name": "U. Pad\u00f3",
                        "slug": "U.-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10375802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fc56fac2ab7ac0554d2d1e2569e48bf3b259442",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves significant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone."
            },
            "slug": "Flexible,-Corpus-Based-Modelling-of-Human-Pad\u00f3-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "Flexible, Corpus-Based Modelling of Human Plausibility Judgements"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A completely corpus-driven model is obtained for this task which rivals or exceeds deeper, resource-driven models while exhibiting higher coverage and can be combined with deeper models to obtain better predictions than from either model alone."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729490"
                        ],
                        "name": "W. Charles",
                        "slug": "W.-Charles",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Charles",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Charles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145580646,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "402627e4eb8c95e4aae3026fd921aa08cd792006",
            "isKey": false,
            "numCitedBy": 1678,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be."
            },
            "slug": "Contextual-correlates-of-semantic-similarity-Miller-Charles",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of semantic similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144967725"
                        ],
                        "name": "J. Quesada",
                        "slug": "J.-Quesada",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Quesada",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Quesada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103940467"
                        ],
                        "name": "Walter Kinstich",
                        "slug": "Walter-Kinstich",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kinstich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Kinstich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2978506"
                        ],
                        "name": "Praful Mangalath",
                        "slug": "Praful-Mangalath",
                        "structuredName": {
                            "firstName": "Praful",
                            "lastName": "Mangalath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Praful Mangalath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18322403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2fda5af92b78650490680e2a34e4b73fbcc3ec0",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Analogy-making as Predication Using Relational Information and LSA Vectors Jose Quesada, Walter Kintsch, Praful Mangalath ([quesadaj, wkintsch, praful]@psych.colorado.edu) Institute of Cognitive Science, University of Colorado, Boulder Boulder, CO 80309-0344 USA Current models of analogy comprehension use hand - coded representations. As Hummel and Holyoak (2003) put it, \u201cthe problem pf hand-coded representations is among the most serious problems facing computational modeling as a scientific enterprise: All models are sensitive to their representations, so the choice of representation is among the most powerful wild cards at the modeler\u2019s disposal\u201d (p. 247). French (2002) reviews different computational models of analogy-making, and points out one of the most fundamental problems of the field: case representations are authored (hand-coded) to make the model work. Between the challenges and future directions he presents \u201cthe systematic exploration of experimenter-independent representation-building and learning mechanisms\u201d (p. 204). In this poster, we propose LSA as a method to generate the much-wanted non-hand-coded representations. However, LSA has severe limitations to represent structure. Turney and Littman (2003) pointed out that the similarity of semantic relations between words is not directly reducible to the semantic similarity of individual words. This is also the leitmotiv of some analogy models like Gentner\u2019s (1983; 1989).Thus, LSA alone would fail to explain analogy, where relations (structure) between words are fundamental. We use a predication (Kintsch, 2001) to represent structure comparisons in the LSA semantic space. Predication is able to select the features (neighbors) of one component of the analogy (the source) that are relevant to the other (the target). Table 1(a): a sample SAT question. Ostrich : bird Table 1(b): predication using analogy domains Number Percent T&L (2003) (a) Lion : Cat (b) Goose : Flock (c) Ewe : Sheep Skipped (d) Cub : Bear Total (e) Primate: Monkey Correct Incorrect Precision 157/307 0.51% Recall F We calculated the predication vectors for all the targets and alternatives of 374 items from the Scholastic aptitude test (SAT). This dataset of analogies was collected by Turney and Littman (2003). An example of a SAT item can be seen in Table 1(a). To calculate the correct alternative, we computed the cosine between the target vector and each alternative, and selected the alternative with the highest cosine. However, this method had poor results: using LSA this way leaves out most of the relational information. For example, relations such as is-a, part-of, causal-agent-of, etc. are all substituted by a very basic semantic distance measure when we compute the cosine between the target and the alternatives. To include this relational information in the comparison, we constructed a set of ten possible relations between the components in the 374 SAT analogies (table 2). Then we computed the cosine between the list of words that define the analogy domain and each analogy predication vector in the dataset. That is, for each analogy we created a vector of ten features, where each feature indicates how similar the analogy is to each of the analogy domains. For example, Ostrich::bird would load primarily in the taxonomy and Hyponymy domain components, but also in endonymy, synonymy, and degree. Then, we correlated these loading vectors for the target and each alternative, and selected the alternative that best correlated with the target to solve the SAT question. Table 2: Ten analogy domains and their characteristic words Hyponymy X is a type of Y (for example - Maple:Tree) [Subordinate of, superordinate to, rank, class, category, family, genus, variety, type of, kind of, hyponym] Degree X means Y at a certain degree (Pour:Drip) [level, stage, point, magnitude, extent, greater, lesser, intensity, severity, extreme, degree] Meronymy The parts of X include the Ys (Body:Arm) [part, whole, component, made up of, portion, contains, constituent, segment, piece of, composite, meronym] Taxonomy X is an item in the category Y (Milk:Beverage)[classification, containing, structure, relationship, hierarchy, system, framework, taxonym] Synonymy is the same as Y (Work:Labor) [equivalent, equal, likeness, match, interchangeable, alike, same as, similar, close to, like, synonym] Antonymy is the opposite of Y (Find:Hide) [opposite, unlike, different, antithesis, opposed, contradiction, contrast, reverse, anti, not the same as, antonym] Characteristic X is a characteristic of Y (Dishonesty:Liar) [indicative, representative of, typical of, feature, attribute, trait, property, mannerism, facet, quality, characteristic] Plurality X is many Ys (Throng:People) [mass, bulk, several, many, lots of, numerous, crowd, group, more, number, plural] Endonymy X entails Y (Coop:Poultry) [entails, require, evoke, involve, suggest, imply, presuppose, mean] Use X is used to Y (Scissors:Cut) [do with, manipulate, operate, function, purpose, role, action, utilize, employ, use] The results are displayed in Table 1(b). The performance of our model is very close to the state of the art in automatic analogy making when considering correct answers (42% vs. 47%, Turney & Littman, 2003), and precision, recall and F measures. Furthermore, our model is psychologically plausible."
            },
            "slug": "Analogy-making-as-Predication-Using-Relational-and-Quesada-Kinstich",
            "title": {
                "fragments": [],
                "text": "Analogy-making as Predication Using Relational Information and LSA Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This poster proposes LSA as a method to generate the much-wanted non-hand-coded representations and uses a predication to represent structure comparisons in the LSA semantic space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 143
                            }
                        ],
                        "text": "This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": false,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746503"
                        ],
                        "name": "A. Kilgarriff",
                        "slug": "A.-Kilgarriff",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Kilgarriff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kilgarriff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3269260"
                        ],
                        "name": "P. Rychl\u00fd",
                        "slug": "P.-Rychl\u00fd",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Rychl\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rychl\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143917717"
                        ],
                        "name": "P. Smrz",
                        "slug": "P.-Smrz",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Smrz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smrz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188833"
                        ],
                        "name": "David Tugwell",
                        "slug": "David-Tugwell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tugwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Tugwell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 167
                            }
                        ],
                        "text": "DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 164
                            }
                        ],
                        "text": "For instance, the Sketch Engine1 builds \u201cword sketches\u201d consisting of triples extracted from parsed corpora and formed by two words linked by a grammatical relation (Kilgarriff et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13188196,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "f7d671e3bb784424e798a62db21eb798d3bed18b",
            "isKey": false,
            "numCitedBy": 950,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The chapter deals with word sketches - one-page automatic, corpus-based summaries of a word's grammatical and collocational behaviour.\u00a0 They were first used in the production of the Macmillan English Dictionary and were presented at Euralex 2002.\u00a0 At that point, they only existed for English.\u00a0 Now, we have developed the Sketch Engine, a corpus tool which takes as input a corpus of any language and a corresponding grammar patterns and which generates word sketches for the words of that language.\u00a0 It also generates a thesaurus and 'sketch differences', which specify similarities and differences between near-synonyms. We briefly present a case study investigating applicability of the Sketch Engine to free word-order languages. The results show that word sketches could facilitate lexicographic work in Czech as they have for English."
            },
            "slug": "The-Sketch-Engine-Kilgarriff-Rychl\u00fd",
            "title": {
                "fragments": [],
                "text": "The Sketch Engine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that word sketches could facilitate lexicographic work in Czech as they have for English, and a case study investigating applicability of the Sketch Engine to free word-order languages is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145811519"
                        ],
                        "name": "K. McRae",
                        "slug": "K.-McRae",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "McRae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McRae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2902015"
                        ],
                        "name": "G. S. Cree",
                        "slug": "G.-S.-Cree",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cree",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Cree"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246097"
                        ],
                        "name": "Mark S. Seidenberg",
                        "slug": "Mark-S.-Seidenberg",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Seidenberg",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark S. Seidenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2560198"
                        ],
                        "name": "C. McNorgan",
                        "slug": "C.-McNorgan",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "McNorgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. McNorgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12526452,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b2c04cc369b8f08f399d5fb95ddc884d52cfebd2",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic features have provided insight into numerous behavioral phenomena concerning concepts, categorization, and semantic memory in adults, children, and neuropsychological populations. Numerous theories and models in these areas are based on representations and computations involving semantic features. Consequently, empirically derived semantic feature production norms have played, and continue to play, a highly useful role in these domains. This article describes a set of feature norms collected from approximately 725 participants for 541 living (dog) and nonliving (chair) basic-level concepts, the largest such set of norms developed to date. This article describes the norms and numerous statistics associated with them. Our aim is to make these norms available to facilitate other research, while obviating the need to repeat the labor-intensive methods involved in collecting and analyzing such norms. The full set of norms may be downloaded from www.psychonomic.org/archive."
            },
            "slug": "Semantic-feature-production-norms-for-a-large-set-McRae-Cree",
            "title": {
                "fragments": [],
                "text": "Semantic feature production norms for a large set of living and nonliving things"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A set of feature norms collected from approximately 725 participants for 541 living (dog) and nonliving (chair) basic-level concepts, the largest such set of norms developed to date are described, making these norms available to facilitate other research, while obviating the need to repeat the labor-intensive methods involved in collecting and analyzing such norms."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399974318"
                        ],
                        "name": "Ying Zhao",
                        "slug": "Ying-Zhao",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50877490"
                        ],
                        "name": "G. Karypis",
                        "slug": "G.-Karypis",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Karypis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Karypis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a40f670c985e45e973337284fcb4fd44fe5858fb",
            "isKey": false,
            "numCitedBy": 565,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, we have witnessed a tremendous growth in the volume of text documents available on the Internet, digital libraries, news sources, and company-wide intranets. This has led to an increased interest in developing methods that can help users to effectively navigate, summarize, and organize this information with the ultimate goal of helping them to find what they are looking for. Fast and high-quality document clustering algorithms play an important role towards this goal as they have been shown to provide both an intuitive navigation/browsing mechanism by organizing large amounts of information into a small number of meaningful clusters as well as to greatly improve the retrieval performance either via cluster-driven dimensionality reduction, term-weighting, or query expansion. This ever-increasing importance of document clustering and the expanded range of its applications led to the development of a number of new and novel algorithms with different complexity-quality trade-offs. Among them, a class of clustering algorithms that have relatively low computational requirements are those that treat the clustering problem as an optimization process which seeks to maximize or minimize a particular clustering criterion function defined over the entire clustering solution. The focus of this paper is to evaluate the performance of different criterion functions for the problem of clustering documents. Our study involves a total of seven different criterion functions, three of which are introduced in this paper and four that have been proposed in the past. Our evaluation consists of both a comprehensive experimental evaluation involving fifteen different datasets, as well as an analysis of the characteristics of the various criterion functions and their effect on the clusters they produce. Our experimental results show that there are a set of criterion functions that consistently outperform the rest, and that some of the newly proposed criterion functions lead to the best overall results. Our theoretical analysis of the criterion function shows that their relative performance depends on (i) the degree to which they can correctly operate when the clusters are of different tightness, and (ii) the degree to which they can lead to reasonably balanced clusters."
            },
            "slug": "Criterion-Functions-for-Document-Clustering-\u2217-and-Zhao-Karypis",
            "title": {
                "fragments": [],
                "text": "Criterion Functions for Document Clustering \u2217 Experiments and Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This study involves a total of seven different criterion functions, three of which are introduced in this paper and four that have been proposed in the past, and involves both a comprehensive experimental evaluation and an analysis of the characteristics of the various criterion functions and their effect on the clusters they produce."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2736974"
                        ],
                        "name": "Abdulrahman Almuhareb",
                        "slug": "Abdulrahman-Almuhareb",
                        "structuredName": {
                            "firstName": "Abdulrahman",
                            "lastName": "Almuhareb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdulrahman Almuhareb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678591"
                        ],
                        "name": "Massimo Poesio",
                        "slug": "Massimo-Poesio",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Poesio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimo Poesio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 380201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21287739fdb91a1ae6378c12794377dfce42e565",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all."
            },
            "slug": "Attribute-Based-and-Value-Based-Clustering:-An-Almuhareb-Poesio",
            "title": {
                "fragments": [],
                "text": "Attribute-Based and Value-Based Clustering: An Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison and finds that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2132734"
                        ],
                        "name": "P. Garrard",
                        "slug": "P.-Garrard",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Garrard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Garrard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3731590"
                        ],
                        "name": "M. L. Lambon Ralph",
                        "slug": "M.-L.-Lambon-Ralph",
                        "structuredName": {
                            "firstName": "Matthew.",
                            "lastName": "Lambon Ralph",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Lambon Ralph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2569756"
                        ],
                        "name": "J. Hodges",
                        "slug": "J.-Hodges",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hodges",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hodges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615574"
                        ],
                        "name": "K. Patterson",
                        "slug": "K.-Patterson",
                        "structuredName": {
                            "firstName": "Karalyn",
                            "lastName": "Patterson",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Patterson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "Similar property lists, collected from subjects in elicitation tasks, are widely used in cognitive science as surrogates of mental features (Garrard et al. 2001; McRae et al. 2005; Vinson and Vigliocco 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205517268,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c2cde741f943a5739130dcc9c25691d940a7da94",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Many cognitive psychological, computational, and neuropsychological approaches to the organisation of semantic memory have incorporated the idea that concepts are, at least partly, represented in terms of their fine-grained features. We asked 20 normal volunteers to provide properties of 64 concrete items, drawn from living and nonliving categories, by completing simple sentence stems (e.g., an owl is __, has __, can__). At a later date, the same participants rated the same concepts for prototypicality and familiarity. The features generated were classified as to type of knowledge (sensory, functional, or encyclopaedic), and also quantified with regard to both dominance (the number of participants specifying that property for that concept) and distinctiveness (the proportion of exemplars within a conceptual category of which that feature was considered characteristic). The results demonstrate that rated prototypicality is related to both the familiarity of the concept and its distance from the average of the exemplars within the same category (the category centroid). The feature database was also used to replicate, resolve, and extend a variety of previous observations on the structure of semantic representations. Specifically, the results of our analyses (1) resolve two conflicting claims regarding the relative ratio of sensory to other kinds of attributes in living vs. nonliving concepts; (2) offer new information regarding the types of features\u2212across different domains\u2212that distinguish concepts from their category coordinates; and (3) corroborate some previous claims of higher intercorrelations between features of living things than those of artefacts."
            },
            "slug": "Prototypicality,-distinctiveness,-and-Analyses-of-Garrard-Ralph",
            "title": {
                "fragments": [],
                "text": "Prototypicality, distinctiveness, and intercorrelation: Analyses of the semantic attributes of living and nonliving concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results demonstrate that rated prototypicality is related to both the familiarity of the concept and its distance from the average of the exemplars within the same category (the category centroid)."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive neuropsychology"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816914"
                        ],
                        "name": "Einat Minkov",
                        "slug": "Einat-Minkov",
                        "structuredName": {
                            "firstName": "Einat",
                            "lastName": "Minkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Einat Minkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12385972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "956e096b1e8422c91989938b9508272b956d3070",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the setting of lazy random graph walks over directed graphs, where entities are represented as nodes and typed edges represent the relations between them. This framework has been used in a variety of problems to derive an extended measure of entity similarity. In this paper we contrast two different approaches for applying supervised learning in this framework to improve graph walk performance: a gradient descent algorithm that tunes the transition probabilities of the graph, and a reranking approach that uses features describing global properties of the traversed paths. An empirical evaluation on a set of tasks from the domain of personal information management and multiple corpora show that reranking performance is usually superior to the local gradient descent algorithm, and that the methods often yield best results when combined."
            },
            "slug": "Learning-to-rank-typed-graph-walks:-local-and-Minkov-Cohen",
            "title": {
                "fragments": [],
                "text": "Learning to rank typed graph walks: local and global approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two different approaches for applying supervised learning in this framework to improve graph walk performance are contrasted: a gradient descent algorithm that tunes the transition probabilities of the graph, and a reranking approach that uses features describing global properties of the traversed paths."
            },
            "venue": {
                "fragments": [],
                "text": "WebKDD/SNA-KDD '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375801"
                        ],
                        "name": "M. Pennacchiotti",
                        "slug": "M.-Pennacchiotti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Pennacchiotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pennacchiotti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 134
                            }
                        ],
                        "text": "Given the sentence The teacher eats a red apple, structured DSMs would not consider eat as a legitimate context for red and would distinguish the object relation connecting eat and apple as a different type of co-occurrence from the modifier relation linking red and apple."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7463996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45bc2636d9f5ee728bccebecbd8db0033afd7de3",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision."
            },
            "slug": "Espresso:-Leveraging-Generic-Patterns-for-Semantic-Pantel-Pennacchiotti",
            "title": {
                "fragments": [],
                "text": "Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results show that the exploitation of generic patterns substantially increases system recall with small effect on overall precision, and a principled measure of pattern and instance reliability enabling the filtering algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8394214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ea92d57e3b7d79d5a11d347a23a9f9330d9c838",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The appropriateness of paraphrases for words depends often on context: \"grab\" can replace \"catch\" in \"catch a ball\", but not in \"catch a cold\". Structured Vector Space (SVS) (Erk and Pado, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates \"best-practice\" parameter settings for SVS, and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation."
            },
            "slug": "Paraphrase-Assessment-in-Structured-Vector-Space:-Erk-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "Paraphrase Assessment in Structured Vector Space: Exploring Parameters and Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper investigates \"best-practice\" parameter settings for SVS, and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108977300"
                        ],
                        "name": "Hugo Liu",
                        "slug": "Hugo-Liu",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hugo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40139814"
                        ],
                        "name": "P. Singh",
                        "slug": "P.-Singh",
                        "structuredName": {
                            "firstName": "Push",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "However, we extend and generalize this assumption, by proposing that, once they are formalized as a threeway tensor, tuples can become the backbone of a unified model for distributional semantics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6210630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3fea597033a46d5ae282464a8f16d6715187e70",
            "isKey": false,
            "numCitedBy": 1545,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "ConceptNet is a freely available commonsense knowledge base and natural-language-processing tool-kit which supports many practical textual-reasoning tasks over real-world documents including topic-gisting, analogy-making, and other context oriented inferences. The knowledge base is a semantic network presently consisting of over 1.6 million assertions of commonsense knowledge encompassing the spatial, physical, social, temporal, and psychological aspects of everyday life. ConceptNet is generated automatically from the 700 000 sentences of the Open Mind Common Sense Project \u2014 a World Wide Web based collaboration with over 14 000 authors."
            },
            "slug": "ConceptNet-\u2014-A-Practical-Commonsense-Reasoning-Liu-Singh",
            "title": {
                "fragments": [],
                "text": "ConceptNet \u2014 A Practical Commonsense Reasoning Tool-Kit"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "ConceptNet is a freely available commonsense knowledge base and natural-language-processing tool-kit which supports many practical textual-reasoning tasks over real-world documents including topic-gisting, analogy-making, and other context oriented inferences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5547937"
                        ],
                        "name": "K. Rothenh\u00e4usler",
                        "slug": "K.-Rothenh\u00e4usler",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Rothenh\u00e4usler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rothenh\u00e4usler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15247649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3491e4c0944039073e2e8481f70ee717deb73ce8",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the results of clustering experiments with a number of different evaluation sets using dependency based word spaces. Contrary to previous results we found a clear advantage using a parsed corpus over word spaces constructed with the help of simple patterns. We achieve considerable gains in performance over these spaces ranging between 9 and 13% in absolute terms of cluster purity."
            },
            "slug": "Unsupervised-Classification-with-Dependency-Based-Rothenh\u00e4usler-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Unsupervised Classification with Dependency Based Word Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This work presents the results of clustering experiments with a number of different evaluation sets using dependency based word spaces and finds a clear advantage using a parsed corpus over word spaces constructed with the help of simple patterns."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153924342"
                        ],
                        "name": "Hsin-Hsi Chen",
                        "slug": "Hsin-Hsi-Chen",
                        "structuredName": {
                            "firstName": "Hsin-Hsi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsin-Hsi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362882"
                        ],
                        "name": "M. Lin",
                        "slug": "M.-Lin",
                        "structuredName": {
                            "firstName": "Ming-Shun",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073750"
                        ],
                        "name": "Y. Wei",
                        "slug": "Y.-Wei",
                        "structuredName": {
                            "firstName": "Yu-Chuan",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3017040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a1e182ef35f370dd5401b34c5460985b25c4595",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A web search with double checking model is proposed to explore the web as a live corpus. Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co-Occurrence Double Check (CODC), are presented. In the experiments on Rubenstein-Goodenough's benchmark data set, the CODC measure achieves correlation coefficient 0.8492, which competes with the performance (0.8914) of the model using WordNet. The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable. Further study on named entity clustering shows that the five measures are quite useful. In particular, CODC measure is very stable on word-word and name-name experiments. The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65% and 14.22% increase compared to the system without community expansion. All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web."
            },
            "slug": "Novel-Association-Measures-Using-Web-Search-with-Chen-Lin",
            "title": {
                "fragments": [],
                "text": "Novel Association Measures Using Web Search with Double Checking"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co-Occurrence Double Check (CODC), are presented and show that the five measures are quite useful."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 113
                            }
                        ],
                        "text": "The debate in DSMs has so far mostly focused on the context choice\u2014for example, lexical collocates vs. documents (Sahlgren 2006; Turney and Pantel 2010)\u2014or on the costs and benefits of having structured contexts (Pado\u0301 and Lapata 2007; Rothenha\u0308usler and Schu\u0308tze 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 158
                            }
                        ],
                        "text": "\u2026and cognitive scientists in corpus-basedmodels of semantic representation (Grefenstette 1994; Lund and Burgess 1996; Landauer and Dumais 1997; Schu\u0308tze 1997; Sahlgren 2006; Bullinaria and Levy 2007; Griffiths, Steyvers, and Tenenbaum 2007; Pado\u0301 and Lapata 2007; Lenci 2008; Turney and Pantel 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11917163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1521ddb27860cc8834f8a82e62665bf983c8ad2c",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 166,
            "paperAbstract": {
                "fragments": [],
                "text": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in ter ..."
            },
            "slug": "The-Word-Space-Model-:-Using-distributional-to-and-Sahlgren",
            "title": {
                "fragments": [],
                "text": "The Word-Space Model : Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terabytes of data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785922"
                        ],
                        "name": "Yuexian Hou",
                        "slug": "Yuexian-Hou",
                        "structuredName": {
                            "firstName": "Yuexian",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuexian Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48437245"
                        ],
                        "name": "D. Song",
                        "slug": "D.-Song",
                        "structuredName": {
                            "firstName": "Dawei",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Song"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15442193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3e7b8c26e7915137e15e09f966166082b4f5649",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An emerging topic in Quantuam Interaction is the use of lexical semantic spaces, as Hilbert spaces, to capture the meaning of words. There has been some initial evidence that the phenomenon of quantum entanglement exists in a semantic space and can potentially play a crucial role in determining the embeded semantics. In this paper, we propose to consider pure high-order entanglements that cannot be reduced to the compositional effect of lower-order ones, as an indicator of high-level semantic entities. To characterize the intrinsic order of entanglements and distinguish pure high-order entanglements from lower-order ones, we develop a set of methods in the framework of Information Geometry. Based on the developed methods, we propose an expanded vector space model that involves context-sensitive high-order information and aims at characterizing high-level retrieval contexts. Some initial ideas on applying the proposed methods in query expansion and text classification are also presented."
            },
            "slug": "Characterizing-Pure-High-Order-Entanglements-in-via-Hou-Song",
            "title": {
                "fragments": [],
                "text": "Characterizing Pure High-Order Entanglements in Lexical Semantic Spaces via Information Geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to consider pure high-order entanglements that cannot be reduced to the compositional effect of lower-order ones, as an indicator of high-level semantic entities, and develops a set of methods in the framework of Information Geometry to characterize the intrinsic order of entanglement."
            },
            "venue": {
                "fragments": [],
                "text": "QI"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7803,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153584694"
                        ],
                        "name": "R. Rapp",
                        "slug": "R.-Rapp",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Rapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18528663,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "747bdec947d9b85a40c1fb31bcaafb464f2ce9aa",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A freely available English thesaurus of related words is presented that has been automatically compiled by analyzing the distributional similarities of words in the British National Corpus. The quality of the results has been evaluated by comparison with human judgments as obtained from non-native and native speakers of English who were asked to provide rankings of word similarities. According to this measure, the results generated by our system are better than the judgments of the non-native speakers and come close to the native speakers\u2019 performance. An advantage of our approach is that it does not require syntactic parsing and therefore can be more easily adapted to other languages. As an example, a similar thesaurus for German has already been completed."
            },
            "slug": "A-Freely-Available-Automatically-Generated-of-Words-Rapp",
            "title": {
                "fragments": [],
                "text": "A Freely Available Automatically Generated Thesaurus of Related Words"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A freely available English thesaurus of related words is presented that has been automatically compiled by analyzing the distributional similarities of words in the British National Corpus, which does not require syntactic parsing and therefore can be more easily adapted to other languages."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145811519"
                        ],
                        "name": "K. McRae",
                        "slug": "K.-McRae",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "McRae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McRae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403814853"
                        ],
                        "name": "M. Spivey-Knowlton",
                        "slug": "M.-Spivey-Knowlton",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Spivey-Knowlton",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spivey-Knowlton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144171250"
                        ],
                        "name": "M. Tanenhaus",
                        "slug": "M.-Tanenhaus",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tanenhaus",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanenhaus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 56
                            }
                        ],
                        "text": "The DM performance is also compared with the results of Merlo and Stevenson (2001) for their classifiers tested with the leave-one-out methodology (macroaveraged F has been computed on the class-by-class scores reported in that article)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 185
                            }
                        ],
                        "text": "The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 31
                            }
                        ],
                        "text": "Conversely, the classifiers in Merlo and Stevenson (2001) rely on a much larger range of knowledge-intensive features selected in an ad hoc fashion for this task (on the other hand, their training corpus\nTable 13 Verb classification performance (precision, recall, and F for MS are macro-averaged)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 24
                            }
                        ],
                        "text": "We use the MS data set (Merlo and Stevenson 2001), which includes 19 unergative verbs undergoing the induced action alternation (e.g., race), 19 unaccusative verbs that undergo the causative/inchoative alternation (e.g., break), and 20 object-drop verbs participating in the unexpressed object\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 95
                            }
                        ],
                        "text": "Distributional categorization has been investigated for other POS as well, most notably verbs (Merlo and Stevenson 2001; Schulte imWalde 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11171797,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7a2e9d06002c40fc470b01d67c724d335e031633",
            "isKey": true,
            "numCitedBy": 622,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-course with which readers use event-specific world knowledge (thematic fit) to resolve structural ambiguity was explored through experiments and implementation of constraint-based and two-stage models. In a norming study, subjects completed fragments that ended in the ambiguous region of a reduced relative clause (The crook arrested/by/the/detective). Completion proportions up to and includingthewere influenced by thematic fit. The results were simulated using a competition model in which independently quantified syntactic and semantic constraints simultaneously influenced interpretation. Predictions were then generated for a self-paced reading task using model parameter values established by the off-line simulations. The pattern of reading times matched the predictions of the constraint-based version of the model but differed substantially from a one-region delay garden-path version. In addition, a garden-path model with a very short delay simulated the data better than the one-region delay model, but not as closely as the constraint-based version. The experiment and modeling illustrate that thematic fit is computed and used immediately in on-line sentence comprehension. Furthermore, the modeling highlighted the difficulty of interpreting sentence comprehension experiments without both quantifying the relevant constraints and implementing the mechanisms involved."
            },
            "slug": "Modeling-the-Influence-of-Thematic-Fit-(and-Other-McRae-Spivey-Knowlton",
            "title": {
                "fragments": [],
                "text": "Modeling the Influence of Thematic Fit (and Other Constraints) in On-line Sentence Comprehension"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33696979"
                        ],
                        "name": "G. Murphy",
                        "slug": "G.-Murphy",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Murphy",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Murphy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145167098,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8c3edb92cd6d0f2b6490dc6a8719dd6fa2fb4116",
            "isKey": false,
            "numCitedBy": 2433,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts embody our knowledge of the kinds of things there are in the world. Tying our past experiences to our present interactions with the environment, they enable us to recognize and understand new objects and events. Concepts are also relevant to understanding domains such as social situations, personality types, and even artistic styles. Yet like other phenomenologically simple cognitive processes such as walking or understanding speech, concept formation and use are maddeningly complex. Research since the 1970s and the decline of the \"classical view\" of concepts have greatly illuminated the psychology of concepts. But persistent theoretical disputes have sometimes obscured this progress. The Big Book of Concepts goes beyond those disputes to reveal the advances that have been made, focusing on the major empirical discoveries. By reviewing and evaluating research on diverse topics such as category learning, word meaning, conceptual development in infants and children, and the basic level of categorization, the book develops a much broader range of criteria than is usual for evaluating theories of concepts."
            },
            "slug": "The-Big-Book-of-Concepts-Murphy",
            "title": {
                "fragments": [],
                "text": "The Big Book of Concepts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2032205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cce57b4eb593359862817e93eb47e8655520652",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik\u2019s WordNet-based model and the EM-based clustering model, but has coverage problems."
            },
            "slug": "A-Simple,-Similarity-based-Model-for-Selectional-Erk",
            "title": {
                "fragments": [],
                "text": "A Simple, Similarity-based Model for Selectional Preferences"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics, focuses on the task of semantic role labeling and shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085030"
                        ],
                        "name": "M. Moens",
                        "slug": "M.-Moens",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 154
                            }
                        ],
                        "text": "This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8927694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea419a6e8583d12f6b91be48df41edb7a8b8d6b6",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency. We propose an approximation algorithm, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
            },
            "slug": "Improvements-in-Automatic-Thesaurus-Extraction-Curran-Moens",
            "title": {
                "fragments": [],
                "text": "Improvements in Automatic Thesaurus Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An approximation algorithm is proposed, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3084460"
                        ],
                        "name": "Eyal Sagi",
                        "slug": "Eyal-Sagi",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Sagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eyal Sagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47563576"
                        ],
                        "name": "Stefan Kaufmann",
                        "slug": "Stefan-Kaufmann",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kaufmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kaufmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018722"
                        ],
                        "name": "B. Clark",
                        "slug": "B.-Clark",
                        "structuredName": {
                            "firstName": "Brady",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14245932,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "cb8085d17388045ab2af95b5cb8fbc4067dfaee3",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new statistical method for detecting and tracking changes in word meaning, based on Latent Semantic Analysis. By comparing the density of semantic vector clusters this method allows researchers to make statistical inferences on questions such as whether the meaning of a word changed across time or if a phonetic cluster is associated with a specific meaning. Possible applications of this method are then illustrated in tracing the semantic change of 'dog', 'do', and 'deer' in early English and examining and comparing phonaesthemes."
            },
            "slug": "Semantic-Density-Analysis:-Comparing-Word-Meaning-Sagi-Kaufmann",
            "title": {
                "fragments": [],
                "text": "Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new statistical method for detecting and tracking changes in word meaning, based on Latent Semantic Analysis, which allows researchers to make statistical inferences on questions such as whether the meaning of a word changed across time or if a phonetic cluster is associated with a specific meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840346"
                        ],
                        "name": "S. Peters",
                        "slug": "S.-Peters",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Peters"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 147
                            }
                        ],
                        "text": "\u2026interesting synergies are emerging with research areas such as neural systems (Smolensky 1990; Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole 2005; Hou and Song 2009), holographic models of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14230035,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5160cad9dc3d6b19ae26796d79f69c24cee0e676",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A calculus which combined the flexible geometric structure o f vector models with the crisp efficiency of Boolean logic would be extrem ely beneficial for modelling natural language. With this goal in mind, we present a formulation for logical connectives in vector spaces based on standard linear algebra, giving examples of the use of vector negation to discriminate between different senses of ambiguous words. It turns out that the operators developed in this way are precisely the connectives of quantum logic (Birkhoff and von Neumann, 1936), which to our knowledge have not been exploited before in natural language processing. In quantum logic, arbitrary sets are replaced by linear subs paces of a vector space, and set unions, intersections and complements are replaced by vector sum, intersection and orthogonal complements of subspaces. We demonstrate that these logical connectives (particularly the orthogonal complement for negation) are powerful tools for exploring and analysing word meanings and show distinct advantages over Boolean operators in document retrieval experiments."
            },
            "slug": "Word-Vectors-and-Quantum-Logic-Experiments-with-and-Widdows-Peters",
            "title": {
                "fragments": [],
                "text": "Word Vectors and Quantum Logic Experiments with negation and disjunction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32372837"
                        ],
                        "name": "Joe Pater",
                        "slug": "Joe-Pater",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Pater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joe Pater"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 142
                            }
                        ],
                        "text": "\u2026being explored in corpus-based semantics, and interesting synergies are emerging with research areas such as neural systems (Smolensky 1990; Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 197465099,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3233c665304a393ae6dc7a098005404d23327492",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The harmonic mind is a two-volume collection of 23 chapters authored by Smolensky, Legendre and their collaborators.* Some of these are reprints or slight updates of previously published material, but approximately threequarters of it has never appeared before. Every phonologist who takes at all seriously the notion that phonology is a branch of cognitive science should own a copy of this book. Smolensky & Legendre (vol. 1, p. 1) present a view of cognition that incorporates two distinct, but related levels of formal description: \u2018the continuous, numerical lower-level description of brain\u2019, characterised in terms of a connectionist network, and \u2018the discrete, structural, higher-level description of mind\u2019, characterised in terms of Optimality Theory (OT). The depth and breadth of the presentation are awe-inspiring, and quite likely intimidating in some places for any single reader. In their preface, Smolensky & Legendre characterise the intended audience as interdisciplinary groups of researchers in cognitive science, and a joint reading of this book by such a group would undoubtedly be extremely educational for all. Bridges between the disciplines are built by careful exposition of the fundamental ideas, and the experts in each area will be satisfied by the wealth of insights, and the formal precision of their presentation. Since phonology is a central focus, this book provides phonologists with a unique opportunity to engage with both the research and the researchers in other realms of cognitive science. There is furthermore much that phonologists (and other linguists) working on their own can gain from this book. The presentation of the overall cognitive framework in the first few chapters of Volume 1, Cognitive architecture, should be accessible to readers with little or no background in mathematics or computational modelling of cognition. This section should particularly appeal to phonologists curious about connectionism, but who are put off by reductionist applications that seek to eliminate the types of representations used in generative"
            },
            "slug": "The-harmonic-mind-:-from-neural-computation-to-Pater",
            "title": {
                "fragments": [],
                "text": "The harmonic mind : from neural computation to optimality-theoretic grammar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642122"
                        ],
                        "name": "Amac Herdagdelen",
                        "slug": "Amac-Herdagdelen",
                        "structuredName": {
                            "firstName": "Amac",
                            "lastName": "Herdagdelen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amac Herdagdelen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11152159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2cd57c279adb7f525b6bf4e908b8d9e62914480",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Both vector space models and graph random walk models can be used to determine similarity between concepts. Noting that vectors can be regarded as local views of a graph, we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted."
            },
            "slug": "Measuring-semantic-relatedness-with-vector-space-Herdagdelen-Erk",
            "title": {
                "fragments": [],
                "text": "Measuring semantic relatedness with vector space models and random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work directly compares vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted."
            },
            "venue": {
                "fragments": [],
                "text": "Graph-based Methods for Natural Language Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744364"
                        ],
                        "name": "T. Kolda",
                        "slug": "T.-Kolda",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Kolda",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kolda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738536"
                        ],
                        "name": "Jimeng Sun",
                        "slug": "Jimeng-Sun",
                        "structuredName": {
                            "firstName": "Jimeng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimeng Sun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 961975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d57125ff69aba3dac4f7b76894d9440927b3262",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern applications such as Internet traffic, telecommunication records, and large-scale social networks generate massive amounts of data with multiple aspects and high dimensionalities. Tensors (i.e., multi-way arrays) provide a natural representation for such data. Consequently, tensor decompositions such as Tucker become important tools for summarization and analysis. One major challenge is how to deal with high-dimensional, sparse data. In other words, how do we compute decompositions of tensors where most of the entries of the tensor are zero. Specialized techniques are needed for computing the Tucker decompositions for sparse tensors because standard algorithms do not account for the sparsity of the data. As a result, a surprising phenomenon is observed by practitioners: Despite the fact that there is enough memory to store both the input tensors and the factorized output tensors, memory overflows occur during the tensor factorization process. To address this intermediate blowup problem, we propose Memory-Efficient Tucker (MET). Based on the available memory, MET adaptively selects the right execution strategy during the decomposition. We provide quantitative and qualitative evaluation of MET on real tensors. It achieves over 1000X space reduction without sacrificing speed; it also allows us to work with much larger tensors that were too big to handle before. Finally, we demonstrate a data mining case-study using MET."
            },
            "slug": "Scalable-Tensor-Decompositions-for-Multi-aspect-Kolda-Sun",
            "title": {
                "fragments": [],
                "text": "Scalable Tensor Decompositions for Multi-aspect Data Mining"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Memory-Efficient Tucker (MET) is proposed, which achieves over 1000X space reduction without sacrificing speed; it also allows us to work with much larger tensors that were too big to handle before."
            },
            "venue": {
                "fragments": [],
                "text": "2008 Eighth IEEE International Conference on Data Mining"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50288999"
                        ],
                        "name": "Randall Davis",
                        "slug": "Randall-Davis",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randall Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716356"
                        ],
                        "name": "H. Shrobe",
                        "slug": "H.-Shrobe",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Shrobe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shrobe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679873"
                        ],
                        "name": "Peter Szolovits",
                        "slug": "Peter-Szolovits",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Szolovits",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Szolovits"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1527228,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "8df1e4d7a7c2f288b7ca4645b444b128b076a572",
            "isKey": false,
            "numCitedBy": 1354,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Although knowledge representation is one of the central and, in some ways, most familiar concepts in AI, the most fundamental question about it -- What is it? -- has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, and still others have focused on properties that are important to the notion of representation in general. In this article, we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and, at times, conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field."
            },
            "slug": "What-Is-a-Knowledge-Representation-Davis-Shrobe",
            "title": {
                "fragments": [],
                "text": "What Is a Knowledge Representation?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that keeping in mind all five of these roles that a representation plays provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806905"
                        ],
                        "name": "A. Maedche",
                        "slug": "A.-Maedche",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Maedche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Maedche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752093"
                        ],
                        "name": "Steffen Staab",
                        "slug": "Steffen-Staab",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Staab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Staab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67210567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "828f48bdaab8cf5b38e3fec591caa5d9a7cfbc65",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Ontologies have shown their usefulness in application areas such as information integration, natural language processing, metadata for the world wide, to name but a few. However, there remains the problem of engineering large and adequate ontologies within short time frames in order to keep costs low. We here present a general architecture for discovering conceptual structures and engineering ontologies from text and a new approach for discovering non-taxonomic conceptual relations from text."
            },
            "slug": "Ontology-Learning-from-Text-Maedche-Staab",
            "title": {
                "fragments": [],
                "text": "Ontology Learning from Text"
            },
            "venue": {
                "fragments": [],
                "text": "NLDB"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755431"
                        ],
                        "name": "P. Bruza",
                        "slug": "P.-Bruza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bruza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bruza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144021632"
                        ],
                        "name": "Richard Cole",
                        "slug": "Richard-Cole",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Cole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Cole"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1583338,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "da46b6bda3e7480b1a8ed68aa81f881e028e809f",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This article is an exploratory account of the the non-monotonic behaviour of conceptual associations in the light of context. Computational approximations of conceptual space are furnished by semantic space models which are emerging from the fields of cognition and computational linguistics. Semantic space models not only provide a cognitively motivated basis to underpin human practical reasoning, but from a mathematical perspective, they are real-valued Hilbert spaces. This introduces the highly speculative prospect of formalizing aspects of human practical reasoning via quantum mechanics. This account focuses on how to formalize context effects in relation to concepts as well as keeping an eye on operational issues."
            },
            "slug": "Quantum-Logic-of-Semantic-Space:-An-Exploratory-of-Bruza-Cole",
            "title": {
                "fragments": [],
                "text": "Quantum Logic of Semantic Space: An Exploratory Investigation of Context Effects in Practical Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This article is an exploratory account of the the non-monotonic behaviour of conceptual associations in the light of context, focusing on how to formalize context effects in relation to concepts as well as keeping an eye on operational issues."
            },
            "venue": {
                "fragments": [],
                "text": "We Will Show Them!"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709773"
                        ],
                        "name": "H. Rubenstein",
                        "slug": "H.-Rubenstein",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Rubenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898344"
                        ],
                        "name": "J. Goodenough",
                        "slug": "J.-Goodenough",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Goodenough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodenough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18309234,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "slug": "Contextual-correlates-of-synonymy-Rubenstein-Goodenough",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of synonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The shapes of the functions indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 215
                            }
                        ],
                        "text": "Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026Memory: A General Framework for Corpus-Based Semantics\nMarco Baroni\u2217\nUniversity of Trento\nAlessandro Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59167516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4471e3117cdac2fae74d305d54b237bb3addd749",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index."
            },
            "slug": "Explorations-in-automatic-thesaurus-discovery-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Explorations in automatic thesaurus discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this monograph is to provide a catalog of words and phrases used in ThesaurusGeneration, as well as some examples of other writers' work, which have been used in similar contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49404233"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53745863,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e3b32677a976f5a3d6b7837f6fb0aae8041bc9cf",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Review-of-Ambiguity-resolution-in-language-and-by-Melamed-Li",
            "title": {
                "fragments": [],
                "text": "Review of Ambiguity resolution in language learning: computational and cognitive models by Hinrich Sch\u00fctze. CSLI Publications 1997."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2999050"
                        ],
                        "name": "S. Evert",
                        "slug": "S.-Evert",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Evert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Evert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 255
                            }
                        ],
                        "text": "The measure can also be interpreted as the dominant term of average MI or as a heuristic variant of pointwise MI to avoid its bias towards overestimating the significance of low frequency events, and it is nearly identical to the Poisson\u2013Stirling measure (Evert 2005)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11101008,
            "fieldsOfStudy": [
                "Engineering",
                "Biology"
            ],
            "id": "dda43b361b7d893e141cc3153bf9d4f64619a6d5",
            "isKey": false,
            "numCitedBy": 571,
            "numCiting": 179,
            "paperAbstract": {
                "fragments": [],
                "text": "A platform weighing scale having a load receiving platform structure positioned over and resting on a multiplicity of load cells and having a load cell-removing aperture in registry with each load cell whereby each load cell is upwardly removable through its associated load cell removal aperture."
            },
            "slug": "The-Statistics-of-Word-Cooccur-rences:-Word-Pairs-Evert",
            "title": {
                "fragments": [],
                "text": "The Statistics of Word Cooccur-rences: Word Pairs and Collocations"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "A platform weighing scale has a load receiving platform structure positioned over and resting on a multiplicity of load cells and having a load cell-removing aperture in registry with each load cell whereby each load cells is upwardly removable through its associated load cell removal aperture."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1218598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e6cd8b0cc4076f52f32f3d86d3638bf872d969",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Higher-order tensor decompositions are analogous to the familiar Singular Value Decomposition (SVD), but they transcend the limitations of matrices (second-order tensors). SVD is a powerful tool that has achieved impressive results in information retrieval, collaborative filtering, computational linguistics, computational vision, and other fields. However, SVD is limited to two-dimensional arrays of data (two modes), and many potential applications have three or more modes, which require higher-order tensor decompositions. This paper evaluates four algorithms for higher-order tensor decomposition: Higher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal Iteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We measure the time (elapsed run time), space (RAM and disk space requirements), and fit (tensor reconstruction accuracy) of the four algorithms, under a variety of conditions. We find that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements. We recommend HOOI for tensors that are small enough for the available RAM and MP for larger tensors."
            },
            "slug": "Empirical-Evaluation-of-Four-Tensor-Decomposition-Turney",
            "title": {
                "fragments": [],
                "text": "Empirical Evaluation of Four Tensor Decomposition Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that standard implementations of HO-SVD and HOOI do not scale up to larger tensors, due to increasing RAM requirements, and is recommended to recommend HooI for tensors that are small enough for the available RAM and MP for larger Tensors."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767844"
                        ],
                        "name": "Diederik Aerts",
                        "slug": "Diederik-Aerts",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Aerts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik Aerts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3278816"
                        ],
                        "name": "M. Czachor",
                        "slug": "M.-Czachor",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Czachor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Czachor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16701954,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "ec013e75fb6486994455db8c31d2feef71bb63a6",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A zipper tooth has a body portion with an opening at one end for connection to a tape to form a zipper stringer of a slide fastener. At the other end of the body portion is an engaging head. Just behind the engaging head are a pair of grooves, one on each side of the body portion. The grooves are contoured to accommodate portions of the engaging heads of other similar zipper teeth. At each end of each of the grooves are outwardly extending tapered projections. The projections are rolled over to provide endwalls for the grooves so that a cavity is formed for locking accepting portions of engaging heads of other zipper teeth."
            },
            "slug": "LETTER-TO-THE-EDITOR:-Quantum-aspects-of-semantic-Aerts-Czachor",
            "title": {
                "fragments": [],
                "text": "LETTER TO THE EDITOR: Quantum aspects of semantic analysis and symbolic artificial intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A zipper tooth has a body portion with an opening at one end for connection to a tape to form a zipper stringer of a slide fastener to accommodate portions of the engaging heads of other similar zipper teeth."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143623142"
                        ],
                        "name": "T. Parsons",
                        "slug": "T.-Parsons",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Parsons",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Parsons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52287177"
                        ],
                        "name": "David R. Dowty",
                        "slug": "David-R.-Dowty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Dowty",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Dowty"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 170315716,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "78722fdd0f047baf0159edcd9493c531c05b95c4",
            "isKey": false,
            "numCitedBy": 2434,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The first \u20ac price and the \u00a3 and $ price are net prices, subject to local VAT. Prices indicated with * include VAT for books; the \u20ac(D) includes 7% for Germany, the \u20ac(A) includes 10% for Austria. Prices indicated with ** include VAT for electronic products; 19% for Germany, 20% for Austria. All prices exclusive of carriage charges. Prices and other details are subject to change without notice. All errors and omissions excepted. D.R. Dowty Word Meaning and Montague Grammar"
            },
            "slug": "Word-Meaning-and-Montague-Grammar-Parsons-Dowty",
            "title": {
                "fragments": [],
                "text": "Word Meaning and Montague Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The first \u20ac price and the \u00a3 and $ price are net prices, subject to local VAT, and the \u20ac(D) includes 7% for Germany, the\u20ac(A) includes 10% for Austria."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744364"
                        ],
                        "name": "T. Kolda",
                        "slug": "T.-Kolda",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Kolda",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kolda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31431661"
                        ],
                        "name": "Brett W. Bader",
                        "slug": "Brett-W.-Bader",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Bader",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brett W. Bader"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 107
                            }
                        ],
                        "text": "Other possibilities, such as graph-based algorithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), or deriving unstructured semantic spaces from the tensor by removing one of the indices, are left to future work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 836,
                                "start": 830
                            }
                        ],
                        "text": "Patterns are sequences of one to three words occurring between the targets, with all, none, or any subset of the elements replaced bywildcards (with the,with\nD ow nloaded from http://direct.m it.edu/coli/article-pdf/36/4/673/1810191/coli_a_00016.pdf by guest on 10 Septem ber 2021\nTable 8 Percentage accuracy in solving SAT analogies with 95% binomial confidence intervals (CI).\nmodel accuracy 95% CI model accuracy 95% CI\nHuman1 57.0 52.0\u201362.3 TypeDM 42.4 37.4\u201347.7 LRA-062 56.1 51.0\u201361.2 LSA7 42.0 37.2\u201347.4 PERT3 53.3 48.5\u201358.9 LRA 37.8 32.8\u201342.8 PairClass4 52.1 46.9\u201357.3 PMI-IR-062 35.0 30.2\u201340.1 VSM1 47.1 42.2\u201352.5 DepDM 31.4 26.6\u201336.2 BagPack5 44.1 39.0\u201349.3 LexDM 29.3 24.8\u201334.3 k-means6 44.0 39.0\u201349.3 Random 20.0 16.1\u201324.5\nModel sources: 1Turney and Littman (2005); 2Turney (2006b); 3Turney (2006a); 4Turney (2008); 5Herdag\u030cdelen and Baroni (2009); 6Bicic\u0327i and Yuret (2006); 7Quesada, Mangalath, and Kintsch (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 132
                            }
                        ],
                        "text": "We also report the best results from a range of experiments with different models and parameter settings from Herdag\u030cdelen, Erk, and Baroni (2009) (whose corpus is about half the size of ours) and Pado\u0301 and Lapata (2007) (who use a much smaller corpus)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 34
                            }
                        ],
                        "text": "The Battig test set introduced by Baroni et al. (2010) is based on the expanded Battig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 127
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "This model is based on the idea, motivated and tested by Baroni et al. (2010)\u2014 but see also Davidov and Rappoport (2008a, 2008b) for a related method\u2014that what matters is not so much the frequency of a link, but the variety of surface forms that express it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 119
                            }
                        ],
                        "text": "We test this approach in the ESSLLI 2008 Distributional Semantic Workshop unconstrained property generation challenge (Baroni, Evert, and Lenci 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 211
                            }
                        ],
                        "text": "Table 12 reports the percentage overlap with the gold standard properties (averaged across the 44 concepts) for our models as well as the only ESSLLI 2008 participant that tried this task, and for the models of Baroni et al. (2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "Distributional Memory: A General Framework for Corpus-Based Semantics\nMarco Baroni\u2217\nUniversity of Trento\nAlessandro Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 364,
                                "start": 358
                            }
                        ],
                        "text": "Table 7 reports percentage coverage and correlations for the DM models (the task requires the links to extract typical subjects and objects, so we cannot use DV nor Win), results from Pado\u0301, Pado\u0301, and Erk (2007) (ParCos is the best among their purely corpus-based systems), and the performance on the Pado\u0301 data set of the supervised system of Herdag\u030cdelen and Baroni (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 100
                            }
                        ],
                        "text": "A first step in this direction, within a framework similar to Turney\u2019s, was taken by Herdag\u030cdelen and Baroni (2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 804,
                                "start": 798
                            }
                        ],
                        "text": "Almuhareb & Poesio (AP)\nmodel purity 95% CI model purity 95% CI\nDepPath1 79 NA DV 65 61\u201369 TypeDM 76 72\u201381 DepDM 62 59\u201367 AttrValue-052 71 NA LexDM 59 56\u201365 Win 71 67\u201376 Random 16 14\u201317 VSM3 70 67\u201375\nBattig\nmodel purity 95% CI model purity 95% CI\nWin 96 91\u2013100 DV-104 79 73\u201389 TypeDM 94 89\u201399 LexDM 78 72\u201388 Strudel4 91 85\u201398 SVD-104 71 67\u201383 DepDM 90 84\u201396 AttrValue4 45 44\u201361 DV 84 79\u201393 Random 29 24\u201334\nESSLLI 2008\nmodel 6-way purity 95% CI 3-way purity 2-way purity avg purity\nTypeDM 84 77\u201395 98 100 94.0 Katrenko5 91 NA 100 80 90.3 DV 75 70\u201389 93 100 89.3 DepDM 75 68\u201389 93 100 89.3 LexDM 75 70\u201389 87 100 87.3 Peirsman5 82 NA 84 86 84.0 Win 75 70\u201389 86 59 73.3 Shaoul5 41 NA 52 55 49.3 Random 38 32\u201345 49 57 48.0\nModel sources: 1Rothenha\u0308usler and Schu\u0308tze (2009); 2Almuhareb and Poesio (2005); 3Herdag\u030cdelen, Erk, and Baroni (2009); 4Baroni et al. (2010); 5ESSLLI 2008 shared task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 221
                            }
                        ],
                        "text": "Recently, there has been some interest in the automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 100
                            }
                        ],
                        "text": "Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Workshop shared tasks (Baroni, Evert, and Lenci 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 125
                            }
                        ],
                        "text": "Table 5 reports results (percentage accuracies) on the TOEFL set for our models as well as the best model of Herdag\u030cdelen and Baroni (2009) and the corpus-based models from the ACL Wiki TOEFL state-of-the-art table (we do not include those models from the Wiki that resort to other knowledge sources, such as WordNet or a thesaurus)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 232
                            }
                        ],
                        "text": "All models are based on the natural idea of extracting word\u2013link\u2013word tuples from a dependency parse of a corpus, but this is not a requirement for DM: The links could for example be based on frequent n-grams as in Turney (2006b) and Baroni et al. (2010), or even on very different kinds of relation, such as co-occurring within the same document."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "The C/I data set was introduced by Baroni and Lenci (2009), but not tested in a classification task there."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "See Baroni et al. (2010) for the full list."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 477,
                                "start": 471
                            }
                        ],
                        "text": "The claims to follow about the relative performance of the models must be interpreted cautiously, in light of the spread of the confidence intervals: It suffices to note that,\nD ow nloaded from http://direct.m it.edu/coli/article-pdf/36/4/673/1810191/coli_a_00016.pdf by guest on 10 Septem ber 2021\nModel sources: 1Rapp (2003); 2Matveeva et al. (2005); 3Bullinaria and Levy (2007); 4Ruiz-Casado, Alfonseca, and Castells (2005); 5Terra and Clarke (2003); 6Herdag\u030cdelen and Baroni (2009); 7Turney (2008); 8Turney (2001); 9Pado\u0301 and Lapata (2007); 10Landauer and Dumais (1997).\naccording to a Fisher test, the difference between the second-best model, GLSA, and the twelfthmodel, PMI-IR-01, is not significant at the\u03b1 = .05 level (p= 0.07)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16074195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87e43e9eba01a4eb03436c9946bf6aa031a5d5af",
            "isKey": true,
            "numCitedBy": 7200,
            "numCiting": 343,
            "paperAbstract": {
                "fragments": [],
                "text": "This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors."
            },
            "slug": "Tensor-Decompositions-and-Applications-Kolda-Bader",
            "title": {
                "fragments": [],
                "text": "Tensor Decompositions and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This survey provides an overview of higher-order tensor decompositions, their applications, and available software."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17581,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "36eff99a7f23cec395e4efc80ff7f937934c7be6",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "From Pythagoras's harmonic sequence to Einstein's theory of relativity, geometric models of position, proximity, ratio, and the underlying properties of physical space have provided us with powerful ideas and accurate scientific tools. Currently, similar geometric models are being applied to another type of space the conceptual space of information and meaning, where the contributions of Pythagoras and Einstein are a part of the landscape itself. The rich geometry of conceptual space can be glimpsed, for instance, in internet documents: while the documents themselves define a structure of visual layouts and point-to-point links, search engines create an additional structure by matching keywords to nearby documents in a spatial arrangement of content. What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded.\""
            },
            "slug": "Geometry-and-Meaning-Widdows",
            "title": {
                "fragments": [],
                "text": "Geometry and Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "What the \"Geometry of Meaning\" provides is a much-needed exploration of computational techniques to represent meaning and of the conceptual spaces on which these representations are founded."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50877490"
                        ],
                        "name": "G. Karypis",
                        "slug": "G.-Karypis",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Karypis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Karypis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 131
                            }
                        ],
                        "text": "We cluster the nouns in each set by computing their similarity matrix based on pairwise cosines, and feeding it to the widely used CLUTO toolkit (Karypis 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "nouns are clustered with CLUTO (Karypis 2003), using their similarity matrix based on pairwise cosines repeated bisections algorithm with global optimization method (parameters with default values in CLUTO) cluster quality is evaluated by percentage purity (Zhao & Karypis 2001) Purity = 1 n Pk r=1 max i (n r )"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "We use CLUTO\u2019s built-in repeated bisections with global optimization method, accepting all of CLUTO\u2019s default values for this method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "Cluster quality is evaluated by percentage purity, one of the standard clustering quality measures returned by CLUTO (Zhao and Karypis 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59820993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "345807c4b360d4bb1f04792bb279bf0487e6f33e",
            "isKey": true,
            "numCitedBy": 587,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Clustering algorithms divide data into meaningful or useful groups, called clusters, such that the intra-cluster similarity is maximized and the inter-cluster similarity is minimized. These discovered clusters can be used to explain the characteristics of the underlying data distribution and thus serve as the foundation for various data mining and analysis techniques. The applications of clustering include characterization of different customer groups based upon purchasing patterns, categorization of documents on the World Wide Web, grouping of genes and proteins that have similar functionality, grouping of spatial locations prone to earth quakes from seismological data, etc. CLUTO is a software package for clustering low and high dimensional datasets and for analyzing the characteristics of the various clusters."
            },
            "slug": "CLUTO-A-Clustering-Toolkit-Karypis",
            "title": {
                "fragments": [],
                "text": "CLUTO - A Clustering Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Clustering algorithms divide data into meaningful or useful groups, called clusters, such that the intra-cluster similarity is maximized and the inter-clusters similarity is minimized."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709413"
                        ],
                        "name": "C. J. V. Rijsbergen",
                        "slug": "C.-J.-V.-Rijsbergen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Rijsbergen",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. V. Rijsbergen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 170
                            }
                        ],
                        "text": "\u2026research areas such as neural systems (Smolensky 1990; Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole 2005; Hou and Song 2009), holographic models of memory (Jones and Mewhort 2007), and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5312750,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "99614334a3e9b809e43384777409af7eccde3db6",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Prologue 1. Introduction 2. On sets and kinds in IR 3. Vector and Hilbert spaces 4. Linear transformations, operators and matrices 5. Conditional logic in IR 6. The geometry of IR Appendix I. Linear algebra Appendix II. Quantum mechanics Appendix III. Probability Bibliography Index."
            },
            "slug": "The-geometry-of-information-retrieval-Rijsbergen",
            "title": {
                "fragments": [],
                "text": "The geometry of information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The geometry of IR is studied through the lens of linear algebra, quantum mechanics, and vector and Hilbert spaces, with a focus on linear transformations, operators and matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5659557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0c3033ea7d4e19e1f5ac71934759507e126162",
            "isKey": false,
            "numCitedBy": 4466,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains."
            },
            "slug": "An-Information-Theoretic-Definition-of-Similarity-Lin",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Definition of Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work presents an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model and demonstrates how this definition can be used to measure the similarity in a number of different domains."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125580247,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9438172bfbb74a6a4ea4242b180d4335bb1f18b7",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: 1. Introduction, 2. Connectionist Representation and Tensor Product Binding: Definition and Examples, 3. Tensor Product Representation: Properties, 4. Conclusion"
            },
            "slug": "Tensor-Product-Variable-Binding-and-the-of-Symbolic-Hinton",
            "title": {
                "fragments": [],
                "text": "Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This chapter contains sections titled connectionist Representation and Tensor Product Binding: Definition and Examples, and tensor Product Representation: Properties."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115377844"
                        ],
                        "name": "T. Dunning",
                        "slug": "T.-Dunning",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Dunning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dunning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6465096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "025464b73f805e76689a7a20a48a9e9c0f4ff3ef",
            "isKey": false,
            "numCitedBy": 2829,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text."
            },
            "slug": "Accurate-Methods-for-the-Statistics-of-Surprise-and-Dunning",
            "title": {
                "fragments": [],
                "text": "Accurate Methods for the Statistics of Surprise and Coincidence"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basis of a measure based on likelihood ratios that can be applied to the analysis of text is described, and in cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 224
                            }
                        ],
                        "text": "This approach to significance testing is problematic in many respects, the most important being that we ignore dependencies in correct and wrong counts due to the fact that the algorithms are evaluated on the same test set (Dietterich 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 683036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22f0579f212dfb568fbda317cba67c8654d84ccd",
            "isKey": false,
            "numCitedBy": 3143,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 52 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set."
            },
            "slug": "Approximate-Statistical-Tests-for-Comparing-Dietterich",
            "title": {
                "fragments": [],
                "text": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task and measures the power (ability to detect algorithm differences when they do exist) of these tests."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2400157"
                        ],
                        "name": "M. Kenward",
                        "slug": "M.-Kenward",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kenward",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kenward"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19878149,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "75f8a4d7ed6a0f32fa098cac967de247938d9ce5",
            "isKey": false,
            "numCitedBy": 14168,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "15 Empirical Bayes Method, 2nd edition J.S. Maritz and T. Lwin (1989) Symmetric Multivariate and Related Distributions K.-T. Fang, S. Kotz and K. Ng (1989) Ieneralized Linear Models, 2nd edition P. McCullagh and J.A. Neider (1989) 38 Cyclic Designs J.A. John (1987) 39 Analog Estimation Methods in Econometrics C.F. Manski (1988) 40 Subset Selection in Regression A.J. Miller (1990) 41 Analysis of Repeated Measures M. Crowder and D .J. Hand (1990) 42 Statistical Reasoning with Imprecise Probabilities P. Walley (1990) ~3 Generalized Additive Models T.J. Hastie and R.J. Tibshirani (1990) lnspection Errors for Attributes in Quality Control N.L. Johnson, S. Kotz and x. Wu (1991) \u00b75 The Analysis of Contingency Tables, 2nd edition B.S. Everitt (1992) 46 The Analysis of Quantal Response Data B.f.T. Morgan (1992) 47 Longitudinal Data with Serial Correlation: A State-Space Approach R.H. Jones (1993) : Differential Geometry and Statistics M.K. Murray and f. W. Rice (1993) 49 Markov Models and Optimization M.H.A. Davies (1993) 50 Chaos and Networks: Statistical and Probabilistic Aspects Edited by O. Barndorff-Nielsen et al. (1993) Number Theoretic Methods in Statistics K.-T. Fang and W. Yuan (1993) 2 Inference and Asymptotics O. Barndorff-Nielsen and D.R. Cox (1993) ;3 Practical Risk Theory for Actuaries C.D. Daykin, T. Pentikainen and M. Pesonen (1993) 54 Statistical Concepts and Applications in Medicine f. Aitchison and I.f. Lauder (1994) 55 Predictive Inference S. Geisser (1993) 56 Model-Free Curve Estimation M. Tarter and M. Lock (1993) 57 An Introduction to the Bootstrap B. Efron and R . Tibshirani (1993) (Full details concerning this series are available from the Publishers.) An Introduction to the Bootstrap"
            },
            "slug": "An-Introduction-to-the-Bootstrap-Kenward",
            "title": {
                "fragments": [],
                "text": "An Introduction to the Bootstrap"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79074509"
                        ],
                        "name": "J. P. Overschelde",
                        "slug": "J.-P.-Overschelde",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Overschelde",
                            "middleNames": [
                                "P.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Overschelde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070899"
                        ],
                        "name": "K. Rawson",
                        "slug": "K.-Rawson",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Rawson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rawson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4203620"
                        ],
                        "name": "J. Dunlosky",
                        "slug": "J.-Dunlosky",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dunlosky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dunlosky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144158836,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2a91c755a410fc5c4ba908d4e5ad5cb1417dc927",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Category-Norms:-An-Updated-and-Expanded-Version-of-Overschelde-Rawson",
            "title": {
                "fragments": [],
                "text": "Category Norms: An Updated and Expanded Version of the Battig and Montague (1969) Norms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796288"
                        ],
                        "name": "D. Geeraerts",
                        "slug": "D.-Geeraerts",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Geeraerts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geeraerts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9998415,
            "fieldsOfStudy": [
                "Philosophy",
                "Linguistics"
            ],
            "id": "65a3cfd5bfbd74856a4a03645753dba12c8150db",
            "isKey": false,
            "numCitedBy": 426,
            "numCiting": 503,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction 1. Historical-Philological Semantics 2. Structuralist Semantics 3. Generativist Semantics 4. Neostructuralist Semantics 5. Cognitive Semantics Conclusion References"
            },
            "slug": "Theories-of-Lexical-Semantics-Geeraerts",
            "title": {
                "fragments": [],
                "text": "Theories of Lexical Semantics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744364"
                        ],
                        "name": "T. Kolda",
                        "slug": "T.-Kolda",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Kolda",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kolda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "\u2026use the linguistic structure of texts to compute co-occurrences, and only record whether the target occurs\nD ow nloaded from http://direct.m it.edu/coli/article-pdf/36/4/673/1810191/coli_a_00016.pdf by guest on 10 Septem ber 2021\nin or close to the context element, without considering the type\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 167
                            }
                        ],
                        "text": "DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 164
                            }
                        ],
                        "text": "For instance, the Sketch Engine1 builds \u201cword sketches\u201d consisting of triples extracted from parsed corpora and formed by two words linked by a grammatical relation (Kilgarriff et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123208904,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "5084d5c3ea15bf6c11d8c8a2dceec6162b776d71",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two new multilinear operators for expressing the matrix compositions that are needed in the Tucker and PARAFAC (CANDECOMP) decompositions. The first operator, which we call the Tucker operator, is shorthand for performing an n-mode matrix multiplication for every mode of a given tensor and can be employed to concisely express the Tucker decomposition. The second operator, which we call the Kruskal operator, is shorthand for the sum of the outer-products of the columns of N matrices and allows a divorce from a matricized representation and a very concise expression of the PARAFAC decomposition. We explore the properties of the Tucker and Kruskal operators independently of the related decompositions. Additionally, we provide a review of the matrix and tensor operations that are frequently used in the context of tensor decompositions."
            },
            "slug": "Multilinear-operators-for-higher-order-Kolda",
            "title": {
                "fragments": [],
                "text": "Multilinear operators for higher-order decompositions"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Two new multilinear operators are proposed for expressing the matrix compositions that are needed in the Tucker and PARAFAC (CANDECOMP) decompositions and one of them is shorthand for performing an n-mode matrix multiplication for every mode of a given tensor."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097381"
                        ],
                        "name": "T. Raghunathan",
                        "slug": "T.-Raghunathan",
                        "structuredName": {
                            "firstName": "Trivellore",
                            "lastName": "Raghunathan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Raghunathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30857171,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1b998dc83c10cd9257c92f168eff5ef38572afa4",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops and evaluates an approximate procedure for testing homogeneity of an arbitrary subset of correlation coefficients among variables measured on the same set of individuals. The sample may have some missing data. The simple test statistic is a multiple of the variance of Fisher r-to-z transformed correlation coefficients relevant to the null hypothesis being tested and is referred to a chi-square distribution. The use of this test is illustrated through several examples. Given the approximate nature of the test statistics, the procedure was evaluated using a simulation study. The accuracy in terms of the nominal and the actual significance levels of this test for several null hypotheses of interest were evaluated."
            },
            "slug": "An-Approximate-Test-for-Homogeneity-of-Correlated-Raghunathan",
            "title": {
                "fragments": [],
                "text": "An Approximate Test for Homogeneity of Correlated Correlation Coefficients"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1904754"
                        ],
                        "name": "C. D. Meyer",
                        "slug": "C.-D.-Meyer",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Meyer",
                            "middleNames": [
                                "Dean"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Meyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 85
                            }
                        ],
                        "text": "A matrix of this sort performs an orthogonal projection of the vector it multiplies (Meyer 2000, chapter 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118122532,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d6e6c3243e9e4e6dd8f0bc783d7612b7d3863d5f",
            "isKey": false,
            "numCitedBy": 4814,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Linear equations 2. Rectangular systems and echelon forms 3. Matrix algebra 4. Vector spaces 5. Norms, inner products, and orthogonality 6. Determinants 7. Eigenvalues and Eigenvectors 8. Perron-Frobenius theory of nonnegative matrices Index."
            },
            "slug": "Matrix-Analysis-and-Applied-Linear-Algebra-Meyer",
            "title": {
                "fragments": [],
                "text": "Matrix Analysis and Applied Linear Algebra"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The author presents Perron-Frobenius theory of nonnegative matrices Index, a theory of matrices that combines linear equations, vector spaces, and matrix algebra with insights into eigenvalues and Eigenvectors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 166
                            }
                        ],
                        "text": "\u2026NOUN (Veale and Hao 2008): as sharp as a knife\u2192 \u3008sharp, as adj as+j+n-a, knife\u3009;\nsuch as: links two nouns occurring in the templates NOUN such as NOUN and such NOUN as NOUN (Hearst 1992, 1998): animals such as cats\u2192 \u3008animal, such as+ns+ns, cat\u3009; such vehicles as cars\u2192 \u3008vehicle, such as+ns+ns, car\u3009."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "For instance, NOUN such as NOUN and such NOUN as NOUN were first proposed by Hearst (1992) as highly reliable patterns for hypernym identification, whereas (the) attribute noun of (a|the) NOUN is ADJ and (a|the) ADJ attribute noun of NOUN were successfully used to identify typical values of concept attributes (Almuhareb and Poesio 2004; Veale and Hao 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "This pattern connects adjectives and nouns that occur in the templates (the) attribute noun of (a|the) NOUN is ADJ (Almuhareb and Poesio 2004) and (a|the) ADJ attribute noun of NOUN (Veale and Hao 2008): the color of strawberries is red\u2192 \u3008red, color+j+ns, strawberry\u3009; the autumnal color of the forest\u2192 \u3008autumnal, color+j+n-the, forest\u3009;\nas adj as: this pattern links an adjective and a noun that match the template as ADJ as (a|the) NOUN (Veale and Hao 2008): as sharp as a knife\u2192 \u3008sharp, as adj as+j+n-a, knife\u3009;\nsuch as: links two nouns occurring in the templates NOUN such as NOUN and such NOUN as NOUN (Hearst 1992, 1998): animals such as cats\u2192 \u3008animal, such as+ns+ns, cat\u3009; such vehicles as cars\u2192 \u3008vehicle, such as+ns+ns, car\u3009."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 91
                            }
                        ],
                        "text": "Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., finding parts)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated discovery of WordNet relations In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database"
            },
            "venue": {
                "fragments": [],
                "text": "Automated discovery of WordNet relations In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 12
                            }
                        ],
                        "text": "E-mail: alessandro.lenci@ling.unipi.it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 124
                            }
                        ],
                        "text": "Attributional similarity is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and Burgess 1996; Schu\u0308tze 1997; Bullinaria and Levy 2007; Pado\u0301 and Lapata 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 60
                            }
                        ],
                        "text": "As an alternative to this \u201cone task, one model\u201d approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Distributional Memory: A General Framework for Corpus-Based Semantics\nMarco Baroni\u2217\nUniversity of Trento\nAlessandro Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 169
                            }
                        ],
                        "text": "Win is an unstructured DSM that does not rely on syntactic structure to select the collocates, but just on their linear proximity to the targets (Lund and Burgess 1996; Schu\u0308tze 1997; Bullinaria and Levy 2007, and many others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 232
                            }
                        ],
                        "text": "\u2026Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 191
                            }
                        ],
                        "text": "The latter can be either documents (Landauer and Dumais 1997; Griffiths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the target (Lund and Burgess 1996; Schu\u0308tze 1997; Rapp 2003; Bullinaria and Levy 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2026and cognitive scientists in corpus-basedmodels of semantic representation (Grefenstette 1994; Lund and Burgess 1996; Landauer and Dumais 1997; Schu\u0308tze 1997; Sahlgren 2006; Bullinaria and Levy 2007; Griffiths, Steyvers, and Tenenbaum 2007; Pado\u0301 and Lapata 2007; Lenci 2008; Turney and Pantel\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ambiguity Resolution in Natural Language Learning. CSLI"
            },
            "venue": {
                "fragments": [],
                "text": "Ambiguity Resolution in Natural Language Learning. CSLI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34884385"
                        ],
                        "name": "E. Barbu",
                        "slug": "E.-Barbu",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Barbu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Barbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840946"
                        ],
                        "name": "B. Murphy",
                        "slug": "B.-Murphy",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Murphy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678591"
                        ],
                        "name": "Massimo Poesio",
                        "slug": "Massimo-Poesio",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Poesio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimo Poesio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 47
                            }
                        ],
                        "text": "Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": "The Battig test set introduced by Baroni et al. (2010) is based on the expanded Battig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 95
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "This model is based on the idea, motivated and tested by Baroni et al. (2010)\u2014 but see also Davidov and Rappoport (2008a, 2008b) for a related method\u2014that what matters is not so much the frequency of a link, but the variety of surface forms that express it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026word\u2013link\u2013word tuples from a dependency parse of a corpus, but this is not a requirement for DM: The links could for example be based on frequent n-grams as in Turney (2006b) and Baroni et al. (2010), or even on very different kinds of relation, such as co-occurring within the same document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1941,
                                "start": 95
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009). Instead, we briefly discuss two other studies that explicitly advocate a uniform approach to corpus-based semantic tasks, and one article that, like us, proposes a tensor-based formalization of corpus-extracted triples. See Turney and Pantel (2010) for a very recent general survey of DSMs. Pad\u00f3 and Lapata (2007), partly inspired by Lowe (2001), have proposed an interesting general formalization of DSMs. In their approach, a corpus-based semantic model is characterized by (1) a set of functions to extract statistics from the corpus, (2) construction of the basis-by-target-elements co-occurrence matrix, and (3) a similarity function operating on the matrix. Our focus is entirely on the second aspect. A DM, according to the characterization in Section 3, is a labeled tensor based on a source weighted tuple structure and coupled with matricization operations. How the tuple structure was built (corpus extraction methods, association measures, etc.) is not part of the DM formalization. At the other end, DM provides sets of vectors in different vector spaces, but it is agnostic about how they are used (measuring similarity via cosines or other measures, reducing the matrices with SVD, etc.). Of course, much of the interesting progress in distributional semantics will occur at the two ends of our tensor, with better tuple extraction and weighting techniques on one side, and better matrix manipulation and similarity measurement on the other. As long as the former operations result in data that can be arranged into a weighted tuple structure, and the latter procedures act on vectors, such innovations fit into the DM framework and can be used to improve performance on tasks defined on any space derivable from the DM tensor. Whereas the model proposed by Pad\u00f3 and Lapata (2007) is designed only to address tasks involving the measurement of the attributional similarity between words, Turney (2008) shares with DM the goal of unifying attributional and relational similarity under the same distributional model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 47
                            }
                        ],
                        "text": "Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 211
                            }
                        ],
                        "text": "Table 12 reports the percentage overlap with the gold standard properties (averaged across the 44 concepts) for our models as well as the only ESSLLI 2008 participant that tried this task, and for the models of Baroni et al. (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 394,
                                "start": 95
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009). Instead, we briefly discuss two other studies that explicitly advocate a uniform approach to corpus-based semantic tasks, and one article that, like us, proposes a tensor-based formalization of corpus-extracted triples. See Turney and Pantel (2010) for a very recent general survey of DSMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 95
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009). Instead, we briefly discuss two other studies that explicitly advocate a uniform approach to corpus-based semantic tasks, and one article that, like us, proposes a tensor-based formalization of corpus-extracted triples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 275
                            }
                        ],
                        "text": "Recently, there has been some interest in the automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 459,
                                "start": 95
                            }
                        ],
                        "text": "We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009). Instead, we briefly discuss two other studies that explicitly advocate a uniform approach to corpus-based semantic tasks, and one article that, like us, proposes a tensor-based formalization of corpus-extracted triples. See Turney and Pantel (2010) for a very recent general survey of DSMs. Pad\u00f3 and Lapata (2007), partly inspired by Lowe (2001), have proposed an interesting general formalization of DSMs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 234
                            }
                        ],
                        "text": "All models are based on the natural idea of extracting word\u2013link\u2013word tuples from a dependency parse of a corpus, but this is not a requirement for DM: The links could for example be based on frequent n-grams as in Turney (2006b) and Baroni et al. (2010), or even on very different kinds of relation, such as co-occurring within the same document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 4
                            }
                        ],
                        "text": "See Baroni et al. (2010) for the full list."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58438864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fa08a45c1cf445fde2a3de625aa4b9cd025465f",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Strudel:-A-distributional-semantic-model-based-on-Baroni-Barbu",
            "title": {
                "fragments": [],
                "text": "Strudel: A distributional semantic model based on properties and types"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 230
                            }
                        ],
                        "text": "\u202631.4 26.6\u201336.2 BagPack5 44.1 39.0\u201349.3 LexDM 29.3 24.8\u201334.3 k-means6 44.0 39.0\u201349.3 Random 20.0 16.1\u201324.5\nModel sources: 1Turney and Littman (2005); 2Turney (2006b); 3Turney (2006a); 4Turney (2008); 5Herdag\u030cdelen and Baroni (2009); 6Bicic\u0327i and Yuret (2006); 7Quesada, Mangalath, and Kintsch (2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 215881965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bb1488931004a5e3cb170323f630ff994a38772",
            "isKey": true,
            "numCitedBy": 7,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Clustering-Word-Pairs-to-Answer-Analogy-Questions-Yuret",
            "title": {
                "fragments": [],
                "text": "Clustering Word Pairs to Answer Analogy Questions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 28
                            }
                        ],
                        "text": "More precisely, in theW1W2\u00d7L tasks where we know the set of target pairs in advance (Sections 6.2.1 and 6.2.2), we smooth the DMmodels by combining in turn one of the words of each target pair with the top 20 nearestW1\u00d7LW2 neighbors of the other word, obtaining a total of 41 pairs (including the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "One distributional memory, many semantic tasks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the EACL GEMS Workshop, pages 1\u20138, Athens."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256003"
                        ],
                        "name": "Vivi Nastase",
                        "slug": "Vivi-Nastase",
                        "structuredName": {
                            "firstName": "Vivi",
                            "lastName": "Nastase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivi Nastase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795595"
                        ],
                        "name": "S. Szpakowicz",
                        "slug": "S.-Szpakowicz",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Szpakowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szpakowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808366"
                        ],
                        "name": "Deniz Yuret",
                        "slug": "Deniz-Yuret",
                        "structuredName": {
                            "firstName": "Deniz",
                            "lastName": "Yuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deniz Yuret"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219306279,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3fe924b38355b16063bd910b2dead6c797586d56",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SemEval-2007-Task-04:-Classification-of-Semantic-Girju-Nakov",
            "title": {
                "fragments": [],
                "text": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals"
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762553"
                        ],
                        "name": "G. Legendre",
                        "slug": "G.-Legendre",
                        "structuredName": {
                            "firstName": "Geraldine",
                            "lastName": "Legendre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Legendre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 142
                            }
                        ],
                        "text": "\u2026being explored in corpus-based semantics, and interesting synergies are emerging with research areas such as neural systems (Smolensky 1990; Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63264598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97f7368710ab8ace806994d633e3870631e01fc0",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Harmonic-Mind:-From-Neural-Computation-to-I:-Smolensky-Legendre",
            "title": {
                "fragments": [],
                "text": "The Harmonic Mind: From Neural Computation to Optimality-Theoretic GrammarVolume I: Cognitive Architecture (Bradford Books)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73271222"
                        ],
                        "name": "Takafumi Ogata",
                        "slug": "Takafumi-Ogata",
                        "structuredName": {
                            "firstName": "Takafumi",
                            "lastName": "Ogata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takafumi Ogata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64024872,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9301aa6150ac2d3d4d458a6aacc28abb9fbd911a",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Causative-Inchoative-Alternation-Ogata",
            "title": {
                "fragments": [],
                "text": "Causative-Inchoative Alternation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 251
                            }
                        ],
                        "text": "Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60830348,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "8cf3a13329f9369f507347f891507a43a784db25",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Ontology-and-the-Lexicon:-The-life-cycle-of-Lenci",
            "title": {
                "fragments": [],
                "text": "Ontology and the Lexicon: The life cycle of knowledge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2736974"
                        ],
                        "name": "Abdulrahman Almuhareb",
                        "slug": "Abdulrahman-Almuhareb",
                        "structuredName": {
                            "firstName": "Abdulrahman",
                            "lastName": "Almuhareb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdulrahman Almuhareb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40718894,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7826741b9ce89bbdf951bea059e5cbb74af42b5c",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Attributes-in-lexical-acquisition-Almuhareb",
            "title": {
                "fragments": [],
                "text": "Attributes in lexical acquisition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742533"
                        ],
                        "name": "Sergei N. Art\u00ebmov",
                        "slug": "Sergei-N.-Art\u00ebmov",
                        "structuredName": {
                            "firstName": "Sergei",
                            "lastName": "Art\u00ebmov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergei N. Art\u00ebmov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737744"
                        ],
                        "name": "H. Barringer",
                        "slug": "H.-Barringer",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Barringer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barringer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2925941"
                        ],
                        "name": "A. Garcez",
                        "slug": "A.-Garcez",
                        "structuredName": {
                            "firstName": "Artur",
                            "lastName": "Garcez",
                            "middleNames": [
                                "S.",
                                "d'Avila"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Garcez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335532"
                        ],
                        "name": "L. Lamb",
                        "slug": "L.-Lamb",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Lamb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143874339"
                        ],
                        "name": "John Woods",
                        "slug": "John-Woods",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Woods",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Woods"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26618352,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "13b3dc778ac84e05abcea3c047c621fcea0e3c02",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "We-Will-Show-Them!-Essays-in-Honour-of-Dov-Gabbay,-Art\u00ebmov-Barringer",
            "title": {
                "fragments": [],
                "text": "We Will Show Them! Essays in Honour of Dov Gabbay, Volume One"
            },
            "venue": {
                "fragments": [],
                "text": "We Will Show Them!"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403837997"
                        ],
                        "name": "Maria Ruiz-Casado",
                        "slug": "Maria-Ruiz-Casado",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Ruiz-Casado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Ruiz-Casado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727837"
                        ],
                        "name": "Enrique Alfonseca",
                        "slug": "Enrique-Alfonseca",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Alfonseca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrique Alfonseca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912921"
                        ],
                        "name": "P. Castells",
                        "slug": "P.-Castells",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Castells",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Castells"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18729302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "114c0b6d9279c74936c62b55918be23ab8448108",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new, unsupervised procedure called Context-window overlapping for calculating the semantic distance between two terms. It is based on the distributional semantics hypothesis, and, in particular, in the fact that synonym words should be interchangeable in every context, and hyponyms can be substituted by their hyperonyms in most contexts. The procedure has been applied to synonym identification, and to ontology extension. In the first task, it has been evaluated with 80 synonym test questions from the TOEFL which already constitute a standard test set in this problem, and attains results similar to most other non-ensemble procedures. Interestingly, it clearly outperforms Latent Semantic Analysis, other procedure grounded on the Distributional Semantic hypothesis. Concerning ontology enrichment, the results obtained are promising, although they can still be much improved. Conclusions are drawn from this result, and we outline several possibilities for future work."
            },
            "slug": "Using-context-window-overlapping-in-synonym-and-Ruiz-Casado-Alfonseca",
            "title": {
                "fragments": [],
                "text": "Using context-window overlapping in synonym discovery and ontology extension"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new, unsupervised procedure called Context-window overlapping for calculating the semantic distance between two terms, based on the distributional semantics hypothesis, which clearly outperforms Latent Semantic Analysis, other procedure grounded on the Distributional Semantic hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised discovery of 717 Downloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00016 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 223
                            }
                        ],
                        "text": "As an alternative to this \u201cone task, one model\u201d approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree of semantic similarity between two words (or other linguistic units) can be modeled\n\u2217\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional structure. Word"
            },
            "venue": {
                "fragments": [],
                "text": "Distributional structure. Word"
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised discovery of 717 Downloaded from http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00016 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 100
                            }
                        ],
                        "text": "Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Workshop shared tasks (Baroni, Evert, and Lenci 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 244
                            }
                        ],
                        "text": "Recently, there has been some interest in the automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 119
                            }
                        ],
                        "text": "We test this approach in the ESSLLI 2008 Distributional Semantic Workshop unconstrained property generation challenge (Baroni, Evert, and Lenci 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI"
            },
            "venue": {
                "fragments": [],
                "text": "Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 234
                            }
                        ],
                        "text": "\u2026Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional approaches in linguistic and cognitive research"
            },
            "venue": {
                "fragments": [],
                "text": "Italian Journal of Linguistics, 20(1):1\u201331."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "MIT Press, Cambridge, MA."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1977.Word Meaning and Montague Grammar. Kluwer, Dordrecht"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 148
                            }
                        ],
                        "text": "\u2026Trento\nAlessandro Lenci\u2217\u2217\nUniversity of Pisa\nResearch into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 161
                            }
                        ],
                        "text": "It seems safe to conclude that structured models are at least not worse than unstructured models\u2014an important conclusion for us, as DM is built upon the structured DSM idea."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Word-Space Model"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. dissertation, Stockholm University."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word vectors and quantum logic"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eighth Mathematics of Language Conference, pages 1\u201314, Bloomington, IN."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Integration of Syntax and Semantic Plausibility in a 719"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploring noun-modifier semantic relations"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Workshop on Computational Semantics, pages 285\u2013301, Tilburg,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "editor. 1998.WordNet: An Electronic Lexical Database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ambiguity Resolution in Natural Language Learning"
            },
            "venue": {
                "fragments": [],
                "text": "CSLI Publications, Stanford, CA."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Statistics of Word Cooccurrences"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. dissertation, Stuttgart University."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised discovery"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Integration of Syntax and Semantic Plausibility in a 719 Downloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00016 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Category 720 Downloaded from http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00016 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Category Baroni and Lenci Distributional Memory norms: An updated and expanded version of the Battig and Montague (1969) norms"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Memory and Language"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Integration of Syntax and Semantic Plausibility in a 719 Downloaded from http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00016 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ambiguity Resolution in Natural Language Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Ambiguity Resolution in Natural Language Learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The life cycle of knowledge"
            },
            "venue": {
                "fragments": [],
                "text": "Chu-Ren Huang, Nicoletta Calzolari, Aldo Gangemi, Alessandro Lenci, Alessandro Oltramari, and Laurent Pr\u00e9vot, editors, Ontology"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Model sources: 1Rapp (2003); 2Matveeva et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bootstrapping distributional feature vector quality"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics, 35(3):435\u2013461. 721"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 187
                            }
                        ],
                        "text": "Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modelling word similarity"
            },
            "venue": {
                "fragments": [],
                "text": "an evaluation of automatic synonymy extraction algorithms. In Proceedings of LREC, pages 3243\u20133249,"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wide-Coverage Model of Sentence Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 91
                            }
                        ],
                        "text": "This model is based on the idea, motivated and tested by Baroni et al. (2010)\u2014 but see also Davidov and Rappoport (2008a, 2008b) for a related method\u2014that what matters is not so much the frequency of a link, but the variety of surface forms that express it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised discovery of 717 Computational Linguistics Volume 36, Number 4 generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ACL"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Category 720 Downloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00016 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 30
                            }
                        ],
                        "text": "Model sources: 1Rapp (2003); 2Matveeva et al. (2005); 3Bullinaria and Levy (2007); 4Ruiz-Casado, Alfonseca, and Castells (2005); 5Terra and Clarke (2003); 6Herda\u01e7delen and Baroni (2009); 7Turney (2008); 8Turney (2001); 9Pad\u00f3 and Lapata (2007); 10Landauer and Dumais (1997)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized latent semantic analysis for term representation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of RANLP, pages 60\u201368, Borovets."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Harmonic Mind"
            },
            "venue": {
                "fragments": [],
                "text": "From Neural Computation to Optimality-theoretic Grammar. MIT Press, Cambridge, MA."
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 47,
            "methodology": 21,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 148,
        "totalPages": 15
    },
    "page_url": "https://www.semanticscholar.org/paper/Distributional-Memory:-A-General-Framework-for-Baroni-Lenci/553fb89d5858826c02f26e94262e8958debc777e?sort=total-citations"
}