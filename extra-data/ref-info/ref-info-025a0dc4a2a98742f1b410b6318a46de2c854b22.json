{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32774629"
                        ],
                        "name": "Alexander Richard",
                        "slug": "Alexander-Richard",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Richard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Richard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123446103"
                        ],
                        "name": "Hilde Kuehne",
                        "slug": "Hilde-Kuehne",
                        "structuredName": {
                            "firstName": "Hilde",
                            "lastName": "Kuehne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hilde Kuehne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055756406"
                        ],
                        "name": "Ahsan Iqbal",
                        "slug": "Ahsan-Iqbal",
                        "structuredName": {
                            "firstName": "Ahsan",
                            "lastName": "Iqbal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahsan Iqbal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "IDEr Zhou et al. (2018c) 4.38 11.55 27.44 0.38 S3D 3.24 9.52 26.09 0.31 VideoBERT 4.33 11.94 28.80 0.55 CBT 5.12( 0.02) 12.97( 0.05) 30.44( 0.08) 0.64( 0.00) Method Frame Acc. Tang et al. (2019) 25.8 Richard et al. (2018) 21.2 Ding &amp; Xu (2018) 34.3 CBT 53.9 Table 4: (Left) Video captioning results on the YouCook2 dataset (Zhou et al., 2018b). We compare with previous state-of-the-art methods by Zhou et al. (2018c)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 4935679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80184c6a88fc97a09393b7336bc2ddb12e9b1030",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks and include these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods."
            },
            "slug": "NeuralNetwork-Viterbi:-A-Framework-for-Weakly-Video-Richard-Kuehne",
            "title": {
                "fragments": [],
                "text": "NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data and shows that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49588480"
                        ],
                        "name": "Austin Myers",
                        "slug": "Austin-Myers",
                        "structuredName": {
                            "firstName": "Austin",
                            "lastName": "Myers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Austin Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(2018c) and Sun et al. (2019a), the caption decoder of all methods share the same architecture, the main difference comes from the visual encoder."
                    },
                    "intents": []
                }
            ],
            "corpusId": 102483628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c41a11c0e9b8b92b4faaf97749841170b760760a",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features."
            },
            "slug": "VideoBERT:-A-Joint-Model-for-Video-and-Language-Sun-Myers",
            "title": {
                "fragments": [],
                "text": "VideoBERT: A Joint Model for Video and Language Representation Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work builds upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively, which can be applied directly to open-vocabulary classification."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806773"
                        ],
                        "name": "Ishan Misra",
                        "slug": "Ishan-Misra",
                        "structuredName": {
                            "firstName": "Ishan",
                            "lastName": "Misra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ishan Misra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some recent work have been done on learning self-supervised video representation (Vondrick et al., 2016; Wang & Gupta, 2015; Misra et al., 2016; Sermanet et al., 2018; Han et al., 2019; Xu et al., 2019; Wang et al., 2019a) by defining pretext tasks such as ordering (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "First we compare against 2DCNN approaches Shuffle&Learn (Misra et al., 2016) and OPN (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, we consider the Shuffle&Learn (Misra et al., 2016) and 3DRotNet (Jing et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9348728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d4e3616d0b27957c4107ae877dc0dd4504b69ab",
            "isKey": false,
            "numCitedBy": 610,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy."
            },
            "slug": "Shuffle-and-Learn:-Unsupervised-Learning-Using-Misra-Zitnick",
            "title": {
                "fragments": [],
                "text": "Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper forms an approach for learning a visual representation from the raw spatiotemporal signals in videos using a Convolutional Neural Network, and shows that this method captures information that is temporally varying, such as human pose."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856712"
                        ],
                        "name": "Olivier J. H\u00e9naff",
                        "slug": "Olivier-J.-H\u00e9naff",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "H\u00e9naff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier J. H\u00e9naff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41207614"
                        ],
                        "name": "A. Srinivas",
                        "slug": "A.-Srinivas",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Srinivas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Srinivas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3364908"
                        ],
                        "name": "J. Fauw",
                        "slug": "J.-Fauw",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Fauw",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fauw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653164"
                        ],
                        "name": "Ali Razavi",
                        "slug": "Ali-Razavi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Razavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Razavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786693"
                        ],
                        "name": "Carl Doersch",
                        "slug": "Carl-Doersch",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Doersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Doersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422336"
                        ],
                        "name": "A\u00e4ron van den Oord",
                        "slug": "A\u00e4ron-van-den-Oord",
                        "structuredName": {
                            "firstName": "A\u00e4ron",
                            "lastName": "Oord",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A\u00e4ron van den Oord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 162168848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cae417456711c4da184f5efcd1b7464a7a0661a",
            "isKey": false,
            "numCitedBy": 769,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers."
            },
            "slug": "Data-Efficient-Image-Recognition-with-Contrastive-H\u00e9naff-Srinivas",
            "title": {
                "fragments": [],
                "text": "Data-Efficient Image Recognition with Contrastive Predictive Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations which make the variability in natural signals more predictable, and produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22237490"
                        ],
                        "name": "Tengda Han",
                        "slug": "Tengda-Han",
                        "structuredName": {
                            "firstName": "Tengda",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tengda Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10096695"
                        ],
                        "name": "Weidi Xie",
                        "slug": "Weidi-Xie",
                        "structuredName": {
                            "firstName": "Weidi",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weidi Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 202542591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0174d263d3a77bf03fce831a9a5ce2678e1959f0",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet."
            },
            "slug": "Video-Representation-Learning-by-Dense-Predictive-Han-Xie",
            "title": {
                "fragments": [],
                "text": "Video Representation Learning by Dense Predictive Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101 and HMDB51, outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29724362"
                        ],
                        "name": "Longlong Jing",
                        "slug": "Longlong-Jing",
                        "structuredName": {
                            "firstName": "Longlong",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longlong Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38101706"
                        ],
                        "name": "Xiaodong Yang",
                        "slug": "Xiaodong-Yang",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800425"
                        ],
                        "name": "Jingen Liu",
                        "slug": "Jingen-Liu",
                        "structuredName": {
                            "firstName": "Jingen",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingen Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35484757"
                        ],
                        "name": "Yingli Tian",
                        "slug": "Yingli-Tian",
                        "structuredName": {
                            "firstName": "Yingli",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingli Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 102353462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7fa3b8ed5f3f75ace4fe8447ccd9abfbb19e621",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of deep neural networks generally requires a vast amount of training data to be labeled, which is expensive and unfeasible in scale, especially for video collections. To alleviate this problem, in this paper, we propose 3DRotNet: a fully self-supervised approach to learn spatiotemporal features from unlabeled videos. A set of rotations are applied to all videos, and a pretext task is defined as prediction of these rotations. When accomplishing this task, 3DRotNet is actually trained to understand the semantic concepts and motions in videos. In other words, it learns a spatiotemporal video representation, which can be transferred to improve video understanding tasks in small datasets. Our extensive experiments successfully demonstrate the effectiveness of the proposed framework on action recognition, leading to significant improvements over the state-of-the-art self-supervised methods. With the self-supervised pre-trained 3DRotNet from large datasets, the recognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51 respectively, compared to the models trained from scratch."
            },
            "slug": "Self-Supervised-Spatiotemporal-Feature-Learning-via-Jing-Yang",
            "title": {
                "fragments": [],
                "text": "Self-Supervised Spatiotemporal Feature Learning via Video Rotation Prediction."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "With the self-supervised pre-trained 3DRotNet from large datasets, the recognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51 respectively, compared to the models trained from scratch."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50854337"
                        ],
                        "name": "D. Xu",
                        "slug": "D.-Xu",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145974111"
                        ],
                        "name": "Jun Xiao",
                        "slug": "Jun-Xiao",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47122432"
                        ],
                        "name": "Zhou Zhao",
                        "slug": "Zhou-Zhao",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549731"
                        ],
                        "name": "Jian Shao",
                        "slug": "Jian-Shao",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Shao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054592958"
                        ],
                        "name": "Di Xie",
                        "slug": "Di-Xie",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143749205"
                        ],
                        "name": "Yueting Zhuang",
                        "slug": "Yueting-Zhuang",
                        "structuredName": {
                            "firstName": "Yueting",
                            "lastName": "Zhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yueting Zhuang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "8 ClipOrder (Xu et al., 2019) UCF101 72."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some recent work have been done on learning self-supervised video representation (Vondrick et al., 2016; Wang & Gupta, 2015; Misra et al., 2016; Sermanet et al., 2018; Han et al., 2019; Xu et al., 2019; Wang et al., 2019a) by defining pretext tasks such as ordering (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Table 4: (Left) Video captioning results on the YouCook2 dataset (Zhou et al., 2018b). We compare with previous state-of-the-art methods by Zhou et al. (2018c) and Sun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195504152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "558aeb7aa38cfcf8dd9951bfd24cf77972bd09aa",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a self-supervised spatiotemporal learning technique which leverages the chronological order of videos. Our method can learn the spatiotemporal representation of the video by predicting the order of shuffled clips from the video. The category of the video is not required, which gives our technique the potential to take advantage of infinite unannotated videos. There exist related works which use frames, while compared to frames, clips are more consistent with the video dynamics. Clips can help to reduce the uncertainty of orders and are more appropriate to learn a video representation. The 3D convolutional neural networks are utilized to extract features for clips, and these features are processed to predict the actual order. The learned representations are evaluated via nearest neighbor retrieval experiments. We also use the learned networks as the pre-trained models and finetune them on the action recognition task. Three types of 3D convolutional neural networks are tested in experiments, and we gain large improvements compared to existing self-supervised methods."
            },
            "slug": "Self-Supervised-Spatiotemporal-Learning-via-Video-Xu-Xiao",
            "title": {
                "fragments": [],
                "text": "Self-Supervised Spatiotemporal Learning via Video Clip Order Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A self-supervised spatiotemporal learning technique which leverages the chronological order of videos to learn the spatiotmporal representation of the video by predicting the order of shuffled clips from the video."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49923155"
                        ],
                        "name": "Hsin-Ying Lee",
                        "slug": "Hsin-Ying-Lee",
                        "structuredName": {
                            "firstName": "Hsin-Ying",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsin-Ying Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3068086"
                        ],
                        "name": "Jia-Bin Huang",
                        "slug": "Jia-Bin-Huang",
                        "structuredName": {
                            "firstName": "Jia-Bin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia-Bin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144688398"
                        ],
                        "name": "Maneesh Kumar Singh",
                        "slug": "Maneesh-Kumar-Singh",
                        "structuredName": {
                            "firstName": "Maneesh",
                            "lastName": "Singh",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maneesh Kumar Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2019a) by defining pretext tasks such as ordering (Lee et al., 2017), rotation (Jing et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2016) and OPN (Lee et al., 2017)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1 OPN (Lee et al., 2017) UCF101 59."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9232547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de40803a3d17dd406e25922695266d6ed580e371",
            "isKey": true,
            "numCitedBy": 391,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised representation learning approach using videos without semantic labels. We leverage the temporal coherence as a supervisory signal by formulating representation learning as a sequence sorting task. We take temporally shuffled frames (i.e., in non-chronological order) as inputs and train a convolutional neural network to sort the shuffled sequences. Similar to comparison-based sorting algorithms, we propose to extract features from all frame pairs and aggregate them to predict the correct order. As sorting shuffled image sequence requires an understanding of the statistical temporal structure of images, training with such a proxy task allows us to learn rich and generalizable visual representation. We validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification, and object detection tasks."
            },
            "slug": "Unsupervised-Representation-Learning-by-Sorting-Lee-Huang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Representation Learning by Sorting Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The experimental results show that the unsupervised representation learning approach using videos without semantic labels compares favorably against state-of-the-art methods on action recognition, image classification, and object detection tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2978413"
                        ],
                        "name": "Chao-Yuan Wu",
                        "slug": "Chao-Yuan-Wu",
                        "structuredName": {
                            "firstName": "Chao-Yuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao-Yuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2322150"
                        ],
                        "name": "Christoph Feichtenhofer",
                        "slug": "Christoph-Feichtenhofer",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Feichtenhofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph Feichtenhofer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146884473"
                        ],
                        "name": "Haoqi Fan",
                        "slug": "Haoqi-Fan",
                        "structuredName": {
                            "firstName": "Haoqi",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haoqi Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562966"
                        ],
                        "name": "Philipp Kr\u00e4henb\u00fchl",
                        "slug": "Philipp-Kr\u00e4henb\u00fchl",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Kr\u00e4henb\u00fchl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Kr\u00e4henb\u00fchl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nds of video. Long-term context can be encoded by recurrent neural networks (Abu Farha et al., 2018; Sun et al., 2019b), graph convolutional networks (Zhang et al., 2019), or long-term feature banks (Wu et al., 2019), but these are all supervised methods. Some recent work have been done on learning self-supervised video representation (Vondrick et al., 2016; Wang &amp; Gupta, 2015; Misra et al., 2016; Sermanet et"
                    },
                    "intents": []
                }
            ],
            "corpusId": 54476257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45cb2cbae5c0b4cc52e524cc191a2f8db674ed42",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank\u2014supportive information extracted over the entire span of a video\u2014to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online."
            },
            "slug": "Long-Term-Feature-Banks-for-Detailed-Video-Wu-Feichtenhofer",
            "title": {
                "fragments": [],
                "text": "Long-Term Feature Banks for Detailed Video Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a long-term feature bank\u2014supportive information extracted over the entire span of a video\u2014to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11797475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67dccc9a856b60bdc4d058d83657a089b8ad4486",
            "isKey": false,
            "numCitedBy": 5478,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. \n \nOur contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification."
            },
            "slug": "Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Two-Stream Convolutional Networks for Action Recognition in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a two-stream ConvNet architecture which incorporates spatial and temporal networks and demonstrates that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367683"
                        ],
                        "name": "H. Pirsiavash",
                        "slug": "H.-Pirsiavash",
                        "structuredName": {
                            "firstName": "Hamed",
                            "lastName": "Pirsiavash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pirsiavash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n learning deep representations for videos have been focusing on capturing the appearance and local motion pattern from short video clips [22, 13, 11], in an supervised [26, 5, 35] or self-supervised [15, 31] fashion. However, to understand longer videos, we need powerful temporal representations for both the semantics and the dynamics. The capacity of reasoning over longtemporal horizons is critical for "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9933254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
            "isKey": false,
            "numCitedBy": 1109,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation."
            },
            "slug": "Generating-Videos-with-Scene-Dynamics-Vondrick-Pirsiavash",
            "title": {
                "fragments": [],
                "text": "Generating Videos with Scene Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background is proposed that can generate tiny videos up to a second at full frame rate better than simple baselines."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47940821"
                        ],
                        "name": "Hang Zhao",
                        "slug": "Hang-Zhao",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158271"
                        ],
                        "name": "Chuang Gan",
                        "slug": "Chuang-Gan",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41020711"
                        ],
                        "name": "Andrew Rouditchenko",
                        "slug": "Andrew-Rouditchenko",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rouditchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rouditchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324658"
                        ],
                        "name": "Josh H. McDermott",
                        "slug": "Josh-H.-McDermott",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "McDermott",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josh H. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since videos contain both visual and audio signals that are roughly synchronized, the two modalities can supervised each other, as explored in prior work such as (Aytar et al., 2016; Owens et al., 2016b;a; Zhao et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4748509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe018f22600d07cbd0452a070e03708886470015",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources."
            },
            "slug": "The-Sound-of-Pixels-Zhao-Gan",
            "title": {
                "fragments": [],
                "text": "The Sound of Pixels"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Qualitative results suggest the PixelPlayer model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources, and experimental results show that the proposed Mix-and-Separate framework outperforms several baselines on source separation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3152281"
                        ],
                        "name": "Y. Aytar",
                        "slug": "Y.-Aytar",
                        "structuredName": {
                            "firstName": "Yusuf",
                            "lastName": "Aytar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aytar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since videos contain both visual and audio signals that are roughly synchronized, the two modalities can supervised each other, as explored in prior work such as (Aytar et al., 2016; Owens et al., 2016b;a; Zhao et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2915490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels."
            },
            "slug": "SoundNet:-Learning-Sound-Representations-from-Video-Aytar-Vondrick",
            "title": {
                "fragments": [],
                "text": "SoundNet: Learning Sound Representations from Unlabeled Video"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge, and suggests some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19200186"
                        ],
                        "name": "Antoine Miech",
                        "slug": "Antoine-Miech",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Miech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Miech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4713953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3448af861bf5d44ce7ab6b25002504815212252e",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Joint understanding of video and language is an active research area with many applications. Prior work in this domain typically relies on learning text-video embeddings. One difficulty with this approach, however, is the lack of large-scale annotated video-caption datasets for training. To address this issue, we aim at learning text-video embeddings from heterogeneous data sources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training. As a result, our framework can learn improved text-video embeddings simultaneously from image and video datasets. We also show the generalization of MEE to other input modalities such as face descriptors. We evaluate our method on the task of video retrieval and report results for the MPII Movie Description and MSR-VTT datasets. The proposed MEE model demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video-to-text retrieval tasks."
            },
            "slug": "Learning-a-Text-Video-Embedding-from-Incomplete-and-Miech-Laptev",
            "title": {
                "fragments": [],
                "text": "Learning a Text-Video Embedding from Incomplete and Heterogeneous Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a Mixture-of-Embedding-Experts (MEE) model with ability to handle missing input modalities during training and demonstrates significant improvements and outperforms previously reported methods on both text-to-video and video- to-text retrieval tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155180275"
                        ],
                        "name": "Yubo Zhang",
                        "slug": "Yubo-Zhang",
                        "structuredName": {
                            "firstName": "Yubo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yubo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2931554"
                        ],
                        "name": "P. Tokmakov",
                        "slug": "P.-Tokmakov",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Tokmakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tokmakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "018; Tran et al., 2014), only captures a few seconds of video. Long-term context can be encoded by recurrent neural networks (Abu Farha et al., 2018; Sun et al., 2019b), graph convolutional networks (Zhang et al., 2019), or long-term feature banks (Wu et al., 2019), but these are all supervised methods. Some recent work have been done on learning self-supervised video representation (Vondrick et al., 2016; Wang &amp"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54463947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0181eb5f6f94df18586fea79d6ad37e583ff6f0c",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "A dominant paradigm for learning-based approaches in computer vision is training generic models, such as ResNet for image recognition, or I3D for video understanding, on large datasets and allowing them to discover the optimal representation for the problem at hand. While this is an obviously attractive approach, it is not applicable in all scenarios. We claim that action detection is one such challenging problem - the models that need to be trained are large, and labeled data is expensive to obtain. To address this limitation, we propose to incorporate domain knowledge into the structure of the model, simplifying optimization. In particular, we augment a standard I3D network with a tracking module to aggregate long-term motion patterns, and use a graph convolutional network to reason about interactions between actors and objects. Evaluated on the challenging AVA dataset, the proposed approach improves over the I3D baseline by 5.5% mAP and over the state-of-the-art by 4.8% mAP."
            },
            "slug": "A-Structured-Model-for-Action-Detection-Zhang-Tokmakov",
            "title": {
                "fragments": [],
                "text": "A Structured Model for Action Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A standard I3D network is augmented with a tracking module to aggregate long-term motion patterns, and a graph convolutional network is used to reason about interactions between actors and objects, simplifying optimization."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50706340"
                        ],
                        "name": "A. Fathi",
                        "slug": "A.-Fathi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Fathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2019) or colorization (Vondrick et al., 2018) but similar to their supervised counterparts they capture only few seconds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49405781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "360ef12906a531733b66e7e15c3d51771e7126d3",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical flow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking."
            },
            "slug": "Tracking-Emerges-by-Colorizing-Videos-Vondrick-Shrivastava",
            "title": {
                "fragments": [],
                "text": "Tracking Emerges by Colorizing Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The natural temporal coherency of color is leveraged to create a model that learns to colorize gray-scale videos by copying colors from a reference frame, which learns to track well enough to outperform the latest methods based on optical flow."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422336"
                        ],
                        "name": "A\u00e4ron van den Oord",
                        "slug": "A\u00e4ron-van-den-Oord",
                        "structuredName": {
                            "firstName": "A\u00e4ron",
                            "lastName": "Oord",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A\u00e4ron van den Oord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144417088"
                        ],
                        "name": "Yazhe Li",
                        "slug": "Yazhe-Li",
                        "structuredName": {
                            "firstName": "Yazhe",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yazhe Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ini-batch. We draw connection of the above training objective with several recent methods for self-supervised learning via mutual information maximization, such as contrastive predictive coding (CPC) [16] and Deep InfoMax (DIM) [9]. In CPC, the authors used a bilinear function f MI(x t;c) = xT t g t(c) as the model to estimation mutual information between the unobserved feature x t and its context c, "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ". For deep neural networks, MINE [4] proposes a method for ef\ufb01cient MI estimation when the encoder is high-dimensional, and has shown promising results for representation learning on images and audio [9, 16]. In particular, contrastive predictive coding (CPC) [16] uses noise contrastive estimation [8] to maximize the MI lower bound. Unlike CPC which relies on auto regressive models to encode context, we "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "training objective of BERT as a special case of mutual information maximization, which has been shown as an effective approach for self-supervised representation learning from images and audio inputs [9, 16]. More speci\ufb01cally, BERT jointly learns an encoder to obtain continuous word embeddings, and a sequence model that encodes the neighboring context and predicts the latent embeddings of the masked word"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 49670925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
            "isKey": true,
            "numCitedBy": 3075,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments."
            },
            "slug": "Representation-Learning-with-Contrastive-Predictive-Oord-Li",
            "title": {
                "fragments": [],
                "text": "Representation Learning with Contrastive Predictive Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a universal unsupervised learning approach to extract useful representations from high-dimensional data, which it calls Contrastive Predictive Coding, and demonstrates that the approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406164179"
                        ],
                        "name": "Li Ding",
                        "slug": "Li-Ding",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026123"
                        ],
                        "name": "Chenliang Xu",
                        "slug": "Chenliang-Xu",
                        "structuredName": {
                            "firstName": "Chenliang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenliang Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4401064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c6ce420976f958e7582a2f452c3a541faa82074",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods."
            },
            "slug": "Weakly-Supervised-Action-Segmentation-with-Soft-Ding-Xu",
            "title": {
                "fragments": [],
                "text": "Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel action modeling framework is proposed, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687325"
                        ],
                        "name": "Du Tran",
                        "slug": "Du-Tran",
                        "structuredName": {
                            "firstName": "Du",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Du Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732879"
                        ],
                        "name": "L. Torresani",
                        "slug": "L.-Torresani",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Torresani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torresani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2210374"
                        ],
                        "name": "Manohar Paluri",
                        "slug": "Manohar-Paluri",
                        "structuredName": {
                            "firstName": "Manohar",
                            "lastName": "Paluri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manohar Paluri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Most existing work on learning video representations, such as (Simonyan & Zisserman, 2014; Varol et al., 2018; Carreira & Zisserman, 2017; Xie et al., 2018; Tran et al., 2014), only captures a few seconds of video."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195346008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Videos have become ubiquitous due to the ease of capturing and sharing via social platforms like Youtube, Facebook, Instagram, and others. The computer vision community has tried to tackle various video analysis problems independently. As a consequence, even though some really good hand-crafted features have been proposed there is a lack of generic features for video analysis. On the other hand, the image domain has progressed rapidly by using features from deep convolutional networks. These deep features are proving to be generic and perform well on variety of image tasks. In this work we propose Convolution 3D (C3D) feature, a generic spatio-temporal feature obtained by training a deep 3-dimensional convolutional network on a large annotated video dataset comprising objects, scenes, actions, and other frequently occurring concepts. We show that by using spatio-temporal convolutions the trained features encapsulate appearance and motion cues and perform well on different video classification tasks. C3D has three main advantages. First, it is generic: achieving state-ofthe-art results on object recognition, scene classification, sport classification, and action similarity labeling in videos. Second, it is compact: obtaining better accuracy than best hand-crafted features and best deep image features with a lower dimensional feature descriptor. Third, it is efficient to compute: 91 times faster than current hand-crafted features, and two orders of magnitude faster than current deeplearning based video classification methods."
            },
            "slug": "C3D:-Generic-Features-for-Video-Analysis-Tran-Bourdev",
            "title": {
                "fragments": [],
                "text": "C3D: Generic Features for Video Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Convolution 3D feature is proposed, a generic spatio-temporal feature obtained by training a deep 3-dimensional convolutional network on a large annotated video dataset comprising objects, scenes, actions, and other frequently occurring concepts that encapsulate appearance and motion cues and perform well on different video classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677364"
                        ],
                        "name": "Luowei Zhou",
                        "slug": "Luowei-Zhou",
                        "structuredName": {
                            "firstName": "Luowei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luowei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34872128"
                        ],
                        "name": "Yingbo Zhou",
                        "slug": "Yingbo-Zhou",
                        "structuredName": {
                            "firstName": "Yingbo",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingbo Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587688"
                        ],
                        "name": "Jason J. Corso",
                        "slug": "Jason-J.-Corso",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Corso",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason J. Corso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4564155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35ed258aede3df17ee20a6635364cb5fd2461049",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively."
            },
            "slug": "End-to-End-Dense-Video-Captioning-with-Masked-Zhou-Zhou",
            "title": {
                "fragments": [],
                "text": "End-to-End Dense Video Captioning with Masked Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an end-to-end transformer model, which employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46584859"
                        ],
                        "name": "Jiangliu Wang",
                        "slug": "Jiangliu-Wang",
                        "structuredName": {
                            "firstName": "Jiangliu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangliu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2840852"
                        ],
                        "name": "Jianbo Jiao",
                        "slug": "Jianbo-Jiao",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780029"
                        ],
                        "name": "Linchao Bao",
                        "slug": "Linchao-Bao",
                        "structuredName": {
                            "firstName": "Linchao",
                            "lastName": "Bao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linchao Bao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2548483"
                        ],
                        "name": "Shengfeng He",
                        "slug": "Shengfeng-He",
                        "structuredName": {
                            "firstName": "Shengfeng",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shengfeng He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46398631"
                        ],
                        "name": "Yunhui Liu",
                        "slug": "Yunhui-Liu",
                        "structuredName": {
                            "firstName": "Yunhui",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunhui Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 102350701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb2f5e00049eb5bd36bea402c2818fe4bbfe0a0e",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of video representation learning without human-annotated labels. While previous efforts address the problem by designing novel self-supervised tasks using video data, the learned features are merely on a frame-by-frame basis, which are not applicable to many video analytic tasks where spatio-temporal features are prevailing. In this paper we propose a novel self-supervised approach to learn spatio-temporal features for video representation. Inspired by the success of two-stream approaches in video classification, we propose to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, given only the input video data. Specifically, we extract statistical concepts (fast-motion region and the corresponding dominant direction, spatio-temporal color diversity, dominant color, etc.) from simple patterns in both spatial and temporal domains. Unlike prior puzzles that are even hard for humans to solve, the proposed approach is consistent with human inherent visual habits and therefore easy to answer. We conduct extensive experiments with C3D to validate the effectiveness of our proposed approach. The experiments show that our approach can significantly improve the performance of C3D when applied to video classification tasks. Code is available at https://github.com/laura-wang/video_repres_mas."
            },
            "slug": "Self-Supervised-Spatio-Temporal-Representation-for-Wang-Jiao",
            "title": {
                "fragments": [],
                "text": "Self-Supervised Spatio-Temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, given only the input video data, and shows that the approach can significantly improve the performance of C3D when applied to video classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39849136"
                        ],
                        "name": "X. Wang",
                        "slug": "X.-Wang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5276358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d4ff172c2d1820f33c0c72286d52b846ab5a216",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation."
            },
            "slug": "Unsupervised-Learning-of-Visual-Representations-Wang-Gupta",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Visual Representations Using Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A simple yet surprisingly powerful approach for unsupervised learning of CNN that uses hundreds of thousands of unlabeled videos from the web to learn visual representations and designs a Siamese-triplet network with a ranking loss function to train this CNN representation."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2668759"
                        ],
                        "name": "G\u00fcl Varol",
                        "slug": "G\u00fcl-Varol",
                        "structuredName": {
                            "firstName": "G\u00fcl",
                            "lastName": "Varol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G\u00fcl Varol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "g. 1 for a summary of our training method and evaluation protocol. 2 RELATED WORK Video representations. Most existing work on learning video representations, such as (Simonyan &amp; Zisserman, 2014; Varol et al., 2018; Carreira &amp; Zisserman, 2017; Xie et al., 2018; Tran et al., 2014), only captures a few seconds of video. Long-term context can be encoded by recurrent neural networks (Abu Farha et al., 2018; Sun"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206767788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47e3ef70f2539386bcef604097fa9235246c6d53",
            "isKey": false,
            "numCitedBy": 712,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101\u00a0(92.7%) and HMDB51 (67.2%)."
            },
            "slug": "Long-Term-Temporal-Convolutions-for-Action-Varol-Laptev",
            "title": {
                "fragments": [],
                "text": "Long-Term Temporal Convolutions for Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that LTC-CNN models with increased temporal extents improve the accuracy of action recognition and the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields, and the importance of high-quality optical flow estimation for learning accurate action models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19200186"
                        ],
                        "name": "Antoine Miech",
                        "slug": "Antoine-Miech",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Miech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Miech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35838466"
                        ],
                        "name": "Dimitri Zhukov",
                        "slug": "Dimitri-Zhukov",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Zhukov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitri Zhukov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285263"
                        ],
                        "name": "Jean-Baptiste Alayrac",
                        "slug": "Jean-Baptiste-Alayrac",
                        "structuredName": {
                            "firstName": "Jean-Baptiste",
                            "lastName": "Alayrac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Baptiste Alayrac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103464"
                        ],
                        "name": "Makarand Tapaswi",
                        "slug": "Makarand-Tapaswi",
                        "structuredName": {
                            "firstName": "Makarand",
                            "lastName": "Tapaswi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makarand Tapaswi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 182952863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9311779489e597315488749ee6c386bfa3f3512e",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models are publicly available."
            },
            "slug": "HowTo100M:-Learning-a-Text-Video-Embedding-by-Video-Miech-Zhukov",
            "title": {
                "fragments": [],
                "text": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367683"
                        ],
                        "name": "H. Pirsiavash",
                        "slug": "H.-Pirsiavash",
                        "structuredName": {
                            "firstName": "Hamed",
                            "lastName": "Pirsiavash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pirsiavash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "port the top-1 accuracy on the test sets. Window (sec.) AvgPool LSTM CBT 1.5 30.2 - - 15 36.8 29.7 38.3 30 34.3 33.9 39.0 45 32.2 35.3 39.9 72 26.7 35.6 41.6 Method Breakfast 50Salads Vondrick et al. [30] 8.1 6.2 Farha et al. [1] - RNN 30.1 30.1 VideoBERT [24] 9.1 5.5 CBT 28.4 41.6 Table 1: Action anticipation accuracy. (Left) Comparison with the average pooling and LSTM baselines on 50Salads. We vary"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ds. For the Breakfast dataset where we have more training examples the fully-supervised approach [1] performs on par with our method. We also compare to two self-supervised approaches Vondrick et al. [30] and VideoBERT [24]. Our approach outperforms both by a very large margin. The difference with VideoBERT, which also relies on a BERT model, can be explained by the fact that it quantizes the visual f"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10533233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932ac3707e1ed84ab67526692a1ef8f064f24ab5",
            "isKey": true,
            "numCitedBy": 390,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future."
            },
            "slug": "Anticipating-Visual-Representations-from-Unlabeled-Vondrick-Pirsiavash",
            "title": {
                "fragments": [],
                "text": "Anticipating Visual Representations from Unlabeled Video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects and applies recognition algorithms on the authors' predicted representation to anticipate objects and actions."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1817030"
                        ],
                        "name": "Saining Xie",
                        "slug": "Saining-Xie",
                        "structuredName": {
                            "firstName": "Saining",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saining Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136435893"
                        ],
                        "name": "Jonathan Huang",
                        "slug": "Jonathan-Huang",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "on Most of the recent efforts on learning deep representations for videos have been focusing on capturing the appearance and local motion pattern from short video clips [22, 13, 11], in an supervised [26, 5, 35] or self-supervised [15, 31] fashion. However, to understand longer videos, we need powerful temporal representations for both the semantics and the dynamics. The capacity of reasoning over longtempor"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Initial deep video representations are based on a two-stream architecture which extracts frame-based features for appearance and \ufb02ow [21]. More recently, 3D convolutions capture temporal information [27, 5, 35, 26]. However, these representations only describe short temporal intervals of a few seconds. Long-term relations can be described by RNNs [1, 25] or graph convolutional networks [36]. An alternative appr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "etting w vt = 1. Pre-trained: use pre-training by setting w v = 1. Fine-tuned: \ufb01ne-tune the model on target datasets. 7 Method BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr Zhou et al. [40] - 1.42 11.20 - - S3D [35] 6.12 3.24 10.00 26.05 0.35 VideoBERT [24] 6.80 4.07 10.99 27.51 0.50 Ours 7.57( 0.07) 4.31( 0.04) 11.91( 0.04) 29.47( 0.16) 0.53( 0.01) Table 3: Video captioning results on the YouCook2 dataset. High"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ") objective for continuous visual inputs. We use noise contrastive estimation to maximize the mutual information between the encoded visual features of a video clip and its bidirectional context. S3D [35] refers to the 3D ConvNet we use to encode video segments. Our \ufb01rst contribution is to extend the BERT training objective to handle continuous inputs. We view the MLM training objective of BERT as a s"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "sformer. For each video we resize its height and width to 224 and sample frames at 20 fps for extracting features with a pre-trained CNN on 30 consecutive frames (1.5 seconds). We use the S3D network [35] pretrained on the Kinetics dataset [5] and use the feature vector before the classi\ufb01cation layer resulting in a vector of size 1024. We follow the same strategy for extracting visual features on the "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195346481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fa0d73b8ba114578744c2ebaf610d2ca9694f45",
            "isKey": true,
            "numCitedBy": 179,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study 3D convolutional networks for video understanding tasks. Our starting point is the state-of-the-art I3D model, which \"inflates\" all the 2D filters of the Inception architecture to 3D. We first consider \"deflating\" the I3D model at various levels to understand the role of 3D convolutions. Interestingly, we found that 3D convolutions at the top layers of the network contribute more than 3D convolutions at the bottom layers, while also being computationally more efficient. This indicates that I3D is better at capturing high-level temporal patterns than low-level motion signals. We also consider replacing 3D convolutions with spatiotemporal-separable 3D convolutions (i.e., replacing convolution using a k * k * k filter with 1 * k * k followed by k * 1 * 1 filters); we show that such a model, which we call S3D, is 1.5x more computationally efficient (in terms of FLOPS) than I3D, and achieves better accuracy. Finally, we explore spatiotemporal feature gating on top of S3D. The resulting model, which we call S3D-G, outperforms the state-of-the-art I3D model by 3.5% accuracy on Kinetics and reduces the FLOPS by 34%. It also achieves a new state-of-the-art performance when transferred to other action classification (UCF-101 and HMDB-51) and detection (UCF-101 and JHMDB) datasets."
            },
            "slug": "Rethinking-Spatiotemporal-Feature-Learning-For-Xie-Sun",
            "title": {
                "fragments": [],
                "text": "Rethinking Spatiotemporal Feature Learning For Video Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Interestingly, it was found that 3D convolutions at the top layers of the network contribute more than 3D Convolutional networks at the bottom layers, while also being computationally more efficient, indicating that I3D is better at capturing high-level temporal patterns than low-level motion signals."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "on Most of the recent efforts on learning deep representations for videos have been focusing on capturing the appearance and local motion pattern from short video clips [22, 13, 11], in an supervised [26, 5, 35] or self-supervised [15, 31] fashion. However, to understand longer videos, we need powerful temporal representations for both the semantics and the dynamics. The capacity of reasoning over longtempor"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Initial deep video representations are based on a two-stream architecture which extracts frame-based features for appearance and \ufb02ow [21]. More recently, 3D convolutions capture temporal information [27, 5, 35, 26]. However, these representations only describe short temporal intervals of a few seconds. Long-term relations can be described by RNNs [1, 25] or graph convolutional networks [36]. An alternative appr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ight and width to 224 and sample frames at 20 fps for extracting features with a pre-trained CNN on 30 consecutive frames (1.5 seconds). We use the S3D network [35] pretrained on the Kinetics dataset [5] and use the feature vector before the classi\ufb01cation layer resulting in a vector of size 1024. We follow the same strategy for extracting visual features on the downstream tasks action anticipation an"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206596127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
            "isKey": true,
            "numCitedBy": 3803,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101."
            },
            "slug": "Quo-Vadis,-Action-Recognition-A-New-Model-and-the-Carreira-Zisserman",
            "title": {
                "fragments": [],
                "text": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101 after pre-training on Kinetics, and a new Two-Stream Inflated 3D Conv net that is based on 2D ConvNet inflation is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2420123"
                        ],
                        "name": "Debidatta Dwibedi",
                        "slug": "Debidatta-Dwibedi",
                        "structuredName": {
                            "firstName": "Debidatta",
                            "lastName": "Dwibedi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debidatta Dwibedi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3152281"
                        ],
                        "name": "Y. Aytar",
                        "slug": "Y.-Aytar",
                        "structuredName": {
                            "firstName": "Yusuf",
                            "lastName": "Aytar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aytar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704494"
                        ],
                        "name": "Jonathan Tompson",
                        "slug": "Jonathan-Tompson",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Tompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Tompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "temporal cycle consistency (Wang et al., 2019b; Dwibedi et al., 2019) or colorization (Vondrick et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118686970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75662c7ab05db37c52a2d750af2a8b712bbf3d53",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a self-supervised representation learning method based on the task of temporal alignment between videos. The method trains a network using temporal cycle-consistency (TCC), a differentiable cycle-consistency loss that can be used to find correspondences across time in multiple videos. The resulting per-frame embeddings can be used to align videos by simply matching frames using nearest-neighbors in the learned embedding space. To evaluate the power of the embeddings, we densely label the Pouring and Penn Action video datasets for action phases. We show that (i) the learned embeddings enable few-shot classification of these action phases, significantly reducing the supervised training requirements; and (ii) TCC is complementary to other methods of self-supervised learning in videos, such as Shuffle and Learn and Time-Contrastive Networks. The embeddings are also used for a number of applications based on alignment (dense temporal correspondence) between video pairs, including transfer of metadata of synchronized modalities between videos (sounds, temporal semantic labels), synchronized playback of multiple videos, and anomaly detection. Project webpage: https://sites.google.com/view/temporal-cycle-consistency ."
            },
            "slug": "Temporal-Cycle-Consistency-Learning-Dwibedi-Aytar",
            "title": {
                "fragments": [],
                "text": "Temporal Cycle-Consistency Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the learned embeddings enable few-shot classification of these action phases, significantly reducing the supervised training requirements; and TCC is complementary to other methods of self-supervised learning in videos, such as Shuffle and Learn and Time-Contrastive Networks."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39849136"
                        ],
                        "name": "X. Wang",
                        "slug": "X.-Wang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14258597"
                        ],
                        "name": "A. Jabri",
                        "slug": "A.-Jabri",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Jabri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jabri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 81983505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "494498bb126b9234f1c1f2fc2dda4ac6f22066d9",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods."
            },
            "slug": "Learning-Correspondence-From-the-Cycle-Consistency-Wang-Jabri",
            "title": {
                "fragments": [],
                "text": "Learning Correspondence From the Cycle-Consistency of Time"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A self-supervised method to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch and demonstrates the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677364"
                        ],
                        "name": "Luowei Zhou",
                        "slug": "Luowei-Zhou",
                        "structuredName": {
                            "firstName": "Luowei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luowei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46184233"
                        ],
                        "name": "Nathan Louis",
                        "slug": "Nathan-Louis",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Louis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Louis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587688"
                        ],
                        "name": "Jason J. Corso",
                        "slug": "Jason-J.-Corso",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Corso",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason J. Corso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13686217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1288aaf45ff85916ccef13668ceba421273a3c36",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We study weakly-supervised video object grounding: given a video segment and a corresponding descriptive sentence, the goal is to localize objects that are mentioned from the sentence in the video. During training, no object bounding boxes are available, but the set of possible objects to be grounded is known beforehand. Existing approaches in the image domain use Multiple Instance Learning (MIL) to ground objects by enforcing matches between visual and semantic features. A naive extension of this approach to the video domain is to treat the entire segment as a bag of spatial object proposals. However, an object existing sparsely across multiple frames might not be detected completely since successfully spotting it from one single frame would trigger a satisfactory match. To this end, we propagate the weak supervisory signal from the segment level to frames that likely contain the target object. For frames that are unlikely to contain the target objects, we use an alternative penalty loss. We also leverage the interactions among objects as a textual guide for the grounding. We evaluate our model on the newly-collected benchmark YouCook2-BoundingBox and show improvements over competitive baselines."
            },
            "slug": "Weakly-Supervised-Video-Object-Grounding-from-Text-Zhou-Louis",
            "title": {
                "fragments": [],
                "text": "Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A weakly-supervised video object grounding model that propagates the weak supervisory signal from the segment level to frames that likely contain the target object and uses the interactions among objects as a textual guide for the grounding."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285263"
                        ],
                        "name": "Jean-Baptiste Alayrac",
                        "slug": "Jean-Baptiste-Alayrac",
                        "structuredName": {
                            "firstName": "Jean-Baptiste",
                            "lastName": "Alayrac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Baptiste Alayrac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329288"
                        ],
                        "name": "Piotr Bojanowski",
                        "slug": "Piotr-Bojanowski",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Bojanowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Bojanowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143688116"
                        ],
                        "name": "Nishant Agrawal",
                        "slug": "Nishant-Agrawal",
                        "structuredName": {
                            "firstName": "Nishant",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nishant Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388317459"
                        ],
                        "name": "S. Lacoste-Julien",
                        "slug": "S.-Lacoste-Julien",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lacoste-Julien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lacoste-Julien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nal textual description. Language can be leveraged by \ufb01nding a joint embedding space for both visual and textual modalities or by learning an alignment between the two modalities (Miech et al., 2018; Alayrac et al., 2016; Zhou et al., 2018a). Recently, several concurrent approaches (Sun et al., 2019a; Lu et al., 2019; Li et al., 2019a; Su et al., 2019; Tan &amp; Bansal, 2019; Li et al., 2019b) generalize the BERT arc"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2617244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e17ba2b5d0769e7f2602d859ea77a153846cf27d",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks1 that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos."
            },
            "slug": "Unsupervised-Learning-from-Narrated-Instruction-Alayrac-Bojanowski",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning from Narrated Instruction Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration to solve two clustering problems, one in text and one in video, and can automatically discover the main steps to achieve the task and locate the steps in the input videos."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108548771"
                        ],
                        "name": "Gen Li",
                        "slug": "Gen-Li",
                        "structuredName": {
                            "firstName": "Gen",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gen Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46429989"
                        ],
                        "name": "Nan Duan",
                        "slug": "Nan-Duan",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nan Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2181664"
                        ],
                        "name": "Yuejian Fang",
                        "slug": "Yuejian-Fang",
                        "structuredName": {
                            "firstName": "Yuejian",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuejian Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71790825"
                        ],
                        "name": "Daxin Jiang",
                        "slug": "Daxin-Jiang",
                        "structuredName": {
                            "firstName": "Daxin",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daxin Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92660691"
                        ],
                        "name": "Ming Zhou",
                        "slug": "Ming-Zhou",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201058752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bc1c8bd00bbf7401afcb5460277840fd8bab029",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM (Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling(MLM), Masked Object Classification(MOC) and Visual-linguistic Matching(VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the cross-modal pre-training."
            },
            "slug": "Unicoder-VL:-A-Universal-Encoder-for-Vision-and-by-Li-Duan",
            "title": {
                "fragments": [],
                "text": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "After pretraining on large-scale image-caption pairs, Unicoder-VL is transferred to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer, and shows the powerful ability of the cross-modal pre-training."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40482726"
                        ],
                        "name": "R. Devon Hjelm",
                        "slug": "R.-Devon-Hjelm",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Hjelm",
                            "middleNames": [
                                "Devon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Devon Hjelm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26920432"
                        ],
                        "name": "A. Fedorov",
                        "slug": "A.-Fedorov",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Fedorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fedorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412884705"
                        ],
                        "name": "Samuel Lavoie-Marchildon",
                        "slug": "Samuel-Lavoie-Marchildon",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Lavoie-Marchildon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Lavoie-Marchildon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14007973"
                        ],
                        "name": "Karan Grewal",
                        "slug": "Karan-Grewal",
                        "structuredName": {
                            "firstName": "Karan",
                            "lastName": "Grewal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karan Grewal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3382568"
                        ],
                        "name": "Adam Trischler",
                        "slug": "Adam-Trischler",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Trischler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Trischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", (Belghazi et al., 2018; Hjelm et al., 2019; Oord et al., 2018; Tian et al., 2019)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", in the deep infomax (DIM) (Hjelm et al., 2019) and contrastive predictive coding (CPC) (Oord et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52055130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eae7d5b15423a148e6bb32d24bbabedfacd0e2df",
            "isKey": false,
            "numCitedBy": 1280,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation\u2019s suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals."
            },
            "slug": "Learning-deep-representations-by-mutual-information-Hjelm-Fedorov",
            "title": {
                "fragments": [],
                "text": "Learning deep representations by mutual information estimation and maximization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation\u2019s suitability for downstream tasks and is an important step towards flexible formulations of representation learning objectives for specific end-goals."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 102352873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6edfe8350da54cd563158b0d7d0c664f16cb91a8",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on multi-person action forecasting in videos. More precisely, given a history of H previous frames, the goal is to detect actors and to predict their future actions for the next T frames. Our approach jointly models temporal and spatial interactions among different actors by constructing a recurrent graph, using actor proposals obtained with Faster R-CNN as nodes. Our method learns to select a subset of discriminative relations without requiring explicit supervision, thus enabling us to tackle challenging visual data. We refer to our model as Discriminative Relational Recurrent Network (DRRN). Evaluation of action prediction on AVA demonstrates the effectiveness of our proposed method compared to simpler baselines. Furthermore, we significantly improve performance on the task of early action classification on J-HMDB, from the previous SOTA of 48% to 60%."
            },
            "slug": "Relational-Action-Forecasting-Sun-Shrivastava",
            "title": {
                "fragments": [],
                "text": "Relational Action Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach jointly models temporal and spatial interactions among different actors by constructing a recurrent graph, using actor proposals obtained with Faster R-CNN as nodes, and learns to select a subset of discriminative relations without requiring explicit supervision, thus enabling the method to tackle challenging visual data."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687325"
                        ],
                        "name": "Du Tran",
                        "slug": "Du-Tran",
                        "structuredName": {
                            "firstName": "Du",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Du Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732879"
                        ],
                        "name": "L. Torresani",
                        "slug": "L.-Torresani",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Torresani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torresani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2210374"
                        ],
                        "name": "Manohar Paluri",
                        "slug": "Manohar-Paluri",
                        "structuredName": {
                            "firstName": "Manohar",
                            "lastName": "Paluri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manohar Paluri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1122604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d25c65d261ea0e6a458be4c50c40ffe5bc508f77",
            "isKey": false,
            "numCitedBy": 5270,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use."
            },
            "slug": "Learning-Spatiotemporal-Features-with-3D-Networks-Tran-Bourdev",
            "title": {
                "fragments": [],
                "text": "Learning Spatiotemporal Features with 3D Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956994"
                        ],
                        "name": "Andrew Owens",
                        "slug": "Andrew-Owens",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Owens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045089"
                        ],
                        "name": "Jiajun Wu",
                        "slug": "Jiajun-Wu",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324658"
                        ],
                        "name": "Josh H. McDermott",
                        "slug": "Josh-H.-McDermott",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "McDermott",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josh H. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "rich source of information for selfsupervised learning of video representations. Since videos contain both visual and audio signals that are synchronized, the two modalities can supervised each other [3, 18, 17, 37]. Another common form of weak supervision is based on video and language, where language is either obtained by automatic speech recognition (ASR) or from additional textual description. Language can b"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11614363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93a87dfa72f22fba14ef243a62c7d0a6906dfed7",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The sound of crashing waves, the roar of fast-moving cars \u2013 sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds."
            },
            "slug": "Ambient-Sound-Provides-Supervision-for-Visual-Owens-Wu",
            "title": {
                "fragments": [],
                "text": "Ambient Sound Provides Supervision for Visual Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work trains a convolutional neural network to predict a statistical summary of the sound associated with a video frame, and shows that this representation is comparable to that of other state-of-the-art unsupervised learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944655894"
                        ],
                        "name": "Jacob Walker",
                        "slug": "Jacob-Walker",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Walker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sentations for both the semantics and the dynamics. The capacity of reasoning over longtemporal horizons is critical for many applications, such as video storytelling [41, 29] and action anticipation [32]. This paper aims at learning representations from long videos. We assume that this task can be decoupled into two sub-tasks: visual representation learning for short video segments, and temporal repr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1303771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0bb8f933e514dd9441e3082a34a9f129e35500",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a conceptually simple but surprisingly powerful method for visual prediction which combines the effectiveness of mid-level visual elements with temporal modeling. Our framework can be learned in a completely unsupervised manner from a large collection of videos. However, more importantly, because our approach models the prediction framework on these mid-level elements, we can not only predict the possible motion in the scene but also predict visual appearances - how are appearances going to change with time. This yields a visual \"hallucination\" of probable events on top of the scene. We show that our method is able to accurately predict and visualize simple future events, we also show that our approach is comparable to supervised methods for event prediction."
            },
            "slug": "Patch-to-the-Future:-Unsupervised-Visual-Prediction-Walker-Gupta",
            "title": {
                "fragments": [],
                "text": "Patch to the Future: Unsupervised Visual Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents a conceptually simple but surprisingly powerful method for visual prediction which combines the effectiveness of mid-level visual elements with temporal modeling and shows that it is comparable to supervised methods for event prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32562635"
                        ],
                        "name": "Liunian Harold Li",
                        "slug": "Liunian-Harold-Li",
                        "structuredName": {
                            "firstName": "Liunian",
                            "lastName": "Li",
                            "middleNames": [
                                "Harold"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liunian Harold Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144508458"
                        ],
                        "name": "Da Yin",
                        "slug": "Da-Yin",
                        "structuredName": {
                            "firstName": "Da",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Da Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 199528533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "isKey": false,
            "numCitedBy": 630,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "slug": "VisualBERT:-A-Simple-and-Performant-Baseline-for-Li-Yatskar",
            "title": {
                "fragments": [],
                "text": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41022481"
                        ],
                        "name": "Yazan Abu Farha",
                        "slug": "Yazan-Abu-Farha",
                        "structuredName": {
                            "firstName": "Yazan",
                            "lastName": "Abu Farha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yazan Abu Farha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32774629"
                        ],
                        "name": "Alexander Richard",
                        "slug": "Alexander-Richard",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Richard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Richard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689714"
                        ],
                        "name": "Juergen Gall",
                        "slug": "Juergen-Gall",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Gall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juergen Gall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201cmix ingredients\u201d. Annotations are also de\ufb01ned at the level of action units by start and end time. Videos have on average a length of 6.4 minutes and contain on average 20 action instances. Following [23, 1], we use the \ufb01ve-fold cross-validation resulting in an average of 722 training and 180 test examples. Due to the small number of training examples, this dataset is very challenging and can bene\ufb01t sign"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " units de\ufb01ned by start and end time. The average duration of the videos is 2.3 minutes and there are on average 5.8 action instances per video. We follow the standard four-fold cross validation split [12, 1] which results in an average of 5,810 training and 1,936 test examples. The 50Salads dataset [23] contains 50 videos with 17 \ufb01ne-grained action classes required for making a salad, such as \u201cadd oil\u201d, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " the test sets. Window (sec.) AvgPool LSTM CBT 1.5 30.2 - - 15 36.8 29.7 38.3 30 34.3 33.9 39.0 45 32.2 35.3 39.9 72 26.7 35.6 41.6 Method Breakfast 50Salads Vondrick et al. [30] 8.1 6.2 Farha et al. [1] - RNN 30.1 30.1 VideoBERT [24] 9.1 5.5 CBT 28.4 41.6 Table 1: Action anticipation accuracy. (Left) Comparison with the average pooling and LSTM baselines on 50Salads. We vary the observation window l"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "tly, 3D convolutions capture temporal information [27, 5, 35, 26]. However, these representations only describe short temporal intervals of a few seconds. Long-term relations can be described by RNNs [1, 25] or graph convolutional networks [36]. An alternative approach uses a long-term feature bank [34], which extracts information over the entire span of a video and selects relevant interactions by cross"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section we evaluate CBT with pre-training for a long-term temporal relation model for action anticipation. Experimental setup. For action prediction we use the experimental setup described in [1]. The model gets as input a video segment up to 1 second before the action starts and predicts the category of the next action. We use the Breakfast and 50Salads datasets and report the accuracy of ac"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4570418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33b1843afc8b76314c9fbe11ca11c23fa0966c08",
            "isKey": true,
            "numCitedBy": 96,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Analyzing human actions in videos has gained increased attention recently. While most works focus on classifying and labeling observed video frames or anticipating the very recent future, making long-term predictions over more than just a few seconds is a task with many practical applications that has not yet been addressed. In this paper, we propose two methods to predict a considerably large amount of future actions and their durations. Both, a CNN and an RNN are trained to learn future video labels based on previously seen content. We show that our methods generate accurate predictions of the future even for long videos with a huge amount of different actions and can even deal with noisy or erroneous input information."
            },
            "slug": "When-will-you-do-what-Anticipating-Temporal-of-Farha-Richard",
            "title": {
                "fragments": [],
                "text": "When will you do what? - Anticipating Temporal Occurrences of Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes two methods to predict a considerably large amount of future actions and their durations using a CNN and an RNN that are trained to learn future video labels based on previously seen content."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799979"
                        ],
                        "name": "K. Soomro",
                        "slug": "K.-Soomro",
                        "structuredName": {
                            "firstName": "Khurram",
                            "lastName": "Soomro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Soomro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40029556"
                        ],
                        "name": "A. Zamir",
                        "slug": "A.-Zamir",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Zamir",
                            "middleNames": [
                                "Roshan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zamir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " state of the art. 1 Introduction Most of the recent efforts on learning deep representations for videos have been focusing on capturing the appearance and local motion pattern from short video clips [22, 13, 11], in an supervised [26, 5, 35] or self-supervised [15, 31] fashion. However, to understand longer videos, we need powerful temporal representations for both the semantics and the dynamics. The capacit"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7197134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da9e411fcf740569b6b356f330a1d0fc077c8d7c",
            "isKey": false,
            "numCitedBy": 3635,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips."
            },
            "slug": "UCF101:-A-Dataset-of-101-Human-Actions-Classes-From-Soomro-Zamir",
            "title": {
                "fragments": [],
                "text": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work introduces UCF101 which is currently the largest dataset of human actions and provides baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3218666"
                        ],
                        "name": "Hao Hao Tan",
                        "slug": "Hao-Hao-Tan",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Tan",
                            "middleNames": [
                                "Hao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Hao Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201103729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79c93274429d6355959f1e4374c2147bb81ea649",
            "isKey": false,
            "numCitedBy": 916,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert"
            },
            "slug": "LXMERT:-Learning-Cross-Modality-Encoder-from-Tan-Bansal",
            "title": {
                "fragments": [],
                "text": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework, a large-scale Transformer model that consists of three encoders, achieves the state-of-the-art results on two visual question answering datasets and shows the generalizability of the pre-trained cross-modality model."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2297229"
                        ],
                        "name": "Stefan Lee",
                        "slug": "Stefan-Lee",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "d textual modalities or by learning an alignment between the two modalities (Miech et al., 2018; Alayrac et al., 2016; Zhou et al., 2018a). Recently, several concurrent approaches (Sun et al., 2019a; Lu et al., 2019; Li et al., 2019a; Su et al., 2019; Tan &amp; Bansal, 2019; Li et al., 2019b) generalize the BERT architecture and MLM objective to learn visual-linguistic representations. They assume the visual rep"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ing them (along with optional ASR tokens) to the BERT model. Unfortunately, VQ loses \ufb01ne-grained information that is often critical for downstream tasks. More recently, several papers (e.g., VilBERT (Lu et al., 2019) and LXMERT (Tan &amp; Bansal, 2019)) proposed to address this limitation by directly measuring the visual similarity between frames using pre-trained visual encoders. In this paper, we propose a way "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 199453025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
            "isKey": false,
            "numCitedBy": 1266,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability."
            },
            "slug": "ViLBERT:-Pretraining-Task-Agnostic-Visiolinguistic-Lu-Batra",
            "title": {
                "fragments": [],
                "text": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language, is presented, extending the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123446103"
                        ],
                        "name": "Hilde Kuehne",
                        "slug": "Hilde-Kuehne",
                        "structuredName": {
                            "firstName": "Hilde",
                            "lastName": "Kuehne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hilde Kuehne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676982"
                        ],
                        "name": "Hueihan Jhuang",
                        "slug": "Hueihan-Jhuang",
                        "structuredName": {
                            "firstName": "Hueihan",
                            "lastName": "Jhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hueihan Jhuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1930964"
                        ],
                        "name": "Est\u00edbaliz Garrote",
                        "slug": "Est\u00edbaliz-Garrote",
                        "structuredName": {
                            "firstName": "Est\u00edbaliz",
                            "lastName": "Garrote",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Est\u00edbaliz Garrote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ures as input to a linear classi\ufb01er, and train the classi\ufb01er on various datasets. For evaluation, we use UCF101 (Soomro et al., 2012), which contains 13,320 videos from 101 human actions, and HMDB51 (Kuehne et al., 2011), which contains 7,000 videos from 51 classes. For both dataset we report the action recognition test accuracy averaged over the 3 standard train/test splits. To pre-train our CBT model, we use a curr"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206769852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b3b8848a311c501e704c45c6d50430ab7068956",
            "isKey": false,
            "numCitedBy": 2546,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion."
            },
            "slug": "HMDB:-A-large-video-database-for-human-motion-Kuehne-Jhuang",
            "title": {
                "fragments": [],
                "text": "HMDB: A large video database for human motion recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper uses the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube, to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476765"
                        ],
                        "name": "Yonglong Tian",
                        "slug": "Yonglong-Tian",
                        "structuredName": {
                            "firstName": "Yonglong",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonglong Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707347"
                        ],
                        "name": "Dilip Krishnan",
                        "slug": "Dilip-Krishnan",
                        "structuredName": {
                            "firstName": "Dilip",
                            "lastName": "Krishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dilip Krishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094770"
                        ],
                        "name": "Phillip Isola",
                        "slug": "Phillip-Isola",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Isola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phillip Isola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 189762205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97f4d09175705be4677d675fa27e55defac44800",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: this http URL."
            },
            "slug": "Contrastive-Multiview-Coding-Tian-Krishnan",
            "title": {
                "fragments": [],
                "text": "Contrastive Multiview Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Key properties of the multiview contrastive learning approach are analyzed, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views the authors learn from, the better the resulting representation captures underlying scene semantics."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This uses the Transformer architecture (Vaswani et al., 2017) to encode long sentences, and trains the model using the \u201cmasked language modeling\u201d (MLM) training objective, in which the model must predict the missing words given their bidirectional context."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ",K}, embeds each one into a dense vector, eyt \u2208, and then emits a sequence of dense output vectors, hyt \u2208 D, which are computed using a transformer (Vaswani et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": false,
            "numCitedBy": 35150,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062879616"
                        ],
                        "name": "Will Kay",
                        "slug": "Will-Kay",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Kay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Kay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125058045"
                        ],
                        "name": "Brian Zhang",
                        "slug": "Brian-Zhang",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38961760"
                        ],
                        "name": "Chloe Hillier",
                        "slug": "Chloe-Hillier",
                        "structuredName": {
                            "firstName": "Chloe",
                            "lastName": "Hillier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chloe Hillier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259154"
                        ],
                        "name": "Sudheendra Vijayanarasimhan",
                        "slug": "Sudheendra-Vijayanarasimhan",
                        "structuredName": {
                            "firstName": "Sudheendra",
                            "lastName": "Vijayanarasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudheendra Vijayanarasimhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47963165"
                        ],
                        "name": "Fabio Viola",
                        "slug": "Fabio-Viola",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Viola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1484039896"
                        ],
                        "name": "Tim Green",
                        "slug": "Tim-Green",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2830305"
                        ],
                        "name": "T. Back",
                        "slug": "T.-Back",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Back",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Back"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820908"
                        ],
                        "name": "A. Natsev",
                        "slug": "A.-Natsev",
                        "structuredName": {
                            "firstName": "Apostol",
                            "lastName": "Natsev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Natsev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573615"
                        ],
                        "name": "Mustafa Suleyman",
                        "slug": "Mustafa-Suleyman",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Suleyman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mustafa Suleyman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27300853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6",
            "isKey": false,
            "numCitedBy": 1731,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers."
            },
            "slug": "The-Kinetics-Human-Action-Video-Dataset-Kay-Carreira",
            "title": {
                "fragments": [],
                "text": "The Kinetics Human Action Video Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The dataset is described, the statistics are described, how it was collected, and some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset are given."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3175258"
                        ],
                        "name": "Fabian Caba Heilbron",
                        "slug": "Fabian-Caba-Heilbron",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Heilbron",
                            "middleNames": [
                                "Caba"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian Caba Heilbron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144201025"
                        ],
                        "name": "Victor Escorcia",
                        "slug": "Victor-Escorcia",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Escorcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Escorcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2931652"
                        ],
                        "name": "Bernard Ghanem",
                        "slug": "Bernard-Ghanem",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Ghanem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernard Ghanem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2014) is composed of 1712 cooking videos and contains 48 fine-grained action classes; the 50Salads dataset (Stein & McKenna, 2013) contains 50 cooking videos with 17 fine-grained action classes; and the ActivityNet 200 dataset (Heilbron et al., 2015) contains 19994 YouTube videos with 200 human action classes (beyond the cooking and instructional domains)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1710722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a28efacb92d16e6e0dd4d87b5aca91b28be8853",
            "isKey": false,
            "numCitedBy": 1307,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper we introduce ActivityNet, a new large-scale video benchmark for human activity understanding. Our benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. We illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: untrimmed video classification, trimmed activity classification and activity detection."
            },
            "slug": "ActivityNet:-A-large-scale-video-benchmark-for-Heilbron-Escorcia",
            "title": {
                "fragments": [],
                "text": "ActivityNet: A large-scale video benchmark for human activity understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper introduces ActivityNet, a new large-scale video benchmark for human activity understanding that aims at covering a wide range of complex human activities that are of interest to people in their daily living."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35299091"
                        ],
                        "name": "Yansong Tang",
                        "slug": "Yansong-Tang",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056669824"
                        ],
                        "name": "Dajun Ding",
                        "slug": "Dajun-Ding",
                        "structuredName": {
                            "firstName": "Dajun",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dajun Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39358728"
                        ],
                        "name": "Yongming Rao",
                        "slug": "Yongming-Rao",
                        "structuredName": {
                            "firstName": "Yongming",
                            "lastName": "Rao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongming Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149515564"
                        ],
                        "name": "Yu Zheng",
                        "slug": "Yu-Zheng",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118333"
                        ],
                        "name": "Danyang Zhang",
                        "slug": "Danyang-Zhang",
                        "structuredName": {
                            "firstName": "Danyang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danyang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145042180"
                        ],
                        "name": "Lili Zhao",
                        "slug": "Lili-Zhao",
                        "structuredName": {
                            "firstName": "Lili",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lili Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697700"
                        ],
                        "name": "Jiwen Lu",
                        "slug": "Jiwen-Lu",
                        "structuredName": {
                            "firstName": "Jiwen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiwen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49640256"
                        ],
                        "name": "Jie Zhou",
                        "slug": "Jie-Zhou",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Right) Action segmentation results on the COIN dataset (Tang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 71147568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e27e78c33288728f66f7dab2fe2696ddbc5c1026",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "There are substantial instruction videos on the Internet, which enables us to acquire knowledge for completing various tasks. However, most existing datasets for instruction video analysis have the limitations in diversity and scale, which makes them far from many real-world applications where more diverse activities occur. Moreover, it still remains a great challenge to organize and harness such data. To address these problems, we introduce a large-scale dataset called \u201cCOIN\" for COmprehensive INstruction video analysis. Organized with a hierarchical structure, the COIN dataset contains 11,827 videos of 180 tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed toolbox, all the videos are annotated effectively with a series of step descriptions and the corresponding temporal boundaries. Furthermore, we propose a simple yet effective method to capture the dependencies among different steps, which can be easily plugged into conventional proposal-based action detection methods for localizing important steps in instruction videos. In order to provide a benchmark for instruction video analysis, we evaluate plenty of approaches on the COIN dataset under different evaluation criteria. We expect the introduction of the COIN dataset will promote the future in-depth research on instruction video analysis for the community."
            },
            "slug": "COIN:-A-Large-Scale-Dataset-for-Comprehensive-Video-Tang-Ding",
            "title": {
                "fragments": [],
                "text": "COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple yet effective method to capture the dependencies among different steps, which can be easily plugged into conventional proposal-based action detection methods for localizing important steps in instruction videos is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677364"
                        ],
                        "name": "Luowei Zhou",
                        "slug": "Luowei-Zhou",
                        "structuredName": {
                            "firstName": "Luowei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luowei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026123"
                        ],
                        "name": "Chenliang Xu",
                        "slug": "Chenliang-Xu",
                        "structuredName": {
                            "firstName": "Chenliang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenliang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587688"
                        ],
                        "name": "Jason J. Corso",
                        "slug": "Jason-J.-Corso",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Corso",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason J. Corso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19713015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e10a5e0baf2aa87d804795af071808a9377cc80a",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n The potential for agents, whether embodied or software, to learn by observing other agents performing procedures involving objects and actions is rich. Current research on automatic procedure learning heavily relies on action labels or video subtitles, even during the evaluation phase, which makes them infeasible in real-world scenarios. This leads to our question: can the human-consensus structure of a procedure be learned from a large set of long, unconstrained videos (e.g., instructional videos from YouTube) with only visual evidence? To answer this question, we introduce the problem of procedure segmentation---to segment a video procedure into category-independent procedure segments. Given that no large-scale dataset is available for this problem, we collect a large-scale procedure segmentation dataset with procedure segments temporally localized and described; we use cooking videos and name the dataset YouCook2. We propose a segment-level recurrent network for generating procedure segments by modeling the dependencies across segments. The generated segments can be used as pre-processing for other tasks, such as dense video captioning and event parsing. We show in our experiments that the proposed model outperforms competitive baselines in procedure segmentation.\n \n"
            },
            "slug": "Towards-Automatic-Learning-of-Procedures-From-Web-Zhou-Xu",
            "title": {
                "fragments": [],
                "text": "Towards Automatic Learning of Procedures From Web Instructional Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A segment-level recurrent network is proposed for generating procedure segments by modeling the dependencies across segments and it is shown that the proposed model outperforms competitive baselines in procedure segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844940337"
                        ],
                        "name": "Yukun Zhu",
                        "slug": "Yukun-Zhu",
                        "structuredName": {
                            "firstName": "Yukun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", we need powerful temporal representations for both the semantics and the dynamics. The capacity of reasoning over longtemporal horizons is critical for many applications, such as video storytelling [41, 29] and action anticipation [32]. This paper aims at learning representations from long videos. We assume that this task can be decoupled into two sub-tasks: visual representation learning for short vide"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6866988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "isKey": false,
            "numCitedBy": 1418,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for."
            },
            "slug": "Aligning-Books-and-Movies:-Towards-Story-Like-by-Zhu-Kiros",
            "title": {
                "fragments": [],
                "text": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "To align movies and books, a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": false,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39599498"
                        ],
                        "name": "Chunhui Gu",
                        "slug": "Chunhui-Gu",
                        "structuredName": {
                            "firstName": "Chunhui",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhui Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259154"
                        ],
                        "name": "Sudheendra Vijayanarasimhan",
                        "slug": "Sudheendra-Vijayanarasimhan",
                        "structuredName": {
                            "firstName": "Sudheendra",
                            "lastName": "Vijayanarasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudheendra Vijayanarasimhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2997956"
                        ],
                        "name": "C. Pantofaru",
                        "slug": "C.-Pantofaru",
                        "structuredName": {
                            "firstName": "Caroline",
                            "lastName": "Pantofaru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pantofaru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711958"
                        ],
                        "name": "David A. Ross",
                        "slug": "David-A.-Ross",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ross",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Ross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805076"
                        ],
                        "name": "G. Toderici",
                        "slug": "G.-Toderici",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Toderici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Toderici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110765504"
                        ],
                        "name": "Yeqing Li",
                        "slug": "Yeqing-Li",
                        "structuredName": {
                            "firstName": "Yeqing",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeqing Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262946"
                        ],
                        "name": "Susanna Ricco",
                        "slug": "Susanna-Ricco",
                        "structuredName": {
                            "firstName": "Susanna",
                            "lastName": "Ricco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susanna Ricco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sentations is the lack of large-scale annotated long videos. In fact, it is not even always clear what should be annotated, as human behavior exhibits a natural hierarchy, ranging from atomic actions [7] to high-level goals. It is thus impossible to exhaustively label these actions at different temporal scales, and a \u201cself-supervised\u201d approach is needed for this task. The success of Bidirectional Enc"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 688013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54c7c3909c7e1e827befdbe8d2595a3b196ba1b8",
            "isKey": false,
            "numCitedBy": 537,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.8% mAP, underscoring the need for developing new approaches for video understanding."
            },
            "slug": "AVA:-A-Video-Dataset-of-Spatio-Temporally-Localized-Gu-Sun",
            "title": {
                "fragments": [],
                "text": "AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145499378"
                        ],
                        "name": "Weijie Su",
                        "slug": "Weijie-Su",
                        "structuredName": {
                            "firstName": "Weijie",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijie Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578924"
                        ],
                        "name": "Xizhou Zhu",
                        "slug": "Xizhou-Zhu",
                        "structuredName": {
                            "firstName": "Xizhou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xizhou Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112823372"
                        ],
                        "name": "Yue Cao",
                        "slug": "Yue-Cao",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48218753"
                        ],
                        "name": "B. Li",
                        "slug": "B.-Li",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152309485"
                        ],
                        "name": "Lewei Lu",
                        "slug": "Lewei-Lu",
                        "structuredName": {
                            "firstName": "Lewei",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lewei Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201317624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2527626c11a84f15709e943fbfa2356e19930e3b",
            "isKey": false,
            "numCitedBy": 707,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \\url{this https URL}."
            },
            "slug": "VL-BERT:-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu",
            "title": {
                "fragments": [],
                "text": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT), which adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19200186"
                        ],
                        "name": "Antoine Miech",
                        "slug": "Antoine-Miech",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Miech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Miech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732879"
                        ],
                        "name": "L. Torresani",
                        "slug": "L.-Torresani",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Torresani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torresani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687325"
                        ],
                        "name": "Du Tran",
                        "slug": "Du-Tran",
                        "structuredName": {
                            "firstName": "Du",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Du Tran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "etwork that is pretrained on Kinetics, and then \u201cfrozen\u201d. We then pass this sequence of features to the CBT model for self-supervised pretraining with the NCE loss on the unlabeled HowTo100M dataset (Miech et al., 2019b). We also evaluate the effects of running ASR on HowTo100M, and passing this to our cross-modal transformer as an additional signal. We then \ufb01ne-tune various shallow decoders for a variety of tasks,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " supplementary, we also present results on video captioning and action segmentation. For the action anticipation task, we follow the standard setup described from recent work (Abu Farha et al., 2018; Miech et al., 2019a). We consider three datasets: the Breakfast dataset (Kuehne et al., 2014) is composed of 1712 cooking videos and contains 48 \ufb01ne-grained action classes; the 50Salads dataset (Stein &amp; McKenna, 20"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "x t= f enc(x ) for all videos using S3D, and focus on learning global representations hx 1:T = CBT(e x 1:T ). For the self-supervised pre-training, we use unlabeled videos from the HowTo100M dataset (Miech et al., 2019b). This contains \u02d81M instructional videos (details below), so the speech is informative about the vision. Therefore, we also run ASR on this dataset and use cross-modal training to compute hxy 1:T = "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "l pre\ufb01x); video captioning (see section 6.1 in supplementary), and video segmentation (see section 6.2 in supplementary). Details on self-supervised pre-training. We pre-train our model on HowTo100M (Miech et al., 2019b). This is a new large-scale dataset of 1.22M narrated instructional videos available on YouTube and covers among 23k different visual tasks. The average duration of a video is 6.5 minutes and there "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " it quantizes the visual features into tokens and, hence loses discriminative power. Next we compare to some recent methods that train deep classi\ufb01ers end-to-end, namely (Abu Farha et al., 2018) and (Miech et al., 2019a). We outperform both by a large margin. Method Self-super Bkfst Salads ActNet Vondrick et al. (2016) Y 8.1 6.2 - Abu Farha et al. (2018) N 30.1 30.1 - Sun et al. (2019a) Y 9.1 5.5 - Miech et al. (20"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 198186391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "702111d5821ac1962ca7f4c4e3c4ed6a0887b093",
            "isKey": true,
            "numCitedBy": 49,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Anticipating actions before they are executed is crucial for a wide range of practical applications including autonomous driving and the moderation of live video streaming. While most prior work in this area requires partial observation of executed actions, in the paper we focus on anticipating actions seconds before they start. Our proposed approach is the fusion of a purely anticipatory model with a complementary model constrained to reason about the present. In particular, the latter predicts present action and scene attributes, and reasons about how they evolve over time. By doing so, we aim at modeling action anticipation at a more conceptual level than directly predicting future actions. Our model outperforms previously reported methods on the EPIC-KITCHENS and Breakfast datasets."
            },
            "slug": "Leveraging-the-Present-to-Anticipate-the-Future-in-Miech-Laptev",
            "title": {
                "fragments": [],
                "text": "Leveraging the Present to Anticipate the Future in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work focuses on anticipating actions seconds before they start, and proposes a fusion of a purely anticipatory model with a complementary model constrained to reason about the present that predicts present action and scene attributes, and reasons about how they evolve over time."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2039154"
                        ],
                        "name": "Paul Vicol",
                        "slug": "Paul-Vicol",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Vicol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Vicol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103464"
                        ],
                        "name": "Makarand Tapaswi",
                        "slug": "Makarand-Tapaswi",
                        "structuredName": {
                            "firstName": "Makarand",
                            "lastName": "Tapaswi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makarand Tapaswi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3436589"
                        ],
                        "name": "Llu\u00eds Castrej\u00f3n",
                        "slug": "Llu\u00eds-Castrej\u00f3n",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Castrej\u00f3n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds Castrej\u00f3n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", we need powerful temporal representations for both the semantics and the dynamics. The capacity of reasoning over longtemporal horizons is critical for many applications, such as video storytelling [41, 29] and action anticipation [32]. This paper aims at learning representations from long videos. We assume that this task can be decoupled into two sub-tasks: visual representation learning for short vide"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4856028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "523574aca71d8981b4122cce8d132f22391ef26e",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to \"read\" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents."
            },
            "slug": "MovieGraphs:-Towards-Understanding-Human-Centric-Vicol-Tapaswi",
            "title": {
                "fragments": [],
                "text": "MovieGraphs: Towards Understanding Human-Centric Situations from Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50043859"
                        ],
                        "name": "Mark Neumann",
                        "slug": "Mark-Neumann",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ta available online for pre-training as does our approach. Self-supervised context modeling. Recently, there has between a lot of work on self-supervised context modeling for language representations [19, 20, 6]. In particular, the BERT model, which stands for Bidirectional Encoder Representations from Transformers [6], pre-trains deep bidirectional representations by jointly conditioning on both left and ri"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3626819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "isKey": false,
            "numCitedBy": 7987,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
            },
            "slug": "Deep-Contextualized-Word-Representations-Peters-Neumann",
            "title": {
                "fragments": [],
                "text": "Deep Contextualized Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across linguistic contexts, allowing downstream models to mix different types of semi-supervision signals."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956994"
                        ],
                        "name": "Andrew Owens",
                        "slug": "Andrew-Owens",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Owens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094770"
                        ],
                        "name": "Phillip Isola",
                        "slug": "Phillip-Isola",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Isola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phillip Isola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324658"
                        ],
                        "name": "Josh H. McDermott",
                        "slug": "Josh-H.-McDermott",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "McDermott",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josh H. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "rich source of information for selfsupervised learning of video representations. Since videos contain both visual and audio signals that are synchronized, the two modalities can supervised each other [3, 18, 17, 37]. Another common form of weak supervision is based on video and language, where language is either obtained by automatic speech recognition (ASR) or from additional textual description. Language can b"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1697911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac640c2d0f33fb3ab49f37b26982948fc31e3191",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions."
            },
            "slug": "Visually-Indicated-Sounds-Owens-Isola",
            "title": {
                "fragments": [],
                "text": "Visually Indicated Sounds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick, using a recurrent neural network to predict sound features from videos and then producing a waveform from these features with an example-based synthesis procedure."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123446103"
                        ],
                        "name": "Hilde Kuehne",
                        "slug": "Hilde-Kuehne",
                        "structuredName": {
                            "firstName": "Hilde",
                            "lastName": "Kuehne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hilde Kuehne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49106279"
                        ],
                        "name": "A. B. Arslan",
                        "slug": "A.-B.-Arslan",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Arslan",
                            "middleNames": [
                                "Bilgin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. B. Arslan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " units de\ufb01ned by start and end time. The average duration of the videos is 2.3 minutes and there are on average 5.8 action instances per video. We follow the standard four-fold cross validation split [12, 1] which results in an average of 5,810 training and 1,936 test examples. The 50Salads dataset [23] contains 50 videos with 17 \ufb01ne-grained action classes required for making a salad, such as \u201cadd oil\u201d, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "nt up to 1 second before the action starts and predicts the category of the next action. We use the Breakfast and 50Salads datasets and report the accuracy of action prediction. The Breakfast dataset [12] is composed of 1712 videos of 52 different actors making breakfast. There are 48 \ufb01ne-grained action classes, such as \u201cpour milk\u201d, \u201ctake plate\u201d and \u201ccut fruits\u201d. Annotations are at the level of action"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9621856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "185f078accb52be4faa13e4f470a9909cc6fe814",
            "isKey": true,
            "numCitedBy": 309,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a framework for modeling human activities as temporally structured processes. Our approach is motivated by the inherently hierarchical nature of human activities and the close correspondence between human actions and speech: We model action units using Hidden Markov Models, much like words in speech. These action units then form the building blocks to model complex human activities as sentences using an action grammar. To evaluate our approach, we collected a large dataset of daily cooking activities: The dataset includes a total of 52 participants, each performing a total of 10 cooking activities in multiple real-life kitchens, resulting in over 77 hours of video footage. We evaluate the HTK toolkit, a state-of-the-art speech recognition engine, in combination with multiple video feature descriptors, for both the recognition of cooking activities (e.g., making pancakes) as well as the semantic parsing of videos into action units (e.g., cracking eggs). Our results demonstrate the benefits of structured temporal generative approaches over existing discriminative approaches in coping with the complexity of human daily life activities."
            },
            "slug": "The-Language-of-Actions:-Recovering-the-Syntax-and-Kuehne-Arslan",
            "title": {
                "fragments": [],
                "text": "The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The HTK toolkit is evaluated, a state-of-the-art speech recognition engine, in combination with multiple video feature descriptors, for both the recognition of cooking activities as well as the semantic parsing of videos into action units."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145992652"
                        ],
                        "name": "Michael U Gutmann",
                        "slug": "Michael-U-Gutmann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gutmann",
                            "middleNames": [
                                "U"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael U Gutmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "e negative examples come from the \ufb01xed vocabulary. For continuous inputs such as visual features, one can sample the negative examples from a distribution, such as in the noise contrastive estimation [8]. We then propose the cross-modal mutual information maximization objective. This is motivated by the fact that there are other modalities, e.g., speech transcribed into ASR, that naturally co-occur w"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "is high-dimensional, and has shown promising results for representation learning on images and audio [9, 16]. In particular, contrastive predictive coding (CPC) [16] uses noise contrastive estimation [8] to maximize the MI lower bound. Unlike CPC which relies on auto regressive models to encode context, we use BERT to encode bidirectional context within each sequence, and across different modalities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15816723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025",
            "isKey": false,
            "numCitedBy": 1227,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field."
            },
            "slug": "Noise-contrastive-estimation:-A-new-estimation-for-Gutmann-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A new estimation principle is presented to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity, which leads to a consistent (convergent) estimator of the parameters."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607963"
                        ],
                        "name": "Yonghui Wu",
                        "slug": "Yonghui-Wu",
                        "structuredName": {
                            "firstName": "Yonghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghui Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 260422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
            "isKey": false,
            "numCitedBy": 951,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon."
            },
            "slug": "Exploring-the-Limits-of-Language-Modeling-J\u00f3zefowicz-Vinyals",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This work explores recent advances in Recurrent Neural Networks for large scale Language Modeling, and extends current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053817058"
                        ],
                        "name": "Sebastian Stein",
                        "slug": "Sebastian-Stein",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Stein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Stein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " on average 5.8 action instances per video. We follow the standard four-fold cross validation split [12, 1] which results in an average of 5,810 training and 1,936 test examples. The 50Salads dataset [23] contains 50 videos with 17 \ufb01ne-grained action classes required for making a salad, such as \u201cadd oil\u201d, \u201ccut cucumber\u201d and \u201cmix ingredients\u201d. Annotations are also de\ufb01ned at the level of action units by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2333743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e018fa4a1c893f964f76cee8ff735573974f87cc",
            "isKey": true,
            "numCitedBy": 258,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a publicly available dataset of complex activities that involve manipulative gestures. The dataset captures people preparing mixed salads and contains more than 4.5 hours of accelerometer and RGB-D video data, detailed annotations, and an evaluation protocol for comparison of activity recognition algorithms. Providing baseline results for one possible activity recognition task, this paper further investigates modality fusion methods at different stages of the recognition pipeline: (i) prior to feature extraction through accelerometer localization, (ii) at feature level via feature concatenation, and (iii) at classification level by combining classifier outputs. Empirical evaluation shows that fusing information captured by these sensor types can considerably improve recognition performance."
            },
            "slug": "Combining-embedded-accelerometers-with-computer-for-Stein-McKenna",
            "title": {
                "fragments": [],
                "text": "Combining embedded accelerometers with computer vision for recognizing food preparation activities"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Modality fusion methods at different stages of the recognition pipeline are investigated prior to feature extraction through accelerometer localization, at feature level via feature concatenation, and at classification level by combining classifier outputs."
            },
            "venue": {
                "fragments": [],
                "text": "UbiComp"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51692,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422357"
                        ],
                        "name": "Ishmael Belghazi",
                        "slug": "Ishmael-Belghazi",
                        "structuredName": {
                            "firstName": "Ishmael",
                            "lastName": "Belghazi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ishmael Belghazi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818842"
                        ],
                        "name": "Sai Rajeswar",
                        "slug": "Sai-Rajeswar",
                        "structuredName": {
                            "firstName": "Sai",
                            "lastName": "Rajeswar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sai Rajeswar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14398916"
                        ],
                        "name": "A. Baratin",
                        "slug": "A.-Baratin",
                        "structuredName": {
                            "firstName": "Aristide",
                            "lastName": "Baratin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Baratin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40482726"
                        ],
                        "name": "R. Devon Hjelm",
                        "slug": "R.-Devon-Hjelm",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Hjelm",
                            "middleNames": [
                                "Devon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Devon Hjelm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3614357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "372bf2716c53e353be6c3f027493f1a40edb6640",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size. MINE is back-propable and we prove that it is strongly consistent. We illustrate a handful of applications in which MINE is succesfully applied to enhance the property of generative models in both unsupervised and supervised settings. We apply our framework to estimate the information bottleneck, and apply it in tasks related to supervised classification problems. Our results demonstrate substantial added flexibility and improvement in these settings."
            },
            "slug": "MINE:-Mutual-Information-Neural-Estimation-Belghazi-Rajeswar",
            "title": {
                "fragments": [],
                "text": "MINE: Mutual Information Neural Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This paper presents a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, and proves that it is back-propable and it is strongly consistent."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16443937"
                        ],
                        "name": "Ben Poole",
                        "slug": "Ben-Poole",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Poole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Poole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422336"
                        ],
                        "name": "A\u00e4ron van den Oord",
                        "slug": "A\u00e4ron-van-den-Oord",
                        "structuredName": {
                            "firstName": "A\u00e4ron",
                            "lastName": "Oord",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A\u00e4ron van den Oord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122113652"
                        ],
                        "name": "Alexander A. Alemi",
                        "slug": "Alexander-A.-Alemi",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Alemi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander A. Alemi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145499435"
                        ],
                        "name": "G. Tucker",
                        "slug": "G.-Tucker",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Tucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", (Oord et al., 2018; Poole et al., 2019))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 155100374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aea3547974399a32d7aa7c007b10bd665e93fab",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning."
            },
            "slug": "On-Variational-Bounds-of-Mutual-Information-Poole-Ozair",
            "title": {
                "fragments": [],
                "text": "On Variational Bounds of Mutual Information"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work introduces a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance and demonstrates the effectiveness of these new bounds for estimation and representation learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60441316,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality."
            },
            "slug": "BERT-has-a-Mouth,-and-It-Must-Speak:-BERT-as-a-Wang-Cho",
            "title": {
                "fragments": [],
                "text": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "It is shown that BERT (Devlin et al., 2018) is a Markov random field language model, and this formulation gives way to a natural procedure to sample sentences from BERT, which can produce high quality, fluent generations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation"
            },
            "year": 2019
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 16,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 65,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Video-Representations-using-Contrastive-Sun-Baradel/025a0dc4a2a98742f1b410b6318a46de2c854b22?sort=total-citations"
}