{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Conditional random fields (CRFs) are a recently-introduced formalism [12] for representing a conditional model Pr(y|x), where bothx andy have non-trivial structure (often sequential)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "One common use of CRFs is for seq uential learning problems like NP chunking [16], POS tagging [12], and NER [15 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 24
                            }
                        ],
                        "text": "Following previous work [12, 16] we will define a conditional random field (CRF) to be an estimator of the form"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274348"
                        ],
                        "name": "Khashayar Rohanimanesh",
                        "slug": "Khashayar-Rohanimanesh",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Rohanimanesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khashayar Rohanimanesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [20, 18]), and one of these methods has also been applied to NER [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6038991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0329663498462521483612649c0dffc85d9d9419",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "slug": "Dynamic-conditional-random-fields:-factorized-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "On a natural-language chunking task, it is shown that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40400230"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11664683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b",
            "isKey": false,
            "numCitedBy": 1233,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character n-grams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998)."
            },
            "slug": "Early-results-for-Named-Entity-Recognition-with-and-McCallum-Li",
            "title": {
                "fragments": [],
                "text": "Early results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that conditionally-trained models, such as conditional maximum entropy models, handle inter-dependent features of greedy sequence modeling in NLP well."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Following the notation of Sha and Pereira [16], we express t he loglikelihood over the training sequences as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "One common use of CRFs is for seq uential learning problems like NP chunking [16], POS tagging [12], and NER [15 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 24
                            }
                        ],
                        "text": "Following previous work [12, 16] we will define a conditional random field (CRF) to be an estimator of the form"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 26
                            }
                        ],
                        "text": "Following the notation of Sha and Pereira [16], we express the loglikelihood over the training sequences as\nL(W) = \u2211\n`\nlog Pr(s`|x`,W) = \u2211\n`\n(W \u00b7 G(x`, s`) \u2212 log ZW(x`)) (5)\n1Assuming that non-entity words are placed in unit-length segments, as we do below."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "We focus here on named entity recognition (NER), in whi ch a segment corresponds to an extracted entity; however, similar arguments might be made for several other tasks, such as gene-finding [11] or NP-chunking [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "As in the forward-backward algorithm for chain CRFs [16], space req ui ments here can be reduced fromML|Y| to M |Y|, whereM is the length of the sequence, by pre-computing an appropriate set of\u03b2 values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13936575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6",
            "isKey": true,
            "numCitedBy": 1544,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models."
            },
            "slug": "Shallow-Parsing-with-Conditional-Random-Fields-Sha-Pereira",
            "title": {
                "fragments": [],
                "text": "Shallow Parsing with Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [20, 18]), and one of these methods has also been applied to NER [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2282762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc36397e1fef5c922d64e88211a7e08ecc64759",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies."
            },
            "slug": "Discriminative-Probabilistic-Models-for-Relational-Taskar-Abbeel",
            "title": {
                "fragments": [],
                "text": "Discriminative Probabilistic Models for Relational Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach is presented, showing how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In recent prior work [6], we investigated semi-Markov learn ing methods for NER based on a voted perceptron training algorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "M ore detail on the distance metrics, feature sets, and datasets above can be found elsew h re [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4108765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2559417f8a3d6ab922cfa824b43f9f0c642a1dae",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER."
            },
            "slug": "Exploiting-dictionaries-in-named-entity-extraction:-Cohen-Sarawagi",
            "title": {
                "fragments": [],
                "text": "Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A semi-Markov extraction process is formalized, which is based on sequentially classifying segments of several adjacent words, rather than single words, and provides a more natural formulation of the NER problem than sequential word classification."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1860208"
                        ],
                        "name": "Vinayak R. Borkar",
                        "slug": "Vinayak-R.-Borkar",
                        "structuredName": {
                            "firstName": "Vinayak",
                            "lastName": "Borkar",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinayak R. Borkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38266369"
                        ],
                        "name": "K. Deshmukh",
                        "slug": "K.-Deshmukh",
                        "structuredName": {
                            "firstName": "Kaustubh",
                            "lastName": "Deshmukh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Deshmukh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 275033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "440cb2336fb8c63c065a79130733de1110ce306a",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a method for automatically segmenting unformatted text records into structured elements. Several useful data sources today are human-generated as continuous text whereas convenient usage requires the data to be organized as structured records. A prime motivation is the warehouse address cleaning problem of transforming dirty addresses stored in large corporate databases as a single text field into subfields like \u201cCity\u201d and \u201cStreet\u201d. Existing tools rely on hand-tuned, domain-specific rule-based systems.\nWe describe a tool DATAMOLD that learns to automatically extract structure when seeded with a small number of training examples. The tool enhances on Hidden Markov Models (HMM) to build a powerful probabilistic model that corroborates multiple sources of information including, the sequence of elements, their length distribution, distinguishing words from the vocabulary and an optional external data dictionary. Experiments on real-life datasets yielded accuracy of 90% on Asian addresses and 99% on US addresses. In contrast, existing information extraction methods based on rule-learning techniques yielded considerably lower accuracy."
            },
            "slug": "Automatic-segmentation-of-text-into-structured-Borkar-Deshmukh",
            "title": {
                "fragments": [],
                "text": "Automatic segmentation of text into structured records"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A tool DATAMOLD is described that learns to automatically extract structure when seeded with a small number of training examples and enhances on Hidden Markov Models (HMM) to build a powerful probabilistic model that corroborates multiple sources of information."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144900955"
                        ],
                        "name": "Xianping Ge",
                        "slug": "Xianping-Ge",
                        "structuredName": {
                            "firstName": "Xianping",
                            "lastName": "Ge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xianping Ge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 76
                            }
                        ],
                        "text": "Semi-Markov mode ls are fairly common in certain applications of statistics [8, 9], and are also used in reinforcement learning to model hierarchical Markov decision processes [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59676523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5169b45deb25b9201fb82e4f284a8502fd459eda",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this dissertation, we derive inference and learning procedures for the segmental semi-Markov model, and propose novel learning algorithms and applications within this framework. The application of the model to real-world supervised and unsupervised learning problems is investigated for both real-valued sequences (for time series patterns) and discrete symbol sequences. A variety of experiments are reported on real-world data such as semiconductor manufacturing time series data, ECG biomedical time series, Unix command sequences, and English text, as well as on simulated data sources. The experimental results confirm that the model can outperform competing non-Markov methods across a variety of selected pattern recognition tasks."
            },
            "slug": "Segmental-semi-markov-models-and-applications-to-Ge-Smyth",
            "title": {
                "fragments": [],
                "text": "Segmental semi-markov models and applications to sequence analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This dissertation derives inference and learning procedures for the segmental semi-Markov model, and proposes novel learning algorithms and applications within this framework for real-world supervised and unsupervised learning problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Following previous NER work [7]), we also used indicators for capitalization/letter patterns (such as \u201cAa+\u201d for a capitalized word, or \u201cD\u201d for a single-digit number)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In recent prior work [6], we investigated semi-Markov learning methods for NER based on a voted perceptron training algorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149586"
                        ],
                        "name": "Marios Skounakis",
                        "slug": "Marios-Skounakis",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Skounakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marios Skounakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145527877"
                        ],
                        "name": "Soumya Ray",
                        "slug": "Soumya-Ray",
                        "structuredName": {
                            "firstName": "Soumya",
                            "lastName": "Ray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumya Ray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "Semi-CRFs are similar to nested HMMs [1], which can also be tr ained discriminitively [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60904798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e704a33f74c2da4b65cb5db01b26f305f434bd4",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction can be defined as the task of automatically extracting instances of specified classes or relations from text. We consider the case of using machine learning methods to induce models for extracting relation instances from biomedical articles. We propose and evaluate an approach that is based on using hierarchical hidden Markov models to represent the grammatical structure of the sentences being processed. Our approach first uses a shallow parser to construct a multi-level representation of each sentence being processed. Then we train hierarchical HMMs to capture the regularities of the parses for both positive and negative sentences. We evaluate our method by inducing models to extract binary relations in three biomedical domains. Our experiments indicate that our approach results in more accurate models than several baseline HMM approaches."
            },
            "slug": "Hierarchical-Hidden-Markov-Models-for-Information-Skounakis-Craven",
            "title": {
                "fragments": [],
                "text": "Hierarchical Hidden Markov Models for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes and evaluates an approach that is based on using hierarchical hidden Markov models to represent the grammatical structure of the sentences being processed, which results in more accurate models than several baseline HMM approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "TheJobscorpus contains 73,330 words, and consists of 300 computerrelated job postings [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1646317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3645698144183e6f7dc6d6929d25f2ddfedad3a8",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
            },
            "slug": "Bottom-Up-Relational-Learning-of-Pattern-Matching-Califf-Mooney",
            "title": {
                "fragments": [],
                "text": "Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template, and presents encouraging experimental results on two domains."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145804005"
                        ],
                        "name": "Robert Malouf",
                        "slug": "Robert-Malouf",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Malouf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Malouf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "(In our implementation we use a limited-memory quasi-Newton method [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6249194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "878783964ab23c97052ea82685368099d85c500d",
            "isKey": false,
            "numCitedBy": 741,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices."
            },
            "slug": "A-Comparison-of-Algorithms-for-Maximum-Entropy-Malouf",
            "title": {
                "fragments": [],
                "text": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A number of algorithms for estimating the parameters of ME models are considered, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114798362"
                        ],
                        "name": "Razvan Bunescu and Raymond J. Mooney",
                        "slug": "Razvan-Bunescu-and-Raymond-J.-Mooney",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Mooney",
                            "middleNames": [
                                "Bunescu",
                                "and",
                                "Raymond",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Bunescu and Raymond J. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "Another non-Markovian model recently used for N ER is relational Markov networks (RMNs) [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16248818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aacc2a481a8ae3b1f80bd863db3d525d740cd6d0",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks, which can represent arbitrary dependencies between extractions. This allows for \u201ccollective information extraction\u201d that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach."
            },
            "slug": "Relational-Markov-Networks-for-Collective-Mooney",
            "title": {
                "fragments": [],
                "text": "Relational Markov Networks for Collective Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new IE method is presented that employs Relational Markov Networks, which can represent arbitrary dependencies between extractions, which allows for \u201ccollective information extraction\u201d that exploits the mutual influence between possible extractions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32183233"
                        ],
                        "name": "Andrew Borthwick",
                        "slug": "Andrew-Borthwick",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Borthwick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Borthwick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144360624"
                        ],
                        "name": "J. Sterling",
                        "slug": "J.-Sterling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sterling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6118890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4df361d65a15ca9a7fc27c58c38b04d1f41e6f62",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel statistical namedentity (i.e. \"proper name\") recognition system built around a maximum entity framework. By working v,ithin the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published. 1 I N T R O D U C T I O N Named entity recognition is one of the simplest of the common message understanding tasks. The objective is to identify and categorize all members of certain categories of \"proper names\" from a given corpus. The specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference (MUC-7), in which the task was to identify \"names\" falling into one of seven categories: person, organization, location, date, time, percentage, and monetary amount. This paper describes a new system called \"Maximum Entropy Named Entity\" or \"MENE\" (pronounced \"meanie\"). By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision. These knowledge sources include capitalization features, lexical features, and features indicating the current section of text. It makes use of a broad array of dictionaries of useful single or multi-word terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary. Our dictio152 naries required no manual editing and were either downloaded from the web or were simply \"obvious\" lists entered by hand. This system, built from off-the-shelf knowledge sources, contained no hand-generated pat terns and achieved a result which is comparable with that of the best statistical systems. Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation. Given appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on upper-case English. We also feel that there are plenty of avenues to explore in enhancing the system's performance on English-language newspaper"
            },
            "slug": "Exploiting-Diverse-Knowledge-Sources-via-Maximum-in-Borthwick-Sterling",
            "title": {
                "fragments": [],
                "text": "Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper describes a novel statistical namedentity recognition system built around a maximum entity framework using the framework of maximum entropy theory and utilizing a flexible object-based architecture to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@COLING/ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For applications like NER and gene-finding [ 11 ], these featu res can be quite natural."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We focus here on named entity recognition (NER), in which a segment corresponds to an extracted entity; however, similar arguments might be made for several other tasks, such as gene-finding [ 11 ] or NP-chunking [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6094938,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f1adc7e91a7990512b0081c313d8fc7f50a957b4",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "11-\u2013-Gene-Finding:-Putting-the-Parts-Together-Krogh",
            "title": {
                "fragments": [],
                "text": "11 \u2013 Gene Finding: Putting the Parts Together"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5895004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1291583873fb890e7922ec0dfefd4846df46c9",
            "isKey": false,
            "numCitedBy": 5481,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stacked-generalization-Wolpert",
            "title": {
                "fragments": [],
                "text": "Stacked generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145969795"
                        ],
                        "name": "Pradeep Ravikumar",
                        "slug": "Pradeep-Ravikumar",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Ravikumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pradeep Ravikumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684961"
                        ],
                        "name": "S. Fienberg",
                        "slug": "S.-Fienberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Fienberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fienberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "We used three different similar ity metrics (Jaccard, TFIDF, and JaroWinkler) which are known to work well for name-match ing in data integration tasks [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10625463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9578679e028777dd709881f938114aa59fbbf481",
            "isKey": false,
            "numCitedBy": 1652,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Using an open-source, Java toolkit of name-matching methods, we experimentally compare string distance metrics on the task of matching entity names. We investigate a number of different metrics proposed by different communities, including edit-distance metrics, fast heuristic string comparators, token-based distance metrics, and hybrid methods. Overall, the best-performing method is a hybrid scheme combining a TFIDF weighting scheme, which is widely used in information retrieval, with the Jaro-Winkler string-distance scheme, which was developed in the probabilistic record linkage community."
            },
            "slug": "A-Comparison-of-String-Distance-Metrics-for-Tasks-Cohen-Ravikumar",
            "title": {
                "fragments": [],
                "text": "A Comparison of String Distance Metrics for Name-Matching Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work investigates a number of different metrics proposed by different communities, including edit-distance metrics, fast heuristic string comparators, token-based distance metrics, and hybrid methods, and finds the best-performing method is a hybrid scheme combining a TFIDF weighting scheme with the Jaro-Winkler string-distance scheme."
            },
            "venue": {
                "fragments": [],
                "text": "IIWeb"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "Generative semi-Markov models are fairly common in certain applications of statistics [8, 9], and are also used in reinforcement learning to model hierarchical Markov decision processes [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 76564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d",
            "isKey": false,
            "numCitedBy": 2807,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Between-MDPs-and-Semi-MDPs:-A-Framework-for-in-Sutton-Precup",
            "title": {
                "fragments": [],
                "text": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409964566"
                        ],
                        "name": "Dong C. Liu",
                        "slug": "Dong-C.-Liu",
                        "structuredName": {
                            "firstName": "Dong C.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong C. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "(In our implementation we use a limited-memory quasi-Newton method [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5681609,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1267fe36b5ece49a9d8f913eb67716a040bbcced",
            "isKey": false,
            "numCitedBy": 5863,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems."
            },
            "slug": "On-the-limited-memory-BFGS-method-for-large-scale-Liu-Nocedal",
            "title": {
                "fragments": [],
                "text": "On the limited memory BFGS method for large scale optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence, and the convergence properties are studied to prove global convergence on uniformly convex problems."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702853"
                        ],
                        "name": "R. Kraut",
                        "slug": "R.-Kraut",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kraut",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kraut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692772"
                        ],
                        "name": "Susan R. Fussell",
                        "slug": "Susan-R.-Fussell",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Fussell",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan R. Fussell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38428515"
                        ],
                        "name": "F. Lerch",
                        "slug": "F.-Lerch",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Lerch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lerch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122567536"
                        ],
                        "name": "A. Espinosa",
                        "slug": "A.-Espinosa",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Espinosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Espinosa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "2.3 Semi-Markov CRFsvsorder-L CRFs\nSince conventional CRFs need not maximize over possible segment lengthsd, inference for semi-CRFs is more expensive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8552152,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "4731dfe9b2299b6e58b3fa6eee5c76f158f2695a",
            "isKey": true,
            "numCitedBy": 48,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "Most research examining the influence of coordination on team performance has not distinguished between coordinating (the processes by which teams attempt to manage interdependencies among individuals) and the resultant state of coordination (the degree to which interdependencies are managed well). Similarly, most research has not distinguished between the state of coordination and the performance outcomes that are often influenced by coordination. We demonstrate the usefulness of these distinctions in a study of 50 teams engaged in a realistic 14-week management simulation. Results using a panel design show that two processes for coordinating (use of shared cognition about the distribution of expertise within the team, and working together for a longer time period) improved coordination. Shared cognition seemed to compensate for low levels of communication and lack of working together. The resulting coordination, in turn, directly influenced teams\u2019 financial performance and external evaluations. All effects of the coordination processes, however, were indirect, and operated by helping the teams achieve a more coordinated state. Coordination in teams Page 3 Coordination in Teams: Evidence from Simulated Management Teams Groups are inherently different from individuals performing similar tasks because group members need to coordinate. Whenever the work of individuals is interdependent, they must manage these interdependencies to achieve success (Van de Ven et al., 1976). Malone and Crowston (1994) defined the process of coordination as the extra work individuals must complete when working in concert to accomplish some goal, over and above what they would need to do to accomplish the goal individually. To work together effectively, team members must coordinate their efforts in both broad and detailed ways. At the broad level, for example, a group developing a new product rollout must align goals. If the marketing manager is positioning a product as a prestige item, the engineering manager cannot skimp on quality. At a more detailed level, the team must assign tasks to the people most capable of doing them, hand off work products when others need them, and build components so that they interoperate, for example. Although coordination has long been assumed by group and organizational scholars to be a component of group effectiveness, as Argote and McGrath (1993) note, there has been little systematic research examining the methods teams use to achieve coordination or the effects of coordination on performance. Recent research has started to fill this gap (Gittell, 2001, 2002; Hoegl & Gemuenden, 2001; Lewis, 2003), but this work has not clearly distinguished between two distinct aspects of coordination\u2014the processes teams use to become coordinated and the resultant state of coordination. In addition, some work has failed to distinguish these two dimensions of coordination from the effects that the state of coordination has on team Coordination in Teams Page 4 performance and cohesion. This confusion may arise from the simple fact that the noun \u201ccoordination\u201d is used in the vernacular to mean both \u201cthe act or action of coordinating\u201d and \u201cthe harmonious function of parts for effective results\u201d (Merriam-Webster Online, 2005), which often leads to misinterpretation in both reading and writing research about coordination. These distinctions are important from a managerial point of view, because coordination processes are the levers teams can directly and indirectly manipulate in their attempts to improve coordination. Because coordination processes can be costly, teams should invest in them only if the resulting coordination results in higher performance. For example, in the management simulation we describe below, most teams met three to six hours per week to exchange analyses individual workers had conducted, plan work for the upcoming week, and set broad directions. These lengthy group meetings took time away from individuals\u2019 analyses and writing, but the teams invested in them because they believed that they needed the resulting coordination to have coherent plans and presentations. If managers and organizational scholars do not distinguish among coordination processes, coordination as a state, and the production outcomes, they cannot make informed judgments about the types of investments that are valuable in setting up workgroups nor trace the mechanisms through which managerial decisions have their impact on production. In this paper we attempt to clarify these distinctions, both conceptually and through confirmatory factor analyses. We then empirically examine several processes that teams in a managerial simulation used to achieve the state of coordination, concentrating on team communication, shared cognition, and tradeoffs between them. We show empirically that coordination as a state directly influences teams\u2019 performance outcomes and that shared cognition can substitute for communication and experience working together in achieving Coordination in Teams Page 5 successful coordination. Distinguishing coordination as a process, coordination as a state, and performance outcomes Coordination as a process When individuals are working together in a group, certain tasks can be carried out independently and therefore don\u2019t need to be coordinated with those performed by other members. Other tasks are interdependent with the activities of other members, so teams must employ various coordination processes to manage these interdependencies (Malone et al., 1999). Managers and self-managed groups can avail themselves of a wide range of coordination processes when trying to manage interdependencies. Some, like the use of configuration management systems in software engineering or the employment of assembly lines in automotive manufacturing, are highly industryand task-specific. Others, like communication and use of shared cognition, are quite general and can potentially be applied to many different types of groups performing different tasks. In this paper, we emphasize communication and shared cognition because of their generality. All teams use them in varying degrees to coordinate. In terms of March and Simon\u2019s classic distinction between mechanistic coordinating through planning and organic coordinating through mutual adjustment, unstructured interpersonal communication is an example of an organic coordination process (March & Simon, 1958). Individuals exchange information about their current states and adjust their behavior to others\u2019 goals and actions (Thompson, 1967). When a bicyclist extends an arm to signal a left turn and catches a motorist\u2019s eye to insure the signal was understood, they are coordinating through interpersonal communication. Groups can directly control how much they use Coordination in Teams Page 6 interpersonal communication as a coordination process. To increase interpersonal communication, they can schedule regular group meetings, locate team members in the same area to foster ad hoc meetings, and invest in telecommunication technology for communication at a distance (Olson & Olson, 2000). In contrast, shared cognition is on the mechanistic end of the March and Simon continuum. Shared cognition is a belief held in common among members of a team about how each will behave. Like standard operating procedures and other pre-formed courses of action, shared cognition is a mechanism that allows members of a group to coordinate without explicit communication. Shared cognition is regularly used to coordinate in daily life. For example, motorists can move smoothly through an intersection with four-way stop signs if they all defer to the motorist on their right and believe that the others will do so as well. Similarly, visitors to Grand Central Station in New York can rendezvous with friends at the Biltmore clock without making explicit plans, if they believe the clock is a sensible landmark for meeting and think that their friends will believe this as well. Managers cannot mandate shared cognition in the same way that they can schedule group meetings. They can, however, make decisions to increase the likelihood that shared cognition will be available in a group, or to increase the degree to which it will develop. For example, they can select members with a common background (O'Leary et al., 2002; Ouchi, 1980), adopt personnel policies that discourage turnover (Katz, 1982), or invest in training regimes designed to foster common views, such as group training (Moreland et al., 1998) or cross-training (Volpe et al., 1996). Coordination as a state Although managers have some control over the coordination processes used in their organizations, they have less control over the resultant state of coordination\u2014that is, the extent Coordination in Teams Page 7 to which members in their organization have successfully managed interdependencies. Coordination can be more or less successfully achieved. For example, in concurrent development projects, failures of coordination can consume up to 50% of the engineering capacity and up to one-third of the development budget (Terwiesch et al., 2002). One can think of coordination in this sense as an index of group efficiency. When coordination is high, a unit of individual work translates into more or better team output. In contrast, when coordination is low, the same quality and quantity of individual work results in less or poorer group output, because of what Steiner (1972) terms process losses. Failures of coordination are characterized by redundant work, delays in production, and incompatibilities between intermediate outputs that will need to be redone before they can be integrated. Research from the classic contingency theory tradition in organizational behavior demonstrates that identical coordination mechanisms can have different consequences depending upon the uncertainty of the task to which they are applied (Katz & Tushman, 1979). Although most of the early conti"
            },
            "slug": "Coordination-in-Teams:-Evidence-from-a-Simulated-Kraut-Fussell",
            "title": {
                "fragments": [],
                "text": "Coordination in Teams: Evidence from a Simulated Management Game"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 86
                            }
                        ],
                        "text": "For instance, for city names in theAddress data, we used a web page listing cities in India."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Semi-CRFs are similar to nested HMMs [1], which can also be tr ained discriminitively [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Recall thatsemi-Markov chain modelsextend hidden Markov models (HMMs) by allowing each statesi to persist for a non-unit length of time di."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "The primary difference is that the \u201cinner model\u201d for semi-CRFs is of short, uniformly-labeled segments with non-Markovian properties, while nested HMMs allow longer, diversely-labeled, Markovian \u201csegments\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Semi-CRFs are similar to nested HMMs [1], which can also be trained discriminitively [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "TheAddresscorpus contains 4,226 words, and consists of 395 home addres ses of students in a major university in India [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "TheAddresscorpus contains 4,226 words, and consists of 395 home addresses of students in a major university in India [1]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic text segm  entation for extracting structured records"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ACM SIGMOD International Conf. on Management of Data"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 86
                            }
                        ],
                        "text": "For instance, for city names in theAddress data, we used a web page listing cities in India."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Semi-CRFs are similar to nested HMMs [1], which can also be tr ained discriminitively [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Recall thatsemi-Markov chain modelsextend hidden Markov models (HMMs) by allowing each statesi to persist for a non-unit length of time di."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "The primary difference is that the \u201cinner model\u201d for semi-CRFs is of short, uniformly-labeled segments with non-Markovian properties, while nested HMMs allow longer, diversely-labeled, Markovian \u201csegments\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Semi-CRFs are similar to nested HMMs [1], which can also be trained discriminitively [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "TheAddresscorpus contains 4,226 words, and consists of 395 home addres ses of students in a major university in India [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "TheAddresscorpus contains 4,226 words, and consists of 395 home addresses of students in a major university in India [1]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic tex t segmentation for extracting structured records"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. ACM SIGMOD International Conf. on Management of Data , S nta Barabara,USA,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 86
                            }
                        ],
                        "text": "For instance, for city names in theAddress data, we used a web page listing cities in India."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Recall thatsemi-Markov chain modelsextend hidden Markov models (HMMs) by allowing each statesi to persist for a non-unit length of time di."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "The primary difference is that the \u201cinner model\u201d for semi-CRFs is of short, uniformly-labeled segments with non-Markovian properties, while nested HMMs allow longer, diversely-labeled, Markovian \u201csegments\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Semi-CRFs are similar to nested HMMs [1], which can also be trained discriminitively [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "TheAddresscorpus contains 4,226 words, and consists of 395 home addresses of students in a major university in India [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic text segmentation for extracting structured records"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. ACM SIGMOD International Conf. on Management of Data, Santa Barabara,USA,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In recent prior work [6], we investigated semi-Markov learn ing methods for NER based on a voted perceptron training algorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "M ore detail on the distance metrics, feature sets, and datasets above can be found elsew h re [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting dictionaries in n  amed entity extraction: Combining semi-markov extraction processes and data integration met  hods"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discover  y and Data Mining,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "The 18,121-wordEmail corpus contains 216 email messages taken from the CSPACE ema il corpus [10], which is mail associated with a 14-week, 277-pe rson management game."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Coo  rdination in teams: evidence from a simulated management"
            },
            "venue": {
                "fragments": [],
                "text": "game. To appear in the Journal of Organization  al Behavior,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Dyanamic CRFs [18] can, with an appropriate network archite ture, be used to implement semi-CRFs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic co  nditional random fields: Factorized probabilistic models for labeling and segmenting sequ  ence data"
            },
            "venue": {
                "fragments": [],
                "text": "InICML,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "TheJobscorpus contains 73,330 words, and consists of 300 computerrelated job postings [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bottom-up relational learning of patter  n matching rules for information extraction.Journal of Machine Learning Research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [20, 18]), and one of these methods has also been applied to NER [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative probabilistic mo dels for relational data"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of Eighteenth Conference on Uncertainty in Artificial Intelligen ce (UAI02),"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-Markov Models and Applications. Kluwer Academic"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-Markov Models and Applications. Kluwer Academic"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "TheJobscorpus contains 73,330 words, and consists of 300 computerrelated job postings [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bottom-up relational learn  ing of pattern matching rules for information extraction.Journal of Machine Learning Research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "The second version, called CRF/4, replaces the I tag with four tagsB, E, C, andU , which depend on where the word appears in an entity [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting di verse knowledge sources via maximum entropy in named entity recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Sixth Workshop on Very Large Corpora New Brunswick,"
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 15
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Semi-Markov-Conditional-Random-Fields-for-Sarawagi-Cohen/135ace829b6ad2ec9db040d8e5fd137034e83665?sort=total-citations"
}