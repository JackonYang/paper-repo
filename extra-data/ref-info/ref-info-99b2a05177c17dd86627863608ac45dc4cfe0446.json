{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46956825"
                        ],
                        "name": "Shubham Paliwal",
                        "slug": "Shubham-Paliwal",
                        "structuredName": {
                            "firstName": "Shubham",
                            "lastName": "Paliwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shubham Paliwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144819655"
                        ],
                        "name": "D. Vishwanath",
                        "slug": "D.-Vishwanath",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "Vishwanath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Vishwanath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35888656"
                        ],
                        "name": "R. Rahul",
                        "slug": "R.-Rahul",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Rahul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rahul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145092223"
                        ],
                        "name": "Monika Sharma",
                        "slug": "Monika-Sharma",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Monika Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3213990"
                        ],
                        "name": "L. Vig",
                        "slug": "L.-Vig",
                        "structuredName": {
                            "firstName": "Lovekesh",
                            "lastName": "Vig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "le Recognition (TR). enables the new tree-edit-distance-based evaluation metric that we propose in Section V B. Model Traditional table detection and recognition methods rely on pre-de\ufb01ned rules [11]\u2013[16] and statistical machine learning [17]\u2013[21]. Recently, deep learning exhibit great performance in image-based table detection and structure recognition. Hao et al. used a set of primitive rules to pro"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 209862087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7284721854bd9db96a9e442caef0609d4324415",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "With the widespread use of mobile phones and scanners to photograph and upload documents, the need for extracting the information trapped in unstructured document images such as retail receipts, insurance claim forms and financial invoices is becoming more acute. A major hurdle to this objective is that these images often contain information in the form of tables and extracting data from tabular sub-images presents a unique set of challenges. This includes accurate detection of the tabular region within an image, and subsequently detecting and extracting information from the rows and columns of the detected table. While some progress has been made in table detection, extracting the table contents is still a challenge since this involves more fine grained table structure(rows & columns) recognition. Prior approaches have attempted to solve the table detection and structure recognition problems independently using two separate models. In this paper, we propose TableNet: a novel end-to-end deep learning model for both table detection and structure recognition. The model exploits the interdependence between the twin tasks of table detection and table structure recognition to segment out the table and column regions. This is followed by semantic rule-based row extraction from the identified tabular sub-regions. The proposed model and extraction approach was evaluated on the publicly available ICDAR 2013 and Marmot Table datasets obtaining state of the art results. Additionally, we demonstrate that feeding additional semantic features further improves model performance and that the model exhibits transfer learning across datasets. Another contribution of this paper is to provide additional table structure annotations for the Marmot data, which currently only has annotations for table detection."
            },
            "slug": "TableNet:-Deep-Learning-Model-for-End-to-end-Table-Paliwal-Vishwanath",
            "title": {
                "fragments": [],
                "text": "TableNet: Deep Learning Model for End-to-end Table Detection and Tabular Data Extraction from Scanned Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed TableNet is a novel end-to-end deep learning model that exploits the interdependence between the twin tasks of table detection and table structure recognition to segment out the table and column regions."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057440295"
                        ],
                        "name": "Sebastian Schreiber",
                        "slug": "Sebastian-Schreiber",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582412"
                        ],
                        "name": "S. Agne",
                        "slug": "S.-Agne",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Agne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651288"
                        ],
                        "name": "I. Wolf",
                        "slug": "I.-Wolf",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734717217"
                        ],
                        "name": "Sheraz Ahmed",
                        "slug": "Sheraz-Ahmed",
                        "structuredName": {
                            "firstName": "Sheraz",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheraz Ahmed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ion [23]\u2013[25]. In addition, deep neural networks for object detection, such as Faster-RCNN [26], Mask-RCNN [27], and YOLO [28] have been exploited for table detection and row/column segmentation [7], [29]\u2013[31]. Furthermore, graph neural networks are used for table detection and recognition by encoding document images as graphs [5], [32]. There are several tools (see Table II) that can convert tables i"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10191334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8bead3ae810cd3f7427d3004e45b4158da9b744",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel end-to-end system for table understanding in document images called DeepDeSRT. In particular, the contribution of DeepDeSRT is two-fold. First, it presents a deep learning-based solution for table detection in document images. Secondly, it proposes a novel deep learning-based approach for table structure recognition, i.e. identifying rows, columns, and cell positions in the detected tables. In contrast to existing rule-based methods, which rely on heuristics or additional PDF metadata (like, for example, print instructions, character bounding boxes, or line segments), the presented system is data-driven and does not need any heuristics or metadata to detect as well as to recognize tabular structures in document images. Furthermore, in contrast to most existing table detection and structure recognition methods, which are applicable only to PDFs, DeepDeSRT processes document images, which makes it equally suitable for born-digital PDFs (as they can automatically be converted into images) as well as even harder problems, e.g. scanned documents. To gauge the performance of DeepDeSRT, the system is evaluated on the publicly available ICDAR 2013 table competition dataset containing 67 documents with 238 pages overall. Evaluation results reveal that DeepDeSRT outperforms state-of-the-art methods for table detection and structure recognition and achieves F1-measures of 96.77% and 91.44% for table detection and structure recognition, respectively. Additionally, DeepDeSRT is evaluated on a closed dataset from a real use case of a major European aviation company comprising documents which are highly unlike those in ICDAR 2013. Tested on a randomly selected sample from this dataset, DeepDeSRT achieves high detection accuracy for tables which demonstrates the sound generalization capabilities of our system."
            },
            "slug": "DeepDeSRT:-Deep-Learning-for-Detection-and-of-in-Schreiber-Agne",
            "title": {
                "fragments": [],
                "text": "DeepDeSRT: Deep Learning for Detection and Structure Recognition of Tables in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In contrast to most existing table detection and structure recognition methods, which are applicable only to PDFs, DeepDeSRT processes document images, which makes it equally suitable for born-digital PDFs as well as even harder problems, e.g. scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402440928"
                        ],
                        "name": "Azka Gilani",
                        "slug": "Azka-Gilani",
                        "structuredName": {
                            "firstName": "Azka",
                            "lastName": "Gilani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Azka Gilani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39404123"
                        ],
                        "name": "S. Qasim",
                        "slug": "S.-Qasim",
                        "structuredName": {
                            "firstName": "Shah",
                            "lastName": "Qasim",
                            "middleNames": [
                                "Rukh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Qasim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49012494"
                        ],
                        "name": "M. I. Malik",
                        "slug": "M.-I.-Malik",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Malik",
                            "middleNames": [
                                "Imran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 180
                            }
                        ],
                        "text": "In addition, deep neural networks for object detection, such as Faster-RCNN [28], Mask-RCNN [12], and YOLO [27] have been exploited for table detection and row/column segmentation [8, 30, 35, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206777650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d6931dd0ba9e492b0ceab00268ca4be62ef663a",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is a crucial step in many document analysis applications as tables are used for presenting essential information to the reader in a structured manner. It is a hard problem due to varying layouts and encodings of the tables. Researchers have proposed numerous techniques for table detection based on layout analysis of documents. Most of these techniques fail to generalize because they rely on hand engineered features which are not robust to layout variations. In this paper, we have presented a deep learning based method for table detection. In the proposed method, document images are first pre-processed. These images are then fed to a Region Proposal Network followed by a fully connected neural network for table detection. The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines. We have done our evaluations on publicly available UNLV dataset where it beats Tesseract's state of the art table detection system by a significant margin."
            },
            "slug": "Table-Detection-Using-Deep-Learning-Gilani-Qasim",
            "title": {
                "fragments": [],
                "text": "Table Detection Using Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines and beats Tesseract's state of the art table detection system by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67319819"
                        ],
                        "name": "Chris Tensmeyer",
                        "slug": "Chris-Tensmeyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Tensmeyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Tensmeyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2852035"
                        ],
                        "name": "Vlad I. Morariu",
                        "slug": "Vlad-I.-Morariu",
                        "structuredName": {
                            "firstName": "Vlad",
                            "lastName": "Morariu",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vlad I. Morariu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844147"
                        ],
                        "name": "Brian L. Price",
                        "slug": "Brian-L.-Price",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Price",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian L. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145823372"
                        ],
                        "name": "Scott D. Cohen",
                        "slug": "Scott-D.-Cohen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143769852"
                        ],
                        "name": "Tony R. Martinez",
                        "slug": "Tony-R.-Martinez",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Martinez",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tony R. Martinez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "olutional neural network to determine whether the regions contain a table [22]. Fullyconvolutional neural networks, followed by a conditional random \ufb01eld, have also been used for table detection [23]\u2013[25]. In addition, deep neural networks for object detection, such as Faster-RCNN [26], Mask-RCNN [27], and YOLO [28] have been exploited for table detection and row/column segmentation [7], [29]\u2013[31]. Fu"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 211027046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "658eef14952dc6dee87f5ef1c0f7313441aadf98",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Given the large variety and complexity of tables, table structure extraction is a challenging task in automated document analysis systems. We present a pair of novel deep learning models (Split and Merge models) that given an input image, 1) predicts the basic table grid pattern and 2) predicts which grid elements should be merged to recover cells that span multiple rows or columns. We propose projection pooling as a novel component of the Split model and grid pooling as a novel part of the Merge model. While most Fully Convolutional Networks rely on local evidence, these unique pooling regions allow our models to take advantage of the global table structure. We achieve state-of-the-art performance on the public ICDAR 2013 Table Competition dataset of PDF documents. On a much larger private dataset which we used to train the models, we significantly outperform both a state-ofthe-art deep model and a major commercial software system."
            },
            "slug": "Deep-Splitting-and-Merging-for-Table-Structure-Tensmeyer-Morariu",
            "title": {
                "fragments": [],
                "text": "Deep Splitting and Merging for Table Structure Decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A pair of novel deep learning models (Split and Merge models) that given an input image, predicts the basic table grid pattern and predicts which grid elements should be merged to recover cells that span multiple rows or columns are presented."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113309578"
                        ],
                        "name": "Xu Zhong",
                        "slug": "Xu-Zhong",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2328282"
                        ],
                        "name": "Jianbin Tang",
                        "slug": "Jianbin-Tang",
                        "structuredName": {
                            "firstName": "Jianbin",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbin Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399097376"
                        ],
                        "name": "Antonio Jimeno-Yepes",
                        "slug": "Antonio-Jimeno-Yepes",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Jimeno-Yepes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Jimeno-Yepes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201124789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5799d10df17de3232540e990da69553800d6376",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis."
            },
            "slug": "PubLayNet:-Largest-Dataset-Ever-for-Document-Layout-Zhong-Tang",
            "title": {
                "fragments": [],
                "text": "PubLayNet: Largest Dataset Ever for Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The PubLayNet dataset for document layout analysis is developed by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central and demonstrated that deep neural networks trained on Pub LayNet accurately recognize the layout of scientific articles."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40420775"
                        ],
                        "name": "Pau Riba",
                        "slug": "Pau-Riba",
                        "structuredName": {
                            "firstName": "Pau",
                            "lastName": "Riba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pau Riba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149815080"
                        ],
                        "name": "Anjan Dutta",
                        "slug": "Anjan-Dutta",
                        "structuredName": {
                            "firstName": "Anjan",
                            "lastName": "Dutta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anjan Dutta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145029669"
                        ],
                        "name": "L. Goldmann",
                        "slug": "L.-Goldmann",
                        "structuredName": {
                            "firstName": "Lutz",
                            "lastName": "Goldmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goldmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686569"
                        ],
                        "name": "A. Forn\u00e9s",
                        "slug": "A.-Forn\u00e9s",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "Forn\u00e9s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Forn\u00e9s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045937"
                        ],
                        "name": "O. R. Terrades",
                        "slug": "O.-R.-Terrades",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Terrades",
                            "middleNames": [
                                "Ramos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. R. Terrades"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826881"
                        ],
                        "name": "J. Llad\u00f3s",
                        "slug": "J.-Llad\u00f3s",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Llad\u00f3s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Llad\u00f3s"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "en exploited for table detection and row/column segmentation [7], [29]\u2013[31]. Furthermore, graph neural networks are used for table detection and recognition by encoding document images as graphs [5], [32]. There are several tools (see Table II) that can convert tables in text-based PDF format into structured representations. However, there is limited work on image-based table recognition. Attention-ba"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 211027013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e45d822c782fd9e52b4e5abf60b3b7d6d5e3df0",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Tabular structures in documents offer a complementary dimension to the raw textual data, representing logical or quantitative relationships among pieces of information. In digital mail room applications, where a large amount of administrative documents must be processed with reasonable accuracy, the detection and interpretation of tables is crucial. Table recognition has gained interest in document image analysis, in particular in unconstrained formats (absence of rule lines, unknown information of rows and columns). In this work, we propose a graph-based approach for detecting tables in document images. Instead of using the raw content (recognized text), we make use of the location, context and content type, thus it is purely a structure perception approach, not dependent on the language and the quality of the text reading. Our framework makes use of Graph Neural Networks (GNNs) in order to describe the local repetitive structural information of tables in invoice documents. Our proposed model has been experimentally validated in two invoice datasets and achieved encouraging results. Additionally, due to the scarcity of benchmark datasets for this task, we have contributed to the community a novel dataset derived from the RVL-CDIP invoice data. It will be publicly released to facilitate future research."
            },
            "slug": "Table-Detection-in-Invoice-Documents-by-Graph-Riba-Dutta",
            "title": {
                "fragments": [],
                "text": "Table Detection in Invoice Documents by Graph Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a graph-based approach for detecting tables in document images that makes use of Graph Neural Networks (GNNs) in order to describe the local repetitive structural information of tables in invoice documents."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145104988"
                        ],
                        "name": "M. Fan",
                        "slug": "M.-Fan",
                        "structuredName": {
                            "firstName": "Miao",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153586399"
                        ],
                        "name": "Doo Soon Kim",
                        "slug": "Doo-Soon-Kim",
                        "structuredName": {
                            "firstName": "Doo",
                            "lastName": "Kim",
                            "middleNames": [
                                "Soon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doo Soon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditional table detection and recognition methods rely on pre-defined rules [11]\u2013[16] and statistical machine learning [17]\u2013[21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2360436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d8d00709eec584a597891a0773086a457d7cd0e",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Superior to state-of-the-art approaches which compete performances of table recognition on 67 annotated government documents released by ICDAR 2013 Table Competition, this paper contributes a novel paradigm on large-scale unlabeled PDF files. We integrate the paradigm into our latest developed system (PdfExtra) to detect the region of tables by means of 9,466 academic articles from the entire repository of ACL Anthology, where almost all papers are archived by PDF format without annotation. The paradigm first adopts heuristics to automatically construct weakly labeled data, then feeds diverse evidences, such as font-styles, layouts and even linguistic features extracted by Apache PDFBox and processed by Stanford NLP toolkit, into different canonical classifiers, and finally uses these models to collaboratively vote on the boundary of tables. Experimental results show that PdfExtra performs a great leap forward, compared with the state-ofthe-arts. Moreover, we discuss the factors of different features, learning models and even domains that may impact the performance. Extensive evaluations demonstrate that our paradigm is compatible enough to leverage various features and learning models for cross-domain table region detection."
            },
            "slug": "Table-Region-Detection-on-Large-scale-PDF-Files-Fan-Kim",
            "title": {
                "fragments": [],
                "text": "Table Region Detection on Large-scale PDF Files without Labeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel paradigm is integrated into the latest developed system (PdfExtra) to detect the region of tables by means of 9,466 academic articles from the entire repository of ACL Anthology, where almost all papers are archived by PDF format without annotation."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656315"
                        ],
                        "name": "Leipeng Hao",
                        "slug": "Leipeng-Hao",
                        "structuredName": {
                            "firstName": "Leipeng",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leipeng Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777642"
                        ],
                        "name": "Liangcai Gao",
                        "slug": "Liangcai-Gao",
                        "structuredName": {
                            "firstName": "Liangcai",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangcai Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3412797"
                        ],
                        "name": "Xiaohan Yi",
                        "slug": "Xiaohan-Yi",
                        "structuredName": {
                            "firstName": "Xiaohan",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohan Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143830636"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "used a set of primitive rules to propose candidate table regions and a convolutional neural network to determine whether the regions contain a table [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2870724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06bf934004b6f93711298f905b1e447683a8d0b9",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the better performance of deep learning on many computer vision tasks, researchers in the area of document analysis and recognition begin to adopt this technique into their work. In this paper, we propose a novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models. In the proposed method, some table-like areas are selected first by some loose rules, and then the convolutional networks are built and refined to determine whether the selected areas are tables or not. Besides, the visual features of table areas are directly extracted and utilized through the convolutional networks, while the non-visual information (e.g. characters, rendering instructions) contained in original PDF documents is also taken into consideration to help achieve better recognition results. The primary experimental results show that the approach is effective in table detection."
            },
            "slug": "A-Table-Detection-Method-for-PDF-Documents-Based-on-Hao-Gao",
            "title": {
                "fragments": [],
                "text": "A Table Detection Method for PDF Documents Based on Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models, which shows that the approach is effective in table detection."
            },
            "venue": {
                "fragments": [],
                "text": "2016 12th IAPR Workshop on Document Analysis Systems (DAS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39404123"
                        ],
                        "name": "S. Qasim",
                        "slug": "S.-Qasim",
                        "structuredName": {
                            "firstName": "Shah",
                            "lastName": "Qasim",
                            "middleNames": [
                                "Rukh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Qasim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588430"
                        ],
                        "name": "Hassan Mahmood",
                        "slug": "Hassan-Mahmood",
                        "structuredName": {
                            "firstName": "Hassan",
                            "lastName": "Mahmood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hassan Mahmood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 195791577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f39b5b60981bc3c37f0557963737f67e9241b8eb",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Document structure analysis, such as zone segmentation and table recognition, is a complex problem in document processing and is an active area of research. The recent success of deep learning in solving various computer vision and machine learning problems has not been reflected in document structure analysis since conventional neural networks are not well suited to the input structure of the problem. In this paper, we propose an architecture based on graph networks as a better alternative to standard neural networks for table recognition. We argue that graph networks are a more natural choice for these problems, and explore two gradient-based graph neural networks. Our proposed architecture combines the benefits of convolutional neural networks for visual feature extraction and graph networks for dealing with the problem structure. We empirically demonstrate that our method outperforms the baseline by a significant margin. In addition, we identify the lack of large scale datasets as a major hindrance for deep learning research for structure analysis and present a new large scale synthetic dataset for the problem of table recognition. Finally, we open-source our implementation of dataset generation and the training framework of our graph networks to promote reproducible research in this direction."
            },
            "slug": "Rethinking-Table-Recognition-using-Graph-Neural-Qasim-Mahmood",
            "title": {
                "fragments": [],
                "text": "Rethinking Table Recognition using Graph Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an architecture based on graph networks as a better alternative to standard neural networks for table recognition, argues that graph networks are a more natural choice for these problems, and explores two gradient-based graph neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115379063"
                        ],
                        "name": "Jing Fang",
                        "slug": "Jing-Fang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070898875"
                        ],
                        "name": "Xin Tao",
                        "slug": "Xin-Tao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087321561"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29035564"
                        ],
                        "name": "Ruiheng Qiu",
                        "slug": "Ruiheng-Qiu",
                        "structuredName": {
                            "firstName": "Ruiheng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruiheng Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49420283"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23786594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f7a62139dfd09bfad667691332bd55e31736887",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is an important task in the field of document analysis. It has been extensively studied since a couple of decades. Various kinds of document mediums are involved, from scanned images to web pages, from plain texts to PDF files. Numerous algorithms published bring up a challenging issue: how to evaluate algorithms in different context. Currently, most work on table detection conducts experiments on their in-house dataset. Even the few sources of online datasets are targeted at image documents only. Moreover, Precision and recall measurement are usual practice in order to account performance based on human evaluation. In this paper, we provide a dataset that is representative, large and most importantly, publicly available. The compatible format of the ground truth makes evaluation independent of document medium. We also propose a set of new measures, implement them, and open the source code. Finally, three existing table detection algorithms are evaluated to demonstrate the reliability of the dataset and metrics."
            },
            "slug": "Dataset,-Ground-Truth-and-Performance-Metrics-for-Fang-Tao",
            "title": {
                "fragments": [],
                "text": "Dataset, Ground-Truth and Performance Metrics for Table Detection Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A dataset that is representative, large and most importantly, publicly available, and the compatible format of the ground truth makes evaluation independent of document medium is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047516905"
                        ],
                        "name": "Max C. G\u00f6bel",
                        "slug": "Max-C.-G\u00f6bel",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "G\u00f6bel",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max C. G\u00f6bel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18592170"
                        ],
                        "name": "Tamir Hassan",
                        "slug": "Tamir-Hassan",
                        "structuredName": {
                            "firstName": "Tamir",
                            "lastName": "Hassan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamir Hassan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759801"
                        ],
                        "name": "Ermelinda Oro",
                        "slug": "Ermelinda-Oro",
                        "structuredName": {
                            "firstName": "Ermelinda",
                            "lastName": "Oro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ermelinda Oro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35203250"
                        ],
                        "name": "G. Orsi",
                        "slug": "G.-Orsi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Orsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orsi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is often referred as table recognition [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "4m ICDAR2013 [2] 3 3 3 156 ICDAR2019 [9] 3 3 7 3."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206777311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cbc1582175b5c03b8203ec38bb25bae9d66397d",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Table understanding is a well studied problem in document analysis, and many academic and commercial approaches have been developed to recognize tables in several document formats, including plain text, scanned page images and born-digital, object-based formats such as PDF. Despite the abundance of these techniques, an objective comparison of their performance is still missing. The Table Competition held in the context of ICDAR 2013 is our first attempt at objectively evaluating these techniques against each other in a standardized way, across several input formats. The competition independently addresses three problems: (i) table location, (ii) table structure recognition, and (iii) these two tasks combined. We received results from seven academic systems, which we have also compared against four commercial products. This paper presents our findings."
            },
            "slug": "ICDAR-2013-Table-Competition-G\u00f6bel-Hassan",
            "title": {
                "fragments": [],
                "text": "ICDAR 2013 Table Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Table Competition held in the context of ICDAR 2013 is the first attempt at objectively evaluating these techniques against each other in a standardized way, across several input formats."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912641"
                        ],
                        "name": "I. Kavasidis",
                        "slug": "I.-Kavasidis",
                        "structuredName": {
                            "firstName": "Isaak",
                            "lastName": "Kavasidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kavasidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46792082"
                        ],
                        "name": "S. Palazzo",
                        "slug": "S.-Palazzo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Palazzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Palazzo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441118"
                        ],
                        "name": "C. Spampinato",
                        "slug": "C.-Spampinato",
                        "structuredName": {
                            "firstName": "Concetto",
                            "lastName": "Spampinato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Spampinato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145587660"
                        ],
                        "name": "C. Pino",
                        "slug": "C.-Pino",
                        "structuredName": {
                            "firstName": "Carmelo",
                            "lastName": "Pino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144027622"
                        ],
                        "name": "D. Giordano",
                        "slug": "D.-Giordano",
                        "structuredName": {
                            "firstName": "Daniela",
                            "lastName": "Giordano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Giordano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076542103"
                        ],
                        "name": "D. Giuffrida",
                        "slug": "D.-Giuffrida",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Giuffrida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Giuffrida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19324186"
                        ],
                        "name": "P. Messina",
                        "slug": "P.-Messina",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Messina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Messina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "Fullyconvolutional neural networks, followed by a conditional random field, have also been used for table detection [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4899629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03672cfa599950f208d424d5298cdc12b72c2492",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks (DCNNs) have recently been applied successfully to a variety of vision and multimedia tasks, thus driving development of novel solutions in several application domains. Document analysis is a particularly promising area for DCNNs: indeed, the number of available digital documents has reached unprecedented levels, and humans are no longer able to discover and retrieve all the information contained in these documents without the help of automation. Under this scenario, DCNNs offers a viable solution to automate the information extraction process from digital documents. Within the realm of information extraction from documents, detection of tables and charts is particularly needed as they contain a visual summary of the most valuable information contained in a document. For a complete automation of visual information extraction process from tables and charts, it is necessary to develop techniques that localize them and identify precisely their boundaries. In this paper we aim at solving the table/chart detection task through an approach that combines deep convolutional neural networks, graphical models and saliency concepts. In particular, we propose a saliency-based fully-convolutional neural network performing multi-scale reasoning on visual cues followed by a fully-connected conditional random field (CRF) for localizing tables and charts in digital/digitized documents. Performance analysis carried out on an extended version of ICDAR 2013 (with annotated charts as well as tables) shows that our approach yields promising results, outperforming existing models."
            },
            "slug": "A-Saliency-based-Convolutional-Neural-Network-for-Kavasidis-Palazzo",
            "title": {
                "fragments": [],
                "text": "A Saliency-based Convolutional Neural Network for Table and Chart Detection in Digitized Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A saliency-based fully-convolutional neural network performing multi-scale reasoning on visual cues followed by a fully-connected conditional random field (CRF) for localizing tables and charts in digital/digitized documents is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICIAP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500370330"
                        ],
                        "name": "Noah Siegel",
                        "slug": "Noah-Siegel",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Siegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Siegel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35219984"
                        ],
                        "name": "Nicholas Lourie",
                        "slug": "Nicholas-Lourie",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Lourie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas Lourie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39071178"
                        ],
                        "name": "Russell Power",
                        "slug": "Russell-Power",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Power",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Russell Power"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145585097"
                        ],
                        "name": "Waleed Ammar",
                        "slug": "Waleed-Ammar",
                        "structuredName": {
                            "firstName": "Waleed",
                            "lastName": "Ammar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Waleed Ammar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Marmot [5] 3 7 7 958 PubLayNet [6] 3 7 7 113k DeepFigures [7] 3 7 7 1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4698432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fec9d41d372267b4474f18cbeadd806c8b67adb",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels---4,000 times larger than the previous largest figure extraction dataset---with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar,\\footnote\\urlhttps://www.semanticscholar.org/ a large-scale academic search engine, and used to extract figures in 13 million scientific documents.\\footnoteA demo of our system is available at \\urlhttp://labs.semanticscholar.org/deepfigures/,and our dataset of induced labels can be downloaded at \\urlhttps://s3-us-west-2.amazonaws.com/ai2-s2-research-public/deepfigures/jcdl-deepfigures-labels.tar.gz. Code to run our system locally can be found at \\urlhttps://github.com/allenai/deepfigures-open."
            },
            "slug": "Extracting-Scientific-Figures-with-Distantly-Neural-Siegel-Lourie",
            "title": {
                "fragments": [],
                "text": "Extracting Scientific Figures with Distantly Supervised Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper induces high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention, and uses this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work."
            },
            "venue": {
                "fragments": [],
                "text": "JCDL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110192162"
                        ],
                        "name": "Yi-Fan Zhou",
                        "slug": "Yi-Fan-Zhou",
                        "structuredName": {
                            "firstName": "Yi-Fan",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-Fan Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052891217"
                        ],
                        "name": "Runhao Jiang",
                        "slug": "Runhao-Jiang",
                        "structuredName": {
                            "firstName": "Runhao",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Runhao Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153028349"
                        ],
                        "name": "Xiao Wu",
                        "slug": "Xiao-Wu",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753760"
                        ],
                        "name": "Jun-Yan He",
                        "slug": "Jun-Yan-He",
                        "structuredName": {
                            "firstName": "Jun-Yan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun-Yan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071356123"
                        ],
                        "name": "Shuang Weng",
                        "slug": "Shuang-Weng",
                        "structuredName": {
                            "firstName": "Shuang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuang Weng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143740449"
                        ],
                        "name": "Qiang Peng",
                        "slug": "Qiang-Peng",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "iderably improves the performance of the attention-based encoder-decoder method on image-based table recognition with a novel EDD architecture. Our model differs from other existing EDD architectures [35], [36], where the dual decoders are independent from each other. In our model, the cell decoder is triggered only when the structure decoder generates a new cell. In the meanwhile, the hidden state of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195488949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3560ebaec26312ee0ad36b24416d97ac04168c36",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-to-image translation is a fundamental task for a wide range of applications, such as image style transfer, video effect generation, cross-domain retrieval, etc. Due to the limited number of labeled data, complex scenes, abstract semantics and various involved domains, image translation remains a challenging task. Compared to the supervised approaches for image translation that need a large collection of paired images for training, the unsupervised methods can significantly reduce the training cost. In this paper, an unsupervised end-to-end generative adversarial network is proposed, named BranchGAN, for mutual image-to-image transfer between two domains. A structure with one single encoder and dual decoders is novelly proposed to capture the cross-domain distributions and generate the images in both domains. Three factors, that is, pixel-level overall style, region semantics, and domain distinguishability are comprehensively considered to constrain the training process of the proposed model, corresponding to reconstruction loss, encoding loss, and adversarial loss, respectively. Experiments conducted on three benchmark datasets demonstrate the effectiveness of the proposed method that outperforms the unsupervised state-of-the-art approaches and has the competitive performance as the supervised method."
            },
            "slug": "BranchGAN:-Unsupervised-Mutual-Image-to-Image-With-Zhou-Jiang",
            "title": {
                "fragments": [],
                "text": "BranchGAN: Unsupervised Mutual Image-to-Image Transfer With A Single Encoder and Dual Decoders"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments conducted on three benchmark datasets demonstrate the effectiveness of the proposed method that outperforms the unsupervised state-of-the-art approaches and has the competitive performance as the supervised method."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3493665"
                        ],
                        "name": "Dafang He",
                        "slug": "Dafang-He",
                        "structuredName": {
                            "firstName": "Dafang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dafang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145823372"
                        ],
                        "name": "Scott D. Cohen",
                        "slug": "Scott-D.-Cohen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844147"
                        ],
                        "name": "Brian L. Price",
                        "slug": "Brian-L.-Price",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Price",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian L. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Fullyconvolutional neural networks, followed by a conditional random field, have also been used for table detection [23]\u2013[25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29473897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cf063c3e25b2ab30e44ba49920b811d40f7702",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Page segmentation and table detection play an important role in understanding the structure of documents. We present a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures. We propose a multi-scale, multi-task fully convolutional neural network (FCN) for the tasks of semantic page segmentation and element contour detection. The semantic segmentation network accurately predicts the probability at each pixel of the three element classes. The contour detection network accurately predicts instance level \"edges\" around each element occurrence. We propose a conditional random field (CRF) that uses features output from the semantic segmentation and contour networks to improve upon the semantic segmentation network output. Given the semantic segmentation output, we also extract individual table instances from the page using some heuristic rules and a verification network to remove false positives. We show that although we only consider a page image as input, we produce comparable results with other methods that relies on PDF file information and heuristics and hand crafted features tailored to specific types of documents. Our approach learns the representative features for page segmentation from real and synthetic training data. %, and produces good results on real documents. The learning-based property makes it a more general method than existing methods in terms of document types and element appearances. For example, our method reliably detects sparsely lined tables which are hard for rule-based or heuristic methods."
            },
            "slug": "Multi-Scale-Multi-Task-FCN-for-Semantic-Page-and-He-Cohen",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Multi-Task FCN for Semantic Page Segmentation and Table Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures and proposes a conditional random field (CRF) that uses features output from the semantic segmentsation and contour networks to improve upon the semantic segmentation network output."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505751"
                        ],
                        "name": "Yuntian Deng",
                        "slug": "Yuntian-Deng",
                        "structuredName": {
                            "firstName": "Yuntian",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntian Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469155"
                        ],
                        "name": "Anssi Kanervisto",
                        "slug": "Anssi-Kanervisto",
                        "structuredName": {
                            "firstName": "Anssi",
                            "lastName": "Kanervisto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anssi Kanervisto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065574525"
                        ],
                        "name": "Jeffrey Ling",
                        "slug": "Jeffrey-Ling",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Camelot9, PDFPlumber10, and Adobe Acrobat R \u00a9 Pro11) and the WYGIWYS model12 [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "extended it by adding a recurrent layer in the encoder for capturing long horizontal spatial dependencies to convert images of mathematical formulas into LATEX representation [34]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29020029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "071c0b3ec700758dd9b4164ede08b714ea7e3c38",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention."
            },
            "slug": "Image-to-Markup-Generation-with-Coarse-to-Fine-Deng-Kanervisto",
            "title": {
                "fragments": [],
                "text": "Image-to-Markup Generation with Coarse-to-Fine Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work presents a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism that outperforms classical mathematical OCR systems by a large margin on in-domain rendered data and also performs well on out-of-domain handwritten data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "the cell tokens of the training data is 32 and 281, respectively. Training images are rescaled to 448 448 pixels to facilitate batching and each channel is normalized by z-score. We use the ResNet-18 [39] network as the encoder. The default ResNet-18 model downsamples the image resolution by 32. We modify the last CNN layer of ResNet-18 to study if a higher-resolution feature map improves table recogn"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95314,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985360"
                        ],
                        "name": "Raymond W. Smith",
                        "slug": "Raymond-W.-Smith",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Smith",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond W. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "b) Model: Traditional table detection and recognition methods rely on pre-defined rules [10]\u2013[14] and statistical machine learning [15]\u2013[19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2534837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dea52f04a90dc993d2aa30e75b5a5a535e7ca70",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting tables in document images is important since not only do tables contain important information, but also most of the layout analysis methods fail in the presence of tables in the document image. Existing approaches for table detection mainly focus on detecting tables in single columns of text and do not work reliably on documents with varying layouts. This paper presents a practical algorithm for table detection that works with a high accuracy on documents with varying layouts (company reports, newspaper articles, magazine pages, ...). An open source implementation of the algorithm is provided as part of the Tesseract OCR engine. Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system."
            },
            "slug": "Table-detection-in-heterogeneous-documents-Shafait-Smith",
            "title": {
                "fragments": [],
                "text": "Table detection in heterogeneous documents"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nk 4https://github.com/doc-analysis/TableBank Dataset TD TSR TR # tables Marmot [6] 3 7 7 958 PubLayNet [7] 3 7 7 113k DeepFigures [8] 3 7 7 1.4m ICDAR2013 [2] 3 3 3 156 ICDAR2019 [9] 3 3 7 3.6k UNLV [10] 3 3 7 558 TableBank4 3 3 7 417k (TD) 145k (TSR) SciTSR3 7 3 3 15k Table2Latex [4] 7 3 3 450k Synthetic data in [5] 7 3 3 Unbounded PubTabNet 7 3 3 568k TABLE I: Datasets for Table Detection (TD), Tab"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16140787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bff7592f28354b3be632f4bf678b622f765a2b8d",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Table spotting and structural analysis are just a small fraction of tasks relevant when speaking of table analysis. Today, quite a large number of different approaches facing these tasks have been described in literature or are available as part of commercial OCR systems that claim to deal with tables on the scanned documents and to treat them accordingly.\n However, the problem of detecting tables is not yet solved at all. Different approaches have different strengths and weak points. Some fail in certain situations or layouts where others perform better. How shall one know, which approach or system is the best for his specific job? The answer to this question raises the demand for an objective comparison of different approaches which address the same task of spotting tables and recognizing their structure.\n This paper describes our approach towards establishing a complete and publicly available, hence open environment for the benchmarking of table spotting and structural analysis. We provide free access to the ground truthing tool and evaluation mechanism described in this paper, describe the ideas behind and we also provide ground truth for the 547 documents of the UNLV and UW-3 datasets that contain tables.\n In addition, we applied the quality measures to the results that were generated by the T-Recs system which we developed some years ago and which we started to further advance since a few months."
            },
            "slug": "An-open-approach-towards-the-benchmarking-of-table-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "An open approach towards the benchmarking of table structure recognition systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach towards establishing a complete and publicly available, hence open environment for the benchmarking of table spotting and structural analysis is described and free access to the ground truthing tool and evaluation mechanism described is provided."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505751"
                        ],
                        "name": "Yuntian Deng",
                        "slug": "Yuntian-Deng",
                        "structuredName": {
                            "firstName": "Yuntian",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntian Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613795"
                        ],
                        "name": "David S. Rosenberg",
                        "slug": "David-S.-Rosenberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David S. Rosenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065970890"
                        ],
                        "name": "Gideon Mann",
                        "slug": "Gideon-Mann",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Mann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gideon Mann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 211027043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e8c3ad4bdced4c1da46b7e1b559861a497201a",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, end-to-end trained neural models have been applied successfully to various optical character recognition (OCR) tasks. However, the same level of success has not yet been achieved in end-to-end neural scientific table recognition, which involves multi-row/multi-column structures and math formulas in cells. In this paper, we take a step forward to full end-to-end scientific table recognition by constructing a large dataset consisting of 450K table images paired with corresponding LaTeX sources. We apply a popular attentional encoder-decoder model to this dataset and show the potential of end-to-end trained neural systems, as well as associated challenges."
            },
            "slug": "Challenges-in-End-to-End-Neural-Scientific-Table-Deng-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Challenges in End-to-End Neural Scientific Table Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper takes a step forward to full end-to-end scientific table recognition by constructing a large dataset consisting of 450K table images paired with corresponding LaTeX sources and applying a popular attentional encoder-decoder model to this dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768769"
                        ],
                        "name": "F. Cesarini",
                        "slug": "F.-Cesarini",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Cesarini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Cesarini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285734"
                        ],
                        "name": "S. Marinai",
                        "slug": "S.-Marinai",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Marinai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marinai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608269"
                        ],
                        "name": "L. Sarti",
                        "slug": "L.-Sarti",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Sarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1551166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56d3298d06b3eb5d479b33d8d9e59eff774965f4",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach for table location in document images. The documents are described by means of a hierarchical representation that is based on the MXY tree. The presence of a table is hypothesized by searching parallel lines in the MXY tree of the page. This hypothesis is afterwards verified by locating perpendicular lines or white spaces in the region included between the parallel lines. Lastly, located tables can be merged on the basis of proximity and similarity criteria. The use of an optimization method, that relies on the definition of an appropriate table location index, allows us to identify, the optimal values of thresholds involved in the algorithm. In this way the algorithm can be adapted to recognize tables with different features by maximizing the performance on an appropriate training set. The algorithm has been evaluated on two data-sets containing more than 1500 pages, and comparing its results with the tables identified by two commercial OCRs."
            },
            "slug": "Trainable-table-location-in-document-images-Cesarini-Marinai",
            "title": {
                "fragments": [],
                "text": "Trainable table location in document images"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An approach for table location in document images is described by means of a hierarchical representation that is based on the MXY tree and the use of an optimization method allows us to identify the optimal values of thresholds involved in the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "In addition, deep neural networks for object detection, such as Faster-RCNN [23], Mask-RCNN [24], and YOLO [25] have been exploited for table detection and row/column segmentation [6], [26]\u2013[28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777642"
                        ],
                        "name": "Liangcai Gao",
                        "slug": "Liangcai-Gao",
                        "structuredName": {
                            "firstName": "Liangcai",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangcai Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49866988"
                        ],
                        "name": "Yilun Huang",
                        "slug": "Yilun-Huang",
                        "structuredName": {
                            "firstName": "Yilun",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yilun Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2131960"
                        ],
                        "name": "Herv\u00e9 D\u00e9jean",
                        "slug": "Herv\u00e9-D\u00e9jean",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "D\u00e9jean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Herv\u00e9 D\u00e9jean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066113788"
                        ],
                        "name": "J. Meunier",
                        "slug": "J.-Meunier",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Meunier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Meunier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072780764"
                        ],
                        "name": "Qinqin Yan",
                        "slug": "Qinqin-Yan",
                        "structuredName": {
                            "firstName": "Qinqin",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qinqin Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112809518"
                        ],
                        "name": "Yu Fang",
                        "slug": "Yu-Fang",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729636"
                        ],
                        "name": "Florian Kleber",
                        "slug": "Florian-Kleber",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Kleber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Kleber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059294500"
                        ],
                        "name": "E. Lang",
                        "slug": "E.-Lang",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Lang",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "4m ICDAR2013 [9] 3 3 3 156 ICDAR2019 [6] 3 3 7 3."
                    },
                    "intents": []
                }
            ],
            "corpusId": 211026773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a2e147403c6e24fe9e782e85fb472f12891dc7d",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The cTDaR competition aims at benchmarking state-of-the-art table detection (TRACK A) and table recognition (TRACK B) methods. In particular, we wish to investigate and compare general methods that can reliably and robustly identify the table regions within a document image on the one hand, and the table structure on the other hand. Due to the presence of hand-drawn tables and handwritten text, the methods must be robust against various noise conditions, interfering annotations, and variations of the tables. Two new challenging datasets were created to test the behaviour of state-of-the-art table detection and recognition systems on real world data. One dataset consists of modern documents, while the other consists of archival documents with presence of hand-drawn tables and handwritten text. The evaluation scheme is adapted from the ICDAR 2013 Table competition. We received results of Track A from 11 teams and results of Track B from 2 teams. Results for Track A are very good for the top participants. The winner and his runner-up are very close while using very different approaches. Track B was more challenging and only one participant was able to produce good results."
            },
            "slug": "ICDAR-2019-Competition-on-Table-Detection-and-Gao-Huang",
            "title": {
                "fragments": [],
                "text": "ICDAR 2019 Competition on Table Detection and Recognition (cTDaR)"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work wishes to investigate and compare general methods that can reliably and robustly identify the table regions within a document image on the one hand, and the table structure on the other hand in state-of-the-art table detection and recognition methods."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148833"
                        ],
                        "name": "Dimitrios Danatsas",
                        "slug": "Dimitrios-Danatsas",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Danatsas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitrios Danatsas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748249"
                        ],
                        "name": "I. Pratikakis",
                        "slug": "I.-Pratikakis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pratikakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pratikakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2397702"
                        ],
                        "name": "S. Perantonis",
                        "slug": "S.-Perantonis",
                        "structuredName": {
                            "firstName": "Stavros",
                            "lastName": "Perantonis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Perantonis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 84
                            }
                        ],
                        "text": "Model Traditional table detection and recognition methods rely on pre-defined rules [7, 14, 15, 24, 31, 37] and statistical machine learning [1, 4, 18, 20, 34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11485240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "800ce7dc6b97a48b1e721f03269eb4a59adacabe",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel technique for automatic table detection in document images. Lines and tables are among the most frequent graphic, non-textual entities in documents and their detection is directly related to the OCR performance as well as to the document layout description. We propose a workflow for table detection that comprises three distinct steps: (i) image pre-processing; (ii) horizontal and vertical line detection and (iii) table detection. The efficiency of the proposed method is demonstrated by using a performance evaluation scheme which considers a great variety of documents such as forms, newspapers/magazines, scientific journals, tickets/bank cheques, certificates and handwritten documents."
            },
            "slug": "Automatic-Table-Detection-in-Document-Images-Gatos-Danatsas",
            "title": {
                "fragments": [],
                "text": "Automatic Table Detection in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The efficiency of the proposed method is demonstrated by using a performance evaluation scheme which considers a great variety of documents such as forms, newspapers/magazines, scientific journals, tickets/bank cheques, certificates and handwritten documents."
            },
            "venue": {
                "fragments": [],
                "text": "ICAPR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117101253"
                        ],
                        "name": "Ke Xu",
                        "slug": "Ke-Xu",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "for image captioning [33]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The structure decoder and cell decoder are recurrent neural networks (RNN) with the attention mechanism proposed in [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1055111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
            "isKey": false,
            "numCitedBy": 7252,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."
            },
            "slug": "Show,-Attend-and-Tell:-Neural-Image-Caption-with-Xu-Ba",
            "title": {
                "fragments": [],
                "text": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An attention based model that automatically learns to describe the content of images is introduced that can be trained in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, deep neural networks for object detection, such as Faster-RCNN [26], Mask-RCNN [27], and YOLO [28] have been exploited for table detection and row/column segmentation [7], [29]\u2013[31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206771194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea99a5535388196d0d44be5b4d7dd02029a43bb2",
            "isKey": false,
            "numCitedBy": 3476,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available."
            },
            "slug": "Mask-R-CNN-He-Gkioxari",
            "title": {
                "fragments": [],
                "text": "Mask R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work presents a conceptually simple, flexible, and general framework for object instance segmentation, which extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47756656"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32225756"
                        ],
                        "name": "R. Kashi",
                        "slug": "R.-Kashi",
                        "structuredName": {
                            "firstName": "Ramanujan",
                            "lastName": "Kashi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37293831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edbd577d793a083de4f337acac992ec7837609e0",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An important step towards the goal of table understanding is a method for reliable table detection. This paper describes a general solution for detecting tables based on computing an optimal partitioning of a document into some number of tables. A dynamic programming algorithm is given to solve the resulting optimization problem. This high-level framework is independent of any particular table quality measure and independent of the document medium. Moreover, it does not rely on the presence of ruling lines or other table delimiters. We also present table quality measures based on white space correlation and vertical connected component analysis. These measures can be applied equally well to ASCII text and scanned images. We report on some preliminary experiments using this method to detect tables in both ASCII text and scanned images, yielding promising results. We present detailed evaluation of these results using three different criteria which by themselves pose interesting research questions."
            },
            "slug": "Medium-independent-table-detection-Hu-Kashi",
            "title": {
                "fragments": [],
                "text": "Medium-independent table detection"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A general solution for detecting tables based on computing an optimal partitioning of a document into some number of tables is described and a dynamic programming algorithm is given to solve the resulting optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105434437"
                        ],
                        "name": "Scott Tupaj",
                        "slug": "Scott-Tupaj",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Tupaj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Tupaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110396962"
                        ],
                        "name": "Zhong Shi",
                        "slug": "Zhong-Shi",
                        "structuredName": {
                            "firstName": "Zhong",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhong Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145923790"
                        ],
                        "name": "D. H. Chang",
                        "slug": "D.-H.-Chang",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. H. Chang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 84
                            }
                        ],
                        "text": "Model Traditional table detection and recognition methods rely on pre-defined rules [7, 14, 15, 24, 31, 37] and statistical machine learning [1, 4, 18, 20, 34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18379904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebeade30e54c102bf7d806739ea0f963ba07a20e",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents work done in locating and extracting tables and their contents from document images. While most research in the area of table analysis and recognition has focused on analyzing the raster image, our approach builds upon the advances in optical character recognition (OCR) software to preserve the layout of tabular data by means of white space. By using methods to analyze the geometry, syntax, and the semantics of the character data, as well as utilizing some well-known image processing techniques, we are able to 1) isolate embedded tables from documents, and 2) identify table components uch as title blocks, table entries, and footer blocks. Furthermore, the table analysis techniques presented in this paper can also be applied when analyzing blocks of text isolated by traditional methods such as connected component analysis[1] or bounding box [2]."
            },
            "slug": "Extracting-Tabular-Information-From-Text-Files-Tupaj-Shi",
            "title": {
                "fragments": [],
                "text": "Extracting Tabular Information From Text Files"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach builds upon the advances in optical character recognition software to preserve the layout of tabular data by means of white space to isolate embedded tables from documents and identify table components uch as title blocks, table entries, and footer blocks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ree-edit-distance-based evaluation metric that we propose in Section V B. Model Traditional table detection and recognition methods rely on pre-de\ufb01ned rules [11]\u2013[16] and statistical machine learning [17]\u2013[21]. Recently, deep learning exhibit great performance in image-based table detection and structure recognition. Hao et al. used a set of primitive rules to propose candidate table regions and a con"
                    },
                    "intents": []
                }
            ],
            "corpusId": 38477730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc9c56918069b4648806219cfd93b6d35d3cfc27",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to table structure recognition as well as to layout analysis. The discussed recognition process differs significantly from existing approaches as it realizes a bottom-up clustering of given word segments, whereas conventional table structure recognizers all rely on the detection of some separators such as delineation or significant white space to analyze a page from the top-down. The following analysis of the recognized layout elements is based on the construction of a tile structure and detects row- and/or column spanning cells as well as sparse tables with a high degree of confidence. \n \nThe overall system is completely domain independent, optionally neglects textual contents and can thus be applied to arbitrary mixed-mode documents (with or without tables) of any language and even operates on low quality OCR documents (e.g. facsimiles)."
            },
            "slug": "The-T-Recs-Table-Recognition-and-Analysis-System-Kieninger-Dengel",
            "title": {
                "fragments": [],
                "text": "The T-Recs Table Recognition and Analysis System"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A new approach to table structure recognition as well as to layout analysis that realizes a bottom-up clustering of given word segments, whereas conventional table structure recognizers all rely on the detection of some separators such as delineation or significant white space to analyze a page from the top-down."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, deep neural networks for object detection, such as Faster-RCNN [26], Mask-RCNN [27], and YOLO [28] have been exploited for table detection and row/column segmentation [7], [29]\u2013[31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "isKey": false,
            "numCitedBy": 16502,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
            },
            "slug": "You-Only-Look-Once:-Unified,-Real-Time-Object-Redmon-Divvala",
            "title": {
                "fragments": [],
                "text": "You Only Look Once: Unified, Real-Time Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background, and outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "e gain of EDD is due to difference in CNN. 16 and 80, respectively. At inference time, the output of both of the decoders are sampled with beam search (beam=3). The EDD model is trained with the Adam [40] optimizer with two stages. First, we pre-train the encoder and the structure decoder to generate the structural tokens only (= 1), where the batch size is 10, and the learning rate is 0.001 in the \ufb01"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": true,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40019398"
                        ],
                        "name": "T. Kasar",
                        "slug": "T.-Kasar",
                        "structuredName": {
                            "firstName": "Thotreingam",
                            "lastName": "Kasar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kasar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32179591"
                        ],
                        "name": "Philippine Barlas",
                        "slug": "Philippine-Barlas",
                        "structuredName": {
                            "firstName": "Philippine",
                            "lastName": "Barlas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippine Barlas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680806"
                        ],
                        "name": "S\u00e9bastien Adam",
                        "slug": "S\u00e9bastien-Adam",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Adam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712446"
                        ],
                        "name": "Cl\u00e9ment Chatelain",
                        "slug": "Cl\u00e9ment-Chatelain",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Chatelain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cl\u00e9ment Chatelain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690399"
                        ],
                        "name": "T. Paquet",
                        "slug": "T.-Paquet",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Paquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Paquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "Model Traditional table detection and recognition methods rely on pre-defined rules [7, 14, 15, 24, 31, 37] and statistical machine learning [1, 4, 18, 20, 34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18367100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a389e078d490bde455f9ba1e58902818598bd870",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method to detect table regions in document images by identifying the column and row line-separators and their properties. The method employs a run-length approach to identify the horizontal and vertical lines present in the input image. From each group of intersecting horizontal and vertical lines, a set of 26 low-level features are extracted and an SVM classifier is used to test if it belongs to a table or not. The performance of the method is evaluated on a heterogeneous corpus of French, English and Arabic documents that contain various types of table structures and compared with that of the Tesseract OCR system."
            },
            "slug": "Learning-to-Detect-Tables-in-Scanned-Document-Using-Kasar-Barlas",
            "title": {
                "fragments": [],
                "text": "Learning to Detect Tables in Scanned Document Images Using Line Information"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The performance of the method is evaluated on a heterogeneous corpus of French, English and Arabic documents that contain various types of table structures and compared with that of the Tesseract OCR system."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31722415"
                        ],
                        "name": "Mateusz Pawlik",
                        "slug": "Mateusz-Pawlik",
                        "structuredName": {
                            "firstName": "Mateusz",
                            "lastName": "Pawlik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mateusz Pawlik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2893196"
                        ],
                        "name": "Nikolaus Augsten",
                        "slug": "Nikolaus-Augsten",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Augsten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolaus Augsten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "We measure the similarity between two tables using the tree-edit distance proposed by Pawlik and Augsten [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36461211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a1a5439cc0c5601761d4aaa9bd4134fea3571c9",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tree-edit-distance:-Robust-and-memory-efficient-Pawlik-Augsten",
            "title": {
                "fragments": [],
                "text": "Tree edit distance: Robust and memory-efficient"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49915337"
                        ],
                        "name": "A. C. E. Silva",
                        "slug": "A.-C.-E.-Silva",
                        "structuredName": {
                            "firstName": "Ana",
                            "lastName": "Silva",
                            "middleNames": [
                                "Costa",
                                "e"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. C. E. Silva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "Model Traditional table detection and recognition methods rely on pre-defined rules [7, 14, 15, 24, 31, 37] and statistical machine learning [1, 4, 18, 20, 34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7260147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "166e2f3ff52620ffc975acca6281915995498605",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMM) are probabilistic graphical models for interdependent classification. In this paper we experiment with different ways of combining the components of an HMM for document analysis applications, in particular for finding tables in text. We show: a) how to integrate different document structure finders into the HMM; b) that transition probabilities should vary along the chain to embed general knowledge axioms of our field, c) some emission energies can be selectively ignored, and d) emission and transition probabilities can be weighed differently. We conclude these changes increase the expressiveness and usability of HMMs in our field."
            },
            "slug": "Learning-Rich-Hidden-Markov-Models-in-Document-Silva",
            "title": {
                "fragments": [],
                "text": "Learning Rich Hidden Markov Models in Document Analysis: Table Location"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper experiments with different ways of combining the components of an HMM for document analysis applications, in particular for finding tables in text, and shows how to integrate different document structure finders into the HMM."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73282614"
                        ],
                        "name": "Y. Hirayama",
                        "slug": "Y.-Hirayama",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Hirayama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hirayama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditional table detection and recognition methods rely on pre-defined rules [11]\u2013[16] and statistical machine learning [17]\u2013[21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206774876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcf75b367f24919a8a4b0f627960f6d6bf5ae33f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel method for table structure analysis. Many documents have table areas, and some have both table and figure areas. It is very important to be able to classify table and figure areas automatically. Furthermore, in tables, the column and row in which a character string is located are very important pieces of information. To detect and analyze table areas, the following method is applied: First, areas that may contain tables or figures are distinguished from text areas by the presence of horizontal and vertical lines. Next, the areas are assumed to be table areas and are analyzed as such. A judgment is made on whether each of the areas can in fact be a table area or not; in this way, the actual table areas are detected. Finally, the structures of the areas are analyzed and character strings in the areas are arranged by using the DP matching method. This method was applied to sixty-five pages of Japanese technical papers, magazines, manuals for software programs, and pages including 34 table areas, 48 line drawing areas, and 35 image areas. As a result, 96.6 percent of the areas were detected correctly and 91.7 percent of the tables were analyzed and arranged correctly."
            },
            "slug": "A-method-for-table-structure-analysis-using-DP-Hirayama",
            "title": {
                "fragments": [],
                "text": "A method for table structure analysis using DP matching"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents a novel method for table structure analysis that was applied to sixty-five pages of Japanese technical papers, magazines, manuals for software programs, and pages including 34 table areas, 48 line drawing areas, and 35 image areas."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399097376"
                        ],
                        "name": "Antonio Jimeno-Yepes",
                        "slug": "Antonio-Jimeno-Yepes",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Jimeno-Yepes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Jimeno-Yepes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144765178"
                        ],
                        "name": "Karin M. Verspoor",
                        "slug": "Karin-M.-Verspoor",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Verspoor",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karin M. Verspoor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "where \u03bb \u2208 [0, 1] is a hyper-parameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Otherwise, the substitution cost is the normalized Levenshtein similarity [38] (\u2208 [0, 1]) between the content of no and ns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An example of the relevance of tabular information in the biomedical domain is in the curation of genetic databases in which just between 2% to 8% of the information was available in the narrative part of the article compared to the information available in tables or files in tabular format [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10526385,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8fc5a449c87cda9d778edbac8d6d96dac7073063",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "A major focus of modern biological research is the understanding of how genomic variation relates to disease. Although there are significant ongoing efforts to capture this understanding in curated resources, much of the information remains locked in unstructured sources, in particular, the scientific literature. Thus, there have been several text mining systems developed to target extraction of mutations and other genetic variation from the literature. We have performed the first study of the use of text mining for the recovery of genetic variants curated directly from the literature. We consider two curated databases, COSMIC (Catalogue Of Somatic Mutations In Cancer) and InSiGHT (International Society for Gastro-intestinal Hereditary Tumours), that contain explicit links to the source literature for each included mutation. Our analysis shows that the recall of the mutations catalogued in the databases using a text mining tool is very low, despite the well-established good performance of the tool and even when the full text of the associated article is available for processing. We demonstrate that this discrepancy can be explained by considering the supplementary material linked to the published articles, not previously considered by text mining tools. Although it is anecdotally known that supplementary material contains \u2018all of the information\u2019, and some researchers have speculated about the role of supplementary material (Schenck et al. Extraction of genetic mutations associated with cancer from public literature. J Health Med Inform 2012;S2:2.), our analysis substantiates the significant extent to which this material is critical. Our results highlight the need for literature mining tools to consider not only the narrative content of a publication but also the full set of material related to a publication."
            },
            "slug": "Literature-mining-of-genetic-variants-for-curation:-Jimeno-Yepes-Verspoor",
            "title": {
                "fragments": [],
                "text": "Literature mining of genetic variants for curation: quantifying the importance of supplementary material"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This is the first study of the use of text mining for the recovery of genetic variants curated directly from the literature, and highlights the need for literature mining tools to consider not only the narrative content of a publication but also the full set of material related to a publication."
            },
            "venue": {
                "fragments": [],
                "text": "Database J. Biol. Databases Curation"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8842741"
                        ],
                        "name": "Romero Morais",
                        "slug": "Romero-Morais",
                        "structuredName": {
                            "firstName": "Romero",
                            "lastName": "Morais",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Romero Morais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144672395"
                        ],
                        "name": "Vuong Le",
                        "slug": "Vuong-Le",
                        "structuredName": {
                            "firstName": "Vuong",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vuong Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6254479"
                        ],
                        "name": "T. Tran",
                        "slug": "T.-Tran",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735360"
                        ],
                        "name": "Budhaditya Saha",
                        "slug": "Budhaditya-Saha",
                        "structuredName": {
                            "firstName": "Budhaditya",
                            "lastName": "Saha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Budhaditya Saha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761350"
                        ],
                        "name": "M. Mansour",
                        "slug": "M.-Mansour",
                        "structuredName": {
                            "firstName": "Moussa",
                            "lastName": "Mansour",
                            "middleNames": [
                                "Reda"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143761093"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Svetha",
                            "lastName": "Venkatesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our model differs from other existing EDD architectures [35], [36], where the dual decoders are independent from each other."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 72941049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db20d81d40243d66ff90f11b5c6f058d43d3701f",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Appearance features have been widely used in video anomaly detection even though they contain complex entangled factors. We propose a new method to model the normal patterns of human movements in surveillance video for anomaly detection using dynamic skeleton features. We decompose the skeletal movements into two sub-components: global body movement and local body posture. We model the dynamics and interaction of the coupled features in our novel Message-Passing Encoder-Decoder Recurrent Network. We observed that the decoupled features collaboratively interact in our spatio-temporal model to accurately identify human-related irregular events from surveillance video sequences. Compared to traditional appearance-based models, our method achieves superior outlier detection performance. Our model also offers \u201copen-box\u201d examination and decision explanation made possible by the semantically understandable features and a network architecture supporting interpretability."
            },
            "slug": "Learning-Regularity-in-Skeleton-Trajectories-for-in-Morais-Le",
            "title": {
                "fragments": [],
                "text": "Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new method to model the normal patterns of human movements in surveillance video for anomaly detection using dynamic skeleton features using the novel Message-Passing Encoder-Decoder Recurrent Network."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689334"
                        ],
                        "name": "Yasushi Makihara",
                        "slug": "Yasushi-Makihara",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Makihara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasushi Makihara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740241"
                        ],
                        "name": "Masao Takizawa",
                        "slug": "Masao-Takizawa",
                        "structuredName": {
                            "firstName": "Masao",
                            "lastName": "Takizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masao Takizawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700379"
                        ],
                        "name": "Y. Shirai",
                        "slug": "Y.-Shirai",
                        "structuredName": {
                            "firstName": "Yoshiaki",
                            "lastName": "Shirai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shirai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691940"
                        ],
                        "name": "J. Miura",
                        "slug": "J.-Miura",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Miura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Miura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143745419"
                        ],
                        "name": "N. Shimada",
                        "slug": "N.-Shimada",
                        "structuredName": {
                            "firstName": "Nobutaka",
                            "lastName": "Shimada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shimada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 289211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bfc879192182e82b03b19526b26f9792a906e46",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an interactive vision system for a robot that finds an object specified by a user and brings it to the user. The system first registers object models automatically. When the user specifies an object, the system tries to recognize the object automatically. When the recognition result is shown to the user, the user may provide additional information via speech such as pointing out mistakes, choosing the correct object from multiple candidates, or giving the relative position of the object. Based on the advice, the system tries again to recognize the object. Experiments are described using real-world refrigerator scenes."
            },
            "slug": "Object-recognition-supported-by-user-interaction-Makihara-Takizawa",
            "title": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An interactive vision system for a robot that finds an object specified by a user and brings it to the user and the user may provide additional information via speech such as pointing out mistakes and choosing the correct object from multiple candidates."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145501780"
                        ],
                        "name": "Matthew F. Hurst",
                        "slug": "Matthew-F.-Hurst",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hurst",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew F. Hurst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ile, the hidden state of the structure decoder is sent to the cell decoder to help it place its attention on the corresponding cell in the table image. C. Evaluation The evaluation metric proposed in [3] is commonly used in table recognition literature and competitions. This metric \ufb01rst \ufb02attens the ground truth and recognition result of a table are into a list of pairwise adjacency relations between "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ation By modeling tables as a tree structure, we propose a new tree-edit-distance-based evaluate metric for image-based table recognition. We demonstrate that our new metric is superior to the metric [3] commonly used in literature and competitions. II. RELATED WORK A. Data Analyzing tabular data in unstructured documents focuses mainly on three problems: i) table detection: localizing the bounding b"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " vol. 2014, 2014. [2] M. Gbel, T. Hassan, E. Oro, and G. Orsi, \u00d2ICDAR 2013 table competition,\u00d3 in 2013 12th International Conference on Document Analysis and Recognition . IEEE, 2013, pp. 1449\u00d01453. [3] M. Hurst, \u00d2A constraint-based approach to table structure derivation,\u00d3 2003. (a) Attention of structure decoder on the \u00derst header row (b) Attention of structure decoder on the \u00derst body row (c) Atte"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " samples is de\ufb01ned as the mean of the TEDS score between the recognition result and ground truth of each sample. In order to justify that TEDS solves the two problems of the adjacency relation metric [3] described previously in Section II, we add two types of perturbations to the validation set of PubTabNet and examine how TEDS and the adjacency relation metric respond to the perturbations. 1) To dem"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1649340,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a7be096a4ed48b143691bdd2deeae585afebc4d6",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to deriving an abstractgeometric model of a table from a physical representation.The technique developed uses a graph of constraints betweencells which must be satisfied in order to determinetheir relative horizontal and vertical position. The methodis evaluated with a test set of tables drawn from US Securitiesand Exchange Commission (SEC) filings."
            },
            "slug": "A-constraint-based-approach-to-table-structure-Hurst",
            "title": {
                "fragments": [],
                "text": "A constraint-based approach to table structure derivation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An approach to deriving an abstractgeometric model of a table from a physical representation using a graph of constraints which must be satisfied in order to determinate the relative horizontal and vertical position."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154179"
                        ],
                        "name": "V. Levenshtein",
                        "slug": "V.-Levenshtein",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Levenshtein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Levenshtein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Otherwise, the substitution cost is the normalized Levenshtein similarity [38] (\u2208 [0, 1]) between the content of no and ns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60827152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd",
            "isKey": false,
            "numCitedBy": 10967,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Binary-codes-capable-of-correcting-deletions,-and-Levenshtein",
            "title": {
                "fragments": [],
                "text": "Binary codes capable of correcting deletions, insertions, and reversals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mediumindependent table detection,\u201d in Document Recognition and Retrieval VII, vol. 3967"
            },
            "venue": {
                "fragments": [],
                "text": "International Society for Optics and Photonics,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mediumindependent table detection"
            },
            "venue": {
                "fragments": [],
                "text": "Document Recognition and Retrieval VII, vol. 3967. International Society for Optics and Photonics, 1999, pp. 291\u2013302."
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 25
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Image-based-table-recognition%3A-data%2C-model%2C-and-Zhong-Shafieibavani/99b2a05177c17dd86627863608ac45dc4cfe0446"
}