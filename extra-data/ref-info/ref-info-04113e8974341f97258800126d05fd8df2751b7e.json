{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056958855"
                        ],
                        "name": "D. McCaffrey",
                        "slug": "D.-McCaffrey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "McCaffrey",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McCaffrey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117841"
                        ],
                        "name": "A. Gallant",
                        "slug": "A.-Gallant",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gallant",
                            "middleNames": [
                                "Ronald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gallant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8481154,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7d176907eefd6df93a92129a3cfba57e94f46c7a",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convergence-rates-for-single-hidden-layer-networks-McCaffrey-Gallant",
            "title": {
                "fragments": [],
                "text": "Convergence rates for single hidden layer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103188660"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "A.R.",
                            "lastName": "Barron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 51276807,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "72d761afbe35634213849419ff63fad5bc9fabeb",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Convergence properties of empirically estimated neural networks are examined. In this theory, an appropriate size feedforward network is automatically determined from the data. The networks studied include two- and three-layer networks with an increasing number of simple sigmoidal nodes, multiple-layer polynomial networks, and networks with certain fixed structures but an increasing complexity in each unit. Each of these classes of networks is dense in the space of continuous functions on compact subsets of d-dimensional Euclidean space, with respect to the topology of uniform convergence. It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases. Bounds on the rate of convergence are given in terms of an index of the approximation capability of the class of networks.<<ETX>>"
            },
            "slug": "Statistical-properties-of-artificial-neural-Barron",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how, with the use of an appropriate complexity regularization criterion, the statistical risk of network estimators converges to zero as the sample size increases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th IEEE Conference on Decision and Control,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32498041"
                        ],
                        "name": "M. Donahue",
                        "slug": "M.-Donahue",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Donahue",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48491389"
                        ],
                        "name": "L. Gurvits",
                        "slug": "L.-Gurvits",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Gurvits",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gurvits"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6260397,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "01a3956e07bef9474080a70903c27521112130c2",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The set of functions which a single hidden layer neural network can approximate is increasingly well understood, yet our knowledge of how the approximation error depends upon the number of hidden units, i.e. the rate of approximation, remains relatively primitive. Barron [1991] and Jones [1992] give bounds on the rate of approximation valid for Hilbert spaces. We derive bounds for L spaces, 1 < p < m, recovering the 0(1 /&) bounds of Barron and Jones for the case p = 2. The results were motivated in part by the desire to understand approximation in the more \u201crobust\u201d (resistant to exemplar noise) LP, 1 ~ p <2 norms. Consider the task of approximating a given target function f by a linear combination of n functions from a set S. For example, S may be the set of possible sigmoidal activation functions, {g : ~d ~[% 6 ~d, b E ~, s.t. g(z) = a(a . z + b)}, in which case the approximants are single hidden layer neural networks with a linear output layer. It is known that under very weak conditions on IS (it must be Riemann integrable and nonpolynomial), the linear span of S is dense in the set of continuous functions on compact subsets of ~d (i.e. for all positive c there is a linear combination of functions in S which can approximate any continuous function to within c everywhere on a compact domain) [Leshno et al. 1992]. Consider the important rate of approximation issue, i.e. the rate at which the achievable error reduces as we allow larger subsets of S to be used in const rutting the approximant. In the context of neural networks, this is the question of how the approximation error scales with the number of hidden units in the network. Unfortunately, approximation bounds for target functions ~ arbitrarily located in the linear closure (i.e. the closure of the span) of S are unknown. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ACM COLT \u201993 17/931CA, USA @ 1993 ACM 0-89791-61 1-5193 /000710303 . ..$1 .50 Leonid Gurvits Eduardo Sontag Learning Systems Dept. Dept. of Mathematics Siemens Corp. Research Rutgers University 755 College Road East New Brunswick, NJ 08903 Princeton, NJ 08540 sontag@control. rutgers. edu gurvitsrlscr. aiemena. com However, progress haa been made recently by introducing the assumptions that f is in the convex closure of S, and that S is bounded in the relevant norm. This theory depends neither on the continuity of f nor on the form of the functions in S (i.e. the functions in S do not need to be sigmoidal or obey the constraints on ~ listed above), but only on the properties of the function space and some generic properties of S. Definition 1 Let X be a Banach space v.rith norm II ! II. Let S ~ X and f E X. Dejine lllinnS fl[ := inf \u2018&W, \u2013 f , (1) i=l where the injimum is over all gl, . . . . gn c S and al, ..., an E ~. Also define llconS \u2013 fll := inf ~ a~ga f , (2) %=1 where the infimum is over all gl, . . . . g~ E S and al, ..., crn G ~+ U {O} such that ~ ai = 1. That is, lllinnS \u2013 fll is the distance of f from the closest span of n functions from S (linear approximation bound), and llconS \u2013 fll is the distance off from the closest convex hull of n functions from S (convex approximation bound). Note that [1Iinn S \u2013 f II ~ IIco.S f Il. These bounds converge to zero as n ~ co for approximable f and thus represent the convergence rates of the best approximants to the target function. The study of such rates is standard in approximation theory (e.g. [Powell 1981] ), but the cases of interest for neural networks are not among those classically considered. For spaces of square-integrable functions (or more general Hilbert spaces) and bounded sets S, Barron [1991] presented results at this conference to the effect that llco~S \u2013 fllz = 0(1/@). Subsequently, under additional conditions on S, he has shown that the same rate obtains for the uniform norm [Barren 1992]. If we consider the procedure of constructing approximants to f incrementally, bv formimz a convex combination of the last approxirna~t with a-single new element convergence rate in Lz is interestingly again 303 of S, the o(l/fi)"
            },
            "slug": "Rate-of-approximation-results-motivated-by-robust-Darken-Donahue",
            "title": {
                "fragments": [],
                "text": "Rate of approximation results motivated by robust neural network learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Bounds on the rate of approximation valid for Hilbert spaces are derived and bounds for L spaces, 1 < p < m, are derived, recovering the 0(1 /&) bounds of Barron and Jones for the case p = 2."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543541"
                        ],
                        "name": "P. Auer",
                        "slug": "P.-Auer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Auer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15294402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39de69932ee3d70a587bd6c009b6aa0d1775bd75",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently Barron (1993) has given rates for hidden layer feedforward networks with sigmoid activation functions approximating a class of functions satisfying a certain smoothness condition. These rates do not depend on the dimension of the input space. We extend Barron's results to feedforward networks with possibly nonsigmoid activation functions approximating mappings and their derivatives simultaneously. Our conditions are similar but not identical to Barron's, but we obtain the same rates of approximation, showing that the approximation error decreases at rates as fast as n1/2, where n is the number of hidden units. The dimension of the input space appears only in the constants of our bounds."
            },
            "slug": "Degree-of-Approximation-Results-for-Feedforward-and-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Degree of Approximation Results for Feedforward Networks Approximating Unknown Mappings and Their Derivatives"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work extends Barron's results to feedforward networks with possibly nonsigmoid activation functions approximating mappings and their derivatives simultaneously, showing that the approximation error decreases at rates as fast as n1/2, where n is the number of hidden units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491571"
                        ],
                        "name": "L. Jones",
                        "slug": "L.-Jones",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jones",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122808966,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "fc91f9756da56e3ea7f1f18ca565606b96652a0c",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A constructive algorithm for uniformly approximating real continuous mappings by linear combinations of bounded sigmoidal functions is given. G. Cybenko (1989) has demonstrated the existence of uniform approximations to any continuous f provided that sigma is continuous; the proof is nonconstructive, relying on the Hahn-Branch theorem and the dual characterization of C(I/sup n/). Cybenko's result is extended to include any bounded sigmoidal (even nonmeasurable ones). The approximating functions are explicitly constructed. The number of terms in the linear combination is minimal for first-order terms. >"
            },
            "slug": "Constructive-approximations-for-neural-networks-by-Jones",
            "title": {
                "fragments": [],
                "text": "Constructive approximations for neural networks by sigmoidal functions"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "G. Cybenko (1989) has demonstrated the existence of uniform approximations to any continuous f provided that sigma is continuous, relying on the Hahn-Branch theorem and the dual characterization of C(I/sup n/)."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10158697,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "21e82ed12c620fba1f5ee42162962aae74a23510",
            "isKey": false,
            "numCitedBy": 4061,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In the paper \"Approximation by Superpositions of a SigmoidaI Function\" [C], the proof given for Lemma i is incorrect since it relies on the erroneous statement that simple functions are dense in L=(R). The author has pointed out that the proof in I'C] can be corrected by changing, at the bottom of page 307 and the top of page 308, the occurrences of L~(R) to L=(J) for a compact interval, J, containing {yrx lx ~ I,}, where y is fLxed. It should also be noted that the reduction of multidimensional density to one-dimensional density as in the proof of Lemma 1 had previously been obtained by Dahmen and Micchelli, using the same techniques, in work on ridge regression (see Lemma 3.2 of [DM]). We thank Raymond T, Melton, who pointed out the error in the proof of Lemma 1 in [C] and supplied a proof, showing that the Fourier transform of the measure /~ must be zero because the/~-measure of every half-plane is zero [M]."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The reduction of multidimensional density to one-dimensional density as in the proof of Lemma 1 had previously been obtained by Dahmen and Micchelli, using the same techniques, in work on ridge regression."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 22935660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9209f90c02a378720879a3bb93aa2f7181cf5f2",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractFor a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target function f is shown to be bounded by \n$${\\text{O}}\\left( {\\frac{{C_f^2 }}{n}} \\right) + O(\\frac{{ND}}{N}\\log N)$$\nwhere n is the number of nodes, d is the input dimension of the function, N is the number of training observations, and Cf is the first absolute moment of the Fourier magnitude distribution of f. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. With n ~ Cf(N/(dlog N))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to be O(Cf((d/N)log N)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case that d is moderately large. Similar bounds are obtained when the number of nodes n is not preselected as a function of Cf (which is generally not known a priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks."
            },
            "slug": "Approximation-and-Estimation-Bounds-for-Artificial-Barron",
            "title": {
                "fragments": [],
                "text": "Approximation and Estimation Bounds for Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Binding of the mean integrated squared error between the estimated network and a target function f is shown to be bounded by O(Cf((d/N)log N)1/2), which demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case that d is moderately large."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205119351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fdd39ab366b65a617015a72fe8dc9d0b394d64",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-nonparametric-regression:-Multilayer-White",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717635"
                        ],
                        "name": "H. Mhaskar",
                        "slug": "H.-Mhaskar",
                        "structuredName": {
                            "firstName": "Hrushikesh",
                            "lastName": "Mhaskar",
                            "middleNames": [
                                "Narhar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mhaskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121886933,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ebf2bc6f41d19d5fe39f93006be5677d28db2ac1",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-by-superposition-of-sigmoidal-and-Mhaskar-Micchelli",
            "title": {
                "fragments": [],
                "text": "Approximation by superposition of sigmoidal and radial basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491571"
                        ],
                        "name": "L. Jones",
                        "slug": "L.-Jones",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jones",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10663266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a0424daa4da05dc78eac319f31d62bb45c5c042",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Fourier approximation and estimation of discriminant, regression, and density functions are considered. A preference order is established for the frequency weights in multiple Fourier expansions and the connection weights in single hidden-layer neural networks. These preferred weight vectors, called good weights (good lattice weights for estimation of periodic functions), are generalizations for arbitrary periods of the hyperbolic lattice points of Korobov (1959) and Hlawka (1962) associated with classes of smooth functions of period one in each variable. Although previous results on approximation and quadrature are affinely invariant to the scale of the underlying periods, some of our results deal with optimization over finite sets and strongly depend on the choice of scale. It is shown how to count and generate good lattice weights. Finite sample bounds on mean integrated squared error are calculated for ridge estimates of periodic pattern class densities. The bounds are combined with a table of cardinalities of good lattice weight sets to furnish classifier design with prescribed class density estimation errors. Applications are presented for neural networks and projection pursuit. A hyperbolic kernel gradient transform is developed which automatically determines the training weights (projection directions). Its sampling properties are discussed. Algorithms are presented for generating good weights for projection pursuit. >"
            },
            "slug": "Good-weights-and-hyperbolic-kernels-for-neural-and-Jones",
            "title": {
                "fragments": [],
                "text": "Good weights and hyperbolic kernels for neural networks, projection pursuit, and pattern classification: Fourier strategies for extracting information from high-dimensional data"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Fourier approximation and estimation of discriminant, regression, and density functions are considered, and a preference order is established for the frequency weights in multiple Fourier expansions and the connection weights in single hidden-layer neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069581"
                        ],
                        "name": "A. Farag\u00f3",
                        "slug": "A.-Farag\u00f3",
                        "structuredName": {
                            "firstName": "Andr\u00e1s",
                            "lastName": "Farag\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Farag\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The implication of polynomial bounds on C/, B, as a consequence of the bound 2C /, B / yin from Theorem 1, is that a polynomial rather than an exponential number of nodes n is sufficient for accurate approximation by sigmoidal networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195864959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "256fe370199610c14256023eccb777bc08a4af3c",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In statistical pattern recognition, a classifier is called universally consistent if its error probability converges to the Bayes-risk as the size of the training data grows for all possible distributions of the random variable pair of the observation vector and its class. It is proven that if a one-layered neural network with properly chosen number of nodes is trained to minimize the empirical risk on the training data, then a universally consistent classifier results. It is shown that the exponent in the rate of convergence does not depend on the dimension if certain smoothness conditions on the distribution are satisfied. That is, this class of universally consistent classifiers does not suffer from the curse of dimensionality. A training algorithm is presented that finds the optimal set of parameters in polynomial time if the number of nodes and the space dimension is fixed and the amount of training data grows. >"
            },
            "slug": "Strong-Universal-Consistency-of-Neural-Network-Farag\u00f3-Lugosi",
            "title": {
                "fragments": [],
                "text": "Strong Universal Consistency of Neural Network Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the exponent in the rate of convergence does not depend on the dimension if certain smoothness conditions on the distribution are satisfied, and this class of universally consistent classifiers does not suffer from the curse of dimensionality."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. IEEE International Symposium on Information Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118425960,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8b7d60f49e7a5368920457c885c813a912c34997",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts of universal data compression lead to minimum description-length criteria for parsimonious statistical model selection for the estimation of functions. In this paper we define general complexity regularization criteria and establish bounds on the statistical risk of the estimated functions. These bounds establish consistency, yield rates of convergence, and demonstrate the near asymptotic optimality of the model selection criterion in both parametric and nonparametric cases. A fundamental role is played by an index of resolvability that quantifies the tradeoff between complexity and accuracy of candidate models. Applications are given to polynomial regression and artificial neural networks."
            },
            "slug": "Complexity-Regularization-with-Application-to-Barron",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Application to Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper defines general complexity regularization criteria and establishes bounds on the statistical risk of the estimated functions and establishes consistency, yield rates of convergence, and the near asymptotic optimality of the model selection criterion in both parametric and nonparametric cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144297560"
                        ],
                        "name": "K. Siu",
                        "slug": "K.-Siu",
                        "structuredName": {
                            "firstName": "Kai-Yeung",
                            "lastName": "Siu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Siu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143742406"
                        ],
                        "name": "Jehoshua Bruck",
                        "slug": "Jehoshua-Bruck",
                        "structuredName": {
                            "firstName": "Jehoshua",
                            "lastName": "Bruck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jehoshua Bruck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6052220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7c0f8622f5f67dd33d3d38644e7934b77fc0da7",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear threshold elements (LTEs) are the basic processing elements in artificial neural networks. An LTE computes a function that is a sign of a weighted sum of the input variables. The weights are arbitrary integers; actually they can be very big integers-exponential in the number of input variables. However, in practice, it is very difficult to implement big weights. So the natural question that one can ask is whether there is an efficient way to simulate a network of LTEs with big weights by a network of LTEs with small weights. We prove the following results: 1) every LTE with big weights can be simulated by a depth-3, polynomial size network of LTEs with small weights, 2) every depth-d polynomial size network of LTEs with big weights can be simulated by a depth-(2d+1), polynomial size network of LTEs with small weights. To prove these results, we use tools from harmonic analysis of Boolean functions. Our technique is quite general, it provides insights to some other problems. For example, we were able to improve the best known results on the depth of a network of threshold elements that computes the COMPARISON, ADDITION and PRODUCT of two n-bits numbers, and the MAXIMUM and the SORTING of n n-bit numbers."
            },
            "slug": "On-The-Power-Of-Threshold-Circuits-With-Small-Siu-Bruck",
            "title": {
                "fragments": [],
                "text": "On The Power Of Threshold Circuits With Small Weights"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The following results are proved: 1) every LTE with big weights can be simulated by a depth-3, polynomial size network of LTEs with small weights, and 2) every depth-d polynometric sizeNetwork of LTES with big weight can be simulate by aDepth-(2d+1), polynomially size networkof LTEsWith small weights."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Symposium on Information Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66247832"
                        ],
                        "name": "G. Anzellotti",
                        "slug": "G.-Anzellotti",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Anzellotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Anzellotti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15723652,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "0fa209624648b36393fb6643fc91b81ae4602986",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of approximating a function belonging to some function space by a linear combination of $n$ translates of a given function. Using a lemma by Jones (1990) and Barron (1991) we show that it is possible to define function spaces for which the rate of convergence to zero of the error is $O({1 \\over \\sqrt n})$ in any number of dimensions. The apparent avoidance of the ``curse of dimensionality'''' is due to the fact that these function spaces are more and more constrained as the dimension increases."
            },
            "slug": "Convergence-Rates-of-Approximation-by-Translates-Girosi-Anzellotti",
            "title": {
                "fragments": [],
                "text": "Convergence Rates of Approximation by Translates"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that it is possible to define function spaces for which the rate of convergence to zero of the error is O({1 \\over \\sqrt n})$ in any number of dimensions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738470"
                        ],
                        "name": "E. Kushilevitz",
                        "slug": "E.-Kushilevitz",
                        "structuredName": {
                            "firstName": "Eyal",
                            "lastName": "Kushilevitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kushilevitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(that is, L(f) :::: de for some fixed c \ufffd 1), is examined in [5] and [ 27 ].3 In particular, Siu and Bruck [5] show, among examples, that the Boolean function on {O, 1 Fd defined by the comparison of two d-bit"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(which has a simple network representation) is not in the class PL. Kushilevitz and Mansour [ 27 ] that a class of binary decision trees represent Boolean functions satisfying L(f) .:; Tn, where Tn is the nnmber of nodes of the tree."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Bellare [28] generalizes the results of [ 27 ] by allowing decision trees with more general P L functions implemented at the nodes of the tree."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9832023,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "e2c558f1467863238374899f44e6d466fba99f98",
            "isKey": true,
            "numCitedBy": 392,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This work gives apolynomial time algorithm for learning decision trees with respect to the uniform distribution. (This algorithm uses membership queries.) The decision tree model that is considered is an extension of the traditional boolean decision tree model that allows linear operations in each node (i.e., summation of a subset of the input variables over GF(2)). This paper shows how to learn in polynomial time any function that can be approximated (in norm L2) by a polynomially sparse function (i.e., a function with only polynomially many nonzero Fourier coefficients). The authors demonstrate that any functionf whose L -norm (i.e., the sum of absolute value of the Fourier coefficients) is polynomial can be approximated by a polynomially sparse function, and prove that boolean decision trees with linear operations are a subset of this class of functions. Moreover, it is shown that the functions with polynomial L -norm can be learned deterministically. The algorithm can also exactly identify a decision tree of depth d in time polynomial in 2 a and n. This result implies that trees of logarithmic depth can be identified in polynomial time."
            },
            "slug": "Learning-decision-trees-using-the-Fourier-spectrum-Kushilevitz-Mansour",
            "title": {
                "fragments": [],
                "text": "Learning decision trees using the Fourier spectrum"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The authors demonstrate that any functionf whose L -norm is polynomial can be approximated by a polynomially sparse function, and prove that boolean decision trees with linear operations are a subset of this class of functions."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491571"
                        ],
                        "name": "L. Jones",
                        "slug": "L.-Jones",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jones",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The method used here to prove Theorem 2 is motivated by the techniques used in Jones [ 4 ] to prove convergence rate results for projection pursuit approximation, and in Jones [261 to prove the denseness property of sigmoidal networks in the space of continuous functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Jones [ 4 ] showed that if I is in the closure of the convex hull of G, then Ill n -111 2 ::; O(l/n ), for the sequence of approximations defined as in (42)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A recent result of Jones [ 4 ] on iterative appr oximation in a"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As he points out in [ 4 ], together we observed that his theorem applies to artificial neural networks, provided it could be shown that the function to be approximated is in the closure the convex hull of bounded multiples of sigmoidal func tions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Jones [ 4 ] has obtained similar approximation properties for linear combinations of sinusoidal functions, where the frequency variables are the nonlinear parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Section VIII, a key lemma due to Jones [ 4 ] is presented"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Proof\" A proof of this lemma by use of an iterative approximat ion, in which the points of the convex combination are optimized one at a time, is due to Jones [ 4 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122240265,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e7f56734291de81e99976d092b58e4e4a2b6f60",
            "isKey": true,
            "numCitedBy": 519,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A general convergence criterion for certain iterative sequences in Hilbert space is presented. For an important subclass of these sequences, estimates of the rate of convergence are given. Under very mild assumptions these results establish an 0(1/ F4n) nonsampling convergence rate for projection pursuit regression and neural network training; where n represents the number of ridge functions, neurons or coefficients in a greedy basis expansion."
            },
            "slug": "A-Simple-Lemma-on-Greedy-Approximation-in-Hilbert-Jones",
            "title": {
                "fragments": [],
                "text": "A Simple Lemma on Greedy Approximation in Hilbert Space and Convergence Rates for Projection Pursuit Regression and Neural Network Training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703441"
                        ],
                        "name": "M. Bellare",
                        "slug": "M.-Bellare",
                        "structuredName": {
                            "firstName": "Mihir",
                            "lastName": "Bellare",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bellare"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Bellare [ 28 ] generalizes the results of [27] by allowing decision trees with more general P L functions implemented at the nodes of the tree."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117618261,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "047128678671f3cb3e8e34cf4c8f6af8d8654023",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In many recent results in learning and computational complexity theory which rely on Fourier analysis, the spectral norm plays a key role. An understanding of this quantity would appear to be useful in both gauging and exploiting these results, and in understanding the underlying techniques. This paper surveys various aspects of the spectral norm of finite functions. We consider some of the motivating results as well as both upper and lower bounds on the spectral norm and their relationships to the computational complexity of the function. Some of the results included here are new. In particular, we introduce a general technique for upper bounding the spectral norm of a decision tree over an arbitrary basis. We also extend the learning algorithm of Kushilevitz and Mansour [KM] to mutually independent distributions."
            },
            "slug": "THE-SPECTRAL-NORM-OF-FINITE-FUNCTIONS-Bellare",
            "title": {
                "fragments": [],
                "text": "THE SPECTRAL NORM OF FINITE FUNCTIONS"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general technique for upper bounding the spectral norm of a decision tree over an arbitrary basis is introduced and the learning algorithm of Kushilevitz and Mansour [KM] is extended to mutually independent distributions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12319558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a1e67bb9a2cc7a94acbfa4c9174ddbd22ce705",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A hinge function y=h(x) consists of two hyperplanes continuously joined together at a hinge. In regression (prediction), classification (pattern recognition), and noiseless function approximation, use of sums of hinge functions gives a powerful and efficient alternative to neural networks with computation times several orders of magnitude less than is obtained by fitting neural networks with a comparable number of parameters. A simple and effective method for finding good hinges is presented. >"
            },
            "slug": "Hinging-hyperplanes-for-regression,-classification,-Breiman",
            "title": {
                "fragments": [],
                "text": "Hinging hyperplanes for regression, classification, and function approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A simple and effective method for finding good hinges is presented and it is shown that use of sums of hinge functions gives a powerful and efficient alternative to neural networks with computation times several orders of magnitude less than is obtained by fitting neural Networks with a comparable number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17354,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144492106"
                        ],
                        "name": "V. K\u016frkov\u00e1",
                        "slug": "V.-K\u016frkov\u00e1",
                        "structuredName": {
                            "firstName": "V\u011bra",
                            "lastName": "K\u016frkov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. K\u016frkov\u00e1"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5748809,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "82566f380f61e835292e483cda84eb3d22e32cd4",
            "isKey": false,
            "numCitedBy": 644,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Kolmogorov's-theorem-and-multilayer-neural-networks-K\u016frkov\u00e1",
            "title": {
                "fragments": [],
                "text": "Kolmogorov's theorem and multilayer neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118977737"
                        ],
                        "name": "Ying Zhao",
                        "slug": "Ying-Zhao",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Zhao [ 17 ] gives conditions such that uniformly distributed"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62090057,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a3141cf8d7f61b46c1266a08af2843bf3ebe398b",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning model based on a nonparametric statistical technique, projection pursuit regression, is studied. Projection pursuit is a nonparametric statistical technique to find interesting low-dimensional projections of high-dimensional data sets. Projection pursuit regression approximates a function of q variables by a sum of nonlinear functions of linear combinations of the q variables, which is related to current neural network models. A training algorithm for projection pursuit learning, called backfitting, is investigated. An example of the application of this model is demonstrated.<<ETX>>"
            },
            "slug": "Projection-pursuit-learning-Zhao-Atkeson",
            "title": {
                "fragments": [],
                "text": "Projection pursuit learning"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A learning model based on a nonparametric statistical technique, projection pursuit regression, which approximates a function of q variables by a sum of nonlinear functions of linear combinations oflinear combinations of the q variables, is studied."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143797114"
                        ],
                        "name": "A. Pinkus",
                        "slug": "A.-Pinkus",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Pinkus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pinkus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119692918,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c58148ce71dc46f4e5e590e584c795071273fc3f",
            "isKey": false,
            "numCitedBy": 860,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I. Introduction.- II. Basic Properties of n-Widths.- 1. Properties of dn.- 2. Existence of Optimal Subspaces for dn.- 3. Properties of dn.- 4. Properties of ?n.- 5. Inequalities Between n-Widths.- 6. Duality Between dn and dn.- 7. n-Widths of Mappings of the Unit Ball.- 8. Some Relationships Between dn(T), dn(T) and ?n(T).- Notes and References.- III. Tchebycheff Systems and Total Positivity.- 1. Tchebycheff Systems.- 2. Matrices.- 3. Kernels.- 4. More on Kernels.- IV. n-Widths in Hilbert Spaces.- 1. Introduction.- 2. n-Widths of Compact Linear Operators.- 3. n-Widths, with Constraints.- 3.1 Restricted Approximating Subspaces.- 3.2 Restricting the Unit Ball and Optimal Recovery.- 3.3 n-Widths Under a Pair of Constraints.- 3.4 A Theorem of Ismagilov.- 4. n-Widths of Compact Periodic Convolution Operators.- 4.1 n-Widths as Fourier Coefficients.- 4.2 A Return to Ismagilov's Theorem.- 4.3 Bounded mth Modulus of Continuity.- 5. n-Widths of Totally Positive Operators in L2.- 5.1 The Main Theorem.- 5.2 Restricted Approximating Subspaces.- 6. Certain Classes of Periodic Functions.- 6.1 n-Widths of Cyclic Variation Diminishing Operators.- 6.2 n-Widths for Kernels Satisfying Property B.- Notes and References.- V. Exact n-Widths of Integral Operators.- 1. Introduction.- 2. Exact n-Widths of K? in Lq and Kp in L1.- 3. Exact n-Widths of K?r in Lq and Kpr in L1.- 4. Exact n-Widths for Periodic Functions.- 5. n-Widths of Rank n + 1 Kernels.- Notes and References.- VI. Matrices and n-Widths.- 1. Introduction and General Remarks.- 2. n-Widths of Diagonal Matrices.- 2.1 The Exact Solution for q ? p and p = 1, q = 2.- 2.2 Various Estimates for p = 1, q = ?.- 3. n-Widths of Strictly Totally Positive Matrices.- Notes and References.- VII. Asymptotic Estimates for n-Widths of Sobolev Spaces.- 1. Introduction.- 2. Optimal Lower Bounds.- 3. Optimal Upper Bounds.- 4. Another Look at ?n(B1(r) L?).- Notes and References.- VIII. n-Widths of Analytic Functions.- 1. Introduction.- 2. n-Widths of Analytic Functions with Bounded mth Derivative.- 3. n-Widths of Analytic Functions in H2.- 4. n-Widths of Analytic Functions in H?.- 5. n-Widths of a Class of Entire Functions.- Notes and References.- Glossary of Selected Symbols.- Author Index."
            },
            "slug": "n-Widths-in-Approximation-Theory-Pinkus",
            "title": {
                "fragments": [],
                "text": "n-Widths in Approximation Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14460436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fca98082fa9ff8e9dbae9922491ae54976a0ccef",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an index of resolvability that is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length. The results on the index of resolvability demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference. The minimum complexity estimator converges to true density nearly as fast as an estimator based on prior knowledge of the true subclass of densities. Interpretations and basic properties of minimum complexity estimators are discussed. Some regression and classification problems that can be examined from the minimum description-length framework are considered. >"
            },
            "slug": "Minimum-complexity-density-estimation-Barron-Cover",
            "title": {
                "fragments": [],
                "text": "Minimum complexity density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An index of resolvability is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length to demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69859375"
                        ],
                        "name": "E. Stein",
                        "slug": "E.-Stein",
                        "structuredName": {
                            "firstName": "Elias",
                            "lastName": "Stein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Stein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143907410"
                        ],
                        "name": "G. Weiss",
                        "slug": "G.-Weiss",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Weiss",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117042370,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c4a856962e285c4488245c804145608b1bb82fa9",
            "isKey": false,
            "numCitedBy": 5173,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a unified treatment of basic topics that arise in Fourier analysis. Their intention is to illustrate the role played by the structure of Euclidean spaces, particularly the action of translations, dilatations, and rotations, and to motivate the study of harmonic analysis on more general spaces having an analogous structure, e.g., symmetric spaces."
            },
            "slug": "Introduction-to-Fourier-Analysis-on-Euclidean-Stein-Weiss",
            "title": {
                "fragments": [],
                "text": "Introduction to Fourier Analysis on Euclidean Spaces."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069598082"
                        ],
                        "name": "G. Pisier",
                        "slug": "G.-Pisier",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Pisier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pisier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117025586,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "171d6c53de316d9cb53a4947b631eef13b6edb53",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 S\u00e9minaire analyse fonctionnelle (dit \"Maurey-Schwartz\") (\u00c9cole Polytechnique), 1980-1981, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives du s\u00e9minaire d\u2019analyse fonctionnelle implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/conditions). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright."
            },
            "slug": "Remarques-sur-un-r\u00e9sultat-non-publi\u00e9-de-B.-Maurey-Pisier",
            "title": {
                "fragments": [],
                "text": "Remarques sur un r\u00e9sultat non publi\u00e9 de B. Maurey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195857038,
            "fieldsOfStudy": [],
            "id": "b7294ac444ae927f71ac442372903ff5e2a763e1",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Let /L be the uniform probability distribution on the unit cube B = [O, I]d , and let d(f, g) = U[o. l]' (f(.x)  g(x))2 dx) I/2 be the distance between functions in L2 (/L, B)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Remarqucs sur un resultat non publie de B. Maurey,\" presented at the Seminaire d' analyse fonctionelle"
            },
            "venue": {
                "fragments": [],
                "text": "Remarqucs sur un resultat non publie de B. Maurey,\" presented at the Seminaire d' analyse fonctionelle"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Bellare [28] generalizes the results of [27] by allowing decision trees with more general P L functions implemented at the nodes of the tree."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity regularization with applications to artifi cial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nonparametric Functional Estimatiqn"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Bellare [28] generalizes the results of [27] by allowing decision trees with more general P L functions implemented at the nodes of the tree."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity regularization with applications to artifi cial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nonparametric Functional Estimatiqn"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3) A. Pinkus, n\u00b7Widths in Approximation Theory"
            },
            "venue": {
                "fragments": [],
                "text": "3) A. Pinkus, n\u00b7Widths in Approximation Theory"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximations by superpositions of a sigmoidal func\u00ad tion,"
            },
            "venue": {
                "fragments": [],
                "text": "Math Contr. Signals, Syst,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Remarqucs sur un resultat non publie de B. Maurey,\" presented at the Seminaire d' analyse fonctionelle 1980-1981"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionists nonparametric regression: Multilayer feed\u00b7 forward networks can learn arbitrary mappings,\" Neural NetworkS"
            },
            "venue": {
                "fragments": [],
                "text": "voL 3,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence rates of approximation by ! ranslates , \" Art"
            },
            "venue": {
                "fragments": [],
                "text": "Intell . Lab . Tech . Rep . 1288 , Mass . Inst . Techno ! ."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The implication of polynomial bounds on C/, B, as a consequence of the bound 2C /, B / yin from Theorem 1, is that a polynomial rather than an exponential number of nodes n is sufficient for accurate approximation by sigmoidal networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "\ufffdonsequently, f(x) = L:kE{O, l}d ei1rk.x iko where f 2-d \" -,,,k ,xf( ) H G k = L..,xE{O, l}d e x . ere, f, B = 7r L:kE {O , l}d Ik l 1 1fk l which is bounded above by 7rdL(.f), where L(.f) = L:kE{O, l}d lik l is the spectral norm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural net apprOximation"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Ya le Wo rkshop Adaptive Learning Syst. , K Narendra"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Netherlands: K1uwer Academic"
            },
            "venue": {
                "fragments": [],
                "text": "The Netherlands: K1uwer Academic"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical lcarning networks: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "Computing Science and Statistics: Proc. 21 sl Symp. Interf ace"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence rates for sil)gle hidden layer feedforward networks,\" Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep . RAND Corp., Santa Monica, CA, and Dept. Statist., North Carolina State Vniv.,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximations by superpositions of a sigmoidal func\u00ad tionMulti-layer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Ma th Contr. Signals, Sy st Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical lcarning networks: A unifying view,\" in Computing"
            },
            "venue": {
                "fragments": [],
                "text": "Science and Statistics: Proc. 21 sl Symp. Interface, Alexandria: American Statistical Assoc. ,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nllmber Th eoretic Methods of Approximate 1\\.nalysis. Moscow: Fiznatgiz"
            },
            "venue": {
                "fragments": [],
                "text": "Nllmber Th eoretic Methods of Approximate 1\\.nalysis. Moscow: Fiznatgiz"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sup norm approximation bounds for networks via Yap\u00b7 nik-Chervoncnkis classes"
            },
            "venue": {
                "fragments": [],
                "text": "Notes, Dept. of Math"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rate of approxi\u00ad mation results motivated b robust neu\\1ll network learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Sixth ACM Wr okshop on Computat. Learning Theory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Slide functions for projection pursuit regression and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Dep. Stat. Tech. Rep"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical properties of artificial neural n et w orks , \" in"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IEEE Int . Cont Decision Contr ."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approxim\ufffdtion and estimation bounds for artificial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 4th Annu"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constructive a pp r ox i m a ti o n s for neural networks by sill \u00ad moidal functions"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IEEE : Special Issue on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Anzcllotti, \"Convergence rates of approximation by !ranslates,\" Art"
            },
            "venue": {
                "fragments": [],
                "text": "Mass. Inst. Techno!.,"
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 1,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 50,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Universal-approximation-bounds-for-superpositions-a-Barron/04113e8974341f97258800126d05fd8df2751b7e?sort=total-citations"
}