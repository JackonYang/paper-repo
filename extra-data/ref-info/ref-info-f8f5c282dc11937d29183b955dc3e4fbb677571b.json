{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 232
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12368399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc8b866cc58e82e6413367c8d770ef681e5abe66",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene and graphic text can provide important supplemental index information in video sequences. In this paper we address the problem automatically identifying text regions in digital video key frames. The text contained in video frames is typically very noisy because it is aliased and/or digitized at a much lower resolution than typical document images, making identification, extraction and recognition difficult. The proposed method is based on the use of a hybrid wavelet/neural network segmenter on a series of overlapping small windows to classify regions which contain text. To detect text over a wide range of font sizes, the method is applied to a pyramid of images and the regions identified at each level are integrated."
            },
            "slug": "Automatic-identification-of-text-in-digital-video-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic identification of text in digital video key frames"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The proposed method is based on the use of a hybrid wavelet/neural network segmenter on a series of overlapping small windows to classify regions which contain text to detect text over a wide range of font sizes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 64
                            }
                        ],
                        "text": "If I is a color image, color clustering techniques are required [26, 28, 59, 60], whereas for gray-scale images, a binarization process is typically used [30, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 232
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [36] Lienhart extracts text in one frame and then checks if the region corresponds to text in the following frame by using simple region matching."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 232
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856656"
                        ],
                        "name": "Hae-Kwang Kim",
                        "slug": "Hae-Kwang-Kim",
                        "structuredName": {
                            "firstName": "Hae-Kwang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hae-Kwang Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 64
                            }
                        ],
                        "text": "If I is a color image, color clustering techniques are required [26, 28, 59, 60], whereas for gray-scale images, a binarization process is typically used [30, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8081258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f455a0f0e6d402648da33930fb39ef5587aab0e3",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed. Target frames are selected at fixed time intervals from shots detected by a scene-change detection method. For each selected frame, segmentation by color clustering is performed around color peaks using a color histogram. For each color plane, text-lines are detected using heuristics, and the temporal and spatial position and the text-image of each text-line are stored in a database. Experimental results for text detection in video images and the performance of the method are reported for various video documents. A user interface for text-image based browsing is designed for direct content-based access to video documents, and other applications are discussed."
            },
            "slug": "Efficient-Automatic-Text-Location-Method-and-and-of-Kim",
            "title": {
                "fragments": [],
                "text": "Efficient Automatic Text Location Method and Content-Based Indexing and Structuring of Video Database"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "The second approach is texture-based and uses well-known texture analysis methods such as Gabor ltering [25], Gaussian ltering [56] or spatial variance [58] to locate text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 154
                            }
                        ],
                        "text": "Therefore, we think that using supervised learning to classify text and nontext will be more e ective than the unsupervised clustering techniques used in [25, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Wu and Manmatha present a text extraction system based on Gaussian ltering [56] and treating text as a distinctive texture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53908609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87794fa9895507e6469231135d1c9d44a0d7944c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A four-step system which automatically detects and extracts text in images is presented. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second , strokes are extracted from the segmented text regions, and then processed to form rectangular boxes surrounding the corresponding text strings. Multi-scale processing is used to account for signiicant font size variations. Third, text is extracted by cleaning up the background and binarizing the detected text strings. Finally , better text bounding boxes are generated by using the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition. The system is stable, robust, and works well on images (with or without structured layouts) from a wide variety of sources, including digitized video frames, photographs, newspapers, advertisements in magazines/newspapers, stock cer-tiicates, and personal checks. Any opinions, ndings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reeect those of the sponsors."
            },
            "slug": "Automatic-Text-Detection-and-Recognition-Wu",
            "title": {
                "fragments": [],
                "text": "Automatic Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A four-step system which automatically detects and extracts text in images is presented, which is stable, robust, and works well on images from a wide variety of sources, including digitized video frames, photographs, newspapers, advertisements in magazines/newspapers, stock cer-tiicates, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145742543"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108207987"
                        ],
                        "name": "Suzanne Gauch",
                        "slug": "Suzanne-Gauch",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Gauch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suzanne Gauch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701970"
                        ],
                        "name": "J. Gauch",
                        "slug": "J.-Gauch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gauch",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39671532"
                        ],
                        "name": "Kok Meng Pua",
                        "slug": "Kok-Meng-Pua",
                        "structuredName": {
                            "firstName": "Kok",
                            "lastName": "Pua",
                            "middleNames": [
                                "Meng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kok Meng Pua"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditionally, content has been indexed primarily by manual annotation [1], closed caption [ 2 ], or transcribed audio [3], but some work has also been done on the content analysis of the video itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 2 ] B. M. Mehtre, M. S. Kankhanhalli, and W. F. Lee, \u201cShape measures"
                    },
                    "intents": []
                }
            ],
            "corpusId": 207195538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8074281989a6d0bc1771fca34ff50ac96b789a6d",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of the VISION (Video Indexing for SearchIng Over Networks)project is to establish a comprehensive, online digital videolibrary. We are developing automatic mechanisms to populate thelibrary and provide content-based search and retrieval overcomputer networks. The salient feature of our approach is theintegrated application of mature image or video processing,information retrieval, speech feature extraction and word-spottingtechnologies for efficient creation and exploration of the librarymaterials. First, full-motion video is captured in real-time withflexible qualities to meet the requirements of library patronsconnected via a wide range of network bandwidths. Then, the videosare automatically segmented into a number of logically meaningfulvideo clips by our novel two-step algorithm based on video andaudio contents. A closed caption decoder and/or word-spotter isbeing incorporated into the system to extract textual informationto index the video clips by their contents. Finally, allinformation is stored in a full-text information retrieval systemfor content-baaed exploration of the library over networks ofvarying bandwidths."
            },
            "slug": "Vision:-a-digital-video-library-Li-Gauch",
            "title": {
                "fragments": [],
                "text": "Vision: a digital video library"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The goal of the VISION (Video Indexing for SearchIng Over Networks) project is to establish a comprehensive, online digital videolibrary by developing automatic mechanisms to populate the library and provide content-based search and retrieval overcomputer networks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, we have proposed an indexing mechanism for our method which achieves up to three orders of magnitude speed-up over sequential scanning with very high accuracy, providing also for clustering visualization and browsing of a data set [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We feel that using supervised learning to classify text and nontext will be more effective than the unsupervised clustering techniques used in [ 9 ], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the literature text detection methods are typically either connected component-based (CC-based) [4], [8] or texture based [ 9 ], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " [ 9 ] E. Milios and E. Petrakis, \u201cEfficient shape matching and retrieval at mul-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": true,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731709"
                        ],
                        "name": "P. Fieguth",
                        "slug": "P.-Fieguth",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Fieguth",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fieguth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 74
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2511895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1424c2ce3bc9db230cb6455c63f1d6366576e90",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a simple and very fast method for object tracking based exclusively on color information in digitized video images. Running on a Silicon Graphics R4600 Indy system with an IndyCam, our algorithm is capable of simultaneously tracking objects at full frame size (640/spl times/480 pixels) and video frame rate (30 fps). Robustness with respect to occlusion is achieved via can explicit hypothesis-tree model of the occlusion process. We demonstrate the efficacy of our technique in the challenging task of tracking people, especially tracking human heads and hands."
            },
            "slug": "Color-based-tracking-of-heads-and-other-mobile-at-Fieguth-Terzopoulos",
            "title": {
                "fragments": [],
                "text": "Color-based tracking of heads and other mobile objects at video frame rates"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A simple and very fast method for object tracking based exclusively on color information in digitized video images that demonstrates the efficacy of the technique in the challenging task of tracking people, especially tracking human heads and hands."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198175"
                        ],
                        "name": "T. Tasdizen",
                        "slug": "T.-Tasdizen",
                        "structuredName": {
                            "firstName": "Tolga",
                            "lastName": "Tasdizen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tasdizen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 64
                            }
                        ],
                        "text": "If I is a color image, color clustering techniques are required [26, 28, 59, 60], whereas for gray-scale images, a binarization process is typically used [30, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 166
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10184381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bad076c5f89ac43026a2b5fca648d488f54f45e",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of locating and extracting text from WWW images. A previous algorithm based on color clustering and connected components analysis works well as long as the color of each character is relatively uniform and the typography is fairly simple. It breaks down quickly, however, when these assumptions are violated. In this paper, we describe more robust techniques for dealing with this challenging problem. We present an improved color clustering algorithm that measures similarity based on both RGB and spatial proximity. Layout analysis is also incorporated to handle more complex typography. THese changes significantly enhance the performance of our text detection procedure."
            },
            "slug": "Finding-text-in-color-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Finding text in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An improved color clustering algorithm that measures similarity based on both RGB and spatial proximity is presented, and layout analysis is also incorporated to handle more complex typography."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926946"
                        ],
                        "name": "Vikrant Kobla",
                        "slug": "Vikrant-Kobla",
                        "structuredName": {
                            "firstName": "Vikrant",
                            "lastName": "Kobla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikrant Kobla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2457943"
                        ],
                        "name": "King-Ip Lin",
                        "slug": "King-Ip-Lin",
                        "structuredName": {
                            "firstName": "King-Ip",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "King-Ip Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 804750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67fe6f4553bc9da70d908b29457225574e2b08f0",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Fast and efficient storage, indexing, browsing, and retrieval of video is a necessity for the development of various multimedia database applications. This can be achieved by analyzing the video directly in the compressed domain, thereby avoiding the overhead of decompressing video into individual frames in the pixel domain. Our compressed domain parsing of video performs shot change detection and motion detection using the data readily accessible from MPEG, with minimal decoding. Key frames are identified and are used for indexing, retrieval, and browsing. In this paper, we describe feature extraction and key frame indexing and retrieval techniques that are directly applicable to compressed video. The features are derived form the available DCT, macroblock, and motion vector information and the techniques enable fast parsing and archiving of video."
            },
            "slug": "Archiving,-indexing,-and-retrieval-of-video-in-the-Kobla-Doermann",
            "title": {
                "fragments": [],
                "text": "Archiving, indexing, and retrieval of video in the compressed domain"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper describes feature extraction and key frame indexing and retrieval techniques that are directly applicable to compressed video that enable fast parsing and archiving of video."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 64
                            }
                        ],
                        "text": "If I is a color image, color clustering techniques are required [26, 28, 59, 60], whereas for gray-scale images, a binarization process is typically used [30, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 166
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23073168,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "3ef36b0186bb73619d08d02661616ce7df218ecd",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors examine the problem of locating and extracting text from images on the World Wide Web. They describe a text detection algorithm which is based on color clustering and connected component analysis. The algorithm first quantizes the color space of the input image into a number of color classes using a parameter-free clustering procedure. It then identifies text-like connected components in each color class based on their shapes. Finally, a post-processing procedure aligns text-like components into text lines. Experimental results suggest this approach is promising despite the challenging nature of the input data."
            },
            "slug": "Extracting-text-from-WWW-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Extracting text from WWW images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A text detection algorithm which is based on color clustering and connected component analysis is described, which suggests this approach is promising despite the challenging nature of the input data."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971263"
                        ],
                        "name": "K. Etemad",
                        "slug": "K.-Etemad",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Etemad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Etemad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 51
                            }
                        ],
                        "text": "The success of neural networks in related problems [8, 11, 13, 39, 48] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15806531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37c7f31be030b82a0188e5934cbaa8abbbaad98b",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm for layout independent document image segmentation is suggested. Text, image and graphics regions in a document image are treated as three diierent \\texture\" classes. Feature vectors based on multi-scale wavelet packet representation are used for local classiication. Segmentation is performed by propagating soft local decisions made on small windows across neighboring blocks and integrating them to reduce their \\ambiguities\" and increase their \\conndence\" as more contextual evidence is obtained from the image data. Local votes propagate in a neighborhood, within and across scales, and majorities of weighted votes give the nal decisions. The method has been tested on document page decomposition tasks, and the results of these tests are presented. The algorithm is general, can be applied to other segmentation and classiication tasks, is based on parallel, distributed and independent computations and has low complexity."
            },
            "slug": "Multiscale-Document-Page-Segmentation-Using-Soft-Etemad-Doermann",
            "title": {
                "fragments": [],
                "text": "Multiscale Document Page Segmentation Using Soft Decision Integration"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new algorithm for layout independent document image segmentation that is general, can be applied to other segmentation and classiication tasks, is based on parallel, distributed and independent computations and has low complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108487560"
                        ],
                        "name": "J. C. Lee",
                        "slug": "J.-C.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "Chung-Mong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257741"
                        ],
                        "name": "A. Kankanhalli",
                        "slug": "A.-Kankanhalli",
                        "structuredName": {
                            "firstName": "Atreyi",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kankanhalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 154
                            }
                        ],
                        "text": "If I is a color image, color clustering techniques are required [26, 28, 59, 60], whereas for gray-scale images, a binarization process is typically used [30, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 189
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11365167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a0d7a8938fa04cad3a1afc28c3dcaf1724bde8",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images. A scene image may be complex due to the following reasons: (1) the characters are embedded in an image with other objects, such as structural bars, company logos and smears; (2) the characters may be painted or printed in any color including white, and the background color may differ only slightly from that of the characters; (3) the font, size and format of the characters may be different; and (4) the lighting may be uneven. The main contribution of this research is that it permits the quick and accurate extraction of characters in a complex scene. A coarse search technique is used to locate potential characters, and then a fine grouping technique is used to extract characters accurately. Several additional techniques in the postprocessing phase eliminate spurious as well as overlapping characters. Experimental results of segmenting characters written on cargo container surfaces show that the system is feasible under real-life constraints. The program has been installed as part of a vision system which verifies container codes on vehicles passing through the Port of Singapore."
            },
            "slug": "Automatic-Extraction-of-Characters-in-Complex-Scene-Lee-Kankanhalli",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images is developed that permits the quick and accurate extraction of characters in a complex scene."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "While it is computationally e cient, over a long sequence the error will accumulate to the point that the tracker loses the target [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 109
                            }
                        ],
                        "text": "Evidently, the pure translational model is not adequate to handle scale, rotation and perpective distortions [21, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 778478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab46391005cea85fa5c204b6e77a9c870fdbaed",
            "isKey": false,
            "numCitedBy": 8401,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments.<<ETX>>"
            },
            "slug": "Good-features-to-track-Shi-Tomasi",
            "title": {
                "fragments": [],
                "text": "Good features to track"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238841"
                        ],
                        "name": "Stan Birchfield",
                        "slug": "Stan-Birchfield",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Birchfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stan Birchfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 74
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9845332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5df7c31418d6963fd86c9d74bdcd129d5aa61b7d",
            "isKey": false,
            "numCitedBy": 898,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for tracking a person's head is presented. The head's projection onto the image plane is modeled as an ellipse whose position and size are continually updated by a local search combining the output of a module concentrating on the intensity gradient around the ellipse's perimeter with that of another module focusing on the color histogram of the ellipse's interior. Since these two modules have roughly orthogonal failure modes, they serve to complement one another. The result is a robust, real-time system that is able to track a person's head with enough accuracy to automatically control the camera's pan, tilt, and zoom in order to keep the person centered in the field of view at a desired size. Extensive experimentation shows the algorithm's robustness with respect to full 360-degree out-of-plane rotation, up to 90-degree tilting, severe but brief occlusion, arbitrary camera movement, and multiple moving people in the background."
            },
            "slug": "Elliptical-head-tracking-using-intensity-gradients-Birchfield",
            "title": {
                "fragments": [],
                "text": "Elliptical head tracking using intensity gradients and color histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An algorithm that is able to track a person's head with enough accuracy to automatically control the camera's pan, tilt, and zoom in order to keep the person centered in the field of view at a desired size is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 154
                            }
                        ],
                        "text": "If I is a color image, color clustering techniques are required [26, 28, 59, 60], whereas for gray-scale images, a binarization process is typically used [30, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 189
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058487"
                        ],
                        "name": "D. Reynard",
                        "slug": "D.-Reynard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Reynard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Reynard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40118130"
                        ],
                        "name": "Andy Wildenberg",
                        "slug": "Andy-Wildenberg",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Wildenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Wildenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144488718"
                        ],
                        "name": "J. Marchant",
                        "slug": "J.-Marchant",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Marchant",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marchant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 196
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15174932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6c0c7bf30df274f393b8944aa399ede146ef90e",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of Active Contours in tracking is highly dependent on the availability of an appropriate model of shape and motion, to use as a predictor. Models can be hand-built, but it is far more effective and less time-consuming to learn them from a training set. Techniques to do this exist both for shape, and for shape and motion jointly. This paper extends the range of shape and motion models in two significant ways. The first is to model jointly the random variations in shape arising within an object-class and those occuring during object motion. The resulting algorithm is applied to tracking of plants captured by a video camera mounted on an agricultural robot. The second addresses the tracking of coupled objects such as head and lips. In both cases, new algorithms are shown to make important contributions to tracking performance."
            },
            "slug": "Learning-Dynamics-of-Complex-Motions-from-Image-Reynard-Wildenberg",
            "title": {
                "fragments": [],
                "text": "Learning Dynamics of Complex Motions from Image Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The range of shape and motion models is extended in two significant ways, one to model jointly the random variations in shape arising within an object-class and those occuring during object motion and the second to address the tracking of coupled objects such as head and lips."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115245322"
                        ],
                        "name": "Eun Ryung Lee",
                        "slug": "Eun-Ryung-Lee",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Ryung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eun Ryung Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472612"
                        ],
                        "name": "P. Kim",
                        "slug": "P.-Kim",
                        "structuredName": {
                            "firstName": "Pyeoung",
                            "lastName": "Kim",
                            "middleNames": [
                                "Kee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42242315,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "id": "d24eb099f24408ea92b531b5497b97d2dbbb50db",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic recognition method of a car license plate using color image processing is presented. At first, background colors of a plate are extracted from an input car image. A neural network is used for more stable extraction. To find a plate region, a fixed ratio of horizontal and vertical length of a plate is used. To recognize characters in a plate, template matching and postprocessing techniques are used. Since the proposed method does not depend on line information of a plate it is very robust to boundary deformation. Also, this method is strong enough to deal with a car's image which has many similar regions with a plate.<<ETX>>"
            },
            "slug": "Automatic-recognition-of-a-car-license-plate-using-Lee-Kim",
            "title": {
                "fragments": [],
                "text": "Automatic recognition of a car license plate using color image processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The proposed method does not depend on line information of a plate it is very robust to boundary deformation and strong enough to deal with a car's image which has many similar regions with a plate."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1st International Conference on Image Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52322585"
                        ],
                        "name": "T. Gotoh",
                        "slug": "T.-Gotoh",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Gotoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gotoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783505"
                        ],
                        "name": "T. Toriu",
                        "slug": "T.-Toriu",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Toriu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Toriu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144909403"
                        ],
                        "name": "S. Sasaki",
                        "slug": "S.-Sasaki",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Sasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sasaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107828741"
                        ],
                        "name": "M. Yoshida",
                        "slug": "M.-Yoshida",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoshida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1943061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d417988c7b5d8cd089ada273842f0bd25e45ffa",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A flexible vision-based algorithm for a book sorting system is presented. The algorithm is based on a discrimination model that is adaptively generated for the current object classes by learning. The algorithm consists of an image normalization process, a feature element extraction process, a learning process, and a recognition process. The image normalization process extracts the contour of the object in an image, and geometrically normalizes the image. The feature extraction process converts the normalized image to the pyramidal representation, and the feature element is extracted from each resolution level. The learning process generates a discrimination model, which represents the differences between classes, based on hierarchical clustering. In the recognition process, the input images are hierarchically discriminated under the control of the decision tree. To evaluate the algorithm, a simulation system was implemented on a general-purpose computer and an image processor was developed. >"
            },
            "slug": "A-Flexible-Vision-Based-Algorithm-for-a-Book-System-Gotoh-Toriu",
            "title": {
                "fragments": [],
                "text": "A Flexible Vision-Based Algorithm for a Book Sorting System"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A flexible vision-based algorithm for a book sorting system that is adaptively generated for the current object classes by learning is presented, based on a discrimination model based on hierarchical clustering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678633"
                        ],
                        "name": "Gregory Hager",
                        "slug": "Gregory-Hager",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Hager",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Hager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767767"
                        ],
                        "name": "P. Belhumeur",
                        "slug": "P.-Belhumeur",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Belhumeur",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Belhumeur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 74
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 109
                            }
                        ],
                        "text": "Evidently, the pure translational model is not adequate to handle scale, rotation and perpective distortions [21, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11616840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b844126725ab2dad5bf69dc11d58e465742694a",
            "isKey": false,
            "numCitedBy": 1309,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "As an object moves through the field of view of a camera, the images of the object may change dramatically. This is not simply due to the translation of the object across the image plane; complications arise due to the fact that the object undergoes changes in pose relative to the viewing camera, in illumination relative to light sources, and may even become partially or fully occluded. We develop an efficient general framework for object tracking, which addresses each of these complications. We first develop a computationally efficient method for handling the geometric distortions produced by changes in pose. We then combine geometry and illumination into an algorithm that tracks large image regions using no more computation than would be required to track with no accommodation for illumination changes. Finally, we augment these methods with techniques from robust statistics and treat occluded regions on the object as statistical outliers. Experimental results are given to demonstrate the effectiveness of our methods."
            },
            "slug": "Efficient-Region-Tracking-With-Parametric-Models-of-Hager-Belhumeur",
            "title": {
                "fragments": [],
                "text": "Efficient Region Tracking With Parametric Models of Geometry and Illumination"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work develops a computationally efficient method for handling the geometric distortions produced by changes in pose and combines geometry and illumination into an algorithm that tracks large image regions using no more computation than would be required to track with no accommodation for illumination changes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809905"
                        ],
                        "name": "A. Witkin",
                        "slug": "A.-Witkin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Witkin",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Witkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "[7, 45], nonrigid deformations [6, 46] and linear image subspaces [5, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53780196,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "bbcc7f511f66f6b5c7fbd0ccba456839bb40508d",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel solution to the 2-D tracking problem is presented. This solution has two major components: a deformation model that constrains the interpretation of motion, and a set of energy-based match criteria that specify image features to be used in tracking. The separation of the motion model from the match features is an advantage of this approach over previous tracking systems. An implementation of these ideas has been shown to exhibit fast and flexible operation over a wide class of image motions. Experimental results are given for two real-world image sequences.<<ETX>>"
            },
            "slug": "Visual-tracking-with-deformation-models-Rehg-Witkin",
            "title": {
                "fragments": [],
                "text": "Visual tracking with deformation models"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel solution to the 2-D tracking problem has two major components: a deformation model that constrains the interpretation of motion, and a set of energy-based match criteria that specify image features to be used in tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Conference on Robotics and Automation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113579091"
                        ],
                        "name": "S. Harmalkar",
                        "slug": "S.-Harmalkar",
                        "structuredName": {
                            "firstName": "Sayali",
                            "lastName": "Harmalkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmalkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847486"
                        ],
                        "name": "R. Sinha",
                        "slug": "R.-Sinha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Sinha",
                            "middleNames": [
                                "Mahesh",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sinha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62171951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4205a7383f711b5040c22db9be16ca303cb7c13",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A text recognition system capable of correcting character-level confusions is developed. It incorporates word-level knowledge sources in the form of a dictionary and an envelope of the words. This speeds up the word recognition process by recognizing only crucial characters in the word. Also, the same knowledge sources are used to generate possible pairs of characters in a merged blob of characters, so that the blob can be segmented at proper positions. The system is capable of segmenting and classifying merged pairs of characters in the input text. A dictionary of the 9800 most frequently used words in English (taken from Brown Corpus) has been used.<<ETX>>"
            },
            "slug": "Integrating-word-level-knowledge-in-text-Harmalkar-Sinha",
            "title": {
                "fragments": [],
                "text": "Integrating word level knowledge in text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A text recognition system capable of correcting character-level confusions is developed that incorporates word-level knowledge sources in the form of a dictionary and an envelope of the words to speed up the word recognition process."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings. 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 130705203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee1d075126f1fcdf6a49ad9976e84ebee5f00caf",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a vision system for the 3-0 modelbased tracking of unconstrained human movement. Using image sequences acquzred szmultaneously from multiple views, we recover the 3-0 body pose at each time instant without the use of markers. The poserecovery problem is formulated as a search problem and entails finding the pose parameters of a graphical human model whose synthesized appearance is most similar to the actual appearance of the real human in the multi-view images. The models used for this purpose are acquiredfrom the images. We use a decomposition approach and a best-first technique to search through the high dimensional pose parameter space. A robust variant of chamfer matching is used as a fast similarity measure between synthesized and real edge images. We present initial tracking results from a large new Humans-In-Action (HIA) database containing more than 2500 frames tn each of four orthogonal vzews. They contain subjects involved in a variety of activities, of various degrees of complexity, rangzng from the more simple one-person hand waving to the chailenging two-person close interaction in the Argentine Tango."
            },
            "slug": "el-based-tracking-of-humans-in-action:-Gavrila-Davis",
            "title": {
                "fragments": [],
                "text": "el-based tracking of humans in action:"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A vision system for the 3-0 modelbased tracking of unconstrained human movement and initial tracking results from a large new Humans-In-Action (HIA) database containing more than 2500 frames each of four orthogonal vzews."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50591689"
                        ],
                        "name": "B. S. Manjunath",
                        "slug": "B.-S.-Manjunath",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Manjunath",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Manjunath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 51
                            }
                        ],
                        "text": "The success of neural networks in related problems [8, 11, 13, 39, 48] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21048567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a4793c6699c8990d5d8da5691310a7c3a89fd15",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A model consisting of a multistage system which extracts and groups salient features in the image at different spatial scales (or frequencies) is used. In the first stage, a Gabor wavelet decomposition provides a representation of the image which is orientation selective and has optimal localization properties in space and frequency. This decomposition is useful in detecting significant features such as step and line edges at different scales and orientations in the image. Following the wavelet transformation, local competitive interactions are introduced to reduce the effects of noise and changes in illumination. Interscale interactions help in localizing the line ends and corners, and play a crucial role in boundary perception. The final stage groups similar features, aiding in boundary completion. The different stages can be identified with processing by simple, complex, and hypercomplex cells in the visual cortex of mammals. Experimental results demonstrate the performance of this model in detecting boundaries (both real and illusory) in real and synthetic images."
            },
            "slug": "A-unified-approach-to-boundary-perception:-edges,-Manjunath-Chellappa",
            "title": {
                "fragments": [],
                "text": "A unified approach to boundary perception: edges, textures, and illusory contours"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results demonstrate the performance of this model in detecting boundaries in real and synthetic images, and can be identified with processing by simple, complex, and hypercomplex cells in the visual cortex of mammals."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329435"
                        ],
                        "name": "G. Piccioli",
                        "slug": "G.-Piccioli",
                        "structuredName": {
                            "firstName": "Giulia",
                            "lastName": "Piccioli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Piccioli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7409396"
                        ],
                        "name": "E. D. Micheli",
                        "slug": "E.-D.-Micheli",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Micheli",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. D. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34741785"
                        ],
                        "name": "P. Parodi",
                        "slug": "P.-Parodi",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Parodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Parodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35074426"
                        ],
                        "name": "M. Campani",
                        "slug": "M.-Campani",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Campani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Campani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5372532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19aca01bafe52131ec95473dac105889aa6a4d33",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-method-for-road-sign-detection-and-Piccioli-Micheli",
            "title": {
                "fragments": [],
                "text": "Robust method for road sign detection and recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[7, 45], nonrigid deformations [6, 46] and linear image subspaces [5, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14497446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dbb7a71d67fbc1b8afa16aec6b34e788a74050c",
            "isKey": false,
            "numCitedBy": 766,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a probabilistic decomposition of human dynamics at multiple abstractions, and shows how to propagate hypotheses across space, time, and abstraction levels. Recognition in this framework is the succession of very general low level grouping mechanisms to increased specific and learned model based grouping techniques at higher levels. Hard decision thresholds are delayed and resolved by higher level statistical models and temporal context. Low-level primitives are areas of coherent motion found by EM clustering, mid-level categories are simple movements represented by dynamical systems, and high-level complex gestures are represented by Hidden Markov Models as successive phases of ample movements. We show how such a representation can be learned from training data, and apply It to the example of human gait recognition."
            },
            "slug": "Learning-and-recognizing-human-dynamics-in-video-Bregler",
            "title": {
                "fragments": [],
                "text": "Learning and recognizing human dynamics in video sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A probabilistic decomposition of human dynamics at multiple abstractions is described, and how to propagate hypotheses across space, time, and abstraction levels is shown."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 58
                            }
                        ],
                        "text": "To deal with this we use a bootstrap method recommendedby Sung and Poggio [54] to re-train."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "[54] K. Sung and T. Poggio."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "To deal with this we use a bootstrap method recommended by Sung and Poggio [54] to re-train."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7164794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088eb2d102c6bb486f5270d0b2adff76961994cf",
            "isKey": true,
            "numCitedBy": 2061,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system."
            },
            "slug": "Example-Based-Learning-for-View-Based-Human-Face-Sung-Poggio",
            "title": {
                "fragments": [],
                "text": "Example-Based Learning for View-Based Human Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An example-based learning approach for locating vertical frontal views of human faces in complex scenes and shows empirically that the distance metric adopted for computing difference feature vectors, and the \"nonface\" clusters included in the distribution-based model, are both critical for the success of the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971263"
                        ],
                        "name": "K. Etemad",
                        "slug": "K.-Etemad",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Etemad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Etemad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27678281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95dffcc92bda88d9f4f5b112d100f43951745b8c",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information. Multiscale feature vectors are classified locally using a neural network to allow soft/fuzzy multi-class membership assignments. Segmentation is performed by integrating soft local decision vectors to reduce their \"ambiguities\"."
            },
            "slug": "Multiscale-Segmentation-of-Unstructured-Document-Etemad-Doermann",
            "title": {
                "fragments": [],
                "text": "Multiscale Segmentation of Unstructured Document Pages Using Soft Decision Integration"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056405660"
                        ],
                        "name": "T. Frank",
                        "slug": "T.-Frank",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Frank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47200142"
                        ],
                        "name": "M. Haag",
                        "slug": "M.-Haag",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Haag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Haag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2590676"
                        ],
                        "name": "H. Kollnig",
                        "slug": "H.-Kollnig",
                        "structuredName": {
                            "firstName": "Henner",
                            "lastName": "Kollnig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kollnig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144721252"
                        ],
                        "name": "H. Nagel",
                        "slug": "H.-Nagel",
                        "structuredName": {
                            "firstName": "Hans-Hellmut",
                            "lastName": "Nagel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nagel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27723407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f73d891659ba9c2e62d9775d2b5384b87218977",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Vehicles on downtown roads can be occluded by other vehicles or by stationary scene components such as traffic lights or road signs. After having recorded such a scene by a video camera, we noticed that the occlusion may disturb the detection and tracking of vehicles by previous versions of our computer vision approach. In this contribution we demonstrate how our image sequence analysis system can be improved by an explicit model-based recognition of 3D occlusion situations. Results obtained from real world image sequences recording gas station traffic as well as inner-city intersection traffic are presented."
            },
            "slug": "Tracking-of-Occluded-Vehicles-in-Traffic-Scenes-Frank-Haag",
            "title": {
                "fragments": [],
                "text": "Tracking of Occluded Vehicles in Traffic Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This contribution demonstrates how the image sequence analysis system can be improved by an explicit model-based recognition of 3D occlusion situations by obtaining results obtained from real world image sequences recording gas station traffic as well as inner-city intersection traffic."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153701268"
                        ],
                        "name": "M. Mohiuddin",
                        "slug": "M.-Mohiuddin",
                        "structuredName": {
                            "firstName": "Marzia",
                            "lastName": "Mohiuddin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohiuddin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 189
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61109635,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e9f890d2ce8b6ef47301312a768366fec32ba1a0",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Unlike simple letter envelopes which have a high degree of global spatial structure among a limited number of entities, many mail pieces such as magazines usually have an address block printed on a label which can be pasted in an arbitrary position and orientation among text, graphics and images on a magazine cover, which is often shrink wrapped. This work concentrates on address block location for complex mail pieces with an arbitrary layout of printed entities. Using a bottom-up approach, a pyramid model is created for the address block. Based on this model, some features are extracted for assigning a confidence value to the located address blocks. The typical processing time for locating a destination address block is 0.1 seconds on a SGI Indy workstation."
            },
            "slug": "Address-block-location-on-complex-mail-pieces-Yu-Jain",
            "title": {
                "fragments": [],
                "text": "Address block location on complex mail pieces"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Using a bottom-up approach, a pyramid model is created for the address block and some features are extracted for assigning a confidence value to the located address blocks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "[7, 45], nonrigid deformations [6, 46] and linear image subspaces [5, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3175562,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "4ef915fa9e5b2260d4b45927c0033a7ba53bf66e",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the use of local parametrized models of image motion for recovering and recognizing the non-rigid and articulated motion of human faces. Parametric flow models (for example affine) are popular for estimating motion in rigid scenes. We observe that within local regions in space and time, such models not only accurately model non-rigid facial motions but also provide a concise description of the motion in terms of a small number of parameters. These parameters are intuitively related to the motion of facial features during facial expressions and we show how expressions such as anger, happiness, surprise, fear, disgust and sadness can be recognized from the local parametric motions in the presence of significant head motion. The motion tracking and expression recognition approach performs with high accuracy in extensive laboratory experiments involving 40 subjects as well as in television and movie sequences.<<ETX>>"
            },
            "slug": "Tracking-and-recognizing-rigid-and-non-rigid-facial-Black-Yacoob",
            "title": {
                "fragments": [],
                "text": "Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper explores the use of local parametrized models of image motion for recovering and recognizing the non-rigid and articulated motion of human faces and shows how expressions can be recognized from the local parametric motions in the presence of significant head motion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145286523"
                        ],
                        "name": "K. Rohr",
                        "slug": "K.-Rohr",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Rohr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rohr"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122238372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92ab4fc76e2f085dde81626794b79b5e9d1d00e0",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The interpretation of the movements of articulated bodies in image sequences is one of the most challenging problems in computer vision. In this contribution, we introduce a model-based approach for the recognition of pedestrians. We represent the human body by a 3D-model consisting of cylinders, whereas for modelling the movement of walking we use data from medical motion studies. The estimation of model parameters in consecutive images is done by applying a Kalman filter. Experimental results are shown for synthetic as well as for real image data."
            },
            "slug": "Towards-model-based-recognition-of-human-movements-Rohr",
            "title": {
                "fragments": [],
                "text": "Towards model-based recognition of human movements in image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A model-based approach for the recognition of pedestrians is introduced and the human body is represented by a 3D-model consisting of cylinders, whereas for modelling the movement of walking the authors use data from medical motion studies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2220834"
                        ],
                        "name": "D. DeKruger",
                        "slug": "D.-DeKruger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "DeKruger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeKruger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685264"
                        ],
                        "name": "B. Hunt",
                        "slug": "B.-Hunt",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Hunt",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hunt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 51
                            }
                        ],
                        "text": "The success of neural networks in related problems [8, 11, 13, 39, 48] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19918079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "928eafc69e7a3cdc59a07488054ba5ac0b96deab",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Image-processing-and-neural-networks-for-of-area-DeKruger-Hunt",
            "title": {
                "fragments": [],
                "text": "Image processing and neural networks for recognition of cartographic area features"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511333"
                        ],
                        "name": "Jian-Tong Wu",
                        "slug": "Jian-Tong-Wu",
                        "structuredName": {
                            "firstName": "Jian-Tong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Tong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145505338"
                        ],
                        "name": "S. Tamura",
                        "slug": "S.-Tamura",
                        "structuredName": {
                            "firstName": "Shinichi",
                            "lastName": "Tamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35040973"
                        ],
                        "name": "H. Mitsumoto",
                        "slug": "H.-Mitsumoto",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Mitsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mitsumoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144600947"
                        ],
                        "name": "H. Kawai",
                        "slug": "H.-Kawai",
                        "structuredName": {
                            "firstName": "Hideo",
                            "lastName": "Kawai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kawai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917567"
                        ],
                        "name": "K. Kurosu",
                        "slug": "K.-Kurosu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Kurosu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kurosu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128053"
                        ],
                        "name": "K. Okazaki",
                        "slug": "K.-Okazaki",
                        "structuredName": {
                            "firstName": "Kozo",
                            "lastName": "Okazaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Okazaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 236
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35225837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c19f6a46ac2d55475d59e081b61538c4702af224",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-vowel-recognition-jointly-using-and-Wu-Tamura",
            "title": {
                "fragments": [],
                "text": "Neural network vowel-recognition jointly using voice features and mouth shape image"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47993204"
                        ],
                        "name": "P. Letellier",
                        "slug": "P.-Letellier",
                        "structuredName": {
                            "firstName": "Ph.",
                            "lastName": "Letellier",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Letellier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316729"
                        ],
                        "name": "M. Nadler",
                        "slug": "M.-Nadler",
                        "structuredName": {
                            "firstName": "Morton",
                            "lastName": "Nadler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nadler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3348448"
                        ],
                        "name": "J. Abramatic",
                        "slug": "J.-Abramatic",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Abramatic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Abramatic"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38767813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "055e9c39d43f5337b03c41b4154e46725d286cc0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Telesign is designed to offer a means of visual communication over a 56- or 64-kbit/s data network. The purpose is to supply a means of visual telecommunication among the members of the deaf community using sign language or lip reading. The system consists of an edge detector followed by digital compression coding to meet channel requirements. Psychometric experiments have shown the need for 25 frames/s with a minimum definition of 128 \u00d7 128 points. Edge-detection techniques are discussed from the viewpoint of an unusual quality criterion: the subjective resemblance of the contoured image to the original and, closely related to this, the intelligibility of the image sequence. A classification of edge detectors is given, based upon the position of the contour with respect to the dark/light contrast boundaries in the picture. The relationship of this classification to the quality criterion yields the definition of a new gradient display, called the \"shifted gradient,\" with improved visual quality. The pseudo-Laplacian, well-suited to the problem, is analyzed. Various binary coding techniques are reviewed, and the results of compression given. A psychovisual experiment, performed in conjunction with the INJS (National Institute for the Education of Young Deaf), Paris, France, is described; the results validate the proposed approach. Guidelines are proposed for the design of a real-time device. The application of Telesign to closed-caption TV programs for the deaf viewer is also suggested."
            },
            "slug": "The-telesign-project-Letellier-Nadler",
            "title": {
                "fragments": [],
                "text": "The telesign project"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The purpose is to supply a means of visual telecommunication among the members of the deaf community using sign language or lip reading using Telesign, which consists of an edge detector followed by digital compression coding to meet channel requirements."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24593399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cf5a3bb338ab51d70153eab0e0539ba6dbab1c7",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Among the many applications that have been proposed for neural networks, character recognition has been one of the most successful. Compared to other methods used in pattern recognition, the advantage of neural networks is that they offer a lot of flexibility to the designer, i.e. expert knowledge can be introduced into the architecture to reduce the number of parameters determined by training by examples. In this paper, a general introduction to neural network architectures and learning algorithms commonly used for pattern recognition problems is given. The design of a neural network character recognizer for on-line recognition of handwritten characters is then described in detail."
            },
            "slug": "Applications-of-Neural-Networks-to-Character-Guyon",
            "title": {
                "fragments": [],
                "text": "Applications of Neural Networks to Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A general introduction to neural network architectures and learning algorithms commonly used for pattern recognition problems is given and the design of a neural network character recognizer for on-line recognition of handwritten characters is described in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30603673"
                        ],
                        "name": "S. Rogers",
                        "slug": "S.-Rogers",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rogers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145482302"
                        ],
                        "name": "J. Colombi",
                        "slug": "J.-Colombi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Colombi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Colombi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108792390"
                        ],
                        "name": "Curtis E. Martin",
                        "slug": "Curtis-E.-Martin",
                        "structuredName": {
                            "firstName": "Curtis",
                            "lastName": "Martin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Curtis E. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12785071"
                        ],
                        "name": "J. C. Gainey",
                        "slug": "J.-C.-Gainey",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Gainey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Gainey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2247041"
                        ],
                        "name": "K. H. Fielding",
                        "slug": "K.-H.-Fielding",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Fielding",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. H. Fielding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119840322"
                        ],
                        "name": "T. J. Burns",
                        "slug": "T.-J.-Burns",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Burns",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J. Burns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1933935"
                        ],
                        "name": "D. Ruck",
                        "slug": "D.-Ruck",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Ruck",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786615"
                        ],
                        "name": "M. Kabrisky",
                        "slug": "M.-Kabrisky",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kabrisky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kabrisky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693682"
                        ],
                        "name": "M. Oxley",
                        "slug": "M.-Oxley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Oxley",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oxley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 51
                            }
                        ],
                        "text": "The success of neural networks in related problems [8, 11, 13, 39, 48] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Empirical results indicate that this metric provides similar rankings to Pe [48]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17817547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daa364d3516bf593636d0b5d9e57aac9ff7dda87",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-for-automatic-target-recognition-Rogers-Colombi",
            "title": {
                "fragments": [],
                "text": "Neural networks for automatic target recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2212686"
                        ],
                        "name": "T. F. Li",
                        "slug": "T.-F.-Li",
                        "structuredName": {
                            "firstName": "Tze",
                            "lastName": "Li",
                            "middleNames": [
                                "Fen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. F. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985613"
                        ],
                        "name": "Shiaw-Shian Yu",
                        "slug": "Shiaw-Shian-Yu",
                        "structuredName": {
                            "firstName": "Shiaw-Shian",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiaw-Shian Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 198169203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f92a78c2dbebb79f5f7e1e6fdec323bc44b2ade9",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Handprinted-Chinese-Character-Recognition-using-the-Li-Yu",
            "title": {
                "fragments": [],
                "text": "Handprinted Chinese Character Recognition using the Probability Distribution Feature"
            },
            "venue": {
                "fragments": [],
                "text": "Document Image Analysis"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721679"
                        ],
                        "name": "Sang-Kyoon Kim",
                        "slug": "Sang-Kyoon-Kim",
                        "structuredName": {
                            "firstName": "Sang-Kyoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Kyoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145132323"
                        ],
                        "name": "D. Kim",
                        "slug": "D.-Kim",
                        "structuredName": {
                            "firstName": "Dae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Wook"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9889790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53d015192ffef21874606d60b8e3666d05acb399",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting a license plate is an important stage in automatic vehicle identification. It is very difficult because vehicle images are usually degraded and processing the images is computationally intensive. We propose a new method to extract the plate region using a distributed genetic algorithm. The algorithm offers robustness in dealing with deformation of vehicle images and inherent parallelism to improve the processing time. A test with seventy images shows an extraction rate of 92.8%, working well within real world situations. This results suggest that the proposed method is pertinent to be put into practical use."
            },
            "slug": "A-recognition-of-vehicle-license-plate-using-a-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A recognition of vehicle license plate using a genetic algorithm based segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new method to extract the plate region using a distributed genetic algorithm that offers robustness in dealing with deformation of vehicle images and inherent parallelism to improve the processing time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd IEEE International Conference on Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "The low-pass lter creates successive approximations to the image while the detailed signal provides a feature-rich representation in terms of textual content [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2356353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b78626ce1a562c05b1c06f9c805e839f9760b9ab",
            "isKey": false,
            "numCitedBy": 20813,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiresolution representations are effective for analyzing the information content of images. The properties of the operator which approximates a signal at a given resolution were studied. It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/sup j/ (where j is an integer) can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions. In L/sup 2/(R), a wavelet orthonormal basis is a family of functions which is built by dilating and translating a unique function psi (x). This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror filters. Wavelet representation lies between the spatial and Fourier domains. For images, the wavelet representation differentiates several spatial orientations. The application of this representation to data compression in image coding, texture discrimination and fractal analysis is discussed. >"
            },
            "slug": "A-Theory-for-Multiresolution-Signal-Decomposition:-Mallat",
            "title": {
                "fragments": [],
                "text": "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/Sup j/ can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1933935"
                        ],
                        "name": "D. Ruck",
                        "slug": "D.-Ruck",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Ruck",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30603673"
                        ],
                        "name": "S. Rogers",
                        "slug": "S.-Rogers",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rogers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786615"
                        ],
                        "name": "M. Kabrisky",
                        "slug": "M.-Kabrisky",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kabrisky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kabrisky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72277905"
                        ],
                        "name": "W. Afb",
                        "slug": "W.-Afb",
                        "structuredName": {
                            "firstName": "Wright",
                            "lastName": "Afb",
                            "middleNames": [
                                "Patterson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Afb"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "Another saliency metric for features measures the MLP outputs, sampled over an appropriate range of allowable input values [50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11329903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fef88a35a758787cf0af06902bceb70c45ba16f4",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of selecting the best set of features for target r ecognition using a multilayer perceptron is addressed in this paper. A technique has been developed which analyzes the weights in a multilayer perceptron to determine which features the network finds important and which are unimportant. A brief introduction to the use of multilayer perceptrons for classification and the training rules available is followed by the mathematical development of the saliency measure for multilayer perceptrons. The tec hnique is applied to two different image databases and is found to be consistent with statistical techniques and independent of the network initial conditions. The saliency measure is the n used to compare the results of two different training rules on a target recognition problem."
            },
            "slug": "Feature-Selection-Using-a-Multilayer-Perceptron-Ruck-Rogers",
            "title": {
                "fragments": [],
                "text": "Feature Selection Using a Multilayer Perceptron"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A technique has been developed which analyzes the weights in a multilayer perceptron to determine which features the network finds important and which are unimportant, and the saliency measure is used to compare the results of two different training rules on a target recognition problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "To illustrate this point,we collected 500 text blocks and 500 nontext blocks and used Linear Discriminant Analysis(LDA) [17] to project them to a new subspace where the between-class scatter is maximizedand the within-class scatter is minimized, which bene ts the classi cation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "To illustrate this point, we collected 500 text blocks and 500 nontext blocks and used Linear Discriminant Analysis (LDA) [17] to project them to a new subspace where the between-class scatter is maximized and the within-class scatter is minimized, which bene ts the classi cation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "An arti cial neural network is a natural6\nFigure 4: The distribution of text ( ) and nontext (+) in the subspace after LDA projection."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62359231,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f1277592f221ea26fa1d2321a38b64c58b33d75b",
            "isKey": true,
            "numCitedBy": 8010,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition. Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology. Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition. This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field. Each chapter contains computer projects as well as exercises."
            },
            "slug": "Introduction-to-Statistical-Pattern-Recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This completely revised second edition presents an introduction to statistical pattern recognition, which is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34751187"
                        ],
                        "name": "C. R. Wren",
                        "slug": "C.-R.-Wren",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Wren",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Wren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145271456"
                        ],
                        "name": "A. Azarbayejani",
                        "slug": "A.-Azarbayejani",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Azarbayejani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Azarbayejani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9458767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69b7efd02ea06e6aa372b5c1a46167e6a5366bfd",
            "isKey": false,
            "numCitedBy": 3546,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Pfinder is a real-time system for tracking and interpretation of people. It runs on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multi-class statistical model of color and shape to obtain a 2-D representation of head and hands in a wide range of viewing conditions. These representations are useful for applications such as wireless interfaces, video databases, and low-bandwidth coding, without cumbersome wires or attached sensors."
            },
            "slug": "Pfinder:-real-time-tracking-of-the-human-body-Wren-Azarbayejani",
            "title": {
                "fragments": [],
                "text": "Pfinder: Real-Time Tracking of the Human Body"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Pfinder uses a multi-class statistical model of color and shape to obtain a 2-D representation of head and hands in a wide range of viewing conditions, useful for applications such as wireless interfaces, video databases, and low-bandwidth coding."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50494708"
                        ],
                        "name": "Ying Dia",
                        "slug": "Ying-Dia",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Dia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Dia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122737130"
                        ],
                        "name": "N. Zheng",
                        "slug": "N.-Zheng",
                        "structuredName": {
                            "firstName": "Nanning",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021593"
                        ],
                        "name": "Xining Zhang",
                        "slug": "Xining-Zhang",
                        "structuredName": {
                            "firstName": "Xining",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xining Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715881"
                        ],
                        "name": "Guorong Xuan",
                        "slug": "Guorong-Xuan",
                        "structuredName": {
                            "firstName": "Guorong",
                            "lastName": "Xuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guorong Xuan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27111052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81d21abd2c5d7b6e279708ff6790134bf8016f27",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Considering that the characters on the license plate of a moving vehicle may be severely blurred, a method is proposed of recognizing province name on a license plate. Using the projection histogram of Chinese characters, the appropriate feature value is extracted, the fuzzy matching and dynamic program is applied, and the 29 province names of China are correctly classified. The simulation results show that this method has strong noise-resistance and a high processing speed. The recognition rate is more than 90% for the blurred characters, and the recognition time is less than one second.<<ETX>>"
            },
            "slug": "Automatic-recognition-of-province-name-on-the-plate-Dia-Zheng",
            "title": {
                "fragments": [],
                "text": "Automatic recognition of province name on the license plate of moving vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Considering that the characters on the license plate of a moving vehicle may be severely blurred, a method of recognizing province name on a license plate is proposed using the projection histogram of Chinese characters, which has strong noise-resistance and a high processing speed."
            },
            "venue": {
                "fragments": [],
                "text": "[1988 Proceedings] 9th International Conference on Pattern Recognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144562609"
                        ],
                        "name": "Marc Davis",
                        "slug": "Marc-Davis",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 1 ] R. T. Chin and C. R. Dyer, \u201cModel-based recognition in robot vision,\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditionally, content has been indexed primarily by manual annotation [ 1 ], closed caption [2], or transcribed audio [3], but some work has also been done on the content analysis of the video itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44813239,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "3e8802e33f9efe150ac9a5f56ec2a4fc9738c041",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 177,
            "paperAbstract": {
                "fragments": [],
                "text": "Thesis (Ph. D.)--Massachusetts Institute of Technology, Program in Media Arts & Sciences, 1995."
            },
            "slug": "Media-streams:-representing-video-for-retrieval-and-Davis",
            "title": {
                "fragments": [],
                "text": "Media streams: representing video for retrieval and repurposing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Thesis (Ph. D.)--Massachusetts Institute of Technology, Program in Media Arts & Sciences, 1995."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[7, 45], nonrigid deformations [6, 46] and linear image subspaces [5, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10376369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c51f0d3f863fdde988cfada25f1a07a224889129",
            "isKey": false,
            "numCitedBy": 530,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Passive sensing of human hand and limb motion is important for a wide range of applications from human-computer interaction to athletic performance measurement. High degree of freedom articulated mechanisms like the human hand are difficult to track because of their large state space and complex image appearance. This article describes a model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz."
            },
            "slug": "Visual-Tracking-of-High-DOF-Articulated-Structures:-Rehg-Kanade",
            "title": {
                "fragments": [],
                "text": "Visual Tracking of High DOF Articulated Structures: an Application to Human Hand Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107584303"
                        ],
                        "name": "J. P. Jones",
                        "slug": "J.-P.-Jones",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3459632"
                        ],
                        "name": "L. Palmer",
                        "slug": "L.-Palmer",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Palmer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "Jones and Palmer have shown that wavelets can closely represent the response pro les of neurons in the striate cortex [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17527051,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "43e0874cb25bfe27dde52776852b154564a8d3a3",
            "isKey": false,
            "numCitedBy": 833,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "1. A reverse correlation (6, 8, 25, 35) method is developed that allows quantitative determination of visual receptive-field structure in two spatial dimensions. This method is applied to simple cells in the cat striate cortex. 2. It is demonstrated that the reverse correlation method yields results with several desirable properties, including convergence and reproducibility independent of modest changes in stimulus parameters. 3. In contrast to results obtained with moving stimuli, we find that the bright and dark excitatory subregions in simple receptive fields do not overlap to any great extent. This difference in results may be attributed to confounding the independent variables space and time when using moving stimuli. 4. All simple receptive fields have subregions that vary smoothly in all directions in space. There are no sharp transitions either between excitatory subregions or between subregions and the area surrounding the receptive field. 5. Simple receptive fields vary both in the number of subregions observed, in the elongation of each subregion, and in the overall elongation of the field. In contrast with results obtained using moving stimuli, we find that subregions within a given receptive field need not be the same length. 6. The hypothesis that simple receptive fields can be modeled as either even symmetric or odd symmetric about a central axis is evaluated. This hypothesis is found to be false in general. Most simple receptive fields are neither even symmetric nor odd symmetric. 7. The hypothesis that simple receptive fields can be modeled as the product of a width response profile and an orthogonal length response profile (Cartesian separability) is evaluated. This hypothesis is found to be true for only approximately 50% of the cells in our sample."
            },
            "slug": "The-two-dimensional-spatial-structure-of-simple-in-Jones-Palmer",
            "title": {
                "fragments": [],
                "text": "The two-dimensional spatial structure of simple receptive fields in cat striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A reverse correlation method is developed that allows quantitative determination of visual receptive-field structure in two spatial dimensions and it is demonstrated that thereverse correlation method yields results with several desirable properties, including convergence and reproducibility independent of modest changes in stimulus parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14824508"
                        ],
                        "name": "Donald H. Foley",
                        "slug": "Donald-H.-Foley",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Foley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald H. Foley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 112
                            }
                        ],
                        "text": "Some authors have addressed the impact on required training samples of the dimensionality of the input features [3, 9, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29377844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8823ea9fe521258d78e09584ab9f8d5b8d05cfcd",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In many practical pattern-classification problems the underlying probability distributions are not completely known. Consequently, the classification logic must be determined on the basis of vector samples gathered for each class. Although it is common knowledge that the error rate on the design set is a biased estimate of the true error rate of the classifier, the amount of bias as a function of sample size per class and feature size has been an open question. In this paper, the design-set error rate for a two-class problem with multivariate normal distributions is derived as a function of the sample size per class (N) and dimensionality (L) . The design-set error rate is compared to both the corresponding Bayes error rate and the test-set error rate. It is demonstrated that the design-set error rate is an extremely biased estimate of either the Bayes or test-set error rate if the ratio of samples per class to dimensions (N/L) is less than three. Also the variance of the design-set error rate is approximated by a function that is bounded by 1/8N ."
            },
            "slug": "Considerations-of-sample-and-feature-size-Foley",
            "title": {
                "fragments": [],
                "text": "Considerations of sample and feature size"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The design-set error rate for a two-class problem with multivariate normal distributions is derived as a function of the sample size per class (N) and dimensionality (L) and is demonstrated to be an extremely biased estimate of either the Bayes or test- set error rate."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984258"
                        ],
                        "name": "\u00c9. Bardinet",
                        "slug": "\u00c9.-Bardinet",
                        "structuredName": {
                            "firstName": "\u00c9ric",
                            "lastName": "Bardinet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9. Bardinet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049705"
                        ],
                        "name": "L. Cohen",
                        "slug": "L.-Cohen",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144827643"
                        ],
                        "name": "N. Ayache",
                        "slug": "N.-Ayache",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Ayache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ayache"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 160
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16173213,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "194f79547c6d14f15b3bfff8224445f0bd801366",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to surface tracking applied to 3D medical data with a deformable model. It is based on a parametric model composed of a superquadric fit followed by a Free-Form Deformation (FFD), that gives a compact representation of a set of points in a 3D image. We present three different approaches to track surfaces in a sequence of 3D cardiac images. From the tracking, we infer quantitative parameters which are useful for the physician, like the ejection fraction, the variation of the heart wall thickness and of the volume during a cardiac cycle or the torsion component in the deformation of the ventricle. Experimental results are shown for automatic shape tracking and motion analysis of a time sequence of Nuclear Medicine images."
            },
            "slug": "Tracking-Medical-3D-Data-with-a-Deformable-Model-Bardinet-Cohen",
            "title": {
                "fragments": [],
                "text": "Tracking Medical 3D Data with a Deformable Parametric Model"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new approach to surface tracking applied to 3D medical data with a deformable model based on a parametric model composed of a superquadric fit followed by a Free-Form Deformation that gives a compact representation of a set of points in a 3D image."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705627"
                        ],
                        "name": "J. Aggarwal",
                        "slug": "J.-Aggarwal",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aggarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851641"
                        ],
                        "name": "Q. Cai",
                        "slug": "Q.-Cai",
                        "structuredName": {
                            "firstName": "Quin",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33832634"
                        ],
                        "name": "W. Liao",
                        "slug": "W.-Liao",
                        "structuredName": {
                            "firstName": "Wen-Hung",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687650"
                        ],
                        "name": "B. Sabata",
                        "slug": "B.-Sabata",
                        "structuredName": {
                            "firstName": "Bikash",
                            "lastName": "Sabata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sabata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "A detailed survey of tracking algorithms for non-rigid motion can be found in [1], but there appears to be very little appropriate literature on the tracking of text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27611564,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "62c4254429c5397fd792772f2d52b698a7c82fbe",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "Motion of physical objects in the world is, in general, nonrigid. In robotics and computer vision, the motion of nonrigid objects is of growing interest to researchers from a wide spectrum of disciplines. The nonrigid objects being studied may be generally categorized into three groups according to the degree of deformation of body parts: articulated, elastic, and fluid. In articulated motion, individual rigid parts of an object move independently of one another and the motion of the whole object is nonrigid in nature. Elastic motion is nonrigid motion that conforms to a certain degree of continuity or smoothness. Fluid motion violates even the continuity assumption and may involve topological variations and turbulent deformations. This paper presents an overview of existing work on articulated and elastic motion, motivated by problems relating to the motion of the human body and of an animal heart, respectively. We study various approaches for recovering the 3D structure and motion of objects through a sequence of images in a bottom-up fashion, a strategy widely employed by various investigators. These approaches are classified as (1) motion recovery without shape models, and (2) model-based analysis. In the discussion of each algorithm, we also include a description of the complexity of feature and motion constraints, which are highly related to each other."
            },
            "slug": "Nonrigid-Motion-Analysis:-Articulated-and-Elastic-Aggarwal-Cai",
            "title": {
                "fragments": [],
                "text": "Nonrigid Motion Analysis: Articulated and Elastic Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents an overview of existing work on articulated and elastic motion, motivated by problems relating to the motion of the human body and of an animal heart, respectively."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693897"
                        ],
                        "name": "P. Shi",
                        "slug": "P.-Shi",
                        "structuredName": {
                            "firstName": "Pengcheng",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39559062"
                        ],
                        "name": "G. Robinson",
                        "slug": "G.-Robinson",
                        "structuredName": {
                            "firstName": "Glynn",
                            "lastName": "Robinson",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962954"
                        ],
                        "name": "R. Constable",
                        "slug": "R.-Constable",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Constable",
                            "middleNames": [
                                "Todd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Constable"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729449"
                        ],
                        "name": "A. Sinusas",
                        "slug": "A.-Sinusas",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Sinusas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sinusas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145947161"
                        ],
                        "name": "J. Duncan",
                        "slug": "J.-Duncan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Duncan",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Duncan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 160
                            }
                        ],
                        "text": "The literature on object tracking is extensive and includes face tracking [4, 14, 21], human body tracking [18, 49, 55], vehicle tracking [16], medical imaging [2, 52] and agricultural automation [23, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17382232,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e6b0e776067263f26c5485a015cc4185e2fcd553",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Accurate estimation of heart wall dense field motion and deformation could help to better understand the physiological processes associated with ischemic heart diseases, and to provide significant improvement in patient treatment. We present a new method of estimating left ventricular deformation which integrates instantaneous velocity information obtained within the mid-wall region with shape information found on the boundaries of the left ventricle. Velocity information is obtained from phase contrast magnetic resonance images, and boundary information is obtained from shape-based motion tracking of the endo- and cardial boundaries. The integration takes place within a continuum biomechanical heart model which is embedded in a finite element framework. We also employ a feedback mechanism to improve tracking accuracy. The integration of the two disparate but complementary sources overcomes some of the limitations of previous work in the field which concentrates on motion estimation from a single image-derived source.<<ETX>>"
            },
            "slug": "A-model-based-integrated-approach-to-track-using-Shi-Robinson",
            "title": {
                "fragments": [],
                "text": "A model-based integrated approach to track myocardial deformation using displacement and velocity constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new method of estimating left ventricular deformation which integrates instantaneous velocity information obtained within the mid-wall region with shape information found on the boundaries of the left ventricle is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "i2N 1;0 i2N 1;1 i2N 1;2N 1 3777777752N 2N (4) we can use Mallat's algorithm [37] to obtain the two-dimensional Haar wavelet transform of I(x; y):LLx;y = 1 4 1 X k1;k2=0 pk1pk2 ik1+2x;k2+2y = 1 4(i2x;2y + i2x;2y+1 + i2x+1;2y + i2x+1;2y+1) (5) LHx;y = 14 1 X k1;k2=0 pk1qk2 ik1+2x;k2+2y = 14(i2x;2y i2x;2y+1 + i2x+1;2y i2x+1;2y+1) (6) HLx;y = 14 1 X k1;k2=0 qk1pk2 ik1+2x;k2+2y = 14(i2x;2y + i2x;2y+1 i2x+1;2y i2x+1;2y+1) (7) HHx;y = 14 1 X k1;k2=0 qk1qk2 ik1+2x;k2+2y = 14(i2x;2y i2x;2y+1 i2x+1;2y + i2x+1;2y+1) (8) The image decomposition process is equavalent to the convolution of the image I(x; y) with the kernels in Figure 6."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1136690,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fd9fb6a24c5d85f52bb44fb1e4b951bea6234df1",
            "isKey": false,
            "numCitedBy": 2572,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A multiresolution approximation is a sequence of embedded vector spaces \uf8f1 \uf8f3 V j \uf8fc \uf8fejmember Z for approximating L 2 (R) functions. We study the properties of a multiresolution approximation and prove that it is characterized by a 2\u03c0 periodic function which is further described. From any multiresolution approximation, we can derive a function \u03c8(x) called a wavelet such that \uf8f1 \uf8f3 \u221a\uf8e5 \uf8e5 2 j \u03c8(2 j x \u2212k) \uf8fc \uf8fe (k ,j)member Z 2 is an orthonormal basis of L 2 (R). This provides a new approach for understanding and computing wavelet orthonormal bases. Finally, we characterize the asymptotic decay rate of multiresolution approximation errors for functions in a Sobolev space H s ."
            },
            "slug": "Multiresolution-approximations-and-wavelet-bases-of-Mallat",
            "title": {
                "fragments": [],
                "text": "Multiresolution approximations and wavelet orthonormal bases of L^2(R)"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is proved that a multiresolution approximation is characterized by a 2\u03c0 periodic function which is further described and provides a new approach for understanding and computing wavelet orthonormal bases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 112
                            }
                        ],
                        "text": "Some authors have addressed the impact on required training samples of the dimensionality of the input features [3, 9, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 112
                            }
                        ],
                        "text": "Some authors have addressed the impact on required training samples of the dimensionality of the input features [3, 9, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18251470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "445ad69010658097fc317f7b83f1198179eebae8",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces."
            },
            "slug": "Geometrical-and-Statistical-Properties-of-Systems-Cover",
            "title": {
                "fragments": [],
                "text": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50591689"
                        ],
                        "name": "B. S. Manjunath",
                        "slug": "B.-S.-Manjunath",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Manjunath",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Manjunath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1872163"
                        ],
                        "name": "T. Simchony",
                        "slug": "T.-Simchony",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Simchony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Simchony"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 51
                            }
                        ],
                        "text": "The success of neural networks in related problems [8, 11, 13, 39, 48] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59895075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18a50cec692927f91eb56f09d98539ac76cda1e9",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Texture-segmentation-with-neural-networks-Chellappa-Manjunath",
            "title": {
                "fragments": [],
                "text": "Texture segmentation with neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "The second approach is texture-based and uses well-known texture analysis methods such as Gabor ltering [25], Gaussian ltering [56] or spatial variance [58] to locate text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 189
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35696139,
            "fieldsOfStudy": [],
            "id": "d68b95534860e2bddd17d17ef7f362d16c550bde",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144562609"
                        ],
                        "name": "Marc Davis",
                        "slug": "Marc-Davis",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42486402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1037379625b80e753179e275c12b0cb7b12affd",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Media-streams-(demonstration):-representing-video-Davis",
            "title": {
                "fragments": [],
                "text": "Media streams (demonstration): representing video for retrieval and repurposing"
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '94"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 149
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing whole words as single symbols"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings  of ICDAR,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 236
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Voice signal processing and representation techniques for speech recognition  in noisy environments"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 236
                            }
                        ],
                        "text": "For example, there has been tremendous success in the automatic conversion of hard-copy documents via optical character recognition (OCR) technology [20, 22, 35, 41] and the transcription of speech via voice recognition (VR) technology [24, 43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Voice signal processing and representation techniques for speech recognition  in noisy environments"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Riseman, \\Finding text in images"
            },
            "venue": {
                "fragments": [],
                "text": "DL'97: Proceedings of the 2nd ACM International Conference on Digital Libraries Images and Multimedia"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "2 Related Work In related domains there has been work on the extraction of text from road signs [32, 44], license plates [10, 29, 31], library books [19], WWW images [59, 60], scene images [30, 42, 57, 58] and isolated video frames [33, 36, 53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Abramatic. The telesign project"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "12 \\Welcome to the Language and Media Processing Laboratory"
            },
            "venue": {
                "fragments": [],
                "text": "12 \\Welcome to the Language and Media Processing Laboratory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction and tracking in digital video,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "Most related previous work has focused on the extraction of graphic text [4], [5], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient automatic text location method and content-based indexing and structuring of video"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Visual Communication and Image Representation,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kabrisky . Recognizing whole words as single symbols"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedingsof ICDAR"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "One application of our text tracking tool TextTracker is to ground truthing of video data; it is being used for this purpose in the Video Processing Evaluation Resource (ViPER) [12] project in our lab."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Video performance evaluation"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, University of Maryland,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 142
                            }
                        ],
                        "text": "We feel that using supervised learning to classify text and nontext will be more e ective than the unsupervised clustering techniques used in [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 150
                            }
                        ],
                        "text": "Text Detection in Video Frames In the literature text detection methods are typically either connected component (CC) based [4], [8] or texture based [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding text in images,\" in DL'97"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2nd ACM International  Conference on Digital Libraries,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Traditionally, content has been indexed primarily by manual annotation [1], closed caption [2] or transcribed audio [3], but some work has also been done on the content analysis of the video itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "VISION: A digital video library,\" in DL'96"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1st ACM  International Conference on Digital Libraries,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Belhumeur, \\EEcient region tracking with parametric models of geometry and illumination"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. PAMI"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognizing whole words as single symbols"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICDAR"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "The success of neural networks in related problems [11], [12], [13] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simchony, \\Texture segmentation with neural networks,\" in Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing, pp"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "For the text detection task, we can keep the result in the form of blocks since it will bene t image resolution enhancement [34] and OCR (multiple text lines can provide more context information)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text extraction and recognition in digital video"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of DAS,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "The success of neural networks in related problems [11], [12], [13] provides us with further motivation to rely on a neural network as a classi er to identify text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gainey, \\Neural networks for automatic target recognition,"
            },
            "venue": {
                "fragments": [],
                "text": "Neural  Networks,"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 54,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 75,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-text-detection-and-tracking-in-digital-Li-Doermann/f8f5c282dc11937d29183b955dc3e4fbb677571b?sort=total-citations"
}