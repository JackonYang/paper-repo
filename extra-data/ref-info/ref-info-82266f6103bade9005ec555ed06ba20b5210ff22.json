{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 134
                            }
                        ],
                        "text": "Gaussian processes models are closel y related to approaches such as Support Vector Machines, and specially Relevance Vector Machines [1] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "Gaussian processes models are closely related to approaches such as Support Vector Machines, and specially Relevance Vector Machines [1]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52807317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "554894f70b28dba58b396c2d84080ac01051261b",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other \"kernel machines\" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided."
            },
            "slug": "Gaussian-Processes-For-Machine-Learning-Seeger",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes For Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning and shows up precise connections to other \"kernel machines\" popular in the community."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116281095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d3b01a9ce510c80c72a31595045bb40844e404a",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks such as multilayer perceptrons are popular tools for nonlinear regression and classification problems. From a Bayesian perspective, a choice of a neural network model can be viewed as defining a prior probability distribution over non-linear functions, and the neural network's learning process can be interpreted in terms of the posterior probability distribution over the unknown function. (Some learning algorithms search for the function with maximum posterior probability and other Monte Carlo methods draw samples from this posterior probability). In the limit of large but otherwise standard networks, Neal (1996) has shown that the prior distribution over non-linear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes. The hyperparameters of the neural network model determine the characteristic length scales of the Gaussian process. Neal's observation motivates the idea of discarding parameterized networks and working directly with Gaussian processes. Computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the Gaussian process. In this chapter I will review work on this idea by Williams and Rasmussen (1996), Neal (1997), Barber and Williams (1997) and Gibbs and MacKay (1997), and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded."
            },
            "slug": "Introduction-to-Gaussian-processes-Mackay",
            "title": {
                "fragments": [],
                "text": "Introduction to Gaussian processes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter will assess whether the feedforward network has been superceded, for supervised regression and classification tasks, and will review work on this idea by Williams and Rasmussen (1996), Neal (1997), Barber and Williams (1997) and Gibbs and MacKay (1997)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12672378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60388818aa7faf07945d53292a21d3efa2ea841e",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 201,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-parametric models and techniques enjoy a growing popularity in the field of machine learning, and among these Bayesian inference for Gaussian process (GP) models has recently received significant attention. We feel that GP priors should be part of the standard toolbox for constructing models relevant to machine learning in the same way as parametric linear models are, and the results in this thesis help to remove some obstacles on the way towards this goal. In the first main chapter, we provide a distribution-free finite sample bound on the difference between generalisation and empirical (training) error for GP classification methods. While the general theorem (the PAC-Bayesian bound) is not new, we give a much simplified and somewhat generalised derivation and point out the underlying core technique (convex duality) explicitly. Furthermore, the application to GP models is novel (to our knowledge). A central feature of this bound is that its quality depends crucially on task knowledge being encoded faithfully in the model and prior distributions, so there is a mutual benefit between a sharp theoretical guarantee and empirically well-established statistical practices. Extensive simulations on real-world classification tasks indicate an impressive tightness of the bound, in spite of the fact that many previous bounds for related kernel machines fail to give non-trivial guarantees in this practically relevant regime. In the second main chapter, sparse approximations are developed to address the problem of the unfavourable scaling of most GP techniques with large training sets. Due to its high importance in practice, this problem has received a lot of attention recently. We demonstrate the tractability and usefulness of simple greedy forward selection with information-theoretic criteria previously used in active learning (or sequential design) and develop generic schemes for automatic model selection with many (hyper)parameters. We suggest two new generic schemes and evaluate some of their variants on large real-world classification and regression tasks. These schemes and their underlying principles (which are clearly stated and analysed) can be applied to obtain sparse approximations for a wide regime of GP models far beyond the special cases we studied here."
            },
            "slug": "Bayesian-Gaussian-process-models-:-PAC-Bayesian-and-Seeger",
            "title": {
                "fragments": [],
                "text": "Bayesian Gaussian process models : PAC-Bayesian generalisation error bounds and sparse approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The tractability and usefulness of simple greedy forward selection with information-theoretic criteria previously used in active learning is demonstrated and generic schemes for automatic model selection with many (hyper)parameters are developed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234984"
                        ],
                        "name": "R. Herbrich",
                        "slug": "R.-Herbrich",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Herbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herbrich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 410,
                                "start": 176
                            }
                        ],
                        "text": "Importantly, there are efficient methods using suffix trees that can compute a string kernel k(x, x\u2032) in time linear in |x|+ |x\u2032| (with some restrictions on the weights {ws}) [Leslie et al., 2003, Vishwanathan and Smola, 2003]. Work on string kernels was started by Watkins [1999] and Haussler [1999]. There are many further developments of the methods we have described above; for example Lodhi et al. [2001] go beyond substrings to consider subsequences of x which are not necessarily contiguous, and Leslie et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 176
                            }
                        ],
                        "text": "Importantly, there are efficient methods using suffix trees that can compute a string kernel k(x, x\u2032) in time linear in |x|+ |x\u2032| (with some restrictions on the weights {ws}) [Leslie et al., 2003, Vishwanathan and Smola, 2003]. Work on string kernels was started by Watkins [1999] and Haussler [1999]. There are many further developments of the methods we have described above; for example Lodhi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 176
                            }
                        ],
                        "text": "Importantly, there are efficient methods using suffix trees that can compute a string kernel k(x, x\u2032) in time linear in |x|+ |x\u2032| (with some restrictions on the weights {ws}) [Leslie et al., 2003, Vishwanathan and Smola, 2003]. Work on string kernels was started by Watkins [1999] and Haussler [1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 524,
                                "start": 176
                            }
                        ],
                        "text": "Importantly, there are efficient methods using suffix trees that can compute a string kernel k(x, x\u2032) in time linear in |x|+ |x\u2032| (with some restrictions on the weights {ws}) [Leslie et al., 2003, Vishwanathan and Smola, 2003]. Work on string kernels was started by Watkins [1999] and Haussler [1999]. There are many further developments of the methods we have described above; for example Lodhi et al. [2001] go beyond substrings to consider subsequences of x which are not necessarily contiguous, and Leslie et al. [2003] describe mismatch string kernels which allow substrings s and s\u2032 of x and x\u2032 respectively to match if there are at most m mismatches between them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12555103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde4bf50d48d54ba913d574c849566dc4f3d4fde",
            "isKey": true,
            "numCitedBy": 553,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on information-theoretic principles, previously suggested for active learning. Our goal is not only to learn d-sparse predictors (which can be evaluated in O(d) rather than O(n), d \u226a n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n \u00b7 d2), and in large real-world classification experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be significantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities ('error bars'), allows for Bayesian model selection and is less complex in implementation."
            },
            "slug": "Fast-Sparse-Gaussian-Process-Methods:-The-Vector-Lawrence-Seeger",
            "title": {
                "fragments": [],
                "text": "Fast Sparse Gaussian Process Methods: The Informative Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on information-theoretic principles, which allows for Bayesian model selection and is less complex in implementation is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "relevance vectors Empirically it is often observed that the number of relevance vectors is smaller than the number of support vectors on the same problem [Tipping, 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section we present a different kind of analysis within the probably approximately correct (PAC) PAC framework due to Valiant [1984]. Seeger [2002; 2003] has presented a PACBayesian analysis of generalization in Gaussian process classifiers and we get to this in a number of stages; we first present an introduction to the PAC framework (section 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although usually not presented as such, the relevance vector machine (RVM) introduced by Tipping [2001] is actually a special case of a Gaussian process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The original RVM algorithm [Tipping, 2001] was not able to exploit the sparsity very effectively during model fitting as it was initialized with all of the \u03b1is set to finite values, meaning that all of the basis functions contributed to the model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7596571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bff76c25f7c416834655ba664553b14eb67a11c",
            "isKey": true,
            "numCitedBy": 1629,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning."
            },
            "slug": "Sparse-Bayesian-Learning-and-the-Relevance-Vector-Tipping",
            "title": {
                "fragments": [],
                "text": "Sparse Bayesian Learning and the Relevance Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that by exploiting a probabilistic Bayesian learning framework, the 'relevance vector machine' (RVM) can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14632283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38bdbf7cf0572732bd21b299bdbaf2aab8da959d",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximate Bayesian Gaussian process (GP) classification techniques are powerful non-parametric learning methods, similar in appearance and performance to support vector machines. Based on simple probabilistic models, they render interpretable results and can be embedded in Bayesian frameworks for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we prove distribution-free generalisation error bounds for a wide range of approximate Bayesian GP classification techniques. We also provide a new and much simplified proof for this powerful theorem, making use of the concept of convex duality which is a backbone of many machine learning techniques. We instantiate and test our bounds for two particular GPC techniques, including a recent sparse method which circumvents the unfavourable scaling of standard GP algorithms. As is shown in experiments on a real-world task, the bounds can be very tight for moderate training sample sizes. To the best of our knowledge, these results provide the tightest known distribution-free error bounds for approximate Bayesian GPC methods, giving a strong learning-theoretical justification for the use of these techniques."
            },
            "slug": "PAC-Bayesian-Generalisation-Error-Bounds-for-Seeger",
            "title": {
                "fragments": [],
                "text": "PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By applying the PAC-Bayesian theorem of McAllester (1999a), this paper proves distribution-free generalisation error bounds for a wide range of approximate Bayesian GP classification techniques, giving a strong learning-theoretical justification for the use of these techniques."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 207
                            }
                        ],
                        "text": "One can resort to approximations, such as the Laplace approximation [3], or approximations based on projecting the non-Gaussian posterior onto the closest Gaussian (in a KL sense) [4] or sampling techniques [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118922677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6328aaad2b9d4de508868db1de7d081d341efc14",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are a natural way of specifying prior distributions over functions of one or more input variables. When such a function defines the mean response in a regression model with Gaussian errors, inference can be done using matrix computations, which are feasible for datasets of up to about a thousand cases. The covariance function of the Gaussian process can be given a hierarchical prior, which allows the model to discover high-level properties of the data, such as which inputs are relevant to predicting the response. Inference for these covariance hyperparameters can be done using Markov chain sampling. Classification models can be defined using Gaussian processes for underlying latent values, which can also be sampled within the Markov chain. Gaussian processes are in my view the simplest and most obvious way of defining flexible Bayesian regression and classification models, but despite some past usage, they appear to have been rather neglected as a general-purpose technique. This may be partly due to a confusion between the properties of the function being modeled and the properties of the best predictor for this unknown function."
            },
            "slug": "Regression-and-Classification-Using-Gaussian-Priors-Neal",
            "title": {
                "fragments": [],
                "text": "Regression and Classification Using Gaussian Process Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Gaussian processes are in my view the simplest and most obvious way of defining flexible Bayesian regression and classification models, but despite some past usage, they appear to have been rather neglected as a general-purpose technique."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845488"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16700850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2f80c32e5b922a634991ea89bcf31eee2f82a4e",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are a natural way of specifying prior distributions over functions of one or more input variables. When such a function defines the mean response in a regression model with Gaussian errors, inference can be done using matrix computations, which are feasible for datasets of up to about a thousand cases. The covariance function of the Gaussian process can be given a hierarchical prior, which allows the model to discover high-level properties of the data, such as which inputs are relevant to predicting the response. Inference for these covariance hyperparameters can be done using Markov chain sampling. Classification models can be defined using Gaussian processes for underlying latent values, which can also be sampled within the Markov chain. Gaussian processes are in my view the simplest and most obvious way of defining flexible Bayesian regression and classification models, but despite some past usage, they appear to have been rather neglected as a general-purpose technique. This may be partly due to a confusion between the properties of the function being modeled and the properties of the best predictor for this unknown function."
            },
            "slug": "Regression-and-Classification-Using-Gaussian-Priors-Berger-Dawid",
            "title": {
                "fragments": [],
                "text": "Regression and Classification Using Gaussian Process Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Gaussian processes are in my view the simplest and most obvious way of defining flexible Bayesian regression and classification models, but despite some past usage, they appear to have been rather neglected as a general-purpose technique."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More comprehensive treatments can be found in Vapnik [1995], Cristianini and Shawe-Taylor [2000] and Sch\u00f6lkopf and Smola [2002]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60486887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d86e239a9e9741f22be1d8c1feed7a44da1bdc1",
            "isKey": false,
            "numCitedBy": 3135,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. The book also introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc. Their first introduction in the early 1990s lead to a recent explosion of applications and deepening theoretical analysis, that has now established Support Vector Machines along with neural networks as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and application of these techniques. The concepts are introduced gradually in accessible and self-contained stages, though in each stage the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally the book will equip the practitioner to apply the techniques and an associated web site will provide pointers to updated literature, new applications, and on-line software."
            },
            "slug": "An-introduction-to-Support-Vector-Machines-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This book is the first comprehensive introduction to Support Vector Machines, a new generation learning system based on recent advances in statistical learning theory, and introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Rasmussen and Ghahramani [2002] used Gaussian process models as local experts, and based their manager on another type of stochastic process: the Dirichlet process."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "Williams and Rasmussen [1996]. We call the parameterization of M3 in eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16685561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f49a73c42be6dbd851af4599d9911ea1d6ac7f4",
            "isKey": false,
            "numCitedBy": 495,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis develops two Bayesian learning methods relying on Gaussian processes and a rigorous statistical approach for evaluating such methods. In these experimental designs the sources of uncertainty in the estimated generalisation performances due to both variation in training and test sets are accounted for. The framework allows for estimation of generalisation performance as well as statistical tests of significance for pairwise comparisons. Two experimental designs are recommended and supported by the DELVE software environment. \nTwo new non-parametric Bayesian learning methods relying on Gaussian process priors over functions are developed. These priors are controlled by hyperparameters which set the characteristic length scale for each input dimension. In the simplest method, these parameters are fit from the data using optimization. In the second, fully Bayesian method, a Markov chain Monte Carlo technique is used to integrate over the hyperparameters. One advantage of these Gaussian process methods is that the priors and hyperparameters of the trained models are easy to interpret. \nThe Gaussian process methods are benchmarked against several other methods, on regression tasks using both real data and data generated from realistic simulations. The experiments show that small datasets are unsuitable for benchmarking purposes because the uncertainties in performance measurements are large. A second set of experiments provide strong evidence that the bagging procedure is advantageous for the Multivariate Adaptive Regression Splines (MARS) method. \nThe simulated datasets have controlled characteristics which make them useful for understanding the relationship between properties of the dataset and the performance of different methods. The dependency of the performance on available computation time is also investigated. It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time. The Gaussian process methods are shown to consistently outperform the more conventional methods."
            },
            "slug": "Evaluation-of-gaussian-processes-and-other-methods-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Evaluation of gaussian processes and other methods for non-linear regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a Bayesian approach to learning in multi-layer perceptron neural networks achieves better performance than the commonly used early stopping procedure, even for reasonably short amounts of computation time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401755685"
                        ],
                        "name": "Joaquin Qui\u00f1onero-Candela",
                        "slug": "Joaquin-Qui\u00f1onero-Candela",
                        "structuredName": {
                            "firstName": "Joaquin",
                            "lastName": "Qui\u00f1onero-Candela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joaquin Qui\u00f1onero-Candela"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Alternatively, Qui\u00f1onero-Candela [2004] suggests using the approximate log marginal likelihood log pSR(y|X) (see eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Here, we have argued that for localized basis functions, the RVM has undesirable properties, but as argued in Rasmussen and Qui\u00f1onero-Candela [2005] it is actually the degeneracy of the covariance function which is the core of the problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60141962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9dbf08ecd426b9967c920b43086c8b2f66c651ae",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The Relevance Vector Machine (RVM) introduced by Tipping is a probabilistic model similar to the widespread Support Vector Machines (SVM), but where the training takes place in a Bayesian framework, and where predictive distributions of the outputs instead of point estimates are obtained. In this paper we focus on the use of RVM\u2019s for regression. We modify this method for training generalized linear models by adapting automatically the width of the basis functions to the optimal for the data at hand. Our Adaptive RVM is tried for prediction on the chaotic Mackey-Glass time series. Much superior performance than with the standard RVM and than with other methods like neural networks and local linear models is obtained."
            },
            "slug": "Learning-with-Uncertainty:-Gaussian-Processes-and-Qui\u00f1onero-Candela",
            "title": {
                "fragments": [],
                "text": "Learning with Uncertainty: Gaussian Processes and Relevance Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper modify this method for training generalized linear models by adapting automatically the width of the basis functions to the optimal for the data at hand, and tries the Adaptive RVM for prediction on the chaotic Mackey-Glass time series."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675811"
                        ],
                        "name": "Sayan Mukherjee",
                        "slug": "Sayan-Mukherjee",
                        "structuredName": {
                            "firstName": "Sayan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayan Mukherjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 47
                            }
                        ],
                        "text": "An interesting idea suggested by Rasmussen and Qui\u00f1onero-Candela [2005] to mitigate this problem is to define the SR model with m + 1 basis functions, where the extra basis function is centered on the test point x\u2217, so that ySR\u2217(x\u2217) = \u2211m i=1 \u03b1ik(x\u2217,xi) + \u03b1\u2217k(x\u2217,x\u2217)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6244043,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1f078c4e39c21e94eec26a20a6efc0ee67aa436d",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines Regression (SVMR) is a learning technique where the goodness of fit is measured not by the usual quadratic loss function (the mean square error), but by a different loss function called the \u0190-Insensitive Loss Function (ILF), which is similar to loss functions used in the field of robust statistics. The quadratic loss function is well justified under the assumption of Gaussian additive noise. However, the noise model underlying the choice of the ILF is not clear. In this paper the use of the ILF is justified under the assumption that the noise is additive and Gaussian, where the variance and mean of the Gaussian are random variables. The probability distributions for the variance and mean will be stated explicitly. While this work is presented in the framework of SVMR, it can be extended to justify nonquadratic loss functions in any Maximum Likelihood or Maximum AP osteriori approach. It applies not only to the ILF, but to a much broader class of loss functions."
            },
            "slug": "On-the-Noise-Model-of-Support-Vector-Machines-Pontil-Mukherjee",
            "title": {
                "fragments": [],
                "text": "On the Noise Model of Support Vector Machines Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work can be extended to justify nonquadratic loss functions in any Maximum Likelihood or Maximum AP osteriori approach, and applies not only to the ILF, but to a much broader class of loss functions."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16378222,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9bfe7080107d3bdc21bd937593f91932ea40a524",
            "isKey": false,
            "numCitedBy": 471,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are a natural way of defining prior distributions over functions of one or more input variables. In a simple nonparametric regression problem, where such a function gives the mean of a Gaussian distribution for an observed response, a Gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases. Hyperparameters that define the covariance function of the Gaussian process can be sampled using Markov chain methods. Regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations. Software is now available that implements these methods using covariance functions with hierarchical parameterizations. Models defined in this way can discover high-level properties of the data, such as which inputs are relevant to predicting the response."
            },
            "slug": "Monte-Carlo-Implementation-of-Gaussian-Process-for-Neal",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Software is now available that implements Gaussian process methods using covariance functions with hierarchical parameterizations, which can discover high-level properties of the data, such as which inputs are relevant to predicting the response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Compare this with the exact mean, given by Opper and Winther [2000] as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10063289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4323cf65589bc509aea260ccaa4ef32a94b2f7e",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a mean-field algorithm for binary classification with gaussian processes that is based on the TAP approach originally proposed in statistical physics of disordered systems. The theory also yields an approximate leave-one-out estimator for the generalization error, which is computed with no extra computational cost. We show that from the TAP approach, it is possible to derive both a simpler naive mean-field theory and support vector machines (SVMs) as limiting cases. For both mean-field algorithms and support vector machines, simulation results for three small benchmark data sets are presented. They show that one may get state-of-the-art performance by using the leave-one-out estimator for model selection and the built-in leave-one-out estimators are extremely precise when compared to the exact leave-one-out estimate. The second result is taken as strong support for the internal consistency of the mean-field approach."
            },
            "slug": "Gaussian-Processes-for-Classification:-Mean-Field-Opper-Winther",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Classification: Mean-Field Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A mean-field algorithm for binary classification with gaussian processes that is based on the TAP approach originally proposed in statistical physics of disordered systems is derived and an approximate leave-one-out estimator for the generalization error is computed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recent work by Meir and Zhang [2003] provides some PAC bounds directly for Bayesian algorithms (like the predictive classifier) whose predictions are made on the basis of a data-dependent posterior distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14929441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1b3100794bb86323869369a82a1d3fdc1c19754",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian approaches to learning and estimation have played a significant role in the Statistics literature over many years. While they are often provably optimal in a frequentist setting, and lead to excellent performance in practical applications, there have not been many precise characterizations of their performance for finite sample sizes under general conditions. In this paper we consider the class of Bayesian mixture algorithms, where an estimator is formed by constructing a data-dependent mixture over some hypothesis space. Similarly to what is observed in practice, our results demonstrate that mixture approaches are particularly robust, and allow for the construction of highly complex estimators, while avoiding undesirable overfitting effects. Our results, while being data-dependent in nature, are insensitive to the underlying model assumptions, and apply whether or not these hold. At a technical level, the approach applies to unbounded functions, constrained only by certain moment conditions. Finally, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Boosting and Bagging."
            },
            "slug": "Generalization-Error-Bounds-for-Bayesian-Mixture-Meir-Zhang",
            "title": {
                "fragments": [],
                "text": "Generalization Error Bounds for Bayesian Mixture Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper considers the class of Bayesian mixture algorithms, where an estimator is formed by constructing a data-dependent mixture over some hypothesis space, and demonstrates that mixture approaches are particularly robust, and allow for the construction of highly complex estimators, while avoiding undesirable overfitting effects."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686671"
                        ],
                        "name": "L. Csat\u00f3",
                        "slug": "L.-Csat\u00f3",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Csat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Csat\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Tanko, Identifikacija mo delov \u010drne skrinjice na podlagi Gaussovih procesov, IJS de lovno poro\u010dilo, 2010 [3] L."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11375333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df761a4db0be30776366024bf7ecaa60dd4d05b",
            "isKey": false,
            "numCitedBy": 714,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets. The method is based on a combination of a Bayesian on-line algorithm, together with a sequential construction of a relevant subsample of the data that fully specifies the prediction of the GP model. By using an appealing parameterization and projection techniques in a reproducing kernel Hilbert space, recursions for the effective parameters and a sparse gaussian approximation of the posterior process are obtained. This allows for both a propagation of predictions and Bayesian error measures. The significance and robustness of our approach are demonstrated on a variety of experiments."
            },
            "slug": "Sparse-On-Line-Gaussian-Processes-Csat\u00f3-Opper",
            "title": {
                "fragments": [],
                "text": "Sparse On-Line Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets is developed based on a combination of a Bayesian on-line algorithm and a sequential construction of a relevant subsample of data that fully specifies the prediction of the GP model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071649"
                        ],
                        "name": "Anton Schwaighofer",
                        "slug": "Anton-Schwaighofer",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Schwaighofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anton Schwaighofer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Tresp [2000] used a random assignment of data points to partitions but Schwaighofer and Tresp [2003] recommend that clustering the data (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "If we are interested in the predictive classifier sgn(q(f\u2217|x\u2217) \u2212 1/2) then Seeger [2002] shows that if q(f\u2217|x\u2217) is symmetric about its mean then the expected risk of the predictive classifier is less than twice the expected risk of the Gibbs classifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Schwaighofer and Tresp [2003] report some experimental comparisons between the BCM method and some other approximation methods for a number of synthetic regression problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2103027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1a2979ef688517ea2c1aadbfbb95cdd26d052b2",
            "isKey": true,
            "numCitedBy": 82,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we compare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian committee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves significantly better accuracy, yet at a higher computational cost."
            },
            "slug": "Transductive-and-Inductive-Methods-for-Approximate-Schwaighofer-Tresp",
            "title": {
                "fragments": [],
                "text": "Transductive and Inductive Methods for Approximate Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels and on complex low noise data sets, the Bayesian committee machine achieves significantly better accuracy, yet at a higher computational cost."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8358534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15a73f78788ffc8c735b46fbb1f572f7c47446a6",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse approximations to Bayesian inference for nonparametric Gaussian Process models scale linearly in the number of training points, allowing for the application of these powerful kernel-based models to large datasets. We show how to generalize the binary classification informative vector machine (IVM) (Lawrence et.al., 2002) to multiple classes. In contrast to earlier efficient approaches to kernel-based non-binary classification, our method is a principled approximation to Bayesian inference which yields valid uncertainty estimates and allows for hyperparameter adaption via marginal likelihood maximization. While most earlier proposals suggest fitting independent binary discriminants to heuristically chosen partitions of the data and combining these in a heuristic manner, our method operates jointly on the data for all classes. Crucially, we still achieve a linear scaling in both the number of classes and the number of training points."
            },
            "slug": "Sparse-Gaussian-Process-Classification-With-Classes-Seeger-Jordan",
            "title": {
                "fragments": [],
                "text": "Sparse Gaussian Process Classification With Multiple Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows how to generalize the binary classification informative vector machine (IVM) to multiple classes and is a principled approximation to Bayesian inference which yields valid uncertainty estimates and allows for hyperparameter adaption via marginal likelihood maximization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1452854346"
                        ],
                        "name": "Roderick Murray-Smith",
                        "slug": "Roderick-Murray-Smith",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "Murray-Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roderick Murray-Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38387373"
                        ],
                        "name": "A. Girard",
                        "slug": "A.-Girard",
                        "structuredName": {
                            "firstName": "Agathe",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Girard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Also, Murray-Smith and Girard [2001] have used an autoregressive moving-average (ARMA) noise model (see ARMA also eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13752731,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d87563c1ab5227451ca3225f705d7d1df04ef0f1",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the standard covariance function used in the Gaussian Process prior nonparametric modelling approach to include correlated (ARMA) noise models. The improvement in performance is illustrated on some simulation examples of data generated by nonlinear static functions corrupted with additive ARMA noise. 1 Gaussian Process priors In recent years many flexible parametric and semi-parametric approaches to empirical identification of nonlinear systems have been used. In this paper we use nonparametric models which retain the available data and perform inference conditional on the current state and local data (called \u2018smoothing\u2019 in some frameworks). This direct use of the data has potential advantages in many control contexts. The uncertainty of model predictions can be made dependent on local data density, and the model complexity is automatically related to the amount of available data (more complex models need more evidence to make them likely). The nonparametric model used in this paper is a Gaussian Process prior, as developed by O\u2019Hagan [1] and reviewed in [2, 3]. An application to modelling a system within a control context is described in [4], and further developments relating to their use in gain scheduling are described in [5]. Most previous published work has focused on regression tasks with independent identically distributed noise characteristics. Input-dependent noise is described in [6], but we are not aware of previous work with coloured noise covariance functions in Gaussian Process priors. This paper shows how knowledge about correlation structure of additive unmeasured noise or disturbances can be incorporated into the model. This improves the performance of the model in finding optimal parameters for describing the deterministic aspects of the system, and can be used to make online prediction more accurately. We expect this will make the use of Gaussian Process priors more attractive for use in control and signal processing contexts. 2 Modelling with GPs We assume that we are modelling an unknown nonlinear system f(x), with known inputs x, using observed outputs y. These have been corrupted by an additive discrete-time process (t). Here we assume that f(xi) and i are independent. Let y = [y1; : : : ; yN \u2104T , a set of observed data or targets be such that yi = f(xi) + i ; i = 1; : : : n (1) 2.1 The Gaussian Process prior approach A prior is placed directly on the space of functions for modelling the above system. We assume that the values of the function f(x) at inputs x1; : : : ; xn, outputs y1; : : : ; yn, constitute a set of random variables which we assume will have a joint n-dimensional multivariate Normal distribution. The Gaussian Process is then fully specified by its mean and covariance function C(xi; xj). We note (y1; : : : ; yn)T N (0; ); (2) where is the covariance matrix whose entries ij are given by C(xi; xj). We now have a prior distribution for the target values which is a multivariate Normal: p(yjx) = (2 ) n2 j j 1 2 exp 12yT 1y ; (3) 1See a standard text such as [7] for a discussion of disturbance models in the linear system identification context. 2In what follows, we assume a zero mean process."
            },
            "slug": "Gaussian-process-priors-with-ARMA-noise-models-Murray-Smith-Girard",
            "title": {
                "fragments": [],
                "text": "Gaussian process priors with ARMA noise models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145617808"
                        ],
                        "name": "D. Barber",
                        "slug": "D.-Barber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One can resort to approximations, such as the Laplace approximation [3], or approximations based on projecting the non-Gaussian posterior onto the closest Gaussian (in a KL sense) [4] or sampling techniques [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18841569,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f0ddbcb32e50514de5c89c8ceca58345c5a43948",
            "isKey": false,
            "numCitedBy": 769,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the probability of class one given x is estimated by /spl sigma/(y(x)), where /spl sigma/(y)=1/(1+e/sup -y/). A Gaussian process prior is placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation. The method is generalized to multiclass problems (m>2) using the softmax function. We demonstrate the effectiveness of the method on a number of datasets."
            },
            "slug": "Bayesian-Classification-With-Gaussian-Processes-Williams-Barber",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification With Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A Bayesian treatment is provided, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation, and the method is generalized to multiclass problems (m>2) using the softmax function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "in Seeger [2002]. Notice that this has reduced a rather scary infinite-dimensional integration to a more manageable n-dimensional integration; in the case that q(f |y) is Gaussian (as for the Laplace and EP approximations), this KL divergence can be computed using eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Seeger [2000].) Instead, the key idea in the EP algorithm is to update the individual ti approximations sequentially."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, there has also been some work on EP-type methods for the multi-class case, see Seeger and Jordan [2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Williams and Seeger [2001] suggested approximating the GPR equations by replacing the matrix K by K\u0303 in the mean and variance prediction equations 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We will not present the proof here, see Seeger [2005]. Consequently, we only have to take account of the explicit dependencies"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our presentation is based mainly on Seeger [2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1001206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69ef4c14c7cf129747a018861fa688aa8a5769b7",
            "isKey": true,
            "numCitedBy": 128,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a variational Bayesian method for model selection over families of kernels classifiers like Support Vector machines or Gaussian processes. The algorithm needs no user interaction and is able to adapt a large number of kernel parameters to given data without having to sacrifice training cases for validation. This opens the possibility to use sophisticated families of kernels in situations where the small \"standard kernel\" classes are clearly inappropriate. We relate the method to other work done on Gaussian processes and clarify the relation between Support Vector machines and certain Gaussian process models."
            },
            "slug": "Bayesian-Model-Selection-for-Support-Vector-and-Seeger",
            "title": {
                "fragments": [],
                "text": "Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A variational Bayesian method for model selection over families of kernels classifiers like Support Vector machines or Gaussian processes that needs no user interaction and is able to adapt a large number of kernel parameters to given data without having to sacrifice training cases for validation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 112
                            }
                        ],
                        "text": "Such a covariance function implements automatic relevance automatic relevance determination determination (ARD) [Neal, 1996], since the inverse of the length-scale determines how relevant an input is: if the length-scale has a very large value, the 1This contrasts the use of the word in the SVM literature, where \u201ctraining\u201d usually refers to finding the support vectors for a fixed kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 241
                            }
                        ],
                        "text": "In contrast to Gaussian process regression the marginal likelihood for a given ANN model is not analytically tractable, and thus approximation techniques such as the Laplace approximation [MacKay, 1992a] and Markov chain Monte Carlo methods [Neal, 1996] have to be used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 2
                            }
                        ],
                        "text": "10Neal [1999] refers to this as adding \u201cjitter\u201d in the context of Markov chain Monte Carlo (MCMC) based inference; in his work the latent variables f are explicitly represented in the Markov chain which makes addition of jitter difficult to avoid."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 48
                            }
                        ],
                        "text": "Denoting all weights by w, we obtain (following Neal [1996])"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "In this section we first describe the covariance function belonging to a particular type of neural network; this construction is due to Neal [1996]. Consider a network which takes an input x, has one hidden layer with NH units and then linearly combines the outputs of the hidden units with a bias b to obtain f(x)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": true,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60578841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c9529259e180dea589447d9b7414a998286e1c2",
            "isKey": false,
            "numCitedBy": 1466,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams."
            },
            "slug": "Learning-in-Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper presents an introduction to inference for Bayesian networks and a view of the EM algorithm that justifies incremental, sparse and other variants, as well as an information-theoretic analysis of hard and soft assignment methods for clustering."
            },
            "venue": {
                "fragments": [],
                "text": "NATO ASI Series"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720911"
                        ],
                        "name": "T. Kailath",
                        "slug": "T.-Kailath",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kailath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kailath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29483566,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4839029fff6b788aa2dd1f19f5ffa389d535050b",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "First it is shown how the Karhunen-Loeve approach to the detection of a deterministic signal can be given a coordinate-free and geometric interpretation in a particular Hilbert space of functions that is uniquely determined by the covariance function of the additive Gaussian noise. This Hilbert space, which is called a reproducing-kernel Hilbert space (RKHS), has many special properties that appear to make it a natural space of functions to associate with a second-order random process. A mapping between the RKHS and the linear Hilbert space of random variables generated by the random process is studied in some detail. This mapping enables one to give a geometric treatment of the detection problem. The relations to the usual integral-equation approach to this problem are also discussed. Some of the special properties of the RKHS are developed and then used to study the singularity and stability of the detection problem and also to suggest simple means of approximating the detectability of the signal. The RKHS for several multidimensional and multivariable processes is presented; by going to the RKHS of functionals rather than functions it is also shown how generalized random processes, including white noise and stationary processes whose spectra grow at infinity, are treated."
            },
            "slug": "RKHS-approach-to-detection-and-estimation-signals-Kailath",
            "title": {
                "fragments": [],
                "text": "RKHS approach to detection and estimation problems-I: Deterministic signals in Gaussian noise"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is shown how the Karhunen-Loeve approach to the detection of a deterministic signal can be given a coordinate-free and geometric interpretation in a particular Hilbert space of functions that is uniquely determined by the covariance function of the additive Gaussian noise."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849455"
                        ],
                        "name": "C. Paciorek",
                        "slug": "C.-Paciorek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Paciorek",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Paciorek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130768"
                        ],
                        "name": "M. Schervish",
                        "slug": "M.-Schervish",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Schervish",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schervish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Paciorek and Schervish [2004] have generalized Gibbs\u2019 construction to obtain non-stationary versions of arbitrary isotropic covariance functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7339083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "430962596e38e127e52ffe980e6be820e3fd091b",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matern stationary co-variance, in which the differentiability of the regression function is controlled by a parameter, freeing one from fixing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP fitting may allow for implementation of the method on larger datasets."
            },
            "slug": "Nonstationary-Covariance-Functions-for-Gaussian-Paciorek-Schervish",
            "title": {
                "fragments": [],
                "text": "Nonstationary Covariance Functions for Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art BayesianFree-k not spline model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144833733"
                        ],
                        "name": "S. Sundararajan",
                        "slug": "S.-Sundararajan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sundararajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sundararajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Following Szeliski [1987] and Poggio and Girosi [1990] we consider"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18871315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9ef5f8939fd4ec87534a43d75a3913b658c377f",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are powerful regression models specified by parameterized mean and covariance functions. Standard approaches to choose these parameters (known by the name hyperparameters) are maximum likelihood and maximum a posteriori. In this article, we propose and investigate predictive approaches based on Geisser's predictive sample reuse (PSR) methodology and the related Stone's cross-validation (CV) methodology. More specifically, we derive results for Geisser's surrogate predictive probability (GPP), Geisser's predictive mean square error (GPE), and the standard CV error and make a comparative study. Within an approximation we arrive at the generalized cross-validation (GCV) and establish its relationship with the GPP and GPE approaches. These approaches are tested on a number of problems. Experimental results show that these approaches are strongly competitive with the existing approaches."
            },
            "slug": "Predictive-Approaches-for-Choosing-Hyperparameters-Sundararajan-Keerthi",
            "title": {
                "fragments": [],
                "text": "Predictive Approaches for Choosing Hyperparameters in Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Predictive approaches based on Geisser's predictive sample reuse (PSR) methodology and the related Stone's cross-validation (CV) methodology are proposed and investigated and Experimental results show that these approaches are strongly competitive with the existing approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the coming sections, we describe two analytic approximations which both approximate the non-Gaussian joint posterior with a Gaussian one: the first is the straightforward Laplace approximation method [Williams and Barber, 1998], and the second is the more sophisticated expectation propagation (EP) method due to Minka [2001]. (The cavity TAP approximation of Opper and Winther [2000] is closely related to the EP method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12018209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4c47ebf6454e3c5a8417c580c8ecf694e34ad49",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models that include unknown hyperparameters such as regularization constants and noise levels. In the evidence framework, the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a gaussian approximation to the posterior distribution. In the alternative MAP method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a gaussian approximation is made. The similarities of the two approaches and their relative merits are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak, which causes signifi-cant biases to arise in the MAP method. In contrast, the evidence framework is shown to introduce negligible predictive error under straightforward conditions. General lessons are drawn concerning inference in many dimensions."
            },
            "slug": "Comparison-of-Approximate-Methods-for-Handling-Mackay",
            "title": {
                "fragments": [],
                "text": "Comparison of Approximate Methods for Handling Hyperparameters"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Two approximate methods for computational implementation of Bayesian hierarchical models that include unknown hyperparameters such as regularization constants and noise levels are examined, and the evidence framework is shown to introduce negligible predictive error under straightforward conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738441"
                        ],
                        "name": "P. Drineas",
                        "slug": "P.-Drineas",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Drineas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Drineas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143884206"
                        ],
                        "name": "Michael W. Mahoney",
                        "slug": "Michael-W.-Mahoney",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mahoney",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael W. Mahoney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recent work by Drineas and Mahoney [2005] analyzes a similar algorithm to the Nystr\u00f6m approximation, except that they use biased sampling with replacement (choosing column i of K with probability \u221d k ii) and a pseudoinverse of the inner m \u00d7m matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215012,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "11e6b5a30a921e6028662105148fac41a76f0500",
            "isKey": false,
            "numCitedBy": 935,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O(n 3 ), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n x n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the form G k = CW + k C T , where C is a matrix consisting of a small number c of columns of G and W k is the best rank-k approximation to W, the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let \u2225.\u2225 2 and \u2225.\u2225 F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let G k be the best rank-k approximation to G. We prove that by choosing O(k/\u2208 4 ) columns \u2225G-CW + k C T \u2225 \u03be \u2264 \u2225G - G k \u2225 \u03be + \u2208 n \u03a3 i=1 G 2 ii , both in expectation and with high probability, for both \u03be = 2, F, and for all k: 0 < k < rank(W). This approximation can be computed using O(n) additional space and time, after making two passes over the data from external storage."
            },
            "slug": "On-the-Nystr\u00f6m-Method-for-Approximating-a-Gram-for-Drineas-Mahoney",
            "title": {
                "fragments": [],
                "text": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm to compute an easily-interpretable low-rank approximation to an n x n Gram matrix G such that computations of interest may be performed more rapidly."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41192969"
                        ],
                        "name": "E. Solak",
                        "slug": "E.-Solak",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Solak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Solak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402170019"
                        ],
                        "name": "R. Murray-Smith",
                        "slug": "R.-Murray-Smith",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "Murray-Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Murray-Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34711979"
                        ],
                        "name": "W. Leithead",
                        "slug": "W.-Leithead",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Leithead",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Leithead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144576055"
                        ],
                        "name": "D. Leith",
                        "slug": "D.-Leith",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Leith",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Leith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "11)), as shown in Sollich [1999] or Opper and Vivarelli [1999]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 828951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48a5c48e2aa8e1613501bb6001ae893a7c85f47f",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identification of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference process. This derivative information can be in the form of priors specified by an expert or identified from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consistent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational efficiency of Gaussian process models for dynamic system identification, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size - traditionally a problem for Gaussian process models."
            },
            "slug": "Derivative-Observations-in-Gaussian-Process-Models-Solak-Murray-Smith",
            "title": {
                "fragments": [],
                "text": "Derivative Observations in Gaussian Process Models of Dynamic Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper improves dramatically the computational efficiency of Gaussian process models for dynamic system identification, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307712"
                        ],
                        "name": "Xiwu Lin",
                        "slug": "Xiwu-Lin",
                        "structuredName": {
                            "firstName": "Xiwu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiwu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607044"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057622896"
                        ],
                        "name": "Dong Xiang",
                        "slug": "Dong-Xiang",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Xiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2999112"
                        ],
                        "name": "Fangyu Gao",
                        "slug": "Fangyu-Gao",
                        "structuredName": {
                            "firstName": "Fangyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fangyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145434956"
                        ],
                        "name": "R. Klein",
                        "slug": "R.-Klein",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145518768"
                        ],
                        "name": "B. Klein",
                        "slug": "B.-Klein",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Klein",
                            "middleNames": [
                                "E.K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15650987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dd44b7f1652e56486b54a4f66328db54f425603",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the randomized Generalized Approximate Cross Validation (ranGACV) method for choosing multiple smoothing parameters in penalized likelihood estimates for Bernoulli data. The method is intended for application with penalized likelihood smoothing spline ANOVA models. In addition we propose a class of approximate numerical methods for solving the penalized likelihood variational problem which, in conjunction with the ranGACV method allows the application of smoothing spline ANOVA models with Bernoulli data to much larger data sets than previously possible. These methods are based on choosing an approximating subset of the natural (representer) basis functions for the variational problem. Simulation studies with synthetic data, including synthetic data mimicking demographic risk factor data sets is used to examine the properties of the method and to compare the approach with the GRKPACK code of Wang (1997c). Bayesian \u201cconfidence intervals\u201d are obtained for the fits and are shown in the simulation studies to have the \u201cacross the function\u201d property usually claimed for these confidence intervals. Finally the method is applied to an observational data set from the Beaver Dam Eye study, with scientifically interesting results."
            },
            "slug": "Smoothing-spline-ANOVA-models-for-large-data-sets-Lin-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing spline ANOVA models for large data sets with Bernoulli observations and the randomized GACV"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A class of approximate numerical methods for solving the penalized likelihood variational problem which, in conjunction with the ranGACV method, allows the application of smoothing spline ANOVA models with Bernoulli data to much larger data sets than previously possible."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49719429"
                        ],
                        "name": "T. Choi",
                        "slug": "T.-Choi",
                        "structuredName": {
                            "firstName": "Taeryon",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130768"
                        ],
                        "name": "M. Schervish",
                        "slug": "M.-Schervish",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Schervish",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schervish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Choudhuri et al. [2005] show that for the binary classification case under certain assumptions GPC is consistent."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For GPR, Choi and Schervish [2004] show that for a one-dimensional input space of finite length under certain assumptions consistency holds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18249383,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ad4cbf20e740699d853c3581f327c13776cb7e6f",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Posterior consistency can be thought of as a theoretical justification of the Bayesian method. One of the most popular approaches to nonparametric Bayesian regression is to put a nonparametric prior distribution on the unknown regression function using Gaussian processes. In this paper, we study posterior consistency in nonparametric regression problems using Gaussian process priors. We use an extension of the theorem of Schwartz (1965) for nonidentically distributed observations, verifying its conditions when using Gaussian process priors for the regression function with normal or double exponential (Laplace) error distributions. We define a metric topology on the space of regression functions and then establish almost sure consistency of the posterior distribution. Our metric topology is weaker than the popular L topology. With additional assumptions, we prove almost sure consistency when the regression functions have L topologies. When the covariate (predictor) is assumed to be a random variable, we prove almost sure consistency for the joint density function of the response and predictor using the Hellinger metric."
            },
            "slug": "Posterior-Consistency-in-Nonparametric-Regression-Choi-Schervish",
            "title": {
                "fragments": [],
                "text": "Posterior Consistency in Nonparametric Regression Problems under Gaussian Process Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An extension of the theorem of Schwartz (1965) for nonidentically distributed observations is used, verifying its conditions when using Gaussian process priors for the regression function with normal or double exponential (Laplace) error distributions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to produce probabilistic predictions, the method of Platt [2000] was used (as described above for the SVM) using the predictive means only (the predictive variances were ignored), except that instead of the 5-fold cross validation, leave-one-out cross-validation (LOO-CV) was used, and the kernel parameters were also set using LOO-CV."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "18Platt [2000] used a logistic whereas we use a cumulative Gaussian."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One can try to interpret the function value f(x) output by the SVM probabilistically, and Platt [2000] suggested that probabilistic predictions can be generated from the SVM by computing \u03c3(af(x) + b) for some constants a, b that are fitted using some \u201cunbiased version\u201d of the training set (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, there are other algorithms, such as the sequential minimal optimization (SMO) algorithm due to Platt [1999], which often have better scaling in practice."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": true,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083942236"
                        ],
                        "name": "Francesco Vivarelli",
                        "slug": "Francesco-Vivarelli",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Vivarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Vivarelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9596632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b85f341ea0187b7a63dfd217ae50f17c63569e6",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In Gaussian process regression the covariance between the outputs at input locations x and x\u2032 is usually assumed to depend on the distance (x\u2212 x\u2032) W (x\u2212 x\u2032), where W is a positive definite matrix. W is often taken to be diagonal, but if we allow W to be a general positive definite matrix which can be tuned on the basis of training data, then an eigen-analysis of W shows that we are effectively creating hidden features, where the dimensionality of the hidden-feature space is determined by the data. We demonstrate the superiority of predictions using the general matrix over those based on a diagonal matrix on two test problems."
            },
            "slug": "Discovering-Hidden-Features-with-Gaussian-Processes-Vivarelli-Williams",
            "title": {
                "fragments": [],
                "text": "Discovering Hidden Features with Gaussian Processes Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work demonstrates the superiority of predictions using the general matrix over those based on a diagonal matrix on two test problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8389658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "486d5c816be93eae1d83b08c962825fd4297d973",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "These lecture notes are based on the work of Neal (1996), Williams and Rasmussen (1996) and Gibbs (1997). My lectures feature a sequence of computer demonstrations written in the free language octave. The source code, and updates and corrections to these lecture notes, will be made available at: http://wol.ra.phy.cam.ac.uk/mackay/. Mark Gibbs's software for Gaussian processes is available at: http://wol.ra.phy.cam.ac.uk/mng10/GP/GP.html. Radford Neal's is at http://www.cs.toronto.edu/~radford/."
            },
            "slug": "Gaussian-Processes-A-Replacement-for-Supervised-Mackay",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes - A Replacement for Supervised Neural Networks?"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "My lectures feature a sequence of computer demonstrations written in the free language octave, based on the work of Neal, Williams and Rasmussen, Gibbs and Gibbs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575699"
                        ],
                        "name": "S. Vijayakumar",
                        "slug": "S.-Vijayakumar",
                        "structuredName": {
                            "firstName": "Sethu",
                            "lastName": "Vijayakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vijayakumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31979282"
                        ],
                        "name": "Aaron D'Souza",
                        "slug": "Aaron-D'Souza",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "D'Souza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron D'Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 69
                            }
                        ],
                        "text": "This task has previously been used to study regression algorithms by Vijayakumar and Schaal [2000], Vijayakumar et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 100
                            }
                        ],
                        "text": "This task has previously been used to study regression algorithms by Vijayakumar and Schaal [2000], Vijayakumar et al. [2002] and Vijayakumar et al. [2005].(13) Following"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 93
                            }
                        ],
                        "text": "The rigid-body-dynamics (RBD) model has a number of free parameters; these were estimated by Vijayakumar et al. [2005] using a least-squares fitting procedure. We also give results for the locally weighted projection regression (LWPR) method of Vijayakumar et al. [2005] which is an on-line method that cycles through the dataset multiple times."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 100
                            }
                        ],
                        "text": "This task has previously been used to study regression algorithms by Vijayakumar and Schaal [2000], Vijayakumar et al. [2002] and Vijayakumar et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "The rigid-body-dynamics (RBD) model has a number of free parameters; these were estimated by Vijayakumar et al. [2005] using a least-squares fitting procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6728013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b559f50f685c7d28d8ae441b619a85395a1de07d",
            "isKey": true,
            "numCitedBy": 568,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Locally weighted projection regression (LWPR) is a new algorithm for incremental nonlinear function approximation in high-dimensional spaces with redundant and irrelevant input dimensions. At its core, it employs nonparametric regression with locally linear models. In order to stay computationally efficient and numerically robust, each local model performs the regression analysis with a small number of univariate regressions in selected directions in input space in the spirit of partial least squares regression. We discuss when and how local learning techniques can successfully work in high-dimensional spaces and review the various techniques for local dimensionality reduction before finally deriving the LWPR algorithm. The properties of LWPR are that it (1) learns rapidly with second-order learning methods based on incremental training, (2) uses statistically sound stochastic leave-one-out cross validation for learning without the need to memorize training data, (3) adjusts its weighting kernels based on only local information in order to minimize the danger of negative interference of incremental learning, (4) has a computational complexity that is linear in the number of inputs, and (5) can deal with a large number of\u2014possibly redundant\u2014inputs, as shown in various empirical evaluations with up to 90 dimensional data sets. For a probabilistic interpretation, predictive variance and confidence intervals are derived. To our knowledge, LWPR is the first truly incremental spatially localized learning method that can successfully and efficiently operate in very high-dimensional spaces."
            },
            "slug": "Incremental-Online-Learning-in-High-Dimensions-Vijayakumar-D'Souza",
            "title": {
                "fragments": [],
                "text": "Incremental Online Learning in High Dimensions"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Locally weighted projection regression is the first truly incremental spatially localized learning method that can successfully and efficiently operate in very high-dimensional spaces."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40411909"
                        ],
                        "name": "Jon D. McAuliffe",
                        "slug": "Jon-D.-McAuliffe",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "McAuliffe",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon D. McAuliffe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2833811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435",
            "isKey": false,
            "numCitedBy": 1199,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0\u20131 loss function. The convexity makes these algorithms computationally efficient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0\u20131 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function\u2014that it satisfies a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise, and show that in this case, strictly convex loss functions lead to faster rates of convergence of the risk than would be implied by standard uniform convergence arguments. Finally, we present applications of our results to the estimation of convergence rates in function classes that are scaled convex hulls of a finite-dimensional base class, with a variety of commonly used loss functions."
            },
            "slug": "Convexity,-Classification,-and-Risk-Bounds-Bartlett-Jordan",
            "title": {
                "fragments": [],
                "text": "Convexity, Classification, and Risk Bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general quantitative relationship between the risk as assessed using the 0\u20131 loss and the riskAs assessed using any nonnegative surrogate loss function is provided, and it is shown that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2692987"
                        ],
                        "name": "Huaiyu Zhu",
                        "slug": "Huaiyu-Zhu",
                        "structuredName": {
                            "firstName": "Huaiyu",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huaiyu Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34915378"
                        ],
                        "name": "R. Rohwer",
                        "slug": "R.-Rohwer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Rohwer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rohwer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3164369"
                        ],
                        "name": "Michal Morciniec",
                        "slug": "Michal-Morciniec",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Morciniec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Morciniec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11784955,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f3ca3340031d94aeb3f827db625d44b1ff1967ec",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of regression under Gaussian assumptions is treated generally. The relationship between Bayesian prediction, regularization and smoothing is elucidated. The ideal regression is the posterior mean and its computation scales as O(n 3 ) , where n is the sample size. We show that the optimal m -dimensional linear model under a given prior is spanned by the first m eigenfunctions of a covariance operator, which is a trace-class operator. This is an infinite dimensional analogue of principal component analysis. The importance of Hilbert space methods to practical statistics is also discussed."
            },
            "slug": "Gaussian-regression-and-optimal-finite-dimensional-Zhu-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian regression and optimal finite dimensional linear models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144402257"
                        ],
                        "name": "R. Kohn",
                        "slug": "R.-Kohn",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70982312"
                        ],
                        "name": "C. Ansley",
                        "slug": "C.-Ansley",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Ansley",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ansley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kohn and Ansley [1987] give details of an O(n) algorithm (based on Kalman filtering) for the computation of the spline and the marginal likelihood."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119751316,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ef529072772ecdbcc95b038ceb6aedf75762a18e",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new efficient algorithm for optimal spline smoothing as the conditional expectation of a stochastic process observed with noise, using the stochastic model of Wahba (J. Royal Statist. Soc. Ser. B, 40 (1978), pp. 364\u2013372). The conditional expectation is computed by expressing the process in state space form and using the filtering and smoothing results in Ansley and Kohn (Annals Statist., 11 (1985), pp.1286\u20131316). We show how to use our algorithms to estimate the smoothness parameter and how to obtain Bayesian confidence intervals for the unknown function and its derivatives. Algorithms based on other stochastic models are compared to ours, and a stochastic derivation is given for Reinsch\u2019s (Numer. Math. 10 (1967), pp. 177\u2013183) algorithm for polynomial splines."
            },
            "slug": "A-new-algorithm-for-spline-smoothing-based-on-a-Kohn-Ansley",
            "title": {
                "fragments": [],
                "text": "A new algorithm for spline smoothing based on smoothing a stochastic process"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new efficient algorithm for optimal spline smoothing is derived as the conditional expectation of a stochastic process observed with noise, using the stoChastic model of Wahba."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For broader introductions to Gaussian processes, consult [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1] for the weight-space view of Gaussian processes which equivalently leads to Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6884486,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "28667c276ba78ab1d855064d5456d50d9932b775",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems."
            },
            "slug": "Prediction-with-Gaussian-Processes:-From-Linear-to-Williams",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes, starting from Bayesian linear regression, and showing how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on prior over parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11422005"
                        ],
                        "name": "Nidhan Choudhuri",
                        "slug": "Nidhan-Choudhuri",
                        "structuredName": {
                            "firstName": "Nidhan",
                            "lastName": "Choudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nidhan Choudhuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2291789"
                        ],
                        "name": "S. Ghosal",
                        "slug": "S.-Ghosal",
                        "structuredName": {
                            "firstName": "Subhashis",
                            "lastName": "Ghosal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ghosal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3155262"
                        ],
                        "name": "A. Roy",
                        "slug": "A.-Roy",
                        "structuredName": {
                            "firstName": "Anindya",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16917583,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4ce5e6dd29c9177773fd3674091c021cf82955e1",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonparametric-binary-regression-using-a-Gaussian-Choudhuri-Ghosal",
            "title": {
                "fragments": [],
                "text": "Nonparametric binary regression using a Gaussian process prior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33171556"
                        ],
                        "name": "Anita C. Faul",
                        "slug": "Anita-C.-Faul",
                        "structuredName": {
                            "firstName": "Anita",
                            "lastName": "Faul",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anita C. Faul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, careful analysis of the RVM marginal likelihood by Faul and Tipping [2002] showed that one can carry out optimization w."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6723676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f98497d63a286637262c98e07d5325aa22ad2a7a",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The recent introduction of the 'relevance vector machine' has effectively demonstrated how sparsity may be obtained in generalised linear models within a Bayesian framework. Using a particular form of Gaussian parameter prior, 'learning' is the maximisation, with respect to hyperparameters, of the marginal likelihood of the data. This paper studies the properties of that objective function, and demonstrates that conditioned on an individual hyper-parameter, the marginal likelihood has a unique maximum which is computable in closed form. It is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model."
            },
            "slug": "Analysis-of-Sparse-Bayesian-Learning-Faul-Tipping",
            "title": {
                "fragments": [],
                "text": "Analysis of Sparse Bayesian Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that conditioned on an individual hyper-parameter, the marginal likelihood has a unique maximum which is computable in closed form, and it is further shown that if a derived 'sparsity criterion' is satisfied, this maximum is exactly equivalent to 'pruning' the corresponding parameter from the model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the generative topographic GTM mapping (GTM) model [Bishop et al., 1998b] the integral was approximated using a grid of points in z-space. In the original GTM paper the non-linear mapping was taken to be a linear combination of non-linear basis functions, but in Bishop et al. [1998a] this was replaced by a Gaussian process mapping between the latent and visible spaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60563397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b1b1654ce0eea729c4160bfedcbb3246460b1d",
            "isKey": false,
            "numCitedBy": 8594,
            "numCiting": 250,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition."
            },
            "slug": "Neural-networks-for-pattern-recognition-Bishop",
            "title": {
                "fragments": [],
                "text": "Neural networks for pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition, and is designed as a text, with over 100 exercises, to benefit anyone involved in the fields of neural computation and pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145605141"
                        ],
                        "name": "M. Stein",
                        "slug": "M.-Stein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stein",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Stein [1999] argues that such strong smoothness assumptions are unrealistic for modelling many physical processes, and recommends the Mat\u00e9rn class (see below)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55575589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8941b0b9e37048c6867c10f4fc2d2cf59790e257",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose a two-dimensional spatial process z(x) with generalized covariance function G(x, x\u2032) \u03b1 |x \u2212 x\u2032|2 log |x \u2212 x\u2032| (Matheron, 1973, Adv. in Appl. Probab., 5, 439\u2013468) is observed with error at a number of locations. This paper gives a kernel approximation to the optimal linear predictor, or kriging predictor, of z(x) under this model as the observations get increasingly dense. The approximation is in terms of a Kelvin function which itself can be easily approximated by series expansions. This generalized covariance function is of particular interest because the predictions it yields are identical to an order 2 thin plate smoothing spline. For moderate sample sizes, the kernel approximation is seen to work very well when the observations are on a square grid and fairly well when the observations come from a uniform random sample.This manuscript was prepared using computer facilities supported in part by National Science Foundation Grants No. DMS-8601732 and DMS-8404941 to the Department of Statistics at The University of Chicago."
            },
            "slug": "A-kernel-approximation-to-the-kriging-predictor-of-Stein",
            "title": {
                "fragments": [],
                "text": "A kernel approximation to the kriging predictor of a spatial process"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A kernel approximation to the optimal linear predictor, or kriging predictor, of z(x) under this model as the observations get increasingly dense is seen to work very well when the observations are on a square grid and fairly wellwhen the observations come from a uniform random sample."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701847"
                        ],
                        "name": "L. Ghaoui",
                        "slug": "L.-Ghaoui",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Ghaoui",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ghaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lanckriet et al. [2004] show that if K is a convex combination of Gram matrices Ki so that K = \u2211 i \u03bdiKi with \u03bdi \u2265 0 for all i then the optimization of the alignment score w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The subset of datapoints (SD) method for GPC was proposed in Lawrence et al. [2003], using an EP-style approximation of the posterior, and the differential entropy score (see section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1113875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0948365ef39ef153e61e9569ade541cf881c7c2a",
            "isKey": false,
            "numCitedBy": 2470,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem."
            },
            "slug": "Learning-the-Kernel-Matrix-with-Semidefinite-Lanckriet-Cristianini",
            "title": {
                "fragments": [],
                "text": "Learning the Kernel Matrix with Semidefinite Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques and leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057046890"
                        ],
                        "name": "Wei Chu",
                        "slug": "Wei-Chu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Chu and Ghahramani [2005] have described how to use GPs for the ordinal regression problem, where one is given ranked preference information as the target data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2788778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "724a2430522c7c398c01f90330f41e187c6a7243",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach."
            },
            "slug": "Gaussian-Processes-for-Ordinal-Regression-Chu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Ordinal Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A probabilistic kernel approach to ordinal regression based on Gaussian processes is presented, where a threshold model that generalizes the probit function is used as the likelihood function for ordinal variables."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17404261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145512a08a7cd79a0efb1f0503ddc6a4e4ef02dc",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the \"support\" patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically. We demonstrate the model selection capabilities of the algorithm in a range of experiments. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods."
            },
            "slug": "Fast-Forward-Selection-to-Speed-Up-Sparse-Gaussian-Seeger-Williams",
            "title": {
                "fragments": [],
                "text": "Fast Forward Selection to Speed Up Sparse Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection, which leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2916561"
                        ],
                        "name": "Alexei Vinokourov",
                        "slug": "Alexei-Vinokourov",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Vinokourov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei Vinokourov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13026481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21494ed027e1c067563776f63840565a6b2e6b0d",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show how the generation of documents can be thought of as a k-stage Markov process, which leads to a Fisher kernel from which the n-gram and string kernels can be re-constructed. The Fisher kernel view gives a more flexible insight into the string kernel and suggests how it can be parametrised in a way that reflects the statistics of the training corpus. Furthermore, the probabilistic modelling approach suggests extending the Markov process to consider sub-sequences of varying length, rather than the standard fixed-length approach used in the string kernel. We give a procedure for determining which sub-sequences are informative features and hence generate a Finite State Machine model, which can again be used to obtain a Fisher kernel. By adjusting the parametrisation we can also influence the weighting received by the features. In this way we are able to obtain a logarithmic weighting in a Fisher kernel. Finally, experiments are reported comparing the different kernels using the standard Rag of Words kernel as a baseline."
            },
            "slug": "String-Kernels,-Fisher-Kernels-and-Finite-State-Saunders-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "String Kernels, Fisher Kernels and Finite State Automata"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper shows how the generation of documents can be thought of as a k-stage Markov process, which leads to a Fisher kernel from which the n-gram and string kernels can be re-constructed, and suggests how the Fisher kernel view gives a more flexible insight into the string kernel."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16378859,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d167055b0947a880de6afee09559a84b0c2c407e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Many of the processing tasks arising in early vision involve the solution of ill-posed inverse problems. Two techniques that are often used to solve these inverse problems are regularization and Bayesian modeling. Regularization is used to find a solution that both fits the data and is also sufficiently smooth. Bayesian modeling uses a statistical prior model of the field being estimated to determine an optimal solution. One convenient way of specifying the prior model is to associate an energy function with each possible solution, and to use a Boltzmann distribution to relate the solution energy to its probability. This paper shows that regularization is an example of Bayesian modeling, and that using the regularization energy function for the surface interpolation problem results in a prior model that is fractal (self-affine over a range of scales). We derive an algorithm for generating typical (fractal) estimates from the posterior distribution. We also show how this algorithm can be used to estimate the uncertainty associated with a regularized solution, and how this uncertainty can be used at later stages of processing."
            },
            "slug": "Regularization-Uses-Fractal-Priors-Szeliski",
            "title": {
                "fragments": [],
                "text": "Regularization Uses Fractal Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows that regularization is an example of Bayesian modeling, and that using the regularization energy function for the surface interpolation problem results in a prior model that is fractal (self-affine over a range of scales)."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17875902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dd9743183f07b7653cc0335fcc1042aa71032c6",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "There is much current interest in kernel methods for classi cation re gression PCA and other linear methods of data analysis Kernel methods may be particularly valuable for problems in which the input data is not readily described by explicit feature vectors One such problem is where input data consists of symbol sequences of di erent lengths and the re lationships between sequences are best captured by dynamic alignment scores This paper shows that the scores produced by certain dynamic align ment algorithms for sequences are in fact valid kernel functions This is proved by expressing the alignment scores explicitly as dot products Alignment kernels are potentially applicable to biological sequence data speech data and time series data The kernel construction may be extended from pair HMMs to pair probabilistic context free grammars Introduction Linear Methods using Kernel Functions Introduction Linear Methods using Kernel Functions In many types of machine learning the learner is given a training set of cases or examples a al A A denotes the set of all possible cases cases may be vectors pieces of text biological sequences sentences etc For supervised learning the cases are accompanied by a set of corresponding labels or values y yl The cases are mapped to feature vectors x xl X where the X is a real vector space termed the feature space The mapping from A to X is denoted by so that xi ai Sometimes the cases are given as feature vectors to start with in which case may be the identity mapping otherwise denotes the method of assigning numeric feature values to a case Once a feature vector xi has been de ned for each case ai it becomes pos sible to apply a wide range of linear methods such as support vector machines linear regression principal components analysis PCA and k means cluster analysis As shown in Vap for SV machines in for example Wah for linear re gression and in SSM for PCA and k means cluster analysis the calculations for all of these linear methods may be carried out using a dual rather than a primal formulation of the problem For example in linear least squares regression the primal formulation is to nd a coe cient vector that minimises kX yk whereX is the design matrix an l by d matrix in which the ith row is xi and each xi has d elements If l is larger than d the usual method of nding is to solve the normal equations XX Xy This requires the solution of a set of linear equations with coe cients given by the d d matrix XX The dual formulation is to nd a coe cient vector that minimises kXX yk so that one coe cient i is found for each case vector xi This requires the solution of a set of linear equations with coe cients given by the l l matrix XX Both methods lead to the same predicted value y for a new case x If there are more cases than features that is if l d the primal method is more economical because the d d matrix XX is smaller than the l l matrix XX For example if there are cases each described by a vector of measurements then the primal method requires solving a by system of linear equations while the dual method requires solving a by system which will have rank at most For such a problem the dual method has no advantage The potential advantage of the dual method for regression is that it can be applied to very large feature vectors The coe cient matrix XX contains the dot products of pairs of feature vectors the ijth element of XX is xi xj In the dual calculation it is only dot products of feature vectors that are used feature vectors never appear on their own As the feature vectors xi ai appear only in dot products it is often possible to avoid computing the feature vectors and to compute dot products directly in some economical fashion from the case descriptions ai instead A kernel is a function k that computes a dot product of feature vectors from the corresponding cases Applying Linear Methods to Structured Objects De nition A kernel is a function k such that for all a b A"
            },
            "slug": "Dynamic-Alignment-Kernels-Watkins",
            "title": {
                "fragments": [],
                "text": "Dynamic Alignment Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the scores produced by certain dynamic align ment algorithms for sequences are in fact valid kernel functions, proved by expressing the alignment scores explicitly as dot products."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177568"
                        ],
                        "name": "J. Q. Candela",
                        "slug": "J.-Q.-Candela",
                        "structuredName": {
                            "firstName": "Joaquin",
                            "lastName": "Candela",
                            "middleNames": [
                                "Qui\u00f1onero"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Q. Candela"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6352895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a10ed9913a3dcfc33939f73b37732c375d5d8121",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Relevance Vector Machine (RVM) is a sparse approximate Bayesian kernel method. It provides full predictive distributions for test cases. However, the predictive uncertainties have the unintuitive property, that they get smaller the further you move away from the training cases. We give a thorough analysis. Inspired by the analogy to non-degenerate Gaussian Processes, we suggest augmentation to solve the problem. The purpose of the resulting model, RVM*, is primarily to corroborate the theoretical and experimental analysis. Although RVM* could be used in practical applications, it is no longer a truly sparse model. Experiments show that sparsity comes at the expense of worse predictive. distributions."
            },
            "slug": "Healing-the-relevance-vector-machine-through-Rasmussen-Candela",
            "title": {
                "fragments": [],
                "text": "Healing the relevance vector machine through augmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The purpose of the resulting model, RVM*, is primarily to corroborate the theoretical and experimental analysis and show that sparsity comes at the expense of worse predictive."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2Schoenberg [1964] proved the representer theorem for the special case of cubic splines and squared error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7099687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "922b81f11a71aa64cda78914e6356cce89cd4f86",
            "isKey": false,
            "numCitedBy": 797,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study a dual version of the Ridge Regression procedure. It allows us to perform non-linear regression by constructing a linear regression function in a high dimensional feature space. The feature space representation can result in a large increase in the number of parameters used by the algorithm. In order to combat this \u201ccurse of dimensionality\u201d, the algorithm allows the use of kernel functions, as used in Support Vector methods. We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infinite number of nodes. This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinitenode splines. Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms."
            },
            "slug": "Ridge-Regression-Learning-Algorithm-in-Dual-Saunders-Gammerman",
            "title": {
                "fragments": [],
                "text": "Ridge Regression Learning Algorithm in Dual Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A regression estimation algorithm which is a combination of the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinitenode splines and the use of kernel functions, as used in Support Vector methods is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 77
                            }
                        ],
                        "text": "19) was called a regularization network regularization network in Poggio and Girosi [1990]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 68
                            }
                        ],
                        "text": "This has been suggested by a number of authors including Poggio and Girosi [1990] and Suykens and Vanderwalle [1999]. Experimental results reported in Rifkin and Klautau [2004] indicate that performance comparable to SVMs can be obtained using kernel LSC (or as they call it the regularized least-squares classifier, RLSC)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "Following Szeliski [1987] and Poggio and Girosi [1990] we consider"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "This has been suggested by a number of authors including Poggio and Girosi [1990] and Suykens and Vanderwalle [1999]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 16
                            }
                        ],
                        "text": "19], Poggio and Girosi [1990] and also in Vivarelli and Williams [1999]; in the latter work a low-rank M was used to implement a linear dimensionality reduction step from the input space to lower-dimensional feature space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1396,
                                "start": 68
                            }
                        ],
                        "text": "This has been suggested by a number of authors including Poggio and Girosi [1990] and Suykens and Vanderwalle [1999]. Experimental results reported in Rifkin and Klautau [2004] indicate that performance comparable to SVMs can be obtained using kernel LSC (or as they call it the regularized least-squares classifier, RLSC). Consider a single random variable y which takes on the value +1 with probability p and value\u22121 with probability 1\u2212p. Then the value of f which minimizes the squared error function E = p(f \u2212 1)(2) + (1\u2212 p)(f + 1)(2) is f\u0302 = 2p\u2212 1, which is a linear rescaling of p to the interval [\u22121, 1]. (Equivalently if the targets are 1 and 0, we obtain f\u0302 = p.) Hence we observe that LSC will estimate p correctly in the large data limit. If we now consider not just a single random variable, but wish to estimate p(C+|x) (or a linear rescaling of it), then as long as the approximating function f(x) is sufficiently flexible, we would expect that in the limit n \u2192 \u221e it would converge to p(C+|x). (For more technical detail on this issue, see section 7.2.1 on consistency.) Hence LSC is quite a sensible procedure for classification, although note that there is no guarantee that f(x) will be constrained to lie in the interval [y\u2212, y+]. If we wish to guarantee a probabilistic interpretation, we could \u201csquash\u201d the predictions through a sigmoid, as suggested for SVMs by Platt [2000] and described on page 145."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2350,
                                "start": 68
                            }
                        ],
                        "text": "This has been suggested by a number of authors including Poggio and Girosi [1990] and Suykens and Vanderwalle [1999]. Experimental results reported in Rifkin and Klautau [2004] indicate that performance comparable to SVMs can be obtained using kernel LSC (or as they call it the regularized least-squares classifier, RLSC). Consider a single random variable y which takes on the value +1 with probability p and value\u22121 with probability 1\u2212p. Then the value of f which minimizes the squared error function E = p(f \u2212 1)(2) + (1\u2212 p)(f + 1)(2) is f\u0302 = 2p\u2212 1, which is a linear rescaling of p to the interval [\u22121, 1]. (Equivalently if the targets are 1 and 0, we obtain f\u0302 = p.) Hence we observe that LSC will estimate p correctly in the large data limit. If we now consider not just a single random variable, but wish to estimate p(C+|x) (or a linear rescaling of it), then as long as the approximating function f(x) is sufficiently flexible, we would expect that in the limit n \u2192 \u221e it would converge to p(C+|x). (For more technical detail on this issue, see section 7.2.1 on consistency.) Hence LSC is quite a sensible procedure for classification, although note that there is no guarantee that f(x) will be constrained to lie in the interval [y\u2212, y+]. If we wish to guarantee a probabilistic interpretation, we could \u201csquash\u201d the predictions through a sigmoid, as suggested for SVMs by Platt [2000] and described on page 145. When generalizing from the binary to multi-class situation there is some freedom as to how to set the problem up. Sch\u00f6lkopf and Smola [2002, sec. 7.6] identify four methods, namely one-versus-rest (where C binary classifiers are trained to classify each class against all the rest), all pairs (where C(C \u2212 1)/2 binary classifiers are trained), error-correcting output coding (where each class is assigned a binary codeword, and binary classifiers are trained on each bit separately), and multi-class objective functions (where the aim is to train C classifiers simultaneously rather than creating a number of binary classification problems). One also needs to specify how the outputs of the various classifiers that are trained are combined so as to produce an overall answer. For the one-versus-rest(7) method one simple criterion is to choose the classifier which produces the most positive output. Rifkin and Klautau [2004] performed extensive experiments and came to the conclusion that the one-versus-rest scheme using either SVMs or RLSC is as accurate as any other method overall, and has the merit of being conceptually simple and straightforward to implement."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11074771,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2c598e80a7c70cf974fe1e57b5741aeb2fecbd00",
            "isKey": true,
            "numCitedBy": 35,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Given n noisy observations g i of the same quantity f, i t i s c o m m o n use to give an estimate of f by minimizing the function P n i=1 (g i ;f) 2. From a statistical point of view this corresponds to computing the Maximum Likelihood estimate, under the assumption of Gaussian noise. Howeve r , i t i s w ell known that this choice leads to results that are very sensitive to the presence of outliers in the data. For this reason it has been proposed to minimize functions of the form P n i=1 V (g i ; f), where V is a function that increases less rapidly than the square. Several choices for V have been proposed and successfully used to obtain \\robust\" estimates. In this paper we show that, for a class of functions V , using these robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance uctuates according to some given probability distribution, that uniquely determines the shape of V ."
            },
            "slug": "Models-of-Noise-and-Robust-Estimates-Girosi",
            "title": {
                "fragments": [],
                "text": "Models of Noise and Robust Estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, for a class of functions V, using robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance uctuates according to some given probability distribution, that uniquely determines the shape of V."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15514452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7678da8b2eb70a5383f203d948564d8f48c0c62a",
            "isKey": false,
            "numCitedBy": 761,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how closely the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function. The measurement of closeness is characterized by the loss function used in the estimation. We show that such a classification scheme can be generally regarded as a (nonmaximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions that have appeared in the literature. Furthermore, the theoretical insight allows us to design good loss functions with desirable properties. Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization. This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines. It also shows their limitations and suggests possible improvements."
            },
            "slug": "Statistical-behavior-and-consistency-of-methods-on-Zhang",
            "title": {
                "fragments": [],
                "text": "Statistical behavior and consistency of classification methods based on convex risk minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines and shows their limitations and suggests possible improvements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144209397"
                        ],
                        "name": "J. Bernardo",
                        "slug": "J.-Bernardo",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Bernardo",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bernardo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52451583"
                        ],
                        "name": "M. J. Bayarri",
                        "slug": "M.-J.-Bayarri",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Bayarri",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bayarri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845488"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099028"
                        ],
                        "name": "D. Heckerman",
                        "slug": "D.-Heckerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heckerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heckerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109352679"
                        ],
                        "name": "Adrian F. M. Smith",
                        "slug": "Adrian-F.-M.-Smith",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian F. M. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656437"
                        ],
                        "name": "M. West",
                        "slug": "M.-West",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "West",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. West"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5956586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d6b2588d76325015545a8d4491900adb325e8e",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Hybrid Monte Carlo (HMC) is often the method of choice for computing Bayesian integrals that are not analytically tractable. However the success of this method may require a very large number of evaluations of the (un-normalized) posterior and its partial derivatives. In situations where the posterior is computationally costly to evaluate, this may lead to an unacceptable computational load for HMC. I propose to use a Gaussian Process model of the (log of the) posterior for most of the computations required by HMC. Within this scheme only occasional evaluation of the actual posterior is required to guarantee that the samples generated have exactly the desired distribution, even if the GP model is somewhat inaccurate. The method is demonstrated on a 10 dimensional problem, where 200 evaluations suffice for the generation of 100 roughly independent points from the posterior. Thus, the proposed scheme allows Bayesian treatment of models with posteriors that are computationally demanding, such as models involving computer simulation."
            },
            "slug": "Gaussian-Processes-to-Speed-up-Hybrid-Monte-Carlo-Rasmussen-Bernardo",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes to Speed up Hybrid Monte Carlo for Expensive Bayesian Integrals"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to use a Gaussian Process model of the (log of the) posterior for most of the computations required by HMC, allowing Bayesian treatment of models with posteriors that are computationally demanding, such as models involving computer simulation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144114908"
                        ],
                        "name": "Sally Wood",
                        "slug": "Sally-Wood",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Wood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sally Wood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144402257"
                        ],
                        "name": "R. Kohn",
                        "slug": "R.-Kohn",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kohn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123559175,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "e415fd9239e950e7105ab16d2e3d5501d4c5604c",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article presents a Bayesian approach to binary nonparametric regression that assumes that the argument of the link is an additive function of the explanatory variables and their multiplicative interactions. The article makes the following contributions. First, a comprehensive approach is presented in which the function estimates are smoothing splines with the smoothing parameters integrated out and the estimates are made robust to outliers. Second, the approach can handle a wide range of link functions. Third, efficient state-space-based algorithms are used to carry out the computations. Fourth, an extensive set of simulations is carried out, which show that the Bayesian estimator works well and compares favorably to two estimators that have recently been proposed and used in practice."
            },
            "slug": "A-Bayesian-Approach-to-Robust-Binary-Nonparametric-Wood-Kohn",
            "title": {
                "fragments": [],
                "text": "A Bayesian Approach to Robust Binary Nonparametric Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A comprehensive approach is presented in which the function estimates are smoothing splines with the smoothing parameters integrated out and the estimates are made robust to outliers, and can handle a wide range of link functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065748442"
                        ],
                        "name": "P. Goldberg",
                        "slug": "P.-Goldberg",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7482528,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "4d00277ee6bdbfc7cd3282d33897be5758d315fe",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes provide natural non-parametric prior distributions over regression functions. In this paper we consider regression problems where there is noise on the output, and the variance of the noise depends on the inputs. If we assume that the noise is a smooth function of the inputs, then it is natural to model the noise variance using a second Gaussian process, in addition to the Gaussian process governing the noise-free output value. We show that prior uncertainty about the parameters controlling both processes can be handled and that the posterior distribution of the noise rate can be sampled from using Markov chain Monte Carlo methods. Our results on a synthetic data set give a posterior noise variance that well-approximates the true variance."
            },
            "slug": "Regression-with-Input-dependent-Noise:-A-Gaussian-Goldberg-Williams",
            "title": {
                "fragments": [],
                "text": "Regression with Input-dependent Noise: A Gaussian Process Treatment"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows that prior uncertainty about the parameters controlling both processes can be handled and that the posterior distribution of the noise rate can be sampled from using Markov chain Monte Carlo methods and gives a posterior noise variance that well-approximates the true variance."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This method has been suggested by Wahba et al. [1995] (in the context of numerical weather prediction) and by Gibbs and MacKay [1997] (in the context of general GP regression)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716788"
                        ],
                        "name": "M. Kawanabe",
                        "slug": "M.-Kawanabe",
                        "structuredName": {
                            "firstName": "Motoaki",
                            "lastName": "Kawanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414086"
                        ],
                        "name": "G. R\u00e4tsch",
                        "slug": "G.-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029782"
                        ],
                        "name": "S. Sonnenburg",
                        "slug": "S.-Sonnenburg",
                        "structuredName": {
                            "firstName": "S\u00f6ren",
                            "lastName": "Sonnenburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sonnenburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Tsuda et al. [2002] have developed the tangent of posterior odds (TOP) kernel based on TOP kernel \u2207\u03b8(log p(y = +1|x,\u03b8)\u2212log p(y = \u22121|x,\u03b8)), which makes use of class-conditional distributions for the C+ and C\u2212 classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11013893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84cccc9e14e49a6c56147e4cd36bda2ffd70b683",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, Jaakkola and Haussler (1999) proposed a method for constructing kernel functions from probabilistic models. Their so-called Fisher kernel has been combined with discriminative classifiers such as support vector machines and applied successfully in, for example, DNA and protein analysis. Whereas the Fisher kernel is calculated from the marginal log-likelihood, we propose the TOP kernel derived from tangent vectors of posterior log-odds. Furthermore, we develop a theoretical framework on feature extractors from probabilistic models and use it for analyzing the TOP kernel. In experiments, our new discriminative TOP kernel compares favorably to the Fisher kernel."
            },
            "slug": "A-New-Discriminative-Kernel-from-Probabilistic-Tsuda-Kawanabe",
            "title": {
                "fragments": [],
                "text": "A New Discriminative Kernel from Probabilistic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a new discriminative TOP kernel derived from tangent vectors of posterior log-odds and develops a theoretical framework on feature extractors from probabilistic models and uses it for analyzing the TOP kernel."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10536649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93cb06180743fa648d844b9e7883b62468921c84",
            "isKey": false,
            "numCitedBy": 2848,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nPattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader."
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-Ripley",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks in this self-contained account."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613576"
                        ],
                        "name": "Peter Sollich",
                        "slug": "Peter-Sollich",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sollich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Sollich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We are unaware of an exact result in this case, but the following approximation due to Sollich and Williams [2005] is simple but effective."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although this function has a similar number of parameters to the Mat\u00e9rn class, it is (as Stein [1999] notes) in a sense less flexible."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Try setting \u03ba(C) = 1/(1 + exp(\u22122C)) as suggested in Sollich [2002] and observe what effect this has."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar results have also been obtained by Steinwart [2005] with various rates on the decay of \u03bbn depending on the smoothness of the kernel."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6090879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7294862e59c8c3a65167260c0156427f4757c67e",
            "isKey": true,
            "numCitedBy": 46,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "I consider the problem of calculating learning curves (i.e., average generalization performance) of Gaussian processes used for regression. A simple expression for the generalization error in terms of the eigenvalue decomposition of the covariance function is derived, and used as the starting point for several approximation schemes. I identify where these become exact, and compare with existing bounds on learning curves; the new approximations, which can be used for any input space dimension, generally get substantially closer to the truth."
            },
            "slug": "Learning-Curves-for-Gaussian-Processes-Sollich",
            "title": {
                "fragments": [],
                "text": "Learning Curves for Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The problem of calculating learning curves (i.e., average generalization performance) of Gaussian processes used for regression is considered, and a simple expression for the generalization error in terms of the eigenvalue decomposition of the covariance function is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50584571"
                        ],
                        "name": "B. Blight",
                        "slug": "B.-Blight",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Blight",
                            "middleNames": [
                                "J.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Blight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152165551"
                        ],
                        "name": "L. Ott",
                        "slug": "L.-Ott",
                        "structuredName": {
                            "firstName": "Lyman",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This idea was explored explicitly as early as 1975 by Blight and Ott [1975], who used the GP to model the residuals from a polynomial regression, i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122135270,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "21e965c7c51c57b01aa20648edf9456a43cdf88a",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented which, in many cases, appears to be an improvement over the standard approach to the polynomial regression problem. This improvement is achieved by focusing attention on the deviation of the polynomial representation from the true underlying function. By fully utilizing the nature of this deviation, a model is constructed in which its properties are represented in terms of a Bayesian prior distribution. The model is analyzed to give parameter estimates and predictions of further observations. Comparisons are made with standard least squares procedures when the true underlying model is (a) quadratic and (b) linear and quadratic with a superimposed sine wave."
            },
            "slug": "A-Bayesian-approach-to-model-inadequacy-for-Blight-Ott",
            "title": {
                "fragments": [],
                "text": "A Bayesian approach to model inadequacy for polynomial regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Sollich [2002] suggests choosing \u03ba(C) = 1/(1 + exp(\u22122C)) which ensures that \u03bd(f, C) \u2264 1 (with equality only when f = \u00b11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14999264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1486de62910bfc6eac13b14e7a76c9024003d6bc",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-parametric regression using cubic splines is an attractive, flexible and widely-applicable approach to curve estimation. Although the basic idea was formulated many years ago, the method is not as widely known or adopted as perhaps it should be. The topics and examples discussed in this paper are intended to promote the understanding and extend the practicability of the spline smoothing methodology. Particular subjects covered include the basic principles of the method; the relation with moving average and other smoothing methods; the automatic choice of the amount of smoothing; and the use of residuals for diagnostic checking and model adaptation. The question of providing inference regions for curves-and for relevant properties of curves--is approached via a finite-dimensional Bayesian formulation."
            },
            "slug": "Some-Aspects-of-the-Spline-Smoothing-Approach-to-Silverman",
            "title": {
                "fragments": [],
                "text": "Some Aspects of the Spline Smoothing Approach to Non\u2010Parametric Regression Curve Fitting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The topics and examples discussed in this paper are intended to promote the understanding and extend the practicability of the spline smoothing methodology."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2877073,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0e658618c9dad4d70dd7dcd5c519185ec4f845f5",
            "isKey": false,
            "numCitedBy": 1160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results."
            },
            "slug": "Gaussian-Processes-for-Regression-Williams-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92579735"
                        ],
                        "name": "Peter Craven",
                        "slug": "Peter-Craven",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Craven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 0
                            }
                        ],
                        "text": "Cressie [1993, sec. 3.2.3]. One way to induce cokriging correlations between a number of output channels is to obtain them as linear combinations of a number of latent channels, as described in Teh et al. [2005]; see also Micchelli and Pontil [2005]. A related approach is taken by Boyle and Frean [2005] who introduce correlations between two processes by deriving them as different convolutions of the same underlying white noise process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 0
                            }
                        ],
                        "text": "Cressie [1993, sec. 3.2.3]. One way to induce cokriging correlations between a number of output channels is to obtain them as linear combinations of a number of latent channels, as described in Teh et al. [2005]; see also Micchelli and Pontil [2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Craven and Wahba [1979] describe a variant of cross-validation using squared error known as generalized cross-validation which gives different weightings to different datapoints so as to achieve certain invariance properites."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14094416,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b477dd12dd49e44a62c1a303501df5fb6706c7e9",
            "isKey": true,
            "numCitedBy": 3541,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "SummarySmoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.We consider the modelyi(ti)+\u03b5i,i=1, 2, ...,n,ti\u2208[0, 1], whereg\u2208W2(m)={f:f,f\u2032, ...,f(m\u22121) abs. cont.,f(m)\u2208\u21122[0,1]}, and the {\u03b5i} are random errors withE\u03b5i=0,E\u03b5i\u03b5j=\u03c32\u03b4ij. The error variance \u03c32 may be unknown. As an estimate ofg we take the solutiongn, \u03bb to the problem: Findf\u2208W2(m) to minimize\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 + \\lambda \\int\\limits_0^1 {(f^{(m)} (u))^2 du} }$$\n. The functiongn, \u03bb is a smoothing polynomial spline of degree 2m\u22121. The parameter \u03bb controls the tradeoff between the \u201croughness\u201d of the solution, as measured by\n$$\\int\\limits_0^1 {[f^{(m)} (u)]^2 du}$$\n, and the infidelity to the data as measured by\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 }$$\n, and so governs the average square errorR(\u03bb; g)=R(\u03bb) defined by\n$$R(\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g_{n,\\lambda } (t_j ) - g(t_j ))^2 }$$\n. We provide an estimate\n$$\\hat \\lambda$$\n, called the generalized cross-validation estimate, for the minimizer ofR(\u03bb). The estimate\n$$\\hat \\lambda$$\n is the minimizer ofV(\u03bb) defined by\n$$V(\\lambda ) = \\frac{1}{n}\\parallel (I - A(\\lambda ))y\\parallel ^2 /\\left[ {\\frac{1}{n}{\\text{Trace(}}I - A(\\lambda ))} \\right]^2$$\n, wherey=(y1, ...,yn)t andA(\u03bb) is then\u00d7n matrix satisfying(gn, \u03bb (t1), ...,gn, \u03bb (tn))t=A (\u03bb) y. We prove that there exist a sequence of minimizers\n$$\\tilde \\lambda = \\tilde \\lambda (n)$$\n ofEV(\u03bb), such that as the (regular) mesh{ti}i=1n becomes finer,\n$$\\mathop {\\lim }\\limits_{n \\to \\infty } ER(\\tilde \\lambda )/\\mathop {\\min }\\limits_\\lambda ER(\\lambda ) \\downarrow 1$$\n. A Monte Carlo experiment with several smoothg's was tried withm=2,n=50 and several values of \u03c32, and typical values of\n$$R(\\hat \\lambda )/\\mathop {\\min }\\limits_\\lambda R(\\lambda )$$\n were found to be in the range 1.01\u20131.4. The derivativeg\u2032 ofg can be estimated by\n$$g'_{n,\\hat \\lambda } (t)$$\n. In the Monte Carlo examples tried, the minimizer of\n$$R_D (\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g'_{n,\\lambda } (t_j ) - } g'(t_j ))$$\n tended to be close to the minimizer ofR(\u03bb), so that\n$$\\hat \\lambda$$\n was also a good value of the smoothing parameter for estimating the derivative."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Craven-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example if we choose the error function h(z) = erf(z) = neural network covariance function 2/ \u221a \u03c0 \u222b z 0 e\u2212t 2 dt as the transfer function, let h(x;u) = erf(u0 + \u2211D j=1ujxj) and choose u \u223c N (0,\u03a3) then we obtain [Williams, 1998]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "11There is a sign error in equation 23 of Williams and Barber [1998] but not in their implementation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our presentation follows Williams and Barber [1998]. We first introduce the vector of latent function values at all n training points and for all C classes"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "8CW thanks Manfred Opper for pointing out that the upper bound developed in Williams and Vivarelli [2000] is exact for the noise-free OU process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our treatment is based on Williams and Vivarelli [2000]. We first calculate E(X) for a fixed design, and then integrate over possible designs to obtain E(n)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative information theoretic argument is given in Williams and Vivarelli [2000]. Note that while this conclusion is true for Gaussian process priors and Gaussian noise models it does not hold generally, see Barber and Saad [1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44861233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9d2f88abcf919ae4215d5d0d9c332db5ece3e05",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "For neural networks with a wide class of weight priors, it can be shown that in the limit of an infinite number of hidden units, the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones."
            },
            "slug": "Computation-with-Infinite-Neural-Networks-Williams",
            "title": {
                "fragments": [],
                "text": "Computation with Infinite Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33171556"
                        ],
                        "name": "Anita C. Faul",
                        "slug": "Anita-C.-Faul",
                        "structuredName": {
                            "firstName": "Anita",
                            "lastName": "Faul",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anita C. Faul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15179374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4b4d5d26cd3ee092733113dc0ee80d8766367bf",
            "isKey": false,
            "numCitedBy": 896,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The \u2018sparse Bayesian\u2019 modelling approach, as exemplified by the \u2018relevance vector machine\u2019, enables sparse classification and regression functions to be obtained by linearly-weighting a small number of fixed basis functions from a large dictionary of potential candidates. Such a model conveys a number of advantages over the related and very popular \u2018support vector machine\u2019, but the necessary \u2018training\u2019 procedure \u2014 optimisation of the marginal likelihood function \u2014 is typically much slower. We describe a new and highly accelerated algorithm which exploits recently-elucidated properties of the marginal likelihood function to enable maximisation via a principled and efficient sequential addition and deletion of candidate basis functions."
            },
            "slug": "Fast-Marginal-Likelihood-Maximisation-for-Sparse-Tipping-Faul",
            "title": {
                "fragments": [],
                "text": "Fast Marginal Likelihood Maximisation for Sparse Bayesian Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes a new and highly accelerated algorithm which exploits recently-elucidated properties of the marginal likelihood function to enable maximisation via a principled and efficient sequential addition and deletion of candidate basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115816494"
                        ],
                        "name": "Ji Zhu",
                        "slug": "Ji-Zhu",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15413835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "746e1c10ca4af0f736682dda3c967d371eeb086c",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector machine (SVM) is known for its good performance in two-class classification, but its extension to multiclass classification is still an ongoing research issue. In this article, we propose a new approach for classification, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in two-class classification, but also can naturally be generalized to the multiclass case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the support points of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a potential computational advantage over the SVM."
            },
            "slug": "Kernel-Logistic-Regression-and-the-Import-Vector-Zhu-Hastie",
            "title": {
                "fragments": [],
                "text": "Kernel Logistic Regression and the Import Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that the IVM not only performs as well as the SVM in two-class classification, but also can naturally be generalized to the multiclass case, and provides an estimate of the underlying probability."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The expectation propagation (EP) algorithm [Minka, 2001] is a general approximation tool with a wide range of applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The expectation propagation (EP) algorithm [Minka, 2001] is a general approximation tool with a wide range of applications. In this section we present only its application to the specific case of a GP model for binary classification. We note that Opper and Winther [2000] presented a similar method for binary GPC based on the fixed-point equations of the Thouless-Anderson-Palmer (TAP) type of mean-field approximation from statistical physics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, note that Minka [2003] provides evidence that other optimization methods (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8632802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf527ca11d7d81a15ff5b5603374a4e9d53b55b6",
            "isKey": true,
            "numCitedBy": 986,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, \u201cExpectation Propagation,\u201d unifies and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. \nLoopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction\u2014propagating richer belief states which incorporate correlations between variables. \nThis framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "A-family-of-algorithms-for-approximate-Bayesian-Minka",
            "title": {
                "fragments": [],
                "text": "A family of algorithms for approximate Bayesian inference"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible, and is found to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145369815"
                        ],
                        "name": "K. Ritter",
                        "slug": "K.-Ritter",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ritter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732421"
                        ],
                        "name": "G. Wasilkowski",
                        "slug": "G.-Wasilkowski",
                        "structuredName": {
                            "firstName": "Grzegorz",
                            "lastName": "Wasilkowski",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wasilkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749153"
                        ],
                        "name": "H. Wozniakowski",
                        "slug": "H.-Wozniakowski",
                        "structuredName": {
                            "firstName": "Henryk",
                            "lastName": "Wozniakowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wozniakowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 14341746,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b02dab748538be5ecbf65df92a2ed2c58fc5d666",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction. We study multivariate integration and L2-approximation for random fields Y that are defined on the d-dimensional unit cube D = [0, 1]d and that have mean zero and known covariance kernel K. We assume that K is at least continuous, and hence we may assume that Y is a measurable random field whose realizations are in L2(D) with probability 1. For integration we want to estimate the integral fD Y(t) dt, whereas for L2approximation we want to estimate the values Y(t) for all t, and we study the distance of the estimate and the realization of the field in the space L2(D). For both problems we mainly consider linear estimators that use n observations of the random field. These estimators are of the form n n"
            },
            "slug": "MULTIVARIATE-INTEGRATION-AND-APPROXIMATION-FOR-Ritter-Wasilkowski",
            "title": {
                "fragments": [],
                "text": "MULTIVARIATE INTEGRATION AND APPROXIMATION FOR RANDOM FIELDS SATISFYING SACKS-YLVISAKER CONDITIONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5062147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "480d517574a079d3e0159b978cb19b3f014e59a3",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Using an input-dependent adaptation of the Dirichlet Process, we implement a gating network for an infinite number of Experts. Inference in this model may be done efficiently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets \u2013 thus potentially overcoming two of the biggest hurdles with GP models. Simulations show the viability of this approach."
            },
            "slug": "Infinite-Mixtures-of-Gaussian-Process-Experts-Rasmussen-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Infinite Mixtures of Gaussian Process Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An extension to the Mixture of Experts model, where the individual experts are Gaussian Process (GP) regression models, using an input-dependent adaptation of the Dirichlet Process to implement a gating network for an infinite number of Experts."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845488"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071974222"
                        ],
                        "name": "J. M. Bernardo",
                        "slug": "J.-M.-Bernardo",
                        "structuredName": {
                            "firstName": "Joao",
                            "lastName": "Bernardo",
                            "middleNames": [
                                "Manuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Bernardo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2664086"
                        ],
                        "name": "J. Oakley",
                        "slug": "J.-Oakley",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Oakley",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Oakley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144581339"
                        ],
                        "name": "M. Kennedy",
                        "slug": "M.-Kennedy",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Kennedy",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kennedy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53896630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac045b96d8a6c546f84c8f556a4a49ba29fc229f",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY This paper builds on work by Haylock and O'Hagan which developed a Bayesian approach to uncertainty analysis. The generic problem is to make posterior inference about the output of a complex computer code, and the speciic problem of uncertainty analysis is to make inference when the \\true\" values of the input parameters are unknown. Given the distribution of the input parameters (which is often a subjective distribution derived from expert opinion), we wish to make inference about the implied distribution of the output. The computer code is suuciently complex that the time to compute the output for any input connguration is substantial. The Bayesian approach was shown to improve dramatically on the classical approach, which is based on drawing a sample of values of the input parameters and thereby obtaining a sample from the output distribution. We review the basic Bayesian approach to the generic problem of inference for complex computer codes, and present some recent advances|inference about the distribution of quantile functions of the uncertainty distribution, calibration of models, and the use of runs of the computer code at diierent levels of complexity to make eecient use of the quicker, cruder, versions of the code. The emphasis is on practical applications. 1. INTRODUCTION 1.1. Complex computer codes In many elds, complex computer programs are used to model and predict real phenomena. For example, weather forecasting uses highly sophisticated models for atmospheric pressures and humidities; the behaviour of large or complex engineering structures is typically modelled in great detail as an aid to their design; astronomers and physicists have long required massive computations to model and predict the movements of planets or atomic particles. A feature of such computer programs is that they generally require substantial amounts of computing time, even on powerful computers. When it is necessary to use many runs of the program, in order to compute the output over a range of input conngurations, the time required for each run becomes important. This paper presents some statistical tools for such problems. We consider the computer model as a black box. Its output is represented as a function () taking value (x) for input x 2 X: The input space X is typically multidi-mensional, with many individual quantities needing to be speciied to deene the input connguration for a given run, so we can think of x as a vector. In practice, computer codes usually also produce \u2026"
            },
            "slug": "Uncertainty-analysis-and-other-inference-tools-for-Smith-Dawid",
            "title": {
                "fragments": [],
                "text": "Uncertainty analysis and other inference tools for complex computer codes"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The basic Bayesian approach to the generic problem of inference for complex computer codes is reviewed, some recent advances about the distribution of quantile functions of the uncertainty distribution are presented, and the use of runs of the computer code at diierent levels of complexity to make eecient use of the quicker, cruder, versions of the code is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404093319"
                        ],
                        "name": "G. Ferrari-Trecate",
                        "slug": "G.-Ferrari-Trecate",
                        "structuredName": {
                            "firstName": "Giancarlo",
                            "lastName": "Ferrari-Trecate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ferrari-Trecate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18083694,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b16da4a8973b1f39c44c98a9c6fde5e26d72bb3c",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian process (GP) prediction suffers from O(n3) scaling with the data set size n. By using a finite-dimensional basis to approximate the GP predictor, the computational complexity can be reduced. We derive optimal finite-dimensional predictors under a number of assumptions, and show the superiority of these predictors over the Projected Bayes Regression method (which is asymptotically optimal). We also show how to calculate the minimal model size for a given n. The calculations are backed up by numerical experiments."
            },
            "slug": "Finite-Dimensional-Approximation-of-Gaussian-Ferrari-Trecate-Williams",
            "title": {
                "fragments": [],
                "text": "Finite-Dimensional Approximation of Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work derives optimal finite-dimensional predictors under a number of assumptions, and shows the superiority of these predictors over the Projected Bayes Regression method (which is asymptotically optimal)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 22
                            }
                        ],
                        "text": ", Xmod(t+p,N)), which Geman and Geman [1984] call the \u201ctwo-sided\u201d Markov property."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 460,
                                "start": 22
                            }
                        ],
                        "text": ", Xmod(t+p,N)), which Geman and Geman [1984] call the \u201ctwo-sided\u201d Markov property. Notice that it is the zeros in the inverse covariance matrix that indicate the conditional independence structure; see also section B.5. The properties of eq. (B.44) have been studied by a number of authors, e.g. Whittle [1963] (under the name of circulant processes), Kashyap and Chellappa [1981] (under the name of circular autoregressive models) and Grenander et al. [1991] (as cyclic Markov process)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 22
                            }
                        ],
                        "text": ", Xmod(t+p,N)), which Geman and Geman [1984] call the \u201ctwo-sided\u201d Markov property. Notice that it is the zeros in the inverse covariance matrix that indicate the conditional independence structure; see also section B.5. The properties of eq. (B.44) have been studied by a number of authors, e.g. Whittle [1963] (under the name of circulant processes), Kashyap and Chellappa [1981] (under the name of circular autoregressive models) and Grenander et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": true,
            "numCitedBy": 18710,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The proof can be found in McAllester [2003]. The Kullback-Leibler (KL) divergence KL(q||p) is defined in section A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14704908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1af5c57357bbe22364ce106c23ea7b016c316f96",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "PAC-Bayesian learning methods combine the informative priors of Bayesian methods with distribution-free PAC guarantees. Stochastic model selection predicts a class label by stochastically sampling a classifier according to a \u201cposterior distribution\u201d on classifiers. This paper gives a PAC-Bayesian performance guarantee for stochastic model selection that is superior to analogous guarantees for deterministic model selection. The guarantee is stated in terms of the training error of the stochastic classifier and the KL-divergence of the posterior from the prior. It is shown that the posterior optimizing the performance guarantee is a Gibbs distribution. Simpler posterior distributions are also derived that have nearly optimal performance guarantees."
            },
            "slug": "PAC-Bayesian-Stochastic-Model-Selection-McAllester",
            "title": {
                "fragments": [],
                "text": "PAC-Bayesian Stochastic Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A PAC-Bayesian performance guarantee for stochastic model selection that is superior to analogous guarantees for deterministic model selection and shown that the posterior optimizing the performance guarantee is a Gibbs distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145579972"
                        ],
                        "name": "L. K. Hansen",
                        "slug": "L.-K.-Hansen",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Hansen",
                            "middleNames": [
                                "Kai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. K. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3150049"
                        ],
                        "name": "C. Liisberg",
                        "slug": "C.-Liisberg",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Liisberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Liisberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115778938"
                        ],
                        "name": "P. Salamon",
                        "slug": "P.-Salamon",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Salamon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Salamon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hansen et al. [1997] provide an analysis of the error-reject trade-off."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14479543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28e0b7e588947ec1b9d648f056f92279417484f8",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the error versus reject tradeoff for classifiers. Our analysis is motivated by the remarkable similarity in error-reject tradeoff curves for widely differing algorithms classifying handwritten characters. We present the data in a new scaled version that makes this universal character particularly evident. Based on Chow's theory of the error-reject tradeoff and its underlying Bayesian analysis we argue that such universality is in fact to be expected for general classification problems. Furthermore, we extend Chow's theory to classifiers working from finite samples on a broad, albeit limited, class of problems. The problems we consider are effectively binary, i.e., classification problems for which almost all inputs involve a choice between the right classification and at most one predominant alternative. We show that for such problems at most half of the initially rejected inputs would have been erroneously classified. We show further that such problems arise naturally as small perturbations of the PAC model for large training sets. The perturbed model leads us to conclude that the dominant source of error comes from pairwise overlapping categories. For infinite training sets, the overlap is due to noise and/or poor preprocessing. For finite training sets there is an additional contribution from the inevitable displacement of the decision boundaries due to finiteness of the sample. In either case, a rejection mechanism which rejects inputs in a shell surrounding the decision boundaries leads to a universal form for the error-reject tradeoff. Finally, we analyze a specific reject mechanism based on the extent of consensus among an ensemble of classifiers. For the ensemble reject mechanism we find an analytic expression for the error-reject tradeoff based on a maximum entropy estimate of the problem difficulty distribution."
            },
            "slug": "The-Error-Reject-Tradeoff-Hansen-Liisberg",
            "title": {
                "fragments": [],
                "text": "The Error-Reject Tradeoff"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "It is argued that universality in error-reject tradeoff curves for widely differing algorithms classifying handwritten characters is in fact to be expected for general classification problems and extended to classifiers working from finite samples on a broad, albeit limited, class of problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109729302"
                        ],
                        "name": "Donald R. Johnson",
                        "slug": "Donald-R.-Johnson",
                        "structuredName": {
                            "firstName": "Donald R.",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750989"
                        ],
                        "name": "F. Gao",
                        "slug": "F.-Gao",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47782928"
                        ],
                        "name": "J. Gong",
                        "slug": "J.-Gong",
                        "structuredName": {
                            "firstName": "Jianjian",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17919171,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "eac71efc12d7e78aea23288e8fc47c27a140f2a9",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In variational data assimilation, optimal ingestion of the observational data, and optimal use of prior physical and statistical information involve the choice of numerous weighting, smoothing, and tuning parameters that control the filtering and merging of diverse sources of information. Generally these weights must be obtained from a partial and imperfect understanding of various sources of errors and are frequently chosen by a combination of historical information, physical reasoning, and trial and error. Generalized cross validation (GCV) has long been one of the methods of choice for choosing certain tuning, smoothing, regularization parameters in ill-posed inverse problems, smoothing, and filtering problems. In theory, it is well suited for the adaptive choice of certain parameters that occur in variational objective analysis and for data assimilation problems that are mathematically equivalent to variational problems. The main drawback of the use of GCV in data assimilation problems was th..."
            },
            "slug": "Adaptive-Tuning-of-Numerical-Weather-Prediction-GCV-Wahba-Johnson",
            "title": {
                "fragments": [],
                "text": "Adaptive Tuning of Numerical Weather Prediction Models: Randomized GCV in Three- and Four-Dimensional Data Assimilation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work has shown that generalized cross validation is well suited for the adaptive choice of certain parameters that occur in variational objective analysis and for data assimilation problems that are mathematically equivalent to variational problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684722"
                        ],
                        "name": "S. Fine",
                        "slug": "S.-Fine",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Fine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To save computation, one could use an incomplete Cholesky factorization in the incomplete Cholesky factorization Newton steps, as suggested by Fine and Scheinberg [2002]. Sometimes it is suggested that it can be useful to replace K by K+ I where is a small constant, to improve the numerical conditioning of K."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13899309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            },
            "slug": "Efficient-SVM-Training-Using-Low-Rank-Kernel-Fine-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Efficient SVM Training Using Low-Rank Kernel Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity and derives an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 110
                            }
                        ],
                        "text": "12) although it would be better practice numerically to use a Cholesky decomposition approach as described in Lawrence et al. [2003]. The scheme evaluates \u2206j over all j \u2208 R at each step to choose the inclusion site. This makes sense when m is small, but as it gets larger it can make sense to select candidate inclusion sites from a subset of R. Lawrence et al. [2003] call this the randomized greedy selection method and give further ideas on how to choose the subset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 110
                            }
                        ],
                        "text": "12) although it would be better practice numerically to use a Cholesky decomposition approach as described in Lawrence et al. [2003]. The scheme evaluates \u2206j over all j \u2208 R at each step to choose the inclusion site."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Lawrence et al. [2003] suggest choosing as the next point (or site) for inclusion into the active set the one that maximizes the differential entropy score \u2206j , H[p(fj)] \u2212 H[p(fj)], where H[p(fj)] is the entropy of the Gaussian at site j \u2208 R (which is a function of the variance at site j as the posterior is Gaussian, see eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Lawrence et al. [2003] call their method the informative vector machine (IVM) IVM"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 785,
                                "start": 110
                            }
                        ],
                        "text": "12) although it would be better practice numerically to use a Cholesky decomposition approach as described in Lawrence et al. [2003]. The scheme evaluates \u2206j over all j \u2208 R at each step to choose the inclusion site. This makes sense when m is small, but as it gets larger it can make sense to select candidate inclusion sites from a subset of R. Lawrence et al. [2003] call this the randomized greedy selection method and give further ideas on how to choose the subset. The differential entropy score \u2206j is not the only criterion that can be used for site selection. For example the information gain criterion KL(p(fj)||p(fj)) can also be used (see Seeger et al., 2003). The use of greedy selection heuristics here is similar to the problem of active learning, see e.g. MacKay [1992c]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 413085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1c20634be8686c950ef21c57b39b98024ce3ead",
            "isKey": true,
            "numCitedBy": 760,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs."
            },
            "slug": "Gaussian-Process-Latent-Variable-Models-for-of-High-Lawrence",
            "title": {
                "fragments": [],
                "text": "Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new underlying probabilistic model for principal component analysis (PCA) is introduced that shows that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, and is extended by considering less restrictive covariance functions which allow non-linear mappings."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120908503,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5bf249445466ac041694525d8969d7879cd259ce",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A general approach to the first order asymptotic analysis ofpenalized likelihood and related estimators is described. The method gives expansions for the systematic and random error. Asymptotic convergence rates in a family of spectral norms are obtained. The theory applies to a broad range of function estimation prob~erns including non\"paxametric. dellSity, hazard and generalized regression curve estimation. Some examples are provided. AMS 1980 subject classifications. Primary, 62-G05, Secondary, 62J05, 41-A35, 41-A25, 47-A53, 45-LlO, 45-M05."
            },
            "slug": "Asymptotic-Analysis-of-Penalized-Likelihood-and-Cox-O\u2019Sullivan",
            "title": {
                "fragments": [],
                "text": "Asymptotic Analysis of Penalized Likelihood and Related Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575699"
                        ],
                        "name": "S. Vijayakumar",
                        "slug": "S.-Vijayakumar",
                        "structuredName": {
                            "firstName": "Sethu",
                            "lastName": "Vijayakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vijayakumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31979282"
                        ],
                        "name": "Aaron D'Souza",
                        "slug": "Aaron-D'Souza",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "D'Souza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron D'Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774593"
                        ],
                        "name": "T. Shibata",
                        "slug": "T.-Shibata",
                        "structuredName": {
                            "firstName": "Tomohiro",
                            "lastName": "Shibata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shibata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302681"
                        ],
                        "name": "J. Conradt",
                        "slug": "J.-Conradt",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Conradt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Conradt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8370683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ede16a3785b1db1f6ae6c522a749db7f7da1fd41",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The complexity of the kinematic and dynamic structure of humanoid robots make conventional analytical approaches to control increasingly unsuitable for such systems. Learning techniques offer a possible way to aid controller design if insufficient analytical knowledge is available, and learning approaches seem mandatory when humanoid systems are supposed to become completely autonomous. While recent research in neural networks and statistical learning has focused mostly on learning from finite data sets without stringent constraints on computational efficiency, learning for humanoid robots requires a different setting, characterized by the need for real-time learning performance from an essentially infinite stream of incrementally arriving data. This paper demonstrates how even high-dimensional learning problems of this kind can successfully be dealt with by techniques from nonparametric regression and locally weighted learning. As an example, we describe the application of one of the most advanced of such algorithms, Locally Weighted Projection Regression (LWPR), to the on-line learning of three problems in humanoid motor control: the learning of inverse dynamics models for model-based control, the learning of inverse kinematics of redundant manipulators, and the learning of oculomotor reflexes. All these examples demonstrate fast, i.e., within seconds or minutes, learning convergence with highly accurate final peformance. We conclude that real-time learning for complex motor system like humanoid robots is possible with appropriately tailored algorithms, such that increasingly autonomous robots with massive learning abilities should be achievable in the near future."
            },
            "slug": "Statistical-Learning-for-Humanoid-Robots-Vijayakumar-D'Souza",
            "title": {
                "fragments": [],
                "text": "Statistical Learning for Humanoid Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is concluded that real-time learning for complex motor system like humanoid robots is possible with appropriately tailored algorithms, such that increasingly autonomous robots with massive learning abilities should be achievable in the near future."
            },
            "venue": {
                "fragments": [],
                "text": "Auton. Robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145395550"
                        ],
                        "name": "P. D. Thompson",
                        "slug": "P.-D.-Thompson",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Thompson",
                            "middleNames": [
                                "Duncan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222559382,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "ff6ec91e349899d91addf6d67c757a44663a8e32",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of smoothing out nonsystematic errors in a two-dimensional field of measurements has been studied from the standpoint of finding the type of weighted area average for which the RMS difference between the true field and the weighted average of the field of observations is the least. For fields whose space-autocorrelation functions are invariant with rotation and have a simple and rather typical form, the optimum weighting function is a linear combination of Bessel functions, whose rate of decrease away from the origin depends partially on the so-called \u201csignal-to-noise\u201d ratio, but primarily on the ratio of the scales of the true field and error field. A comparison of optimum averaging with the analyst's subjective process of smoothing indicates that the former is significantly superior in its ability to distinguish between random small-scale fluctuations and minor synoptic features of only slightly greater scale. Finally, the minimum RMS error of linearly smoothed fields is expressed in terms of the statistical properties of the true fields and the observing system itself. DOI: 10.1111/j.2153-3490.1956.tb01236.x"
            },
            "slug": "Optimum-Smoothing-of-Two-Dimensional-Fields-Thompson",
            "title": {
                "fragments": [],
                "text": "Optimum Smoothing of Two-Dimensional Fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For the noiseless case with k1 = k0, there is a simple lower bound E(n) \u2265 \u2211\u221e i=n+1 \u03bbi due to Micchelli and Wahba [1981]. This bound is obtained by demonstrating that the optimal n pieces of information are the projections of the random function f onto the first n eigenfunctions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7051002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1abfc1c96fe3f2dd2b9282835dea1fd6906fedb0",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a foundation for multi-task learning using reproducing kernel Hilbert spaces of vector-valued functions. In this setting, the kernel is a matrix-valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix- valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi-task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation."
            },
            "slug": "Kernels-for-Multi--task-Learning-Micchelli-Pontil",
            "title": {
                "fragments": [],
                "text": "Kernels for Multi--task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper provides a foundation for multi-task learning using reproducing kernel Hilbert spaces of vector-valued functions using classes of matrix- valued kernels which are linear and are of the dot product or the translation invariant type."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40132383"
                        ],
                        "name": "Phillip Boyle",
                        "slug": "Phillip-Boyle",
                        "structuredName": {
                            "firstName": "Phillip",
                            "lastName": "Boyle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phillip Boyle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A related approach is taken by Boyle and Frean [2005] who introduce correlations between two processes by deriving them as different convolutions of the same underlying white noise process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6491673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad3fb8664178e269ebe8671471c4938baed9ac6b",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are usually parameterised in terms of their covariance functions. However, this makes it difficult to deal with multiple outputs, because ensuring that the covariance matrix is positive definite is problematic. An alternative formulation is to treat Gaussian processes as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead. Using this, we extend Gaussian processes to handle multiple, coupled outputs."
            },
            "slug": "Dependent-Gaussian-Processes-Boyle-Frean",
            "title": {
                "fragments": [],
                "text": "Dependent Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work extends Gaussian processes to handle multiple, coupled outputs, by treating them as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Dawid [1976] calls the generative and discriminative approaches the sampling and diagnostic paradigms, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46644233,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "fc34b14dc7c8170433e7f0f502e78b5a670f3bb0",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In applications of statistical methods to medical diagnosis, information on patients' diseases and symptoms is collected and the resulting data-base is used to diagnose new patients. The data-structure is complicated by a number of factors, two of which are examined here: selection bias and unstable population. Under reasonable conditions, no correction for selection bias is required when assessing probabilities for diseases based on symptom information, and it is suggested that these \"diagnostic distributions\" should form the principal object of study. Transformation of these distributions under changing population structure is considered and shown to take on a simple form in many situations. It is argued that the prevailing paradigm of diagnostic statistics, which concentrates on incidence of symptoms for given disease, is largely inappropriate and should be replaced by an emphasis on diagnostic distributions. The generalized logistic model is seen to fit naturally into the new framework."
            },
            "slug": "Properties-of-diagnostic-data-distributions.-Dawid",
            "title": {
                "fragments": [],
                "text": "Properties of diagnostic data distributions."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that the prevailing paradigm of diagnostic statistics, which concentrates on incidence of symptoms for given disease, is largely inappropriate and should be replaced by an emphasis on diagnostic distributions, and the generalized logistic model is seen to fit naturally into the new framework."
            },
            "venue": {
                "fragments": [],
                "text": "Biometrics"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sollich [1999] derives a number of more accurate approximations to the learning curve than eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8981636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e597460557d44de07ec570738cd2b42cdcc2580",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n2m), storage is O(nm), the cost for prediction is O(n) and the cost to compute confidence bounds is O(nm), where n \u226a m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems."
            },
            "slug": "Sparse-Greedy-Gaussian-Process-Regression-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m, and shows applications to large scale problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2280315,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "id": "93b0eff68081abfa51e499733ffea7188f7a4cf0",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evidences). We find that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain."
            },
            "slug": "Bayesian-Monte-Carlo-Rasmussen-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Bayesian Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is found that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083942236"
                        ],
                        "name": "Francesco Vivarelli",
                        "slug": "Francesco-Vivarelli",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Vivarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Vivarelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "11)), as shown in Sollich [1999] or Opper and Vivarelli [1999]. Using the fact that EX [\u03a6\u03a6>] = nI, a n\u00e4\u0131ve approximation would replace \u03a6\u03a6> inside the trace with its expectation; in fact Opper and Vivarelli [1999] showed that this gives a lower bound, so that"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9137917,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "be1e6fb6e0469089bd680d94129ab8f6402b02af",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Based on a simple convexity lemma, we develop bounds for different types of Bayesian prediction errors for regression with Gaussian processes. The basic bounds are formulated for a fixed training set. Simpler expressions are obtained for sampling from an input distribution which equals the weight function of the covariance kernel, yielding asymptotically tight results. The results are compared with numerical experiments."
            },
            "slug": "General-Bounds-on-Bayes-Errors-for-Regression-with-Opper-Vivarelli",
            "title": {
                "fragments": [],
                "text": "General Bounds on Bayes Errors for Regression with Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Based on a simple convexity lemma, bounds for different types of Bayesian prediction errors for regression with Gaussian processes are developed, yielding asymptotically tight results."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547829"
                        ],
                        "name": "B. Yandell",
                        "slug": "B.-Yandell",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Yandell",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yandell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7237314"
                        ],
                        "name": "W. Raynor",
                        "slug": "W.-Raynor",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Raynor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Raynor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123063504,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4a8beb7eb38042903cd34004f1ef45f88703201b",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the penalized likelihood method for estimating nonparametric regression functions in generalized linear models (Nelder and Wedderburn 1972) and present a generalized cross-validation procedure for empirically assessing an appropriate amount of smoothing in these estimates. Asymptotic arguments and numerical simulations are used to show that the generalized cross-validatory procedure preforms well from the point of view of a weighted mean squared error criterion. The methodology adds to the battery of graphical tools for model building and checking within the generalized linear model framework. Included are two examples motivated by medical and horticultural applications."
            },
            "slug": "Automatic-Smoothing-of-Regression-Functions-in-O\u2019Sullivan-Yandell",
            "title": {
                "fragments": [],
                "text": "Automatic Smoothing of Regression Functions in Generalized Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32854168"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16563958,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0fa0eb19be31b3efaf0acba4e2912a2975bf99d3",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "If there are many independent, identically distributed observations governed by a smooth, finite-dimensional statistical model, the Bayes estimate and the maximum likelihood estimate will be close. Furthermore, the posterior distribution of the parameter vector around the posterior mean will be close to the distribution of the maximum likelihood estimate around truth. Thus, Bayesian confidence sets have good frequentist coverage properties, and conversely. However, even for the simplest infinite-dimensional models, such results do not hold. The object here is to give some examples."
            },
            "slug": "On-the-Bernstein-von-Mises-Theorem-with-Infinite-Freedman",
            "title": {
                "fragments": [],
                "text": "On the Bernstein-von Mises Theorem with Infinite Dimensional Parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7691428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d4f4601940d5b13455541a643a39538bb54b6f3",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms."
            },
            "slug": "Kernel-independent-component-analysis-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Kernel independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A class of algorithms for independent component analysis which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space is presented, showing that these algorithms outperform many of the presently known algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8231010"
                        ],
                        "name": "M. Kuss",
                        "slug": "M.-Kuss",
                        "structuredName": {
                            "firstName": "Malte",
                            "lastName": "Kuss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kuss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This explanation is corroborated further by Kuss and Rasmussen [2005]. It should be noted that all the methods compared on the binary digits classification task except for the linear probit model are using the squared distance between the digitized digit images measured directly in the image space as the suitablility of the covariance function sole input to the algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "12 we show the results Monte Carlo results of running a sophisticated Markov chain Monte Carlo method called Annealed Importance Sampling [Neal, 2001] carried out by Kuss and Rasmussen [2005]. The USPS dataset for these experiments was identical to the one used in Figures 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10524277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68d45f335295f60755003b78bfe5195f4d91d9d7",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are attractive models for probabilistic classification but unfortunately exact inference is analytically intractable. We compare Laplace's method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate."
            },
            "slug": "Assessing-Approximations-for-Gaussian-Process-Kuss-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Assessing Approximations for Gaussian Process Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work compares Laplace's method and Expectation Propagation focusing on marginal likelihood estimates and predictive performance and explains theoretically and corroborate empirically that EP is superior to Laplace."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48377731"
                        ],
                        "name": "G. Winkler",
                        "slug": "G.-Winkler",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Winkler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Winkler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A useful reference on Markov random fields is Winkler [1995]. A simple example of a Gaussian Markov random field has the form"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Wong [1971] showed that the only isotropic quasi-Markov Gaussian field with a continuous covariance function is the degenerate case X(t) = X(0), where X(0) is a Gaussian variate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45306254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "769a3188b73fdd9c5ea10970989827bd6d5c769d",
            "isKey": false,
            "numCitedBy": 697,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The book is mainly concerned with the mathematical foundations of Bayesian image analysis and its algorithms. This amounts to the study of Markov random fields and dynamic Monte Carlo algorithms like sampling, simulated annealing and stochastic gradient algorithms. The approach is introductory and elementary: given basic concepts from linear algebra and real analysis it is self-contained. No previous knowledge from image analysis is required. Knowledge of elementary probability theory and statistics is certainly beneficial but not absolutely necessary. The necessary background from imaging is sketched and illustrated by a number of concrete applications like restoration, texture segmentation and motion analysis."
            },
            "slug": "Image-analysis,-random-fields-and-dynamic-Monte-a-Winkler",
            "title": {
                "fragments": [],
                "text": "Image analysis, random fields and dynamic Monte Carlo methods: a mathematical introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The book is mainly concerned with the mathematical foundations of Bayesian image analysis and its algorithms, which amounts to the study of Markov random fields and dynamic Monte Carlo algorithms like sampling, simulated annealing and stochastic gradient algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Applications of mathematics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32583901"
                        ],
                        "name": "R. Kashyap",
                        "slug": "R.-Kashyap",
                        "structuredName": {
                            "firstName": "Rangasami",
                            "lastName": "Kashyap",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashyap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36297738,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "914c6bfc9eb2fd2d29deb30e4b1fe93d52eeff07",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of closed boundaries of arbitrary shapes on a plane is discussed. Specifically, the problems of representation and reconstruction are considered. A one-to-one correspondence between the given closed boundary and a univariate or multivariate sequence of real numbers is set up. Univariate or multivariate circular autoregressive models are suggested for the representation of the sequence of numbers derived from the closed boundary. The stochastic model representing the closed boundary is invariant to transformations like sealing, translation, choice of starting point, and rotation over angles that are multiples of 2\\pi/N , where N is the number of observations. Methods for estimating the unknown parameters of the model are given and a decision rule for choosing the appropriate number of coefficients is included. Constraints on the estimates are derived so that the estimates are invariant to the transformations of the boundaries. The stochastic model enables the reconstruction of a dosed boundary using FFT algorithms. Results of simulations are included and the application to contour coding is discussed."
            },
            "slug": "Stochastic-models-for-closed-boundary-analysis:-and-Kashyap-Chellappa",
            "title": {
                "fragments": [],
                "text": "Stochastic models for closed boundary analysis: Representation and reconstruction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The stochastic model representing the closed boundary is invariant to transformations like sealing, translation, choice of starting point, and rotation over angles that are multiples of 2\\pi/N, where N is the number of observations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102295411"
                        ],
                        "name": "Zhen Luo",
                        "slug": "Zhen-Luo",
                        "structuredName": {
                            "firstName": "Zhen",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhen Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14759690,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1ba3f2e75ce0def9fb7b9037d3a6867caa3a7470",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An adaptive spline method for smoothing is proposed that combines features from both regression spline and smoothing spline approaches. One of its advantages is the ability to vary the amount of smoothing in response to the inhomogeneous \u201ccurvature\u201d of true functions at different locations. This method can be applied to many multivariate function estimation problems, which is illustrated by an application to smoothing temperature data on the globe. The method's performance in a simulation study is found to be comparable to the wavelet shrinkage methods proposed by Donoho and Johnstone. The problem of how to count the degrees of freedom for an adaptively chosen set of basis functions is addressed. This issue arises also in the MARS procedure proposed by Friedman and other adaptive regression spline procedures."
            },
            "slug": "Hybrid-Adaptive-Splines-Luo-Wahba",
            "title": {
                "fragments": [],
                "text": "Hybrid Adaptive Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108020"
                        ],
                        "name": "L. Shepp",
                        "slug": "L.-Shepp",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Shepp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shepp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Shepp [1966] defined the l-fold integrated Wiener process as"
                    },
                    "intents": []
                }
            ],
            "corpusId": 51695418,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b088c0ef72617cafe0546ea91f9a68b765825c94",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We give simple necessary and sufficient conditions on the mean and covariance for a Gaussian measure to be equivalent to Wiener measure. This was formerly an unsolved problem [26]. An unsolved problem is to obtain the Radom-Nikodym derivative d\u03bc/d\u03bd where \u03bc and \u03bd are equivalent Gaussian measure [28]. We solve this problem for many cases of \u03bc and \u03bd, by writing d\u03bc/d\u03bd in terms of Fredholm determinants and resolvents. The problem is thereby reduced to the calculation of these classical quantities, and explicit formulas can often be given. Our method uses Wiener measure \u03bcw as a catalyst; that is, we compute derivatives with respect to \u03bcw and then use the chain rule: d\u03bc/d\u03bd = (d\u03bc/d\u03bcw) / (d\u03bd/d\u03bcw). Wiener measure is singled out because it has a simple distinctive property--the Wiener process has a random Fourier-type expansion in the integrals of any complete orthonormal system. We show that any process equivalent to the Wiener process W can be realized by a linear transformation of W. This transformation necessarily involves stochastic integration and generalizes earlier nodulation transformations studied by Legal [21] and others [4], [27]. New variants of the Wiener process are introduced, both conditioned Wiener processes and free n-fold integrated Wiener processes. We given necessary and sufficient conditions for a Gaussian process to be equivalent to any one of the variants and also give the corresponding Radon-Niels (R-N) derivative. Last, some novel uses of R-N derivatives are given. We calculate explicitly: (i) the probability that W cross a slanted line in a finite time, (ii) the first passage probability for the process W (T + 1) \u2212 W(t), and (iii) a class of function space integrals. Using (iii) we prove a zero-one law for convergence of certain integrals on Wiener paths. Disciplines Applied Statistics This journal article is available at ScholarlyCommons: http://repository.upenn.edu/statistics_papers/347 Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. www.jstor.org \u00ae"
            },
            "slug": "Radon-Nikodym-Derivatives-of-Gaussian-Measures-Shepp",
            "title": {
                "fragments": [],
                "text": "Radon-Nikodym Derivatives of Gaussian Measures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38387373"
                        ],
                        "name": "A. Girard",
                        "slug": "A.-Girard",
                        "structuredName": {
                            "firstName": "Agathe",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Girard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177568"
                        ],
                        "name": "J. Q. Candela",
                        "slug": "J.-Q.-Candela",
                        "structuredName": {
                            "firstName": "Joaquin",
                            "lastName": "Candela",
                            "middleNames": [
                                "Qui\u00f1onero"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Q. Candela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402170019"
                        ],
                        "name": "R. Murray-Smith",
                        "slug": "R.-Murray-Smith",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "Murray-Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Murray-Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "See Poggio and Girosi [1990] for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Girard et al. [2003] showed that it is possible to compute the mean and variance of the output analytically when using the SE covariance function and Gaussian input noise."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8808015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c12a9e385935108beb003a97c0e2859d8be5df52",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form yt = f(yt-1,...,yt-L), the prediction of y at time t + k is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction."
            },
            "slug": "Gaussian-Process-Priors-with-Uncertain-Inputs-to-Girard-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Gaussian Process Priors with Uncertain Inputs - Application to Multiple-Step Ahead Time Series Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows how an analytical Gaussian approximation can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction of the multi-step ahead prediction in time series analysis."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2) it is infeasible to consider all members of set R for inclusion on each iteration; instead Smola and Sch\u00f6lkopf [2000] suggest finding the best point to include from a randomly chosen subset of set R on each iteration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1888591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c35cc80fe8c6cdea742d4fa1af1f2e698d41aba7",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of exible conditional probability models and techniques for classi cation regression problems Many existing methods such as generalized linear models and support vector machines are subsumed under this class The exibility of this class of techniques comes from the use of kernel functions as in support vector machines and the generality from dual formulations of stan dard regression models"
            },
            "slug": "Probabilistic-kernel-regression-models-Jaakkola-Haussler",
            "title": {
                "fragments": [],
                "text": "Probabilistic kernel regression models"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A class of exible conditional probability models and techniques for classi cation regression problems that comes from the use of kernel functions as in support vector machines and the generality from dual formulations of stan dard regression models is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e33131dc7180e92be2f2dfd366aaf1c0ed50dee8",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a semiparametric model for regression and classification problems involving multiple response variables. The model makes use of a set of Gaussian processes to model the relationship to the inputs in a nonparametric fashion. Conditional dependencies between the responses can be captured through a linear mixture of the driving processes. This feature becomes important if some of the responses of predictive interest are less densely supplied by observed data than related auxiliary ones. We propose an efficient approximate inference scheme for this semiparametric model whose complexity is linear in the number of training data points."
            },
            "slug": "Semiparametric-latent-factor-models-Teh-Seeger",
            "title": {
                "fragments": [],
                "text": "Semiparametric latent factor models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A semiparametric model for regression and classification problems involving multiple response variables makes use of a set of Gaussian processes to model the relationship to the inputs in a nonparametric fashion."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860160"
                        ],
                        "name": "Baver Okutmustur",
                        "slug": "Baver-Okutmustur",
                        "structuredName": {
                            "firstName": "Baver",
                            "lastName": "Okutmustur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baver Okutmustur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 126243841,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5596cbce3b079c23215a865a9a6c38dfb44bce8a",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "REPRODUCING KERNEL HILBERT SPACES Baver Okutmustur M.S. in Mathematics Supervisor: Assist. Prof. Dr. Aurelian Gheondea August, 2005 In this thesis we make a survey of the theory of reproducing kernel Hilbert spaces associated with positive definite kernels and we illustrate their applications for interpolation problems of Nevanlinna-Pick type. Firstly we focus on the properties of reproducing kernel Hilbert spaces, generation of new spaces and relationships between their kernels and some theorems on extensions of functions and kernels. One of the most useful reproducing kernel Hilbert spaces, the Bergman space, is studied in details in chapter 3. After giving a brief definition of Hardy spaces, we dedicate the last part for applications of interpolation problems of NevanlinnaPick type with three main theorems: interpolation with a finite number of points, interpolation with an infinite number of points and interpolation with points on the boundary. Finally we include an Appendix that contains a brief recall of the main results from functional analysis and operator theory."
            },
            "slug": "Reproducing-kernel-Hilbert-spaces-Okutmustur",
            "title": {
                "fragments": [],
                "text": "Reproducing kernel Hilbert spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2753465"
                        ],
                        "name": "H. Voorhees",
                        "slug": "H.-Voorhees",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Voorhees",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Voorhees"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For the regularizer \u2016Pf\u2016 = \u222b (\u2207f)dx in two dimensions, the equivalent kernel is given in terms of the Kelvin function kei (Poggio et al. 1985, Stein 1991). Silverman [1984] has also shown that for splines of order m in 1-d (corresponding to a roughness penalty of \u222b (f ) dx) the width of the equivalent kernel will scale as n\u22121/2m asymptotically."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6006893,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6577a2344e4a0ea5992787352d03bf2478773239",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-regularized-solution-to-edge-detection-Poggio-Voorhees",
            "title": {
                "fragments": [],
                "text": "A regularized solution to edge detection"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Craven and Wahba [1979] describe a variant of cross-validation using squared error known as generalized cross-validation which gives different weightings to different datapoints so as to achieve certain invariance properites."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 189781595,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "02cb8ef325adcc6f29e0b1759920527836ea8b2b",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how to choose the smoothing parameter when a smoothing periodic spline of degree 2m\u22121 is used to reconstruct a smooth periodic curve from noisy ordinate data. The noise is assumed \u201cwhite\u201d, and the true curve is assumed to be in the Sobolev spaceW2(2m) of periodic functions with absolutely continuousv-th derivative,v=0, 1, ..., 2m\u22121 and square integrable 2m-th derivative. The criteria is minimum expected square error, averaged over the data points. The dependency of the optimum smoothing parameter on the sample size, the noise variance, and the smoothness of the true curve is found explicitly."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51222978"
                        ],
                        "name": "P. Mazur",
                        "slug": "P.-Mazur",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Mazur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mazur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123530867,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "927e9d6a05476c8c5f874ec48729d7257bbba79c",
            "isKey": false,
            "numCitedBy": 1084,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-theory-of-brownian-motion-Mazur",
            "title": {
                "fragments": [],
                "text": "On the theory of brownian motion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406409112"
                        ],
                        "name": "A. O'Hagan",
                        "slug": "A.-O'Hagan",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "O'Hagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Hagan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This idea was proposed under the name of Bayes-Hermite quadrature by O\u2019Hagan [1991], and later under the name of Bayesian Monte Carlo in Rasmussen and Ghahramani [2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125181692,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f9ba21ba8e71c19d34ccb754e60ad23ee8054e88",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The optimal design problem is tackled in the framework of a new model and new objectives. A regression model is proposed in which the regression function is permitted to take any form over the space :!l' of independent variables. The design objective is based on fitting a simplified function for prediction. The approach is Bayesian throughout. The new designs are more robust than conventional ones. They also avoid the need to limit artificially design points to a predetermined subset of:!l'. New solutions are also offered for the problems of smoothing, curve fitting and the selection of regressor variables."
            },
            "slug": "Curve-Fitting-and-Optimal-Design-for-Prediction-O'Hagan",
            "title": {
                "fragments": [],
                "text": "Curve Fitting and Optimal Design for Prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463572"
                        ],
                        "name": "M. O. Stitson",
                        "slug": "M.-O.-Stitson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stitson",
                            "middleNames": [
                                "Oliver"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O. Stitson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118468304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "221663c3ec94babfaa0754a00d92ff69e2a4424a",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) [Vapng] are a way of imposing a structure on multi-dimensional kernels which are generated as the tensor product of one-dimensional kernels. This gives more accurate control over the capacity of the learning machine (VCdimension). SVAD uses ideas from ANOVA decomposition methods and extends them to generate kernels which directly implement these ideas. SVAD is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel. The Boston housing data set from UCI has been tested on Bagging [Bre94] and Support Vector methods before [DBK97] and these results are compared to the SVAD method."
            },
            "slug": "Support-vector-regression-with-ANOVA-decomposition-Stitson-Gammerman",
            "title": {
                "fragments": [],
                "text": "Support vector regression with ANOVA decomposition kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613576"
                        ],
                        "name": "Peter Sollich",
                        "slug": "Peter-Sollich",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sollich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Sollich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7633425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f069d697a061c203103cfac7c45539468c584144",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The equivalent kernel [1] is a way of understanding how Gaussian process regression works for large sample sizes based on a continuum limit. In this paper we show (1) how to approximate the equivalent kernel of the widely-used squared exponential (or Gaussian) kernel and related kernels, and (2) how analysis using the equivalent kernel helps to understand the learning curves for Gaussian processes."
            },
            "slug": "Using-the-Equivalent-Kernel-to-Understand-Gaussian-Sollich-Williams",
            "title": {
                "fragments": [],
                "text": "Using the Equivalent Kernel to Understand Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper shows how to approximate the equivalent kernels of the widely-used squared exponential (or Gaussian) kernel and related kernels, and how analysis using the equivalent kernel helps to understand the learning curves for Gaussian processes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example Cristianini et al. [2002] define the alignment between a Gram matrix K and the corresponding +1/\u2212 1 vector of alignment targets y as A(K,y) = y>Ky n\u2016K\u2016F , (5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17466014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36aa0d0936b2cf128c646c36a1981807b5a27aaf",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction."
            },
            "slug": "On-Kernel-Target-Alignment-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "On Kernel-Target Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function, is introduced, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on a test set, giving improved classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103897"
                        ],
                        "name": "Changjiang Yang",
                        "slug": "Changjiang-Yang",
                        "structuredName": {
                            "firstName": "Changjiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changjiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719541"
                        ],
                        "name": "R. Duraiswami",
                        "slug": "R.-Duraiswami",
                        "structuredName": {
                            "firstName": "Ramani",
                            "lastName": "Duraiswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duraiswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An early paper on this subject is by Ylvisaker [1975]. These questions have been addressed both in the statistical literature and in theoretical numerical analysis; for the latter area the book by Ritter [2000] provides a useful overview."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, recent work by Yang et al. [2005] uses the improved fast Gauss transform for this purpose."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 5809817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "303600138609133e1cf558fca94514d9c19470d2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The computation and memory required for kernel machines with N training samples is at least O(N2). Such a complexity is significant even for moderate size problems and is prohibitive for large datasets. We present an approximation technique based on the improved fast Gauss transform to reduce the computation to O(N). We also give an error bound for the approximation, and provide experimental results on the UCI datasets."
            },
            "slug": "Efficient-Kernel-Machines-Using-the-Improved-Fast-Yang-Duraiswami",
            "title": {
                "fragments": [],
                "text": "Efficient Kernel Machines Using the Improved Fast Gauss Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An approximation technique based on the improved fast Gauss transform to reduce the computation to O(N) is presented and an error bound for the approximation is given."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 (Moore-Aronszajn theorem, Aronszajn [1950])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The theory was developed by Aronszajn [1950]; a more recent treatise is Saitoh [1988]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": false,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684150"
                        ],
                        "name": "D. Kammler",
                        "slug": "D.-Kammler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kammler",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kammler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We follow the treatment given by Kammler [2000]. We consider Fourier analysis of functions on the real line R, of periodic functions of period l on the circle Tl, of functions defined on the integer lattice Z, and of functions on PN , the regular N -polygon, which is a discretization of Tl."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120535559,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "365933da4dbf16ce175a1f47fefa4cd7ae0a3b0c",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Fourier's Representation for Functions on R, Tp, Z, and PN. 2. Convolution of Functions on R, Tp, Z and PN. 3. The Calculus for Finding Fourier Transforms of Functions of R. 4. The Calculus for Finding Fourier Transforms of Functions of Tp, Z, and PN. 5. Operator Identities Associated with Fourier Analysis. 6. The Fast Fourier Transform. 7. Generalized Functions on R. 8. Sampling. 9. Partial Differential Equations. 10. Wavelets. 11. Musical Tones. 12. Probability. Appendix 0: The Impact of Fourier Analysis. Appendix 1: Functions and Their Fourier Transforms. Appendix 2: The Fourier Transform Calculus. Appendix 3: Operators and Their Fourier Transforms. Appendix 4: The Whittaker-Robinson Flow Chart for Harmonic Analysis. Appendix 5: FORTRAN Code for a Radix 2 FFT. Appendix 6: The Standard Normal Probability Distribution. Appendix 7: Frequencies of the Piano Keyboard. Index."
            },
            "slug": "A-First-Course-in-Fourier-Analysis-Kammler",
            "title": {
                "fragments": [],
                "text": "A First Course in Fourier Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207605229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2639515c248f220c73d44688c0097a99b01e1474",
            "isKey": false,
            "numCitedBy": 1456,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis, which is based on a linear transformation between the latent space and the data space. In this article, we introduce a form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm. GTM provides a principled alternative to the widely used self-organizing map (SOM) of Kohonen (1982) and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multiphase oil pipeline."
            },
            "slug": "GTM:-The-Generative-Topographic-Mapping-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "GTM: The Generative Topographic Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781982"
                        ],
                        "name": "D. Hand",
                        "slug": "D.-Hand",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hand",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712654"
                        ],
                        "name": "H. Mannila",
                        "slug": "H.-Mannila",
                        "structuredName": {
                            "firstName": "Heikki",
                            "lastName": "Mannila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mannila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1363,
                                "start": 158
                            }
                        ],
                        "text": "The effective number of parameters or degrees of freedom of the smoother is degrees of freedom defined as tr(K(K + \u03c3(2) nI) \u22121) = \u2211n i=1 \u03bbi/(\u03bbi + \u03c3 2 n), see Hastie and Tibshirani [1990, sec. 3.5]. Notice that this counts the number of eigenvectors which are not eliminated. We can define a vector of functions h(x\u2217) = (K + \u03c3(2) nI) k(x\u2217). Thus we have f\u0304(x\u2217) = h(x\u2217)y, making it clear that the mean prediction at a point x\u2217 is a linear combination of the target values y. For a fixed test point x\u2217, h(x\u2217) gives the vector of weights applied to targets y. h(x\u2217) is called the weight function [Silverman, 1984]. As Gaussian process regression is a linear smoother, weight function the weight function does not depend on y. Note the difference between a linear model, where the prediction is a linear combination of the inputs, and a linear smoother, where the prediction is a linear combination of the training set targets. Understanding the form of the weight function is made complicated by the matrix inversion ofK+\u03c3(2) nI and the fact thatK depends on the specific locations of the n datapoints. Idealizing the situation one can consider the observations to be \u201csmeared out\u201d in x-space at some density of observations. In this case analytic tools can be brought to bear on the problem, as shown in section 7.1. By analogy to kernel smoothing, Silverman [1984] called the idealized weight function the equivalent kernel ; see also Girosi et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d1896f86c8d504cf8822468e84534a9d671095a",
            "isKey": true,
            "numCitedBy": 3759,
            "numCiting": 421,
            "paperAbstract": {
                "fragments": [],
                "text": "Data mining is the discovery of interesting, unexpected or valuable structures in large datasets. As such, it has two rather different aspects. One of these concerns large-scale, \u2018global\u2019 structures, and the aim is to model the shapes, or features of the shapes, of distributions. The other concerns small-scale, \u2018local\u2019 structures, and the aim is to detect these anomalies and decide if they are real or chance occurrences. In the context of signal detection in the pharmaceutical sector, most interest lies in the second of the above two aspects; however, signal detection occurs relative to an assumed background model, therefore, some discussion of the first aspect is also necessary. This paper gives a lightning overview of data mining and its relation to statistics, with particular emphasis on tools for the detection of adverse drug reactions."
            },
            "slug": "Principles-of-Data-Mining-Hand-Mannila",
            "title": {
                "fragments": [],
                "text": "Principles of Data Mining"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper gives a lightning overview of data mining and its relation to statistics, with particular emphasis on tools for the detection of adverse drug reactions."
            },
            "venue": {
                "fragments": [],
                "text": "Drug safety"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "19) was called a regularization network regularization network in Poggio and Girosi [1990]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Following Szeliski [1987] and Poggio and Girosi [1990] we consider"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This has been suggested by a number of authors including Poggio and Girosi [1990] and Suykens and Vanderwalle [1999]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "19], Poggio and Girosi [1990] and also in Vivarelli and Williams [1999]; in the latter work a low-rank M was used to implement a linear dimensionality reduction step from the input space to lower-dimensional feature space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Poggio and Girosi [1990]. In section 6."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122927965,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ae6ad5780d3562b0fb4af3fa4e17998fef53a2bc",
            "isKey": true,
            "numCitedBy": 23,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Least squares estimators are very common in statistics, but they lead to results that are very sensitive to outliers, and it has been proposed to minimize other measures of error, that lead to ``robust'''' estimates. In this paper we show that using these robust estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance fluctuates according to some given probability distribution, that uniquely determines the estimator."
            },
            "slug": "Models-of-Noise-and-Robust-Estimation-Girosi",
            "title": {
                "fragments": [],
                "text": "Models of Noise and Robust Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that using robust least squares estimators corresponds to assuming that data are corrupted by Gaussian noise whose variance fluctuates according to some given probability distribution, that uniquely determines the estimator."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47528929"
                        ],
                        "name": "D. L. Hawkins",
                        "slug": "D.-L.-Hawkins",
                        "structuredName": {
                            "firstName": "Doyle",
                            "lastName": "Hawkins",
                            "middleNames": [
                                "L."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. L. Hawkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120397306,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4a6b70b95d3994513a30ae3e17c86d72a8c06b5",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Two problems in the practical implementation of a certain sieve estimator of the Gaussian process mean are discussed and illustrated via Monte Carlo experiments. The problems are: (1) introduction of extra error due to the necessity of estimating certain stochastic integrals; (2) the value of the sieve index."
            },
            "slug": "Some-practical-problems-in-implementing-a-certain-Hawkins",
            "title": {
                "fragments": [],
                "text": "Some practical problems in implementing a certain sieve estimator of the gaussian mean function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1670,
                                "start": 60
                            }
                        ],
                        "text": "In this case universal consistency results can be obtained [Devroye et al., 1996, ch. 30] under certain technical conditions and growth rates on k. Although universal consistency is a \u201cgood thing\u201d, it does not necessarily mean that we should only consider procedures that have this property; for example if on a specific problem we knew that a linear regression model was consistent for that problem then it would be very natural to use it. In the 1980\u2019s there was a large surge in interest in artificial neural networks neural networks (ANNs), which are feedforward networks consisting of an input layer, followed by one or more layers of non-linear transformations of weighted combinations of the activity from previous layers, and an output layer. One reason for this surge of interest was the use of the backpropagation algorithm for training ANNs. Initial excitement centered around that fact that training non-linear networks was possible, but later the focus came onto the generalization performance of ANNs, and how to deal with questions such as how many layers of hidden units to use, how many units there should be in each layer, and what type of non-linearities should be used, etc. For a particular ANN the search for a good set of weights for a given training set is complicated by the fact that there can be local optima in the optimization problem; this can cause significant difficulties in practice. In contrast for Gaussian process regression and classification the posterior for the latent variables is convex. One approach to the problems raised above was to put ANNs in a Bayesian Bayesian neural networks framework, as developed by MacKay [1992a] and Neal [1996]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1686,
                                "start": 60
                            }
                        ],
                        "text": "In this case universal consistency results can be obtained [Devroye et al., 1996, ch. 30] under certain technical conditions and growth rates on k. Although universal consistency is a \u201cgood thing\u201d, it does not necessarily mean that we should only consider procedures that have this property; for example if on a specific problem we knew that a linear regression model was consistent for that problem then it would be very natural to use it. In the 1980\u2019s there was a large surge in interest in artificial neural networks neural networks (ANNs), which are feedforward networks consisting of an input layer, followed by one or more layers of non-linear transformations of weighted combinations of the activity from previous layers, and an output layer. One reason for this surge of interest was the use of the backpropagation algorithm for training ANNs. Initial excitement centered around that fact that training non-linear networks was possible, but later the focus came onto the generalization performance of ANNs, and how to deal with questions such as how many layers of hidden units to use, how many units there should be in each layer, and what type of non-linearities should be used, etc. For a particular ANN the search for a good set of weights for a given training set is complicated by the fact that there can be local optima in the optimization problem; this can cause significant difficulties in practice. In contrast for Gaussian process regression and classification the posterior for the latent variables is convex. One approach to the problems raised above was to put ANNs in a Bayesian Bayesian neural networks framework, as developed by MacKay [1992a] and Neal [1996]. This gives rise"
                    },
                    "intents": []
                }
            ],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31010503"
                        ],
                        "name": "J. Hunter",
                        "slug": "J.-Hunter",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hunter",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hunter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2320788,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "66d409772c6ec165851cbed7263fcf4ee7cb3388",
            "isKey": false,
            "numCitedBy": 2750,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Measure-Theory-Hunter",
            "title": {
                "fragments": [],
                "text": "Measure Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3235534"
                        ],
                        "name": "H. Wendland",
                        "slug": "H.-Wendland",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Wendland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wendland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Prediction with Gaussian processes is certainly not a very recent topic, especially for time series analysis; the basic theory goes back at least as far as the time series work of Wiener [1949] and Kolmogorov [1941] in the 1940\u2019s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117139758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3d3440c64a09ec544d5cf694ae00f6d407138ccd",
            "isKey": false,
            "numCitedBy": 1373,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Applications and motivations 2. Hear spaces and multivariate polynomials 3. Local polynomial reproduction 4. Moving least squares 5. Auxiliary tools from analysis and measure theory 6. Positive definite functions 7. Completely monotine functions 8. Conditionally positive definite functions 9. Compactly supported functions 10. Native spaces 11. Error estimates for radial basis function interpolation 12. Stability 13. Optimal recovery 14. Data structures 15. Numerical methods 16. Generalised interpolation 17. Interpolation on spheres and other manifolds."
            },
            "slug": "Scattered-Data-Approximation-Wendland",
            "title": {
                "fragments": [],
                "text": "Scattered Data Approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Fowlkes et al. [2001] have applied the Nystr\u00f6m method to approximate the top few eigenvectors in a computer vision problem where the matrices in question are larger than 10\u00d710 in size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2886088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e4392c40ea4618c554bf72b74aeec6b1739cbf9",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral graph theoretic methods have recently shown great promise for the problem of image segmentation, but due to the computational demands, applications of such methods to spatiotemporal data have been slow to appear For even a short video sequence, the set of all pairwise voxel similarities is a huge quantity of data: one second of a 256/spl times/384 sequence captured at 30 Hz entails on the order of 10/sup 13/ pairwise similarities. The contribution of this paper is a method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning, making it feasible to apply them to very large spatiotemporal grouping problems. Our approach is based on a technique for the numerical solution of eigenfunction problems known as the Nystrom method This method allows extrapolation of the complete grouping solution using only a small number of \"typical\" samples. In doing so, we successfully exploit the fact that there are far fewer coherent groups in an image sequence than pixels."
            },
            "slug": "Efficient-spatiotemporal-grouping-using-the-Nystrom-Fowlkes-Belongie",
            "title": {
                "fragments": [],
                "text": "Efficient spatiotemporal grouping using the Nystrom method"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning, making it feasible to apply them to very large spatiotemporal grouping problems, based on a technique for the numerical solution of eigenfunction problems known as the Nystrom method."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Below we state some definitions and properties of convex sets and functions taken from Boyd and Vandenberghe [2004]. A set C is convex if the line segment between any two points in C lies in C, convex sets i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": false,
            "numCitedBy": 38725,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144327162"
                        ],
                        "name": "I. Blake",
                        "slug": "I.-Blake",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Blake",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2485933"
                        ],
                        "name": "W. Lindsey",
                        "slug": "W.-Lindsey",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Lindsey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lindsey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39682952,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e67771847e897e53db5cb86352c93f70d7898a3c",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "In a variety of practical problems involving random processes, it is necessary to have statistical information on their level-crossing properties. This paper presents a survey of known results on certain aspects of this problem and provides a basis for further study in the area. The goal has been to give a broad view of the problems considered in the literature and a brief indication of the techniques used in their solution. Much material of a more or less historical nature has been included since, to the authors' knowledge, no other survey of this nature exists."
            },
            "slug": "Level-crossing-problems-for-random-processes-Blake-Lindsey",
            "title": {
                "fragments": [],
                "text": "Level-crossing problems for random processes"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A survey of known results on certain aspects of the level-crossing properties of random processes is presented and provides a basis for further study in the area."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A proof of the positive definiteness of this covariance function can be found in Schoenberg [1938]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18673721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b9a933d2aaeed93d99064f64a8e58814017695ef",
            "isKey": false,
            "numCitedBy": 758,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As poo we get the space Em with the distance function maxi-, ... I xi X. Let, furthermore, lP stand for the space of real sequences with the series of pth powers of the absolute values convergent. Similarly let LP denote the space of real measurable functions in the interval (0, 1) which are summable to the pth power, while C shall mean the space of real continuous functions in the same interval. In all these spaces a distance function is assumed to be defined as usual. t L2 is equivalent to the real Hilbert space t. The spaces EmP, IP and LP are metric only if p > 1, but we shall consider them also for positive values of p O). A general theorem of Banach and Mazur ([1], p. 187) states that any separable metric space (5 may be imbedded isometrically in the space C. Furthermore, as a special case of a well known theorem of Urysohn, any such space (E may be imbedded topologically in t. Isometric imbeddability of (E in '& is, however, a much more restricted property of (B. The chief purpose of this paper is to point out the intimate relationship between the problem of isometric imbedding and the concept of positive definite functions, if this concept is properly enlarged. As a first approach to this connection we consider here isometric imbedding in Hilbert space only. It turns out that the possibility of imbedding$ in 6& is very easily expressible in terms of the elementary function e-t2 and the concept of positive definite functions (Theorem 1). The author's previous result ([10]) to the effect that i(,y), (O <,y < 1), which is the space arising from 6& by raising its metric to a"
            },
            "slug": "Metric-spaces-and-positive-definite-functions-Schoenberg",
            "title": {
                "fragments": [],
                "text": "Metric spaces and positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1938
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218687"
                        ],
                        "name": "P. Rousseeuw",
                        "slug": "P.-Rousseeuw",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rousseeuw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rousseeuw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33519033,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6677822795fd4aecb2c2cd262f3f3a6cca65295d",
            "isKey": false,
            "numCitedBy": 3539,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Classical least squares regression consists of minimizing the sum of the squared residuals. Many authors have produced more robust versions of this estimator by replacing the square by something else, such as the absolute value. In this article a different approach is introduced in which the sum is replaced by the median of the squared residuals. The resulting estimator can resist the effect of nearly 50% of contamination in the data. In the special case of simple regression, it corresponds to finding the narrowest strip covering half of the observations. Generalizations are possible to multivariate location, orthogonal regression, and hypothesis testing in linear models."
            },
            "slug": "Least-Median-of-Squares-Regression-Rousseeuw",
            "title": {
                "fragments": [],
                "text": "Least Median of Squares Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815604"
                        ],
                        "name": "M. Schonlau",
                        "slug": "M.-Schonlau",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schonlau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schonlau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228938"
                        ],
                        "name": "W. Welch",
                        "slug": "W.-Welch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 114
                            }
                        ],
                        "text": "75 EI(x) = minn yn \u2212 ! m(x) ( )\u03a6 minn yn \u2212 ! m(x) ! \u03c3 (x) \u239b \u239d\u239c \u239e \u23a0\u239f + ! \u03c3 (x)\u03c6 minn yn \u2212 ! m(x) ! \u03c3 (x) \u239b \u239d\u239c \u239e \u23a0\u239f Jones et al. (1998)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 13068209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daa63f57c3fbe994c4356f8d986a22e696e776d2",
            "isKey": false,
            "numCitedBy": 5736,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome."
            },
            "slug": "Efficient-Global-Optimization-of-Expensive-Jones-Schonlau",
            "title": {
                "fragments": [],
                "text": "Efficient Global Optimization of Expensive Black-Box Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering and shows how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734566"
                        ],
                        "name": "D. Cornford",
                        "slug": "D.-Cornford",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Cornford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cornford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3263426"
                        ],
                        "name": "I. Nabney",
                        "slug": "I.-Nabney",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Nabney",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Nabney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example a value of \u03bd = 5/2 was used in [Cornford et al., 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120074784,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "499987b1102d58f486495b3bf7ef160d1f173d31",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian procedure for the retrieval of wind vectors over the ocean using satellite-borne scatterometers requires realistic prior near-surface wind field models over the oceans. We have implemented carefully chosen vector Gaussian Process models; however, in some cases these models are too smooth to reproduce real atmospheric features, such as fronts. At the scale of the scatterometer observations, fronts appear as discontinuities in wind direction. Due to the nature of the retrieval problem a simple discontinuity model is not feasible, and hence we have developed a constrained discontinuity vector Gaussian Process model which ensures realistic fronts. We describe the generative model and show how to compute the data likelihood given the model. We show the results of inference using the model with Markov Chain Monte Carlo methods on both synthetic and real data."
            },
            "slug": "Modelling-Frontal-Discontinuities-in-Wind-Fields-Cornford-Nabney",
            "title": {
                "fragments": [],
                "text": "Modelling Frontal Discontinuities in Wind Fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38089959"
                        ],
                        "name": "Mark N. Gibbs",
                        "slug": "Mark-N.-Gibbs",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gibbs",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark N. Gibbs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14456885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d429b63e0b8e329c565766289b4189c9398174",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes are a promising nonlinear regression tool, but it is not straightforward to solve classification problems with them. In this paper the variational methods of Jaakkola and Jordan are applied to Gaussian processes to produce an efficient Bayesian binary classifier."
            },
            "slug": "Variational-Gaussian-process-classifiers-Gibbs-Mackay",
            "title": {
                "fragments": [],
                "text": "Variational Gaussian process classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The variational methods of Jaakkola and Jordan are applied to Gaussian processes to produce an efficient Bayesian binary classifier."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "by looking at matches of subtrees [Collins and Duffy, 2002]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "by looking at matches of subtrees [Collins and Duffy, 2002]. Leslie et al. [2003] have applied string kernels to the classification of protein domains into SCOP superfamilies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 396794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c7adc28e20d361d5c35aa9808094b10f6a34d1",
            "isKey": false,
            "numCitedBy": 932,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees."
            },
            "slug": "Convolution-Kernels-for-Natural-Language-Collins-Duffy",
            "title": {
                "fragments": [],
                "text": "Convolution Kernels for Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and experimental results on the ATIS corpus of parse trees are given."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the splines literature, Cox [1984] showed that for regression problems using the regularizer \u2016f\u2016m = \u2211m k=0 \u2016Of\u2016 (using the definitions in eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119965968,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b14391b13324eb91b7ed1cf4c4a29e0bc47373ae",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Given data $z_i = g(t_i ) + \\varepsilon _i , 1 \\leqq i \\leqq n$, where g is the unknown function, the $t_i $ are known d-dimensional variables in a domain $\\Omega $, and the $\\varepsilon _i $ are i..."
            },
            "slug": "MULTIVARIATE-SMOOTHING-SPLINE-FUNCTIONS-Cox",
            "title": {
                "fragments": [],
                "text": "MULTIVARIATE SMOOTHING SPLINE FUNCTIONS"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Given data, g is the unknown function, the t_i and the varepsilon variables are known d-dimensional variables in a domain $\\Omega $, and the $\\varpsilon _i $ are i..."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117919011,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b73646c80a62fa038b1e9da66f8aa5ba2f5e3e18",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : We consider the problem of interpolating a surface given its values at a finite number of points. We place a special emphasis on the question of choosing the location of the points where the function will be sampled. Using minimal norm interpolation in reproducing kernel Hilbert spaces, equivalently Bayesian interpolation, and N-widths, we provide lower bounds for interpolation error relative to certain error criteria. These lower bounds can be used when evaluating an existing design, or when attempting to obtain a good design by iterative procedures to decide whether further minimization is worthwhile. The bounds are given in terms of the eigenvalues of a relevant reproducing kernel and the asymptotic behavior of these eigenvalues for certain tensor product spaces in the unit d-dimensional cube is obtained."
            },
            "slug": "Design-Problems-for-Optimal-Surface-Interpolation.-Micchelli-Wahba",
            "title": {
                "fragments": [],
                "text": "Design Problems for Optimal Surface Interpolation."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Using minimal norm interpolation in reproducing kernel Hilbert spaces, equivalently Bayesian interpolation, and N-widths, this work provides lower bounds for interpolation error relative to certain error criteria."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 417,
                                "start": 14
                            }
                        ],
                        "text": ") As shown in Duda and Hart [1973, section 5.8], choosing y+, y\u2212 appropriately allows us to obtain the same solution as Fisher\u2019s linear discriminant using the decision criterion f(x) \u2277 0. Also, they show that using targets y+ = +1, y\u2212 = \u22121 with the least-squares error function gives a minimum squared-error approximation to the Bayes discriminant function p(C+|x)\u2212p(C\u2212|x) as n\u2192\u221e. Following Rifkin and Klautau [2004] we call such methods least-squares classification (LSC)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": true,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686671"
                        ],
                        "name": "L. Csat\u00f3",
                        "slug": "L.-Csat\u00f3",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Csat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Csat\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Matheron, 1973; Journel and Huijbregts, 1978) where it is known as kriging , and in meteorology [Thompson, 1956, Daley, 1991] although this literakriging ture naturally has focussed mostly on two- and three-dimensional input spaces. Whittle [1963, sec. 5.4] also suggests the use of such methods for spatial prediction. Ripley [1981] and Cressie [1993] provide useful overviews of Gaussian process prediction in spatial statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10406494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89e2d6d74eb6593d9bdf47bcdf9942355345996f",
            "isKey": true,
            "numCitedBy": 36,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a specific sequential minimization of the free energy leads to a generalization of Minka's expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classification and density estimation with Gaussian processes and on an independent component analysis problem."
            },
            "slug": "TAP-Gibbs-Free-Energy,-Belief-Propagation-and-Csat\u00f3-Opper",
            "title": {
                "fragments": [],
                "text": "TAP Gibbs Free Energy, Belief Propagation and Sparsity"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived and how a specific sequential minimization of the free energy leads to a generalization of Minka's expectation propagation is shown."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16910350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33f1522d3cae6f41a0af5247fd7a18755d0b19b0",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we analyze the relationships between the eigenvalues of the m x m Gram matrix K for a kernel k (\u00b7, \u00b7) corresponding to a sample x1,...,xm drawn from a density p(x) and the eigenvalues of the corresponding continuous eigenproblem. We bound the differences between the two spectra and provide a performance bound on kernel PCA."
            },
            "slug": "The-Stability-of-Kernel-Principal-Components-and-to-Shawe-Taylor-Williams",
            "title": {
                "fragments": [],
                "text": "The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The relationships between the eigenvalues of the m x m Gram matrix K for a kernel k corresponding to a sample x1,...,xm drawn from a density p(x) are analyzed and a performance bound on kernel PCA is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121062339,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3065c5e37a0c1f1be365e88ddf2d5cd02faa5db1",
            "isKey": false,
            "numCitedBy": 1330,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-results-on-Tchebycheffian-spline-functions-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheffian spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709847"
                        ],
                        "name": "E. Eskin",
                        "slug": "E.-Eskin",
                        "structuredName": {
                            "firstName": "Eleazar",
                            "lastName": "Eskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 4
                            }
                        ],
                        "text": "See Leslie et al. [2003] and Saunders et al. [2003] for an interesting discussion of the similarities of the features used in the k-spectrum kernel and the score vector derived from an order k \u2212 1 Markov model; see also exercise"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 83
                            }
                        ],
                        "text": "\u2022 If we only consider substrings of length k, then we obtain the k-spectrum kernel [Leslie et al., 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5112756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2aec4a2aa286a0093bf124482ed106f7e965ee8b",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem. These kernels measure sequence similarity based on shared occurrences of -length subsequences, counted with up to mismatches, and do not rely on any generative model for the positive training sequences. We compute the kernels efficiently using a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVM classifier performs as well as the Fisher kernel, the most successful method for remote homology detection, while achieving considerable computational savings."
            },
            "slug": "Mismatch-String-Kernels-for-SVM-Protein-Leslie-Eskin",
            "title": {
                "fragments": [],
                "text": "Mismatch String Kernels for SVM Protein Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A class of string kernels, called mismatch kernels, are introduced for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem, and show that the mismatch kernel used with an SVM classifier performs as well as the Fisher kernel, the most successful method for remote homology detection."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32183271"
                        ],
                        "name": "A. E. Hoerl",
                        "slug": "A.-E.-Hoerl",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Hoerl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. E. Hoerl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94158926"
                        ],
                        "name": "R. Kennard",
                        "slug": "R.-Kennard",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kennard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kennard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "is known in this case as ridge regression [Hoerl and Kennard, 1970] because of ridge regression the effect of the quadratic penalty term 12w >\u03a3\u22121 p w from the log prior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28142999,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1473110f6c33b483251ade10b79416d3efee2da4",
            "isKey": false,
            "numCitedBy": 4992,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X\u2032X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X\u2032X to obtain biased estimates with smaller mean square error."
            },
            "slug": "Ridge-Regression:-Biased-Estimation-for-Problems-Hoerl-Kennard",
            "title": {
                "fragments": [],
                "text": "Ridge Regression: Biased Estimation for Nonorthogonal Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The ridge trace is introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality, and how to augment X\u2032X to obtain biased estimates with smaller mean square error."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This form of analysis was used by Silverman [1984] for splines in one dimension."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Whittle in the discussion of Silverman [1985], and shows the need for the condition 2m > D for spline smoothing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116344014,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "82b84d5ba9b7dfa427ca83a28878e4a2c7fe90b4",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A method is developed for the estimation of the logarithm of the ratio of two probability density functions. The method has applications in several contexts, notably in data analysis and in the construction of empirical versions of statistical procedures based on likelihood ratios. In this paper, the method is applied to a problem arising from the investigation of the causes of \"cot death\"."
            },
            "slug": "Density-Ratios,-Empirical-Likelihood-and-Cot-Death-Silverman",
            "title": {
                "fragments": [],
                "text": "Density Ratios, Empirical Likelihood and Cot Death"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312459"
                        ],
                        "name": "D. Malzahn",
                        "slug": "D.-Malzahn",
                        "structuredName": {
                            "firstName": "D\u00f6rthe",
                            "lastName": "Malzahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malzahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although we have focused on GP regression with squared loss, we note that Malzahn and Opper [2002] have developed more general techniques that can be used to analyze learning curves for other situations such as GP classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2905977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47cfdfd881cdcaae94d04afe4b8d59fcbf3bf465",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We combine the replica approach from statistical physics with a varia-tional approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance."
            },
            "slug": "A-Variational-Approach-to-Learning-Curves-Malzahn-Opper",
            "title": {
                "fragments": [],
                "text": "A Variational Approach to Learning Curves"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The replica approach from statistical physics with a varia-tional approach to analyze learning curves analytically is applied to Gaussian process regression and derives approximative relations between empirical error measures, the generalization error and the posterior variance."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862374"
                        ],
                        "name": "U. Grenander",
                        "slug": "U.-Grenander",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Grenander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Grenander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2661433"
                        ],
                        "name": "Yunshyong Chow",
                        "slug": "Yunshyong-Chow",
                        "structuredName": {
                            "firstName": "Yunshyong",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunshyong Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963249"
                        ],
                        "name": "D. M. Keenan",
                        "slug": "D.-M.-Keenan",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keenan",
                            "middleNames": [
                                "Macrae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Keenan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195592621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac891dff9effb37aeb5800406addc8f71f86b0b6",
            "isKey": false,
            "numCitedBy": 407,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The text develops a global shape model and applies it to the analysis of real pictures acquired with a visible light camera under varying conditions of optical degradation. Computational feasibility of the algorithms derived from this model is achieved by analytical means. The aim is to develop methods for image understanding based on structured restoration, for example automatic detection of abnormalities. The limits of applicability of the algorithms are also traced by making the optical degradations more and more severe until the algorithms no longer succeed in their task. This book is suitable for an advanced undergraduate or graduate seminar in pattern theory, or as an accompanying book for applied probability, computer vision or pattern recognition."
            },
            "slug": "Hands:-A-Pattern-Theoretic-Study-of-Biological-Grenander-Chow",
            "title": {
                "fragments": [],
                "text": "Hands: A Pattern Theoretic Study of Biological Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The text develops a global shape model and applies it to the analysis of real pictures acquired with a visible light camera under varying conditions of optical degradation to develop methods for image understanding based on structured restoration, for example automatic detection of abnormalities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729640"
                        ],
                        "name": "P. Gr\u00fcnwald",
                        "slug": "P.-Gr\u00fcnwald",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Gr\u00fcnwald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gr\u00fcnwald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10379509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a24f8830089b74df7a6aa3eb09c884817a0629e0",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that forms of Bayesian and MDL inference that are often applied to classification problems can be inconsistent. This means that there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error. From a Bayesian point of view, the result can be reinterpreted as saying that Bayesian inference can be inconsistent under misspecification, even for countably infinite models. We extensively discuss the result from both a Bayesian and an MDL perspective."
            },
            "slug": "Suboptimal-behavior-of-Bayes-and-MDL-in-under-Gr\u00fcnwald-Langford",
            "title": {
                "fragments": [],
                "text": "Suboptimal behavior of Bayes and MDL in classification under misspecification"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is shown that forms of Bayesian and MDL inference that are often applied to classification problems can be inconsistent, which means that there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4191,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144789007"
                        ],
                        "name": "M. Kohler",
                        "slug": "M.-Kohler",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kohler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kohler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9745969"
                        ],
                        "name": "A. Krzy\u017cak",
                        "slug": "A.-Krzy\u017cak",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Krzy\u017cak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krzy\u017cak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158720"
                        ],
                        "name": "Harro Walk",
                        "slug": "Harro-Walk",
                        "structuredName": {
                            "firstName": "Harro",
                            "lastName": "Walk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harro Walk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43315484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e9e495ff046b0da2948b1de10c9ea008804f40a5",
            "isKey": false,
            "numCitedBy": 1712,
            "numCiting": 204,
            "paperAbstract": {
                "fragments": [],
                "text": "Why is Nonparametric Regression Important? * How to Construct Nonparametric Regression Estimates * Lower Bounds * Partitioning Estimates * Kernel Estimates * k-NN Estimates * Splitting the Sample * Cross Validation * Uniform Laws of Large Numbers * Least Squares Estimates I: Consistency * Least Squares Estimates II: Rate of Convergence * Least Squares Estimates III: Complexity Regularization * Consistency of Data-Dependent Partitioning Estimates * Univariate Least Squares Spline Estimates * Multivariate Least Squares Spline Estimates * Neural Networks Estimates * Radial Basis Function Networks * Orthogonal Series Estimates * Advanced Techniques from Empirical Process Theory * Penalized Least Squares Estimates I: Consistency * Penalized Least Squares Estimates II: Rate of Convergence * Dimension Reduction Techniques * Strong Consistency of Local Averaging Estimates * Semi-Recursive Estimates * Recursive Estimates * Censored Observations * Dependent Observations"
            },
            "slug": "A-Distribution-Free-Theory-of-Nonparametric-Gy\u00f6rfi-Kohler",
            "title": {
                "fragments": [],
                "text": "A Distribution-Free Theory of Nonparametric Regression"
            },
            "venue": {
                "fragments": [],
                "text": "Springer series in statistics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152558939"
                        ],
                        "name": "Peter J. Green",
                        "slug": "Peter-J.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It turns out that the coefficients \u03b1 and \u03b2 can be computed in time O(n) using an algorithm due to Reinsch; see Green and Silverman [1994, sec. 2.3.3] for details. Splines were first used in regression problems. However, by using generalized linear modelling [McCullagh and Nelder, 1983] they can be extended to classification problems and other non-Gaussian likelihoods, as we did for GP classification in section 3.3. Early references in this direction include Silverman [1978] and O\u2019Sullivan et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122440103,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "a24e46a309111f3f55f15aaba06e0c6b11a01da4",
            "isKey": true,
            "numCitedBy": 1889,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Of course, from childhood to forever, we are always thought to love reading. It is not only reading the lesson book but also reading everything good is the choice of getting new inspirations. Religion, sciences, politics, social, literature, and fictions will enrich you for not only one aspect. Having more aspects to know and understand will lead you become someone more precious. Yea, becoming precious can be situated with the presentation of how your knowledge much."
            },
            "slug": "Nonparametric-regression-and-generalized-linear-Green-Silverman",
            "title": {
                "fragments": [],
                "text": "Nonparametric regression and generalized linear models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5981344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c2a39d24613aa8a1f6fa23c8376b20a37969024",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work."
            },
            "slug": "Occam's-Razor-Rasmussen-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Occam's Razor"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well; the two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The BCM approach is transductive [Vapnik, 1995] rather than inductive, in the sense that the method computes a test-set dependent model making use of the test set input locations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 45
                            }
                        ],
                        "text": "This was result extended to general RKHSs in Kimeldorf and Wahba [1971]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 111
                            }
                        ],
                        "text": "The correspondence between the GP posterior mean and the solution of the regularization problem f\u0302 was made in Kimeldorf and Wahba [1970]. In case (ii) we have seen in chapter 3 for classification problems using the logistic, probit or softmax response functions that Q(y, f) is convex."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 46
                            }
                        ],
                        "text": "1 The representer theorem was first stated by Kimeldorf and Wahba [1971] for the case of squared error.(2) O\u2019Sullivan et al. [1986] showed that the representer theorem could be extended to likelihood"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 46
                            }
                        ],
                        "text": "1 The representer theorem was first stated by Kimeldorf and Wahba [1971] for the case of squared error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120654716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f75d859e750961d1d094f166fc3b564d9cfe99b",
            "isKey": true,
            "numCitedBy": 975,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The report presents classes of prior distributions for which the Bayes' estimate of an unknown function given certain observations is a spline function. (Author)"
            },
            "slug": "A-Correspondence-Between-Bayesian-Estimation-on-and-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16034217"
                        ],
                        "name": "R. Mazo",
                        "slug": "R.-Mazo",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mazo",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mazo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 46
                            }
                        ],
                        "text": "More comprehensive treatments can be found in Vapnik [1995], Cristianini and Shawe-Taylor [2000] and Sch\u00f6lkopf and Smola [2002]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 61
                            }
                        ],
                        "text": "This process is known as the Ornstein-Uhlenbeck (OU) process [Uhlenbeck and Ornstein, 1930] and was introduced as a mathematical model of the velocity of a particle undergoing Brownian motion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 93073745,
            "fieldsOfStudy": [
                "Mathematics",
                "Physics"
            ],
            "id": "6aeb22e31b1d808754bfca8ba2bf597d92972d06",
            "isKey": false,
            "numCitedBy": 1482,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The spectrum of the Fokker-Planck operator for weakly coupled gases is considered. The operator is decomposed into operators acting on functions whose angular dependence is given by spherical harmonics. It is shown that the operator corresponding to l = 0 has zero for a point eigenvalue (the eigenfunction is the Maxwell distribution). There are no other point eigenvalues and the continuous spectrum of all of the operators is the entire negative real axis. Some consequences are briefly discussed."
            },
            "slug": "On-the-theory-of-brownian-motion-Mazo",
            "title": {
                "fragments": [],
                "text": "On the theory of brownian motion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738575"
                        ],
                        "name": "T. V. Gestel",
                        "slug": "T.-V.-Gestel",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Gestel",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. Gestel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145355963"
                        ],
                        "name": "J. D. Brabanter",
                        "slug": "J.-D.-Brabanter",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Brabanter",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Brabanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750713"
                        ],
                        "name": "B. Moor",
                        "slug": "B.-Moor",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Moor",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53940663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b24972552161cd9eda729e748762a73430983e3a",
            "isKey": false,
            "numCitedBy": 1837,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines Basic Methods of Least Squares Support Vector Machines Bayesian Inference for LS-SVM Models Robustness Large Scale Problems LS-SVM for Unsupervised Learning LS-SVM for Recurrent Networks and Control."
            },
            "slug": "Least-Squares-Support-Vector-Machines-Suykens-Gestel",
            "title": {
                "fragments": [],
                "text": "Least Squares Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Support Vector Machines Basic Methods of Least Squares Support Vector Machines Bayesian Inference for LS-SVM Models Robustness Large Scale Problems LS- sVM for Unsupervised Learning LS- SVM for Recurrent Networks and Control."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our experiments were conducted using the SVMTorch software [Collobert and Bengio, 2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our experiments were conducted using the SVMTorch software [Collobert and Bengio, 2001]. In order to compute probabilistic predictions, we squashed the test-activities through a cumulative Gaussian, using the methods proposed by Platt [2000]: we made a parameterized linear transformation of the test-activities and fed this through the cumulative Gaussian."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8952183,
            "fieldsOfStudy": [
                "Economics",
                "Education"
            ],
            "id": "7141ea996fc449807b14c071716cecac0999f4ce",
            "isKey": false,
            "numCitedBy": 985,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: learning Reference EPFL-REPORT-82604 URL: http://publications.idiap.ch/downloads/reports/2000/rr00-17.pdf Record created on 2006-03-10, modified on 2017-05-10"
            },
            "slug": "SVMTorch:-Support-Vector-Machines-for-Large-Scale-Collobert-Bengio",
            "title": {
                "fragments": [],
                "text": "SVMTorch: Support Vector Machines for Large-Scale Regression Problems"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1139278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80a992011660efa9c2916663312affdb6a8b407c",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a tutorial describing the Expectation Propagation (EP) algorithm for a general exponential family. Our focus is on simplicity of exposition. Although the overhead of translating a specific model into its exponential family representation can be considerable, many apparent complications of EP can simply be sidestepped by working in this canonical representation."
            },
            "slug": "Expectation-Propagation-for-Exponential-Families-Seeger",
            "title": {
                "fragments": [],
                "text": "Expectation Propagation for Exponential Families"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This is a tutorial describing the Expectation Propagation algorithm for a general exponential family and its focus is on simplicity of exposition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39733718"
                        ],
                        "name": "M. Arat\u00f3",
                        "slug": "M.-Arat\u00f3",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Arat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Arat\u00f3"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117866743,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9cf77128c29d427c78b5827954a201c604dfd4da",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Case studies, problems and their statistical investigation.- Elementary Gaussian processes.- The maximum likelihood estimators and their distributions in the one dimensional case.- The multi-dimensional processes."
            },
            "slug": "Linear-Stochastic-Systems-with-Constant-Arat\u00f3",
            "title": {
                "fragments": [],
                "text": "Linear Stochastic Systems with Constant Coefficients"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35112676"
                        ],
                        "name": "S. Kuhnt",
                        "slug": "S.-Kuhnt",
                        "structuredName": {
                            "firstName": "Sonja",
                            "lastName": "Kuhnt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kuhnt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50676519"
                        ],
                        "name": "D. Steinberg",
                        "slug": "D.-Steinberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Steinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Sacks et al. [1989]. 9More complicated noise models with non-trivial covariance structure can also be handled, see section 9."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15210862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b617b85b5add24d55bdbd952cb5bde52b01e4869",
            "isKey": false,
            "numCitedBy": 2763,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The design and analysis of computer experiments as a relatively young research field is not only of high importance for many industrial areas but also presents new challenges and open questions for statisticians. This editorial introduces a special issue devoted to the topic. The included papers present an interesting mixture of recent developments in the field as they cover fundamental research on the design of experiments, models and analysis methods as well as more applied research connected to real-life applications."
            },
            "slug": "Design-and-analysis-of-computer-experiments-Kuhnt-Steinberg",
            "title": {
                "fragments": [],
                "text": "Design and analysis of computer experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The included papers present an interesting mixture of recent developments in the field as they cover fundamental research on the design of experiments, models and analysis methods as well as more applied research connected to real-life applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5118862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72a7e7bc1911b6a327c4614553bfcde98194d4ef",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time, thus obviating dynamic programming with quadratic time complexity. Furthermore, prediction cost in many cases can be reduced to linear cost in the length of the sequence to be classified, regardless of the number of support vectors. This improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner."
            },
            "slug": "Fast-Kernels-for-String-and-Tree-Matching-Vishwanathan-Smola",
            "title": {
                "fragments": [],
                "text": "Fast Kernels for String and Tree Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A new algorithm suitable for matching discrete objects such as strings and trees in linear time is presented, thus obviating dynamic programming with quadratic time complexity and improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2376576,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4f961395bd0f281e54d2399b3e21bdb937b7a94a",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this note is to extend some of the recent work on spline interpolation so as to include also a solution of the problem of graduation of data. The well-known method of graduation due to E. T. Whittaker suggests how this should be done. Here we merely describe the idea and the qualitative aspects of the new method, while proofs and the computational side will be discussed elsewhere."
            },
            "slug": "SPLINE-FUNCTIONS-AND-THE-PROBLEM-OF-GRADUATION.-Schoenberg",
            "title": {
                "fragments": [],
                "text": "SPLINE FUNCTIONS AND THE PROBLEM OF GRADUATION."
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The aim of this note is to extend some of the recent work on spline interpolation so as to include also a solution of the problem of graduation of data and the qualitative aspects of the new method are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588491"
                        ],
                        "name": "B. \u00d8ksendal",
                        "slug": "B.-\u00d8ksendal",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "\u00d8ksendal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. \u00d8ksendal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "\u00d8ksendal [1985]. As for the discrete-time case, one can write eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122743997,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fbbc2c872544225397d0cff190844c99fb872792",
            "isKey": false,
            "numCitedBy": 3434,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We now return to the possible solutions X t (\u03c9) of the stochastic differential equation \n \n \n \n(5.1) \n \nwhere W t is 1-dimensional \u201cwhite noise\u201d. As discussed in Chapter III the Ito interpretation of (5.1) is that X t satisfies the stochastic integral equation \n \n \n \nor in differential form \n \n \n \n(5.2) \n \n."
            },
            "slug": "Stochastic-Differential-Equations-\u00d8ksendal",
            "title": {
                "fragments": [],
                "text": "Stochastic Differential Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "h(x\u2217) is called the weight function [Silverman, 1984]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119733144,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69e535bb138886d10cdba8a53d0b297a5444a86a",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere une approche spline pour la regression non parametrique et l'estimation de courbe. On montre que dans un certain sens, le lissage spline correspond approximativement au lissage par une methode du noyau avec une largeur de bande dependant de la densite locale des points du plan de regression"
            },
            "slug": "Spline-Smoothing:-The-Equivalent-Variable-Kernel-Silverman",
            "title": {
                "fragments": [],
                "text": "Spline Smoothing: The Equivalent Variable Kernel Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145969624"
                        ],
                        "name": "C. Chatfield",
                        "slug": "C.-Chatfield",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Chatfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chatfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Chatfield [1989]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 188532463,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6518b268fbf215d1196cf707c854ae8eb86f6a7b",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Simple descriptive techniques probability models for time series estimation in the time domain forecasting stationary processes in the frequency domain spectral analysis bivariate processes linear systems state-space models and the Kalman filter non-linear models multivariate time series modelling some other topics."
            },
            "slug": "The-Analysis-of-Time-Series:-An-Introduction-Chatfield",
            "title": {
                "fragments": [],
                "text": "The Analysis of Time Series: An Introduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69518685"
                        ],
                        "name": "G. Arfken",
                        "slug": "G.-Arfken",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Arfken",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Arfken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Arfken [1985] provides an introduction to calculus of variations and Green\u2019s functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For preparation in calculus and linear algebra any good university-level textbook on mathematics for physics or engineering such as Arfken [1985] would be fine."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122141371,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2250807f672b4fbb6634347c79d5973d87e66ed9",
            "isKey": false,
            "numCitedBy": 8354,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector Analysis. Curved Coordinates, Tensors. Determinants and Matrices. Group Theory. Infinite Series. Functions of a Complex Variable I. Functions of a Complex Variable II. Differential Equations. Sturm-Liouville Theory. Gamma-Factrial Function. Bessel Functions. Legendre Functions. Special Functions. Fourier Series. Integral Transforms. Integral Equations. Calculus of Variations. Nonlinear Methods and Chaos."
            },
            "slug": "Mathematical-Methods-for-Physicists-Arfken",
            "title": {
                "fragments": [],
                "text": "Mathematical Methods for Physicists"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47966942"
                        ],
                        "name": "G. Grimmett",
                        "slug": "G.-Grimmett",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Grimmett",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grimmett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70059285"
                        ],
                        "name": "D. Stirzaker",
                        "slug": "D.-Stirzaker",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stirzaker",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stirzaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Grimmett and Stirzaker [1992] for further information on these processes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120474741,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a746ef7f4060f9417bc8f13e5ce7e79b862f1ec0",
            "isKey": false,
            "numCitedBy": 2587,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Events and their probabilities random variables and their distributions discrete random variables continuous random variables generating functions and their applications Markov chains convergence of random variables random processes stationary processes renewals queues Martingales diffusion processes. Appendices: Foundations and notations history and varieties of probability John Arburthnot's preface to \"Of the Laws of Chance\" (1692)."
            },
            "slug": "Probability-and-random-processes-Grimmett-Stirzaker",
            "title": {
                "fragments": [],
                "text": "Probability and random processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145369815"
                        ],
                        "name": "K. Ritter",
                        "slug": "K.-Ritter",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These questions have been addressed both in the statistical literature and in theoretical numerical analysis; for the latter area the book by Ritter [2000] provides a useful overview."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This approach was used by Sampson and Guttorp [1992] to model patterns of solar radiation in southwestern British Columbia using Gaussian processes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57454822,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b18264587cc5821f0de32c9aac72e21ef212de37",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction Linear Problems: Definitions and a Classical Example Second-Order Results for Linear Problems Integration and Approximation of Univariate Functions Linear Problems for Univariate Functions with Noisy Data Integration and Approximation of Multivariate Functions Nonlinear Methods for Linear Problems Nonlinear Problems."
            },
            "slug": "Average-case-analysis-of-numerical-problems-Ritter",
            "title": {
                "fragments": [],
                "text": "Average-case analysis of numerical problems"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This chapter discusses Linear Problems for Univariate Functions with Noisy Data Integration and Approximation of Multivariate Functions Nonlinear Methods for Linear Problems Nonlinear Problems."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture notes in mathematics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66642349"
                        ],
                        "name": "R. Bartle",
                        "slug": "R.-Bartle",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bartle",
                            "middleNames": [
                                "Gardner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bartle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Bartlett et al. [2003] have characterized the loss functions that lead to universal consistency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in Doob [1994] and Bartle [1995]. Let \u03a9 be the set of all possible outcomes of an experiment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Smola and Bartlett [2001] take a similar approach, but choose as their criterion the quadratic form 1 \u03c32 n |y \u2212Knm\u1fb1m| + \u1fb1mKmm\u1fb1m = y>(K\u0303 + \u03c3 nIn)y, (8."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123250353,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c63a67cf2de559edfdf92ff6ff1d7a087dee1860",
            "isKey": true,
            "numCitedBy": 372,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE ELEMENTS OF INTEGRATION. Measurable Functions. Measures. The Integral. Integrable Functions. The Lebesgue Spaces L p . Modes of Convergence. Decomposition of Measures. Generation of Measures. Product Measures. THE ELEMENTS OF LEBESGUE MEASURE. Volumes of Cells and Intervals. The Outer Measure. Measurable Sets. Examples of Measurable Sets. Approximation of Measurable Sets. Additivity and Nonadditivity. Nonmeasurable and Non--Borel Sets. References. Index."
            },
            "slug": "The-elements-of-integration-and-Lebesgue-measure-Bartle",
            "title": {
                "fragments": [],
                "text": "The elements of integration and Lebesgue measure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The standard solution [Wahba, 1985, Ansley and Kohn, 1985] in this case is to project y onto the directions orthogonal to the span of H> and compute the marginal likelihood in this subspace. Let the rank of H> be m. Then as shown in Ansley and Kohn [1985] this means that we must discard the terms \u2212 12 log |B| \u2212 m 2 log 2\u03c0 from eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Following Wahba [1978], we consider the random function"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121176122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88ce531f22108f687cbb576bcb0cd660b2a694bc",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere des procedures de lissage spline etudiees en 1978 et 1983 et leur extension a la resolution d'equations d'operateurs lineaires avec donnees bruitees"
            },
            "slug": "A-Comparison-of-GCV-and-GML-for-Choosing-the-in-the-Wahba",
            "title": {
                "fragments": [],
                "text": "A Comparison of GCV and GML for Choosing the Smoothing Parameter in the Generalized Spline Smoothing Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060034584"
                        ],
                        "name": "R. Daley",
                        "slug": "R.-Daley",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Daley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Daley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122133736,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "7b54cea6d04228fcde7c13c01f53eb96092bce14",
            "isKey": false,
            "numCitedBy": 2192,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction 2. Function fitting 3. The method of successive corrections 4. Statistical interpolation - univariate 5. Statistical interpolation - multivariate 6. The initialisation problem 7. Quasi-geostrophic constraints 8. Variational procedures 9. Normal mode initialisation - theory 10. Normal mode initialisation - applications 11. Dynamic initialisation 12. Continuous data assimilation 13. Future directions Appendixes."
            },
            "slug": "Atmospheric-Data-Analysis-Daley",
            "title": {
                "fragments": [],
                "text": "Atmospheric Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074730628"
                        ],
                        "name": "J. Duchon",
                        "slug": "J.-Duchon",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Duchon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Duchon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The result, as shown by Duchon [1977] and Meinguet [1979] is"
                    },
                    "intents": []
                }
            ],
            "corpusId": 123055447,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd573acce51d862cfb576bf9588f7ccad07b7272",
            "isKey": false,
            "numCitedBy": 1451,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a family of semi-norms \u2016\u03bc\u2016m,s=(\u222b\u211d n\u2223\u03c4\u22232s\u2223\u2131 Dmu(\u03c4)\u22232 d\u03c4)1/2 Minimizing such semi-norms, subject to some interpolating conditions, leads to functions of very simple forms, providing interpolation methods that: 1\u00b0) preserve polynomials of degree\u2264m\u22121; 2\u00b0) commute with similarities as well as translations and rotations of \u211dn; and 3\u00b0) converge in Sobolev spaces Hm+s(\u03a9)."
            },
            "slug": "Splines-minimizing-rotation-invariant-semi-norms-in-Duchon",
            "title": {
                "fragments": [],
                "text": "Splines minimizing rotation-invariant semi-norms in Sobolev spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A family of semi-norms is defined, subject to some interpolating conditions, providing interpolation methods that preserve polynomials of degree\u2264m\u22121 and converge in Sobolev spaces Hm+s(\u03a9)."
            },
            "venue": {
                "fragments": [],
                "text": "Constructive Theory of Functions of Several Variables"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5436619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7f15848cd0fbb3d08f351595da833b1627de9c3",
            "isKey": false,
            "numCitedBy": 8764,
            "numCiting": 249,
            "paperAbstract": {
                "fragments": [],
                "text": "Fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "slug": "Information-Theory,-Inference,-and-Learning-Mackay",
            "title": {
                "fragments": [],
                "text": "Information Theory, Inference, and Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64295966,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b7caf811d6980627caad1a8b3053f40348693508",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Fitting a Sigmoid After the SVM, Empirical Tests, Conclusions, Appendix: Pseudo-code for the Sigmoid Training"
            },
            "slug": "Probabilities-for-SV-Machines-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Probabilities for SV Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, Fitting a Sigmoid After the SVM, Empirical Tests, Conclusions, Appendix: Pseudo-code for the Sigmoids Training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "using the \u201cterm frequency inverse document frequency\u201d (TF-IDF) weighting scheme developed in the information retrieval area [Salton and Buckley, 1988]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7725217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50a316f97c9a405aa000d883a633bd5707f1a34",
            "isKey": false,
            "numCitedBy": 9463,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Term-Weighting-Approaches-in-Automatic-Text-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "Term-Weighting Approaches in Automatic Text Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4052106"
                        ],
                        "name": "S. Geisser",
                        "slug": "S.-Geisser",
                        "structuredName": {
                            "firstName": "Seymour",
                            "lastName": "Geisser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geisser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7721316"
                        ],
                        "name": "W. Eddy",
                        "slug": "W.-Eddy",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Eddy",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Eddy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "see [Geisser and Eddy, 1979] for a discussion of this and related approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1062,
                                "start": 5
                            }
                        ],
                        "text": "see [Geisser and Eddy, 1979] for a discussion of this and related approaches. LLOO in eq. (5.11) is sometimes called the log pseudo-likelihood. Notice, that pseudo-likelihood in each of the n LOO-CV rotations, inference in the Gaussian process model (with fixed hyperparameters) essentially consists of computing the inverse covariance matrix, to allow predictive mean and variance in eq. (2.23) and (2.24) to be evaluated (i.e. there is no parameter-fitting, such as there would be in a parametric model). The key insight is that when repeatedly applying the prediction eq. (2.23) and (2.24), the expressions are almost identical: we need the inverses of covariance matrices with a single column and row removed in turn. This can be computed efficiently from the inverse of the complete covariance matrix using inversion by partitioning, see eq. (A.11-A.12). A similar insight has also been used for spline models, see e.g. Wahba [1990, sec. 4.2]. The approach was used for hyperparameter selection in Gaussian process models in Sundararajan and Keerthi [2001]. The expressions for the LOO-CV predictive mean and variance are"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120771971,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "ab8ff2b08acb1e673fd0c6fabea46d60db5031b4",
            "isKey": true,
            "numCitedBy": 901,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article offers a synthesis of Bayesian and sample-reuse approaches to the problem of high structure model selection geared to prediction. Similar methods are used for low structure models. Nested and nonnested paradigms are discussed and examples given."
            },
            "slug": "A-Predictive-Approach-to-Model-Selection-Geisser-Eddy",
            "title": {
                "fragments": [],
                "text": "A Predictive Approach to Model Selection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125105198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e003f0a280275de163269d32046950ad37aa37f0",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction: Linear Methods using Kernel function, Applying Linear Methods to Structured Objects, Conditional Symmetric Independence Kernels, Pair Hidden Markov Models, Conditionally Symmetrically Independent PHMMs, Conclusion"
            },
            "slug": "Dynamic-Alignment-Kernels-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Dynamic Alignment Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction: Linear Methods using Kernel function, Applying Linear Methods to Structured Objects, Conditional Symmetric Independence Kernels, Pair Hidden Markov Models, Conditionally Symmetrically Independent PHMMs, Conclusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145785254"
                        ],
                        "name": "P. Diggle",
                        "slug": "P.-Diggle",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Diggle",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diggle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "in Doob [1994] and Bartle [1995]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The relationship of stationary solutions of pth-order SDEs to rational spectral densities can be traced back at least as far as Doob [1944]. Above we have assumed that the process is stationary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122679957,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47e782597333f3e1ad2f249a3fe1c3d979ce0e6c",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction 1. Simple descriptive methods of analysis 2. Theory of stationery processes 3. Spectral analysis 4. Repeated measurements 5. Fitting autoregressive moving average processes to data 6. Forecasting 7. Elements of bivariate time-series analysis References Appendix A, B & C"
            },
            "slug": "Time-Series:-A-Biostatistical-Introduction-Diggle",
            "title": {
                "fragments": [],
                "text": "Time Series: A Biostatistical Introduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608024"
                        ],
                        "name": "W. Vetterling",
                        "slug": "W.-Vetterling",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Vetterling",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vetterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35046585"
                        ],
                        "name": "B. Flannery",
                        "slug": "B.-Flannery",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Flannery",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Flannery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61769312,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ca2832d2c30287a9ee5b8584cc498d2b1cb14753",
            "isKey": false,
            "numCitedBy": 16689,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08"
            },
            "slug": "Numerical-recipes-in-C-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Jones [2001] examines a number of criteria that have been suggested for where to make the next function evaluation based on the predictive mean and variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8723392,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "a3db48fdc9aaf6921f269817ba4ed16b9b198394",
            "isKey": false,
            "numCitedBy": 1862,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach."
            },
            "slug": "A-Taxonomy-of-Global-Optimization-Methods-Based-on-Jones",
            "title": {
                "fragments": [],
                "text": "A Taxonomy of Global Optimization Methods Based on Response Surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper presents a taxonomy of existing approaches for using response surfaces for global optimization, illustrating each method with a simple numerical example that brings out its advantages and disadvantages."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34286280"
                        ],
                        "name": "R. Berk",
                        "slug": "R.-Berk",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Berk",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126723222,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "73e77044c4909bc6189b2365f3d8189409a931ad",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "(1996). Continuous Univariate Distributions, Volume 2. Technometrics: Vol. 38, No. 2, pp. 189-189."
            },
            "slug": "Continuous-Univariate-Distributions,-Volume-2-Berk",
            "title": {
                "fragments": [],
                "text": "Continuous Univariate Distributions, Volume 2"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011917"
                        ],
                        "name": "T. Santner",
                        "slug": "T.-Santner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Santner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Santner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50475322"
                        ],
                        "name": "B. Williams",
                        "slug": "B.-Williams",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50238440"
                        ],
                        "name": "W. Notz",
                        "slug": "W.-Notz",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Notz",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Notz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58225730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ff39cebaea5755dd60d9bdbac22f24668c3cba8",
            "isKey": false,
            "numCitedBy": 2524,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "Physical Experiments and Computer Experiments.- Basic Elements of Computer Experiments.- Analyzing Output from Computer Experiments-Predicting Output from Training Data.- Space Filling Designs for Computer Experiments.- Criteria Based Designs for Computer Experiments.- Other Issues."
            },
            "slug": "The-Design-and-Analysis-of-Computer-Experiments-Santner-Williams",
            "title": {
                "fragments": [],
                "text": "The Design and Analysis of Computer Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper presents a meta-modelling framework for estimating Output from Computer Experiments-Predicting Output from Training Data and Criteria Based Designs for computer Experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Springer series in statistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10670765"
                        ],
                        "name": "R. Cormack",
                        "slug": "R.-Cormack",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Cormack",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cormack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48393332"
                        ],
                        "name": "N. Cressie",
                        "slug": "N.-Cressie",
                        "structuredName": {
                            "firstName": "Noel",
                            "lastName": "Cressie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cressie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Ripley [1981] and Cressie [1993] provide useful overviews of Gaussian process prediction in spatial statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125893266,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "457cb7db00b9a3abfef41f48f9d5243b7347b48f",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistics for Spatial Data GEOSTATISTICAL DATA Geostatistics Spatial Prediction and Kriging Applications of Geostatistics Special Topics in Statistics for Spatial Data LATTICE DATA Spatial Models on Lattices Inference for Lattice Models SPATIAL PATTERNS Spatial Point Patterns Modeling Objects References Author Index Subject Index."
            },
            "slug": "Statistics-for-Spatial-Data.-Cormack-Cressie",
            "title": {
                "fragments": [],
                "text": "Statistics for Spatial Data."
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Statistics for Spatial Data GEOSTATISTICAL DATA Geostatistics Spatial Prediction and Kriging Applications of Geost atistics Special Topics in Statistics for Sp spatial data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144737540"
                        ],
                        "name": "A. Hawkes",
                        "slug": "A.-Hawkes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hawkes",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hawkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144327830"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Cox",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144459170"
                        ],
                        "name": "H. D. Miller",
                        "slug": "H.-D.-Miller",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Miller",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. D. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118699372,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a9a2f1f003918c32fb72f74caa45a1a88a8e37d7",
            "isKey": false,
            "numCitedBy": 1573,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book should be of interest to undergraduate and postgraduate students of probability theory."
            },
            "slug": "The-Theory-of-Stochastic-Processes-Hawkes-Cox",
            "title": {
                "fragments": [],
                "text": "The Theory of Stochastic Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46561670"
                        ],
                        "name": "B. Mandelbrot",
                        "slug": "B.-Mandelbrot",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Mandelbrot",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mandelbrot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43776837,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "d9c33f310c8af5dbdbc79d1609d3e1bc45180847",
            "isKey": false,
            "numCitedBy": 17996,
            "numCiting": 357,
            "paperAbstract": {
                "fragments": [],
                "text": "\"...a blend of erudition (fascinating and sometimes obscure historical minutiae abound), popularization (mathematical rigor is relegated to appendices) and exposition (the reader need have little knowledge of the fields involved) ...and the illustrations include many superb examples of computer graphics that are works of art in their own right.\" Nature"
            },
            "slug": "Fractal-Geometry-of-Nature-Mandelbrot",
            "title": {
                "fragments": [],
                "text": "Fractal Geometry of Nature"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This book is a blend of erudition, popularization, and exposition, and the illustrations include many superb examples of computer graphics that are works of art in their own right."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An additive model [Hastie and Tibshirani, 1990] has the form f(x) = c + \u2211D i=1fi(xi), i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 236049,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5fa2ace8450629a53eea82e2e4174b16bce6182a",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This article reviews flexible statistical methods that are useful for characterizing the effect of potential prognostic factors on disease endpoints. Applications to survival models and binary outcome models are illustrated."
            },
            "slug": "Generalized-additive-models-for-medical-research-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Generalized additive models for medical research"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "Flexible statistical methods that are useful for characterizing the effect of potential prognostic factors on disease endpoints are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Statistical methods in medical research"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346261"
                        ],
                        "name": "L. Plaskota",
                        "slug": "L.-Plaskota",
                        "structuredName": {
                            "firstName": "Leszek",
                            "lastName": "Plaskota",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Plaskota"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35082536,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "id": "04ee508122e915b411b9f8d69deb72c1e5309ba4",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Overview 2. Worst case setting 3. Average case setting 4. Worst-average case setting 5. Average-worst case setting 6. Asymptotic setting Bibliography Glossary Indices."
            },
            "slug": "Noisy-information-and-computational-complexity-Plaskota",
            "title": {
                "fragments": [],
                "text": "Noisy information and computational complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a meta-modelling framework for estimating the severity of the consequences of asymptotic encephalopathy in patients with a history of head injuries or severe burns."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32854168"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Diggle et al. [1998] were concerned with modelling count data measured geographically using a Poisson likelihood with a spatially varying rate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120719658,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5c63ab340634a23b84a233f1a7725c4b5ddffe9c",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "On etudie les proprietes de frequence des regles de Bayes avec une attention particuliere a la consistence"
            },
            "slug": "On-the-consistency-of-Bayes-estimates-Diaconis-Freedman",
            "title": {
                "fragments": [],
                "text": "On the consistency of Bayes estimates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 189
                            }
                        ],
                        "text": "In contrast to Gaussian process regression the marginal likelihood for a given ANN model is not analytically tractable, and thus approximation techniques such as the Laplace approximation [MacKay, 1992a] and Markov chain Monte Carlo methods [Neal, 1996] have to be used. Neal\u2019s observation [1996] that certain ANNs with one hidden layer converge to a Gaussian process prior over functions (see section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 69
                            }
                        ],
                        "text": "Another interesting example of this warping construction is given in MacKay [1998] where the one-dimensional input variable x is mapped to the two-dimensional u(x) = (cos(x), sin(x)) to give rise to a periodic random function of x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "MacKay [1992b] for this framework and MacKay [1992a] for the context of neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 64
                            }
                        ],
                        "text": "The probabilistic framework was pursued using approximations by MacKay [1992b] and using Markov chain Monte Carlo (MCMC) methods by Neal [1996]. Neal was also a graduate student in the same lab, and in his thesis he sought to demonstrate that using the Bayesian formalism, one does not necessarily have problems with \u201coverfitting\u201d when the models get large, and one should pursue the limit of large models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 56
                            }
                        ],
                        "text": "hyperparameters than for the parameters themselves, see MacKay [1999] for an illuminating discussion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 64
                            }
                        ],
                        "text": "The probabilistic framework was pursued using approximations by MacKay [1992b] and using Markov chain Monte Carlo (MCMC) methods by Neal [1996]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2026,
                                "start": 56
                            }
                        ],
                        "text": "hyperparameters than for the parameters themselves, see MacKay [1999] for an illuminating discussion. The prior over models Hi in eq. (5.7) is often taken to be flat, so that a priori we do not favour one model over another. In this case, the probability for the model is proportional to the expression from eq. (5.6). It is primarily the marginal likelihood from eq. (5.4) involving the integral over the parameter space which distinguishes the Bayesian scheme of inference from other schemes based on optimization. It is a property of the marginal likelihood that it automatically incorporates a trade-off between model fit and model complexity. This is the reason why the marginal likelihood is valuable in solving the model selection problem. In Figure 5.2 we show a schematic of the behaviour of the marginal likelihood for three different model complexities. Let the number of data points n and the inputs X be fixed; the horizontal axis is an idealized representation of all possible vectors of targets y, and the vertical axis plots the marginal likelihood p(y|X,Hi). A simple model can only account for a limited range of possible sets of target values, but since the marginal likelihood is a probability distribution over y it must normalize to unity, and therefore the data sets which the model does account for have a large value of the marginal likelihood. Conversely for a complex model: it is capable of accounting for a wider range of data sets, and consequently the marginal likelihood doesn\u2019t attain such large values as for the simple model. For example, the simple model could be a linear model, and the complex model a large neural network. The figure illustrates why the marginal likelihood doesn\u2019t simply favour the models that fit the training data the best. This effect is called Occam\u2019s razor after William of Occam 1285-1349, Occam\u2019s razor whose principle: \u201cplurality should not be assumed without necessity\u201d he used to encourage simplicity in explanations. See also Rasmussen and Ghahramani [2001] for an investigation into Occam\u2019s razor in statistical models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "The derivation is adapted from MacKay [1998]. It is straightforward to generalize this construction to multivariate x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation. Neural Computation, 4(3):415\u2013447"
            },
            "venue": {
                "fragments": [],
                "text": "pp. xiii,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885041"
                        ],
                        "name": "C. Baker",
                        "slug": "C.-Baker",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Baker",
                            "middleNames": [
                                "T.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107597903"
                        ],
                        "name": "R. Taylor",
                        "slug": "R.-Taylor",
                        "structuredName": {
                            "firstName": "Robert L.",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 578,
                                "start": 140
                            }
                        ],
                        "text": "The theory of the numerical solution of eigenvalue problems shows that for a fixed i, 1 n\u03bb mat i will converge to \u03bbi in the limit that n\u2192\u221e [Baker, 1977, Theorem 3.4]. It is also possible to study the convergence further; for example it is quite easy using the properties of principal components analysis (PCA) in feature space to show that for any l, 1 \u2264 l \u2264 n, En[ 1 n \u2211l i=1\u03bb mat i ] \u2265 \u2211l i=1\u03bbi and En[ 1 n \u2211n i=l+1\u03bb mat i ] \u2264 \u2211N i=l+1\u03bbi, where En denotes expectation with respect to samples of size n drawn from p(x). For further details see Shawe-Taylor and Williams [2003]. The Nystr\u00f6m method for approximating the ith eigenfunction (see Baker Nystr\u00f6m method [1977] and Press et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123967005,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fecf8f9b8f09c3c87def62a66485558e92065737",
            "isKey": true,
            "numCitedBy": 798,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Numerical-Treatment-of-Integral-Equations-Baker-Taylor",
            "title": {
                "fragments": [],
                "text": "The Numerical Treatment of Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693972"
                        ],
                        "name": "C. Priebe",
                        "slug": "C.-Priebe",
                        "structuredName": {
                            "firstName": "Carey",
                            "lastName": "Priebe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Priebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 781,
                                "start": 64
                            }
                        ],
                        "text": "Amos Storkey, Volker Tresp, Sethu Vijayakumar, Grace Wahba, Joe Whittaker and Tong Zhang for valuable discussions on specific issues. We also thank Bob Prior and the staff at MIT Press for their support during the writing of the book. We thank the Gatsby Computational Neuroscience Unit (UCL) and Neil Lawrence at the Department of Computer Science, University of Sheffield for hosting our visits and kindly providing space for us to work, and the Department of Computer Science at the University of Toronto for computer support. Thanks to John and Fiona for their hospitality on numerous occasions. Some of the diagrams in this book have been inspired by similar diagrams appearing in published work, as follows: Figure 3.5, Sch\u00f6lkopf and Smola [2002]; Figure 5.2, MacKay [1992b]. CER gratefully acknowledges financial support from the German Research Foundation (DFG)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 753,
                                "start": 64
                            }
                        ],
                        "text": "Amos Storkey, Volker Tresp, Sethu Vijayakumar, Grace Wahba, Joe Whittaker and Tong Zhang for valuable discussions on specific issues. We also thank Bob Prior and the staff at MIT Press for their support during the writing of the book. We thank the Gatsby Computational Neuroscience Unit (UCL) and Neil Lawrence at the Department of Computer Science, University of Sheffield for hosting our visits and kindly providing space for us to work, and the Department of Computer Science at the University of Toronto for computer support. Thanks to John and Fiona for their hospitality on numerous occasions. Some of the diagrams in this book have been inspired by similar diagrams appearing in published work, as follows: Figure 3.5, Sch\u00f6lkopf and Smola [2002]; Figure 5."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17493949,
            "fieldsOfStudy": [],
            "id": "7f922938fb881cc8517616f2f96c130b9dcc5148",
            "isKey": true,
            "numCitedBy": 42,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Mixtures-Priebe",
            "title": {
                "fragments": [],
                "text": "Adaptive Mixtures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 112
                            }
                        ],
                        "text": "If = 0 then the error model is a Laplacian distribution, which corresponds to least absolute values regression (Edgeworth [1887], cited in Rousseeuw [1984]); this is a heavier-tailed distribution than the Gaussian and provides some protection against outliers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 112
                            }
                        ],
                        "text": "If = 0 then the error model is a Laplacian distribution, which corresponds to least absolute values regression (Edgeworth [1887], cited in Rousseeuw [1984]); this is a heavier-tailed distribution than the Gaussian and provides some protection against outliers. Girosi [1991] showed that the Laplacian distribution can be viewed as a continuous mixture of zeromean Gaussians with a certain distribution over their variances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 445,
                                "start": 112
                            }
                        ],
                        "text": "If = 0 then the error model is a Laplacian distribution, which corresponds to least absolute values regression (Edgeworth [1887], cited in Rousseeuw [1984]); this is a heavier-tailed distribution than the Gaussian and provides some protection against outliers. Girosi [1991] showed that the Laplacian distribution can be viewed as a continuous mixture of zeromean Gaussians with a certain distribution over their variances. Pontil et al. [1998] extended this result by allowing the means to uniformly shift in [\u2212 , ] in order to obtain a probabilistic model corresponding to the -insensitive error function."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Observations Relating to Several Quantities"
            },
            "venue": {
                "fragments": [],
                "text": "Hermathena, 6:279\u2013285. p. 146"
            },
            "year": 1887
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 209075926,
            "fieldsOfStudy": [],
            "id": "a9a2f1f003918c32fb72f74caa45a1a88a8e37d7",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103223485"
                        ],
                        "name": "A. Bello",
                        "slug": "A.-Bello",
                        "structuredName": {
                            "firstName": "Abdul",
                            "lastName": "Bello",
                            "middleNames": [
                                "Lateef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bello"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 187993553,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c05c7eeef1c8c6b392b6567224b14f830af2c0bb",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tables-of-Integrals,-Series,-and-Products-Bello",
            "title": {
                "fragments": [],
                "text": "Tables of Integrals, Series, and Products"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4442175"
                        ],
                        "name": "C. D. Keeling",
                        "slug": "C.-D.-Keeling",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Keeling",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Keeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7524896"
                        ],
                        "name": "T. Whorf",
                        "slug": "T.-Whorf",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Whorf",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Whorf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 9
                            }
                        ],
                        "text": "The data [Keeling and Whorf, 2004] consists of monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) derived from in situ air samples collected at the Mauna Loa Observatory, Hawaii, between 1958 and 2003 (with some missing values)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 128377816,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "7ca23f2237be9ccf2738dcdab93fbbe83b36bfa2",
            "isKey": false,
            "numCitedBy": 752,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Atmospheric-CO-2-records-from-sites-in-the-SIO-air-Keeling-Whorf",
            "title": {
                "fragments": [],
                "text": "Atmospheric CO 2 records from sites in the SIO air sampling network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47363524"
                        ],
                        "name": "P. R. Nelson",
                        "slug": "P.-R.-Nelson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Nelson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. R. Nelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126416994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e03973b0ce53ef053637c5bb9e717ef0c96ae26",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Continuous-Univariate-Distributions-Volume-2-Nelson",
            "title": {
                "fragments": [],
                "text": "Continuous Univariate Distributions Volume 2"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225ca57add3b3fb12ef01cc97c4683350dc93fe4",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143639508"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102992139"
                        ],
                        "name": "Vasiliy Yakovlevich Arsenin",
                        "slug": "Vasiliy-Yakovlevich-Arsenin",
                        "structuredName": {
                            "firstName": "Vasiliy",
                            "lastName": "Arsenin",
                            "middleNames": [
                                "Yakovlevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasiliy Yakovlevich Arsenin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122072756,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc14819e745cd7af37efd09ea29773dc0065119e",
            "isKey": false,
            "numCitedBy": 7884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solutions-of-ill-posed-problems-Tikhonov-Arsenin",
            "title": {
                "fragments": [],
                "text": "Solutions of ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145192271"
                        ],
                        "name": "P. Whittle",
                        "slug": "P.-Whittle",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Whittle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Whittle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122793670,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fdd557f057d24707927268f91e76f99ef26fd468",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Prediction-and-Regulation-by-Linear-Least-Square-Whittle",
            "title": {
                "fragments": [],
                "text": "Prediction and Regulation by Linear Least-Square Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102407518"
                        ],
                        "name": "R. V. Mises",
                        "slug": "R.-V.-Mises",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Mises",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. V. Mises"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100846323"
                        ],
                        "name": "H. Geiringer",
                        "slug": "H.-Geiringer",
                        "structuredName": {
                            "firstName": "Hilda",
                            "lastName": "Geiringer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Geiringer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065560575"
                        ],
                        "name": "J. Gillis",
                        "slug": "J.-Gillis",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Gillis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gillis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124439213,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1034ae8e7d58e6e6a98de7d0386197972d1ebcb3",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mathematical-Theory-of-Probability-and-Statistics-Mises-Geiringer",
            "title": {
                "fragments": [],
                "text": "Mathematical Theory of Probability and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152746370"
                        ],
                        "name": "Cki Williams",
                        "slug": "Cki-Williams",
                        "structuredName": {
                            "firstName": "Cki",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cki Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102934096"
                        ],
                        "name": "A. Scwaighofer",
                        "slug": "A.-Scwaighofer",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "Scwaighofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Scwaighofer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29983583"
                        ],
                        "name": "Tresp",
                        "slug": "Tresp",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Tresp",
                            "middleNames": [],
                            "suffix": "V"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tresp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121333478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93ee729418053ef7c30ad5b2348eb6e8eb2b815e",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Observations-on-the-Nystr\u00f6m-Method-for-Gaussian-Williams-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Observations on the Nystr\u00f6m Method for Gaussian Process Prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145506999"
                        ],
                        "name": "E. Wong",
                        "slug": "E.-Wong",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118807829,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c8da9fcc021b34644b9421c0d702735204aed8f0",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-processes-in-information-and-dynamical-Wong",
            "title": {
                "fragments": [],
                "text": "Stochastic processes in information and dynamical systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707626"
                        ],
                        "name": "N. Wiener",
                        "slug": "N.-Wiener",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Wiener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiener"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60881246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80e9826f5a4238251340b5bf8e81bed7337e7c12",
            "isKey": false,
            "numCitedBy": 3191,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extrapolation,-Interpolation,-and-Smoothing-of-Time-Wiener",
            "title": {
                "fragments": [],
                "text": "Extrapolation, Interpolation, and Smoothing of Stationary Time Series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39282374"
                        ],
                        "name": "H. K\u00f6nig",
                        "slug": "H.-K\u00f6nig",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "K\u00f6nig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K\u00f6nig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118374805,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f60c972081657e9087e8d57a5dd8cf31b35c3410",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Eigenvalue-Distribution-of-Compact-Operators-K\u00f6nig",
            "title": {
                "fragments": [],
                "text": "Eigenvalue Distribution of Compact Operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100909650"
                        ],
                        "name": "A. Yaglom",
                        "slug": "A.-Yaglom",
                        "structuredName": {
                            "firstName": "Akiva",
                            "lastName": "Yaglom",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yaglom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117143547,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47ba00a0a67e07cc1e4eef1343c236775f4fca9a",
            "isKey": false,
            "numCitedBy": 1055,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Correlation-Theory-of-Stationary-and-Related-Random-Yaglom",
            "title": {
                "fragments": [],
                "text": "Correlation Theory of Stationary and Related Random Functions I: Basic Results"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49165342"
                        ],
                        "name": "Zhe Jiang",
                        "slug": "Zhe-Jiang",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe Jiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 12
                            }
                        ],
                        "text": "For example Ritter et al. [1995] showed that in 1-d with \u03bc uniform on [0, 1], processes which are r-times mean-square differentiable have \u03bbi \u221d i\u2212(2r+2) asymptotically."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5076462,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eb7a0617fa8b88dd4680b3f690220a93f0c56d1e",
            "isKey": false,
            "numCitedBy": 787,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spatial-Statistics-Jiang",
            "title": {
                "fragments": [],
                "text": "Spatial Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Database Systems"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sch\u00f6lkopf and Smola [2002] and Wegman [1982]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 221303277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96cff156cdc8a479aa994eae875a0860d663d317",
            "isKey": false,
            "numCitedBy": 2181,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-with-kernels-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115033499"
                        ],
                        "name": "M. Gibbs",
                        "slug": "M.-Gibbs",
                        "structuredName": {
                            "firstName": "Marilyn",
                            "lastName": "Gibbs",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gibbs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Gibbs and MacKay [2000], Jaakkola and Haussler [1999], and Seeger [2000]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1995] (in the context of numerical weather prediction) and by Gibbs and MacKay [1997] (in the context of general GP regression)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117952703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64b6ce4ad4624cb3544da1199aa4ec3416ce4386",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-implementation-of-gaussian-processes-Gibbs",
            "title": {
                "fragments": [],
                "text": "Efficient implementation of gaussian processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32856741,
            "fieldsOfStudy": [],
            "id": "c3c01d9d68bdfcfc36f78a7a238d31dcd390824c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Suboptimal Behavior of Bayes and MDL in Classification Under Misspecification"
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Smola and Sch\u00f6lkopf [2000] suggest a greedy algorithm to choose points to include into the active set so as to minimize the error criterion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41680909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "isKey": false,
            "numCitedBy": 726,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-Greedy-Matrix-Approximation-for-Machine-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Matrix Approximation for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42041158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6fff8b8ea77f157913986e7af53951d9fc1128e",
            "isKey": false,
            "numCitedBy": 2170,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m < n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m2n). We report experiments on the USPS and abalone data sets and show that we can set m \u226a n without any significant decrease in the accuracy of the solution."
            },
            "slug": "Using-the-Nystr\u00f6m-Method-to-Speed-Up-Kernel-Williams-Seeger",
            "title": {
                "fragments": [],
                "text": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems) and the computational complexity of a predictor using this approximation is O(m2n)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advances in Large Margin Classifiers, pages 39\u201350"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "O\u2019Hagan [1978] suggested making w a function of x to allow for different values of w to be appropriate in different regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "12 we show the results Monte Carlo results of running a sophisticated Markov chain Monte Carlo method called Annealed Importance Sampling [Neal, 2001] carried out by Kuss and Rasmussen [2005]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Annealed Importance Sampling. Statistics and Computing, 11:125\u2013139"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "T\u00fcbingen and Edinburgh, summer 2005 Second printing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 A.9 Convexity"
            },
            "venue": {
                "fragments": [],
                "text": "Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 A.9 Convexity"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correlation Theory of Stationary and Related Random Functions Volume I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Monte-Carlo Algorithms for Finding Low-Rank"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": ") The dichotomy theorem for Gaussian processes (due to Hajek [1958] and, independently, Feldman [1958]) states that two Gaussian processes are either equivalent or orthogonal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On a Property of Normal Distributions of Any Stochastic Process (In Russian)"
            },
            "venue": {
                "fragments": [],
                "text": "Czechoslovak Math. J., 8:610\u2013618. Translated in Selected Trans. Math. Statist. Probab. 1 245-252 (1961). Also available in Collected Works of Jaroslav Hajek, eds. M. Hu\u0161kov\u00e1, R. Beran, V. Dupa\u010d, Wiley, (1998)."
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On a property of normal distributions of any stochastic process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sprotno u\u010denje modelov na podlagi Gaussovih procesov. Diplomsko delo"
            },
            "venue": {
                "fragments": [],
                "text": "Sprotno u\u010denje modelov na podlagi Gaussovih procesov. Diplomsko delo"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 74
                            }
                        ],
                        "text": "Gaussian process prediction is also well known in the geostatistics field (see, geostatistics e.g. Matheron, 1973; Journel and Huijbregts, 1978) where it is known as kriging ,(17) and in meteorology [Thompson, 1956, Daley, 1991] although this literakriging ture naturally has focussed mostly on two- and three-dimensional input spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mining Geostatistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Link Between Kriging and Thin-plate Splines"
            },
            "venue": {
                "fragments": [],
                "text": "Probability, Statsitics and Optimization,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "208 B.1.1 Sampling and"
            },
            "venue": {
                "fragments": [],
                "text": "209 B.2 Continuous-time Gaussian Markov Processes . . . . . . . . . . . . . . . . 211 B.2.1 Continuous The Solution of the Corresponding Difference Equation on P N . . 215 B.4 The Relationship Between Discrete-time and Sampled Continuous"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Theory of Stochastic Processes, volume 1"
            },
            "venue": {
                "fragments": [],
                "text": "Springer Verlag, Berlin."
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Luo and Wahba [1997] choose the next kernel so as to minimize the residual sum of squares (RSS) |y\u2212Knm\u03b1m|(2) after optimizing \u03b1m."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "3This is known as Cromwell\u2019s dictum [Lindley, 1985] after Oliver Cromwell who on August 5th, 1650 wrote to the synod of the Church of Scotland: \u201cI beseech you, in the bowels of Christ, consider it possible that you are mistaken."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Making Decisions. John Wiley and Sons, London, UK, second edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A.1 Joint, Marginal and Conditional Probability . . . . . . . . . . . . . . . . . 199 A.2 Gaussian Identities"
            },
            "venue": {
                "fragments": [],
                "text": "A.1 Joint, Marginal and Conditional Probability . . . . . . . . . . . . . . . . . 199 A.2 Gaussian Identities"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Theory for Continuous Systems. World Scientific, Singapore"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computational Biology,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Hawkins [1989] gives the exact eigenvalue spectrum for the OU process on [0, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Machine Learning, the MIT Press, 2006, ISBN 026218253X"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical Behaviour and Consistency of Classification Methods based on Convex Risk Minimization (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "As usual the question of how to choose a subset of points arises; Lin et al. [2000] select these using a clustering method, while Zhu and Hastie [2002] propose a forward selection strategy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Time Series Analysis in 1880: A Discussion of Contributions Made by T"
            },
            "venue": {
                "fragments": [],
                "text": "N. Thiele. International Statistical Review, 49:319\u2013333."
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": "For further discussion of Markov properties of random fields see the Appendix in Adler [1981]. If instead of R we wish to define a Markov random field (MRF) on a graphical structure (for example the lattice Z) things become more straightforward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 81
                            }
                        ],
                        "text": "For further discussion of Markov properties of random fields see the Appendix in Adler [1981]. If instead of R we wish to define a Markov random field (MRF) on a graphical structure (for example the lattice Z) things become more straightforward. We follow the presentation in Jordan [2005]. Let G = (X,E) be a graph where X is a set of nodes that are in one-to-one correspondence with a set of random variables, and E be the set of undirected edges of the graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Geometry of Random Fields. Wiley, Chichester"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics,"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Upper and Lower Bounds on the Learning Curve for Gaussian Proccesses"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning, 40:77\u2013102."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 198
                            }
                        ],
                        "text": "Prediction with Gaussian processes is certainly not a very recent topic, especially for time series analysis; the basic theory goes back at least as far as the time series work of Wiener [1949] and Kolmogorov [1941] in the 1940\u2019s. Indeed Lauritzen [1981] discusses relevant work by the Danish astronomer T."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 198
                            }
                        ],
                        "text": "Prediction with Gaussian processes is certainly not a very recent topic, especially for time series analysis; the basic theory goes back at least as far as the time series work of Wiener [1949] and Kolmogorov [1941] in the 1940\u2019s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolation und Extrapolation von station\u00e4ren zuf\u00e4ligen Folgen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1941
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identifikacija mo  delov \u010drne skrinjice na podlagi Gaussovih procesov"
            },
            "venue": {
                "fragments": [],
                "text": "IJS de  lovno poroc\u030cilo,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sprotno u  \u010denje modelov na podlagi Gaussovih procesov"
            },
            "venue": {
                "fragments": [],
                "text": "Diploms  k delo,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designs on Random Fields"
            },
            "venue": {
                "fragments": [],
                "text": "Srivastava, J. N., editor, A Survey of Statistical Design and Linear Models, pages 593\u2013608. North-Holland."
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric Estimation of Nonstationary Covariance Structure"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Statistical Association,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LWPR: An O(n) Algorithm for Incremental Real Time Learning in High Dimensional Space"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the Seventeenth International Conference on Machine Learning (ICML 2000), pages 1079\u20131086."
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 65,
            "methodology": 63,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 233,
        "totalPages": 24
    },
    "page_url": "https://www.semanticscholar.org/paper/Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams/82266f6103bade9005ec555ed06ba20b5210ff22?sort=total-citations"
}