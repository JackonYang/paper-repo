{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "independent of the choice of the origin) in H work with cpd kernels [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Some Properties of Kernels [56] If k1, k2, ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 44
                            }
                        ],
                        "text": "Hence only need to store \u03b3 with accuracy \u0394\u03b3 [56, 75]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": ", (xm, ym, f (xm))) = 1 m m \u2211 i=1 (yi \u2212 f (xi))(2), and \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 (\u03bb > 0): [37] \u2022 generalization to non-quadratic cost functions: [16] \u2022 present form: [56]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Learning with Kernels\nBernhard Scho\u0308lkopf\nMax-Planck-Institut fu\u0308r biologische Kybernetik 72076 Tu\u0308bingen, Germany\nbs@tuebingen.mpg.de\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Roadmap\n\u2022 Elements of Statistical Learning Theory \u2022 Kernels and feature spaces \u2022 Support vector algorithms and other kernel methods \u2022 Applications\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "\u2217 A rigorous treatment would need to use a second randomization over permutations of the 2m-sample, see [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": true,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47934783"
                        ],
                        "name": "J. Davenport",
                        "slug": "J.-Davenport",
                        "structuredName": {
                            "firstName": "Janina",
                            "lastName": "Davenport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Davenport"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 513,
                                "start": 497
                            }
                        ],
                        "text": "The Kernel Trick \u2014 Summary \u2022 any algorithm that only depends on dot products can benefit from the kernel trick \u2022 this way, we can apply linear methods to vectorial as well as non-vectorial data \u2022 think of the kernel as a nonlinear similarity measure \u2022 examples of common kernels: Polynomial k(x, x\u2032) = ( \u3008 x, x\u2032 \u3009 + c)d Sigmoid k(x, x\u2032) = tanh(\u03ba \u3008 x, x\u2032 \u3009 + \u0398) Gaussian k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20162/(2\u03c32)) \u2022 Kernels are studied also in the Gaussian Process prediction community (covariance functions) [79, 76, 81, 41]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 220072940,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "56b332dc42f9bb3f3202e53eb6a9a63147bc6564",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editor-Davenport",
            "title": {
                "fragments": [],
                "text": "Editor"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 513,
                                "start": 497
                            }
                        ],
                        "text": "The Kernel Trick \u2014 Summary \u2022 any algorithm that only depends on dot products can benefit from the kernel trick \u2022 this way, we can apply linear methods to vectorial as well as non-vectorial data \u2022 think of the kernel as a nonlinear similarity measure \u2022 examples of common kernels: Polynomial k(x, x\u2032) = ( \u3008 x, x\u2032 \u3009 + c)d Sigmoid k(x, x\u2032) = tanh(\u03ba \u3008 x, x\u2032 \u3009 + \u0398) Gaussian k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20162/(2\u03c32)) \u2022 Kernels are studied also in the Gaussian Process prediction community (covariance functions) [79, 76, 81, 41]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5073,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": ", (xm, ym, f (xm))) = 1 m m \u2211 i=1 (yi \u2212 f (xi))(2), and \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 (\u03bb > 0): [37] \u2022 generalization to non-quadratic cost functions: [16] \u2022 present form: [56]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 121062339,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3065c5e37a0c1f1be365e88ddf2d5cd02faa5db1",
            "isKey": false,
            "numCitedBy": 1330,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-results-on-Tchebycheffian-spline-functions-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheffian spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47312726"
                        ],
                        "name": "J. Mercer",
                        "slug": "J.-Mercer",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Mercer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "i=1 \u03bbi\u03c8i(x)\u03c8i(x \u2032) using eigenfunctions \u03c8i and eigenvalues \u03bbi \u2265 0 [42]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121070291,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b48694cb275eba60b48026f3159373c92c1b286c",
            "isKey": false,
            "numCitedBy": 1592,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The present memoir is the outcome of an attempt to obtain the conditions under which a given symmetric and continuous function k ( s, t ) is definite, in the sense of Hilbert. At an early stage, however, it was found that the class of definite functions was too restricted to allow the determination of necessary and sufficient conditions in terms of the determinants of \u00a7 10. The discovery that this could be done for functions of positive or negative type, and the fact that almost all the theorems which are true of definite functions are, with slight modification, true of these, led finally to the abandonment of the original plan in favour of a discussion of the properties of functions belonging to the wider classes. The first part of the memoir is devoted to the definition of various terms employed, and to the re-statement of the consequences which follow from Hilbert\u2019s theorem."
            },
            "slug": "Functions-of-Positive-and-Negative-Type,-and-their-Mercer",
            "title": {
                "fragments": [],
                "text": "Functions of Positive and Negative Type, and their Connection with the Theory of Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1909
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120654716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5f75d859e750961d1d094f166fc3b564d9cfe99b",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The report presents classes of prior distributions for which the Bayes' estimate of an unknown function given certain observations is a spline function. (Author)"
            },
            "slug": "A-Correspondence-Between-Bayesian-Estimation-on-and-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35080630"
                        ],
                        "name": "H. Weinert",
                        "slug": "H.-Weinert",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Weinert",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Weinert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118235835,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bfadc8299539818c21da34cda6c12aab1da3d744",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reproducing-kernel-Hilbert-spaces:-Applications-in-Weinert",
            "title": {
                "fragments": [],
                "text": "Reproducing kernel Hilbert spaces: Applications in statistical signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144403112"
                        ],
                        "name": "C. Berg",
                        "slug": "C.-Berg",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Berg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144027552"
                        ],
                        "name": "J. Christensen",
                        "slug": "J.-Christensen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Christensen",
                            "middleNames": [
                                "Peter",
                                "Reus"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101172055"
                        ],
                        "name": "P. Ressel",
                        "slug": "P.-Ressel",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Ressel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ressel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Roadmap\n\u2022 Elements of Statistical Learning Theory \u2022 Kernels and feature spaces \u2022 Support vector algorithms and other kernel methods \u2022 Applications\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 33
                            }
                        ],
                        "text": "The Feature Space for PD Kernels [4, 1, 50] \u2022 define a feature map \u03a6 : X \u2192 RX x \u2192 k(."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Learning with Kernels\nBernhard Scho\u0308lkopf\nMax-Planck-Institut fu\u0308r biologische Kybernetik 72076 Tu\u0308bingen, Germany\nbs@tuebingen.mpg.de\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                }
            ],
            "corpusId": 117450983,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b9f4febf74f3802df63f3d73dd49cabd37eece15",
            "isKey": false,
            "numCitedBy": 1021,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Harmonic-Analysis-on-Semigroups-Berg-Christensen",
            "title": {
                "fragments": [],
                "text": "Harmonic Analysis on Semigroups"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2734323"
                        ],
                        "name": "Y. Sawano",
                        "slug": "Y.-Sawano",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Sawano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sawano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087748"
                        ],
                        "name": "S. Saitoh",
                        "slug": "S.-Saitoh",
                        "structuredName": {
                            "firstName": "Saburou",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saitoh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Roadmap\n\u2022 Elements of Statistical Learning Theory \u2022 Kernels and feature spaces \u2022 Support vector algorithms and other kernel methods \u2022 Applications\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 33
                            }
                        ],
                        "text": "The Feature Space for PD Kernels [4, 1, 50] \u2022 define a feature map \u03a6 : X \u2192 RX x \u2192 k(."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Learning with Kernels\nBernhard Scho\u0308lkopf\nMax-Planck-Institut fu\u0308r biologische Kybernetik 72076 Tu\u0308bingen, Germany\nbs@tuebingen.mpg.de\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                }
            ],
            "corpusId": 117252811,
            "fieldsOfStudy": [],
            "id": "636b46471adea4916ec1b2e38c8e8265218f6952",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Reproducing-Kernels-and-Its-Applications-Sawano-Saitoh",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels and Its Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115205884,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7cabbdf6a7288d15e26fa6ea504009bab3d1edf4",
            "isKey": false,
            "numCitedBy": 1137,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-recognition-using-generalized-portrait-Vapnik",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79783680"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099637865"
                        ],
                        "name": "Mozer",
                        "slug": "Mozer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mozer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054400760"
                        ],
                        "name": "M. Jordan",
                        "slug": "M.-Jordan",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2214848"
                        ],
                        "name": "T. Petsche",
                        "slug": "T.-Petsche",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Petsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petsche"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60518954,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "271c040ea880abc2470f72690ed89bc3d8a11a2c",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-accuracy-and-speed-of-support-vector-Burges-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Improving the accuracy and speed of support vector learning machines"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Roadmap\n\u2022 Elements of Statistical Learning Theory \u2022 Kernels and feature spaces \u2022 Support vector algorithms and other kernel methods \u2022 Applications\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 33
                            }
                        ],
                        "text": "The Feature Space for PD Kernels [4, 1, 50] \u2022 define a feature map \u03a6 : X \u2192 RX x \u2192 k(."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "Learning with Kernels\nBernhard Scho\u0308lkopf\nMax-Planck-Institut fu\u0308r biologische Kybernetik 72076 Tu\u0308bingen, Germany\nbs@tuebingen.mpg.de\nB. Scho\u0308lkopf, Canberra, February 2006"
                    },
                    "intents": []
                }
            ],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": false,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49128898"
                        ],
                        "name": "U. A. M\u00fcller",
                        "slug": "U.-A.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "M\u00fcller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. A. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776424"
                        ],
                        "name": "Eduard S\u00e4ckinger",
                        "slug": "Eduard-S\u00e4ckinger",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "S\u00e4ckinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduard S\u00e4ckinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46946958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55f58ee028e8d86ddf80f68b7538bfb5d6005dc8",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold."
            },
            "slug": "Comparison-of-classifier-methods:-a-case-study-in-Bottou-Cortes",
            "title": {
                "fragments": [],
                "text": "Comparison of classifier methods: a case study in handwritten digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits by considering not only raw accuracy, but also training time, recognition time, and memory requirements."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 52810328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1061ff8a216a8d00f5f189d7ea593c6f0703b771",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Support Vector Machine SVM is a uni versal learning machine whose decision sur face is parameterized by a set of support vec tors and by a set of corresponding weights An SVM is also characterized by a kernel function Choice of the kernel determines whether the resulting SVM is a polynomial classi er a two layer neural network a ra dial basis function machine or some other learning machine SVMs are currently considerably slower in test phase than other approaches with sim ilar generalization performance To address this we present a general method to signif icantly decrease the complexity of the deci sion rule obtained using an SVM The pro posed method computes an approximation to the decision rule in terms of a reduced set of vectors These reduced set vectors are not support vectors and can in some cases be computed analytically We give ex perimental results for three pattern recogni tion problems The results show that the method can decrease the computational com plexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods Fur ther the method allows the generalization performance complexity trade o to be di rectly controlled The proposed method is not speci c to pattern recognition and can be applied to any problem where the Sup port Vector algorithm is used for example regression INTRODUCTION SUPPORT VECTOR MACHINES Consider a two class classi er for which the decision rule takes the form"
            },
            "slug": "Simplified-Support-Vector-Decision-Rules-Burges",
            "title": {
                "fragments": [],
                "text": "Simplified Support Vector Decision Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show that the method can decrease the computational complexity of the decision rule by a factor of ten with no loss in generalization perfor mance making the SVM test speed com petitive with that of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709847"
                        ],
                        "name": "E. Eskin",
                        "slug": "E.-Eskin",
                        "structuredName": {
                            "firstName": "Eleazar",
                            "lastName": "Eskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Properties, II: Functional Calculus [60] \u2022K symmetric m\u00d7m matrix with spectrum \u03c3(K) \u2022 f a continuous function on \u03c3(K) \u2022 Then there is a symmetric matrix f (K) with eigenvalues in f (\u03c3(K))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": ", 84] \u2013 complex kernels from simple ones [30, 2], global kernels from local ones [38] \u2013 functional calculus for kernel matrices [60] \u2013 model selection, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26520923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae2e70bbb35efb2f00f13e259de3e4efdf33dc6d",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In kernel methods, all the information about the training data is contained in the Gram matrix. If this matrix has large diagonal values, which arises for many types of kernels, then kernel methods do not perform well. We propose and test several methods for dealing with this problem by reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine."
            },
            "slug": "A-Kernel-Approach-for-Learning-from-Almost-Patterns-Sch\u00f6lkopf-Weston",
            "title": {
                "fragments": [],
                "text": "A Kernel Approach for Learning from Almost Orthogonal Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes and test several methods for dealing with the problem of reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine."
            },
            "venue": {
                "fragments": [],
                "text": "PKDD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [5]) Introduce Lagrange multipliers \u03b1i \u2265 0 and a Lagrangian L(w, b,\u03b1) = 1 2 \u2016w\u20162 \u2212 m \u2211 i=1 \u03b1i (yi \u00b7 [\u3008w,xi\u3009 + b] \u2212 1) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": false,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 41
                            }
                        ],
                        "text": ", 84] \u2013 complex kernels from simple ones [30, 2], global kernels from local ones [38] \u2013 functional calculus for kernel matrices [60] \u2013 model selection, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60001180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "609776ee2cd58040d82150bcea44f9bb1fc6f03d",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-kernels-for-structured-data-Bartlett-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Some kernels for structured data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116357032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "340e4ad19181e04717e6fe802c01e362d0495ea2",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a common probabilistic framework for kernel or spline smooth- ing methods, including popular architectures such as Gaussian processes and Support Vector machines. We identify the problem of unnormalized loss func- tions and suggest a general technique to overcome this problem at least ap- proximately. We give an intuitive interpretation of the effect an unnormalized loss function can induce, by comparing Support Vector classification (SVC) with Gaussian process classification (GPC) as a nonparametric generalization of logistic regression. This interpretation relates SVC to boosting techniques. We propose a variational Bayesian model selection algorithm for general nor- malized loss functions. This algorithm has a wider applicability than other previously suggested Bayesian techniques and exhibits comparable perfor- mance in cases where both techniques are applicable. We present and discuss results of a substantial number of experiments in which we applied the vari- ational algorithm to common real-world classification tasks and compared it to a range of other known methods. The wider scope of this thesis is to provide a bridge between the fields of probabilistic Bayesian techniques and Statistical Learning Theory, and we present some material of tutorial nature which we hope will be useful to researchers of both fields."
            },
            "slug": "Bayesian-methods-for-Support-Vector-machines-and-Seeger",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for Support Vector machines and Gaussian processes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A variational Bayesian model selection algorithm for general nor- malized loss functions has a wider applicability than other previously suggested Bayesian techniques and exhibits comparable perfor- mance in cases where both techniques are applicable."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Kernel Dependency Estimation [80] Given two sets X and Y with kernels k and k\u2032, and training data (xi, yi)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 813046,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "359300ac4a78687d60e18681925709b404b8fa54",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images."
            },
            "slug": "Kernel-Dependency-Estimation-Weston-Chapelle",
            "title": {
                "fragments": [],
                "text": "Kernel Dependency Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work considers the learning problem of finding a dependency between a general class of objects and another, possibly different, generalclass of objects, made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "\u2022 the extreme case: by making the sub-problems very small (just two points), one can solve them analytically [46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": ", vM with minimal M such that (\u2217) holds (Tipping & Sch\u00f6lkopf, 2001 [66]) B."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": ", Linde-Buzo-Gray (LBG) algorithm \u2022 kernel approach [66]: \u2013 specify a maximum distortion guarantee: D [v(xn), xn] \u2264 R (\u2217) \u2013 constrain the codebook to be a subset of the data set: {v1, ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11591423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab2642d36e8e95ffb49f15b87e2c7ebde4096c8b",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Kernel-Approach-for-Vector-Quantization-with-Tipping-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "A Kernel Approach for Vector Quantization with Guaranteed Distortion Bounds"
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2637505"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": ", (xm, ym, f (xm))) = 1 m m \u2211 i=1 (yi \u2212 f (xi))(2), and \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 (\u03bb > 0): [37] \u2022 generalization to non-quadratic cost functions: [16] \u2022 present form: [56]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 120908503,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5bf249445466ac041694525d8969d7879cd259ce",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A general approach to the first order asymptotic analysis ofpenalized likelihood and related estimators is described. The method gives expansions for the systematic and random error. Asymptotic convergence rates in a family of spectral norms are obtained. The theory applies to a broad range of function estimation prob~erns including non\"paxametric. dellSity, hazard and generalized regression curve estimation. Some examples are provided. AMS 1980 subject classifications. Primary, 62-G05, Secondary, 62J05, 41-A35, 41-A25, 47-A53, 45-LlO, 45-M05."
            },
            "slug": "Asymptotic-Analysis-of-Penalized-Likelihood-and-Cox-O\u2019Sullivan",
            "title": {
                "fragments": [],
                "text": "Asymptotic Analysis of Penalized Likelihood and Related Estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14727192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53",
            "isKey": false,
            "numCitedBy": 13352,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software."
            },
            "slug": "An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory, and will guide practitioners to updated literature, new applications, and on-line software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087528"
                        ],
                        "name": "L. Gy\u00f6rfi",
                        "slug": "L.-Gy\u00f6rfi",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Gy\u00f6rfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gy\u00f6rfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [69, 20])) For m 2 \u2265 2 we have P{ sup f\u2208F (R[f ]\u2212Remp[f ]) > } \u2264 2P{ sup f\u2208F (Remp[f ]\u2212R\u2032 emp[f ]) > /2} Here, the first P refers to the distribution of iid samples of size m, while the second one refers to iid samples of size 2m."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116929976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fcdee6c6d885ac2bd32e122dbf282f93720c22",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 557,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface * Introduction * The Bayes Error * Inequalities and alternatedistance measures * Linear discrimination * Nearest neighbor rules *Consistency * Slow rates of convergence Error estimation * The regularhistogram rule * Kernel rules Consistency of the k-nearest neighborrule * Vapnik-Chervonenkis theory * Combinatorial aspects of Vapnik-Chervonenkis theory * Lower bounds for empirical classifier selection* The maximum likelihood principle * Parametric classification *Generalized linear discrimination * Complexity regularization *Condensed and edited nearest neighbor rules * Tree classifiers * Data-dependent partitioning * Splitting the data * The resubstitutionestimate * Deleted estimates of the error probability * Automatickernel rules * Automatic nearest neighbor rules * Hypercubes anddiscrete spaces * Epsilon entropy and totally bounded sets * Uniformlaws of large numbers * Neural networks * Other error estimates *Feature extraction * Appendix * Notation * References * Index"
            },
            "slug": "A-Probabilistic-Theory-of-Pattern-Recognition-Devroye-Gy\u00f6rfi",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Theory of Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Bayes Error and Vapnik-Chervonenkis theory are applied as guide for empirical classifier selection on the basis of explicit specification and explicit enforcement of the maximum likelihood principle."
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Modelling and Applied Probability"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49086005"
                        ],
                        "name": "T. McElroy",
                        "slug": "T.-McElroy",
                        "structuredName": {
                            "firstName": "Tucker",
                            "lastName": "McElroy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McElroy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2230454"
                        ],
                        "name": "D. Politis",
                        "slug": "D.-Politis",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Politis",
                            "middleNames": [
                                "Nicolas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Politis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 38084019,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "82ad666e0293b6307c37223030b89cd3276530df",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "THE current mathemetical theories concerned with the estimation of one or more unknown parameters in a frequency distribution have been the occasion of much controversy, and the absence of a systematic review of this field at a sufficiently elementary level has been a serious obstacle to the student. Two recent articles by Dr. Leon Solomon published in the Journal of the Institute of Actuaries Students9Society (7, 144 and 213) go a long way towards meeting this need, and form a most welcome addition to the literature. The first article is chiefly concerned with 'point' estimation. The concept of sufficiency is discussed, the minimum-variance theorem is proved, and the method of maximum likelihood is introduced, the proofs of its main properties being given in outline. The second article is concerned with 'interval' estimation, first from the point of view of the theory of confidence intervals and then in terms of fiducial probability, the contrast between these two methods of approach being illustrated by a discussion of the Behrens\u2013Fisher problem. Rigorous proofs of the theorems are not attempted, and the emphasis throughout is laid on the new ideas which are involved. Another welcome feature of the presentation is the inclusion of many worked examples."
            },
            "slug": "Statistical-Estimation-McElroy-Politis",
            "title": {
                "fragments": [],
                "text": "Statistical Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509099"
                        ],
                        "name": "M. Kauffman",
                        "slug": "M.-Kauffman",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kauffman",
                            "middleNames": [
                                "R.",
                                "Hern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kauffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81426350"
                        ],
                        "name": "San Mateo",
                        "slug": "San-Mateo",
                        "structuredName": {
                            "firstName": "San",
                            "lastName": "Mateo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "San Mateo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3011435"
                        ],
                        "name": "E. Prescott",
                        "slug": "E.-Prescott",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Prescott",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Prescott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15897108,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "604954a3600f749b25a9f52317a42d13a8ec0339",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Plutowski, M., et al. (1994). Cross-validation estimates integrated mean squared error. In Advances in neural information processing systems 6, Cowan et al. some constant set by k and m. It's also true that E(C OTS | \u03c6, d X) is not drastically different if one considers d X 's with a different m'. Accordingly, our summand doesn't vary drastically between d X 's of one m' and d X 's of another. Since n >> m and \u03c0(x) is uniform though, almost all of the terms in the sum have m' = m."
            },
            "slug": "The-Lack-of-A-Priori-Distinctions-Between-Learning-Kauffman-Mateo",
            "title": {
                "fragments": [],
                "text": "The Lack of A Priori Distinctions Between Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It's true that n >> m and \u03c0(x) is uniform though, almost all of the terms in the sum have m' = m, so the summand doesn't vary drastically between d X 's of one m' and d X's of another."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413848"
                        ],
                        "name": "B. Shahraray",
                        "slug": "B.-Shahraray",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahraray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shahraray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387879"
                        ],
                        "name": "D. Gibbon",
                        "slug": "D.-Gibbon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gibbon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gibbon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1195744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5383f8ada6fb3a0be6a40d8bb4adbb3d96f46645",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that support vectors machines (SVM\u2019s) are much better than conventional algorithms in a relevancy feedback (RF) environment in information retrieval (IR) of text documents. We track performance as a function of feedback iteration and show that while the conventional algorithms do very well in the initial feedback iteration if the topic searched for has high visibility in the data base, they do very poorly if the relevant documents are a small percentage of the data base. SVM\u2019s however do very well when the number of documents returned in the preliminary search is low and the number of relevant documents is small. The competitive algorithms examined are Rocchio, Ide regular, and Ide dec-hi."
            },
            "slug": "Relevance-Feedback-using-Support-Vector-Machines-Drucker-Shahraray",
            "title": {
                "fragments": [],
                "text": "Relevance Feedback using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "It is shown that support vectors machines (SVM\u2019s) are much better than conventional algorithms in a relevancy feedback (RF) environment in information retrieval (IR) of text documents and conventional algorithms do very poorly if the relevant documents are a small percentage of the data base."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716788"
                        ],
                        "name": "M. Kawanabe",
                        "slug": "M.-Kawanabe",
                        "structuredName": {
                            "firstName": "Motoaki",
                            "lastName": "Kawanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414086"
                        ],
                        "name": "G. R\u00e4tsch",
                        "slug": "G.-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029782"
                        ],
                        "name": "S. Sonnenburg",
                        "slug": "S.-Sonnenburg",
                        "structuredName": {
                            "firstName": "S\u00f6ren",
                            "lastName": "Sonnenburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sonnenburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11013893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84cccc9e14e49a6c56147e4cd36bda2ffd70b683",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, Jaakkola and Haussler (1999) proposed a method for constructing kernel functions from probabilistic models. Their so-called Fisher kernel has been combined with discriminative classifiers such as support vector machines and applied successfully in, for example, DNA and protein analysis. Whereas the Fisher kernel is calculated from the marginal log-likelihood, we propose the TOP kernel derived from tangent vectors of posterior log-odds. Furthermore, we develop a theoretical framework on feature extractors from probabilistic models and use it for analyzing the TOP kernel. In experiments, our new discriminative TOP kernel compares favorably to the Fisher kernel."
            },
            "slug": "A-New-Discriminative-Kernel-from-Probabilistic-Tsuda-Kawanabe",
            "title": {
                "fragments": [],
                "text": "A New Discriminative Kernel from Probabilistic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a new discriminative TOP kernel derived from tangent vectors of posterior log-odds and develops a theoretical framework on feature extractors from probabilistic models and uses it for analyzing the TOP kernel."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Properties of Kernel Matrices, I [51] Suppose we are given distinct training patterns x1, ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30545896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "356125478f5d06b564b420755a4944254045bbbe",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology."
            },
            "slug": "Support-vector-learning-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This book provides a comprehensive analysis of what can be done using Support vector Machines, achieving record results in real-life pattern recognition problems, and proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which it is considered as the most natural and elegant way for generalization of classical Principal Component analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716986"
                        ],
                        "name": "T. Furey",
                        "slug": "T.-Furey",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Furey",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Furey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34700222"
                        ],
                        "name": "D. Bednarski",
                        "slug": "D.-Bednarski",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bednarski",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bednarski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5049147"
                        ],
                        "name": "M. Schummer",
                        "slug": "M.-Schummer",
                        "structuredName": {
                            "firstName": "Mich\u00e8l",
                            "lastName": "Schummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10039376,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "5794141889d0e994c3103b0aaab08a18222c9c43",
            "isKey": false,
            "numCitedBy": 2439,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nDNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.\n\n\nRESULTS\nWe demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment results for 97,802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed. The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.\n\n\nAVAILABILITY\nThe SVM software is available at http://www.cs. columbia.edu/ approximately bgrundy/svm."
            },
            "slug": "Support-vector-machine-classification-and-of-cancer-Furey-Cristianini",
            "title": {
                "fragments": [],
                "text": "Support vector machine classification and validation of cancer tissue samples using microarray expression data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new method to analyse tissue samples using support vector machines for mis-labeled or questionable tissue results and shows that other machine learning methods also perform comparably to the SVM on many of those datasets."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2671293"
                        ],
                        "name": "M. Diekhans",
                        "slug": "M.-Diekhans",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Diekhans",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Diekhans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2048632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e2dd064daaac3603581ec65b580b7b5385e2c2b",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for detecting remote protein homologies is introduced and shown to perform well in classifying protein domains by SCOP superfamily. The method is a variant of support vector machines using a new kernel function. The kernel function is derived from a generative statistical model for a protein family, in this case a hidden Markov model. This general approach of combining generative models like HMMs with discriminative methods such as support vector machines may have applications in other areas of biosequence analysis as well."
            },
            "slug": "A-Discriminative-Framework-for-Detecting-Remote-Jaakkola-Diekhans",
            "title": {
                "fragments": [],
                "text": "A Discriminative Framework for Detecting Remote Protein Homologies"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new method for detecting remote protein homologies is introduced and shown to perform well in classifying protein domains by SCOP superfamily using a new kernel function derived from a generative statistical model for a protein family, in this case a hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Biol."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125657"
                        ],
                        "name": "Phil Knirsch",
                        "slug": "Phil-Knirsch",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Knirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Knirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14669541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a3e0028d99439ce2741d0e147b6e9a34bc4267",
            "isKey": false,
            "numCitedBy": 1231,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel methods. First, we use it to reduce the computational complexity of SV decision functions; second, we combine it with the Kernel PCA algorithm, thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real-world data."
            },
            "slug": "Input-space-versus-feature-space-in-kernel-based-Sch\u00f6lkopf-Mika",
            "title": {
                "fragments": [],
                "text": "Input space versus feature space in kernel-based methods"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The geometry of feature space is reviewed, and the connection between feature space and input space is discussed by dealing with the question of how one can, given some vector in feature space, find a preimage in input space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Soft Margin SVMs C-SVM [15]: for C > 0, minimize \u03c4 (w, \u03be) = 1 2 \u2016w\u20162 + C m \u2211 i=1 \u03bei subject to yi \u00b7 (\u3008w,xi\u3009 + b) \u2265 1 \u2212 \u03bei, \u03bei \u2265 0 (margin 2/\u2016w\u2016) \u03bd-SVM [58]: for 0 \u2264 \u03bd < 1, minimize \u03c4 (w, \u03be, \u03c1) = 1 2 \u2016w\u20162 \u2212 \u03bd\u03c1 + 1 m \u2211 i \u03bei subject to yi \u00b7 (\u3008w,xi\u3009 + b) \u2265 \u03c1\u2212 \u03bei, \u03bei \u2265 0 (margin 2\u03c1/\u2016w\u2016)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 207673395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d73c0d0c92446102fdb6cc728b5d69674a1a387",
            "isKey": false,
            "numCitedBy": 2615,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results."
            },
            "slug": "New-Support-Vector-Algorithms-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "New Support Vector Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of support vector algorithms for regression and classification that eliminates one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35281,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058177533"
                        ],
                        "name": "Simon Tong",
                        "slug": "Simon-Tong",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7806109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8797f1d253c75669d96e6fcceda2be3f8534e1d",
            "isKey": false,
            "numCitedBy": 3138,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings."
            },
            "slug": "Support-Vector-Machine-Active-Learning-with-to-Text-Tong-Koller",
            "title": {
                "fragments": [],
                "text": "Support Vector Machine Active Learning with Applications to Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results showing that employing the active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings are presented."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "1 \u2022 RBF kernel, C and \u03c3 chosen as in [59]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1900499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4a422669ec9b6a60b05d2d2595314008a5fb419",
            "isKey": false,
            "numCitedBy": 1314,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "slug": "Comparing-support-vector-machines-with-Gaussian-to-Sch\u00f6lkopf-Sung",
            "title": {
                "fragments": [],
                "text": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system, and the SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829255"
                        ],
                        "name": "C. Yeang",
                        "slug": "C.-Yeang",
                        "structuredName": {
                            "firstName": "Chen-Hsiang",
                            "lastName": "Yeang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yeang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631654"
                        ],
                        "name": "S. Ramaswamy",
                        "slug": "S.-Ramaswamy",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Ramaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ramaswamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144085315"
                        ],
                        "name": "P. Tamayo",
                        "slug": "P.-Tamayo",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Tamayo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tamayo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153292035"
                        ],
                        "name": "Sayan Mukherjee",
                        "slug": "Sayan-Mukherjee",
                        "structuredName": {
                            "firstName": "Sayan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayan Mukherjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27370905"
                        ],
                        "name": "Michael Angelo",
                        "slug": "Michael-Angelo",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Angelo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Angelo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48940636"
                        ],
                        "name": "Michael Reich",
                        "slug": "Michael-Reich",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Reich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Reich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9311320"
                        ],
                        "name": "E. Lander",
                        "slug": "E.-Lander",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Lander",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727782"
                        ],
                        "name": "J. Mesirov",
                        "slug": "J.-Mesirov",
                        "structuredName": {
                            "firstName": "Jill",
                            "lastName": "Mesirov",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mesirov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2900051"
                        ],
                        "name": "T. Golub",
                        "slug": "T.-Golub",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Golub",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Golub"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9600286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba34b59c82b71d9a2e0ceb2630096d1035504023",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Using gene expression data to classify tumor types is a very promising tool in cancer diagnosis. Previous works show several pairs of tumor types can be successfully distinguished by their gene expression patterns (Golub et al. 1999, Ben-Dor et al. 2000, Alizadeh et al. 2000). However, the simultaneous classification across a heterogeneous set of tumor types has not been well studied yet. We obtained 190 samples from 14 tumor classes and generated a combined expression dataset containing 16063 genes for each of those samples. We performed multi-class classification by combining the outputs of binary classifiers. Three binary classifiers (k-nearest neighbors, weighted voting, and support vector machines) were applied in conjunction with three combination scenarios (one-vs-all, all-pairs, hierarchical partitioning). We achieved the best cross validation error rate of 18.75% and the best test error rate of 21.74% by using the one-vs-all support vector machine algorithm. The results demonstrate the feasibility of performing clinically useful classification from samples of multiple tumor types."
            },
            "slug": "Molecular-classification-of-multiple-tumor-types-Yeang-Ramaswamy",
            "title": {
                "fragments": [],
                "text": "Molecular classification of multiple tumor types"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work obtained 190 samples from 14 tumor classes and generated a combined expression dataset containing 16063 genes for each of those samples, and performed multi-class classification by combining the outputs of binary classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ISMB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "1 \u2022 RBF kernel, C and \u03c3 chosen as in [59]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1900499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4a422669ec9b6a60b05d2d2595314008a5fb419",
            "isKey": false,
            "numCitedBy": 1314,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "slug": "Comparing-support-vector-machines-with-Gaussian-to-Sch\u00f6lkopf-Sung",
            "title": {
                "fragments": [],
                "text": "Comparing support vector machines with Gaussian kernels to radial basis function classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system, and the SV approach is thus not only theoretically well-founded but also superior in a practical application."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 601110,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8985a9637540daa0b7b8295f8a5bbda3a3be1dea",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-connection-between-regularization-operators-and-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "The connection between regularization operators and support vector kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15917152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c5e562437ee94fb6e4d60ec559386dd0a433513",
            "isKey": false,
            "numCitedBy": 796,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A single linear programming formulation is proposed which generates a plane that of minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space. When the convex hulls of the two sets are also disjoint, the plane completely separates the two sets. When the convex hulls intersect, our linear program, unlike all previously proposed linear programs, is guaranteed to generate some error-minimizing plane, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases. The effectiveness of the proposed linear program has been demonstrated by successfully testing it on a number of databases. In addition, it has been used in conjunction with the multisurface method of piecewise-linear separation to train a feed-forward neural network with a single hidden layer."
            },
            "slug": "Robust-linear-programming-discrimination-of-two-Bennett-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Robust linear programming discrimination of two linearly inseparable sets"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A single linear programming formulation is proposed which generates a plane that of minimizes an average sum of misclassified points belonging to two disjoint points sets in n-dimensional real space, without the imposition of extraneous normalization constraints that inevitably fail to handle certain cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10843,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "\u2022 present case [54]: \u03bb(C) \u221d 1 margin2 , where C := {half-spaces in H, not containing the origin}"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2110475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc912ae25797e5f7c0d73300d3968ad8339b411",
            "isKey": false,
            "numCitedBy": 4688,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a simple subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data."
            },
            "slug": "Estimating-the-Support-of-a-High-Dimensional-Sch\u00f6lkopf-Platt",
            "title": {
                "fragments": [],
                "text": "Estimating the Support of a High-Dimensional Distribution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data by carrying out sequential optimization over pairs of input patterns and providing a theoretical analysis of the statistical performance of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414086"
                        ],
                        "name": "G. R\u00e4tsch",
                        "slug": "G.-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21313583"
                        ],
                        "name": "C. Lemmen",
                        "slug": "C.-Lemmen",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lemmen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemmen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49370597"
                        ],
                        "name": "Thomas Lengauer",
                        "slug": "Thomas-Lengauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lengauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Lengauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2154417,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "bdb4a67b8e83de538965181f27bc070e68ced84c",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nIn order to extract protein sequences from nucleotide sequences, it is an important step to recognize points at which regions start that code for proteins. These points are called translation initiation sites (TIS).\n\n\nRESULTS\nThe task of finding TIS can be modeled as a classification problem. We demonstrate the applicability of support vector machines for this task, and show how to incorporate prior biological knowledge by engineering an appropriate kernel function. With the described techniques the recognition performance can be improved by 26% over leading existing approaches. We provide evidence that existing related methods (e.g. ESTScan) could profit from advanced TIS recognition."
            },
            "slug": "Engineering-Support-Vector-Machine-Kerneis-That-Zien-R\u00e4tsch",
            "title": {
                "fragments": [],
                "text": "Engineering Support Vector Machine Kerneis That Recognize Translation Initialion Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "With the described techniques the recognition performance can be improved by 26% over leading existing approaches, and there is evidence that existing related methods could profit from advanced TIS recognition."
            },
            "venue": {
                "fragments": [],
                "text": "German Conference on Bioinformatics"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144551309"
                        ],
                        "name": "P. Pavlidis",
                        "slug": "P.-Pavlidis",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pavlidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646057"
                        ],
                        "name": "Jinsong Cai",
                        "slug": "Jinsong-Cai",
                        "structuredName": {
                            "firstName": "Jinsong",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinsong Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2361327"
                        ],
                        "name": "W. Grundy",
                        "slug": "W.-Grundy",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Grundy",
                            "middleNames": [
                                "Noble"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grundy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1044641,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "36708bbc473ec0b07752782de8a4d00f903eec3d",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data. We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons. We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task. Our results suggest the importance of exploiting prior information about the heterogeneity of the data. In particular, we propose an SVM kernel function that is explicitly heterogeneous. We also show how to use knowledge about heterogeneity to aid in feature selection."
            },
            "slug": "Gene-functional-classification-from-heterogeneous-Pavlidis-Weston",
            "title": {
                "fragments": [],
                "text": "Gene functional classification from heterogeneous data"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work considers the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons and proposes an SVM kernel function that is explicitly heterogeneous."
            },
            "venue": {
                "fragments": [],
                "text": "RECOMB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1888591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c35cc80fe8c6cdea742d4fa1af1f2e698d41aba7",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of exible conditional probability models and techniques for classi cation regression problems Many existing methods such as generalized linear models and support vector machines are subsumed under this class The exibility of this class of techniques comes from the use of kernel functions as in support vector machines and the generality from dual formulations of stan dard regression models"
            },
            "slug": "Probabilistic-kernel-regression-models-Jaakkola-Haussler",
            "title": {
                "fragments": [],
                "text": "Probabilistic kernel regression models"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A class of exible conditional probability models and techniques for classi cation regression problems that comes from the use of kernel functions as in support vector machines and the generality from dual formulations of stan dard regression models is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15109515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51c1519a57a65351a713a3d74f8d477105df0ec3",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions."
            },
            "slug": "Prior-Knowledge-in-Support-Vector-Kernels-Sch\u00f6lkopf-Simard",
            "title": {
                "fragments": [],
                "text": "Prior Knowledge in Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions by exploring methods for incorporating prior knowledge in Support Vector learning machines."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12053807,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b1377fbc559ac7ecbb4b956aefb6bdde60d714c5",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivation: The phylogenetic profile of a protein is a string that encodes the presence or absence of the protein in every fully sequenced genome. Because proteins that participate in a common structural complex or metabolic pathway are likely to evolve in a correlated fashion, the phylogenetic profiles of such proteins are often \u201csimilar\u201d or at least \u201crelated\u201d to each other. The question we address in this paper is the following: how to measure the \u201csimilarity\u201d between two profiles, in an evolutionarily relevant way, in order to develop efficient function prediction methods? Results: We show how the profiles can be mapped to a high-dimensional vector space which incorporates evolutionarily relevant information, and we provide an algorithm to compute efficiently the inner product in that space, which we call the tree kernel. The tree kernel can be used by any kernel-based analysis method for classification or data mining of phylogenetic profiles. As an application a Support Vector Machine (SVM) trained to predict the functional class of a gene from its phylogenetic profile is shown to perform better with the tree kernel than with a naive kernel that does not include any information about the phylogenetic relationships among species. Moreover a kernel principal component analysis (KPCA) of the phylogenetic profiles illustrates the sensitivity of the tree kernel to evolutionarily relevant variations. Availability: All data and softwares used are freely and publicly available upon request. Contact: Jean-Philippe.Vert@mines.org"
            },
            "slug": "A-tree-kernel-to-analyze-phylog-enetic-profi-les-Vert",
            "title": {
                "fragments": [],
                "text": "A tree kernel to analyze phylog enetic profi les"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown how the phylogenetic profiles can be mapped to a high-dimensional vector space which incorporates evolutionarily relevant information, and an algorithm to compute efficiently the inner product in that space, which is called the tree kernel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3293655"
                        ],
                        "name": "S. Romdhani",
                        "slug": "S.-Romdhani",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Romdhani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Romdhani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2639990"
                        ],
                        "name": "A. Psarrou",
                        "slug": "A.-Psarrou",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Psarrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Psarrou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Other applications: face modeling [48], image superresolution (see below)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1817782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1db4ffe4d09918ef3c03114fce2da43e975818d",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recovering the shape of any 3D object using multiple 2D views requires establishing correspondence between feature points at different views. However changes in viewpoint introduce self-occlusions, resulting nonlinear variations in the shape and inconsistent 2D features between views. Here we introduce a multi-view nonlinear shape model utilising 2D view-dependent constraint without explicit reference to 3D structures. For nonlinear model transformation, we adopt Kernel PCA based on Support Vector Machines."
            },
            "slug": "A-Multi-View-Nonlinear-Active-Shape-Model-Using-PCA-Romdhani-Gong",
            "title": {
                "fragments": [],
                "text": "A Multi-View Nonlinear Active Shape Model Using Kernel PCA"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a multi-view nonlinear shape model utilising 2D view-dependent constraint without explicit reference to 3D structures, and adopts Kernel PCA based on Support Vector Machines."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7882,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6082464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d27c7569fdbcbb57ff511f5293e32b547acca7b3",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995). SVM is a technique that can be derived from the structural risk minimization principle (Vapnik, 1982) and can be used to estimate the parameters of several different approximation schemes, including radial basis functions, algebraic and trigonometric polynomials, B-splines, and some forms of multilayer perceptrons. Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary). We show that if the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem. In the appendix, we present a derivation of the SVM technique in the framework of regularization theory, rather than statistical learning theory, establishing a connection between SVM, sparse approximation, and regularization theory."
            },
            "slug": "An-Equivalence-Between-Sparse-Approximation-and-Girosi",
            "title": {
                "fragments": [],
                "text": "An Equivalence Between Sparse Approximation and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "If the data are noiseless, the modified version of basis pursuit denoising proposed in this article is equivalent to SVM in the following sense: if applied to the same data set, the two techniques give the same solution, which is obtained by solving the same quadratic programming problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734990"
                        ],
                        "name": "S. Harmeling",
                        "slug": "S.-Harmeling",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Harmeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522238"
                        ],
                        "name": "A. Ziehe",
                        "slug": "A.-Ziehe",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ziehe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ziehe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716788"
                        ],
                        "name": "M. Kawanabe",
                        "slug": "M.-Kawanabe",
                        "structuredName": {
                            "firstName": "Motoaki",
                            "lastName": "Kawanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": ", 2004), ICA [28], canonical correlations (Bach & Jordan, 2002; Kuss, 2002) \u2022 optimization and implementation: QP, SDP (Lanckriet et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7875856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "080d3bccb37eb85334c17b2f3871131ca6862ccd",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In kernel based learning the data is mapped to a kernel feature space of a dimension that corresponds to the number of training data points. In practice, however, the data forms a smaller submanifold in feature space, a fact that has been used e.g. by reduced set techniques for SVMs. We propose a new mathematical construction that permits to adapt to the intrinsic dimension and to find an orthonormal basis of this submanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experiments demonstrate the good performance and high computational efficiency of our kTDSEP algorithm for the problem of nonlinear BSS."
            },
            "slug": "Kernel-Feature-Spaces-and-Nonlinear-Blind-Souce-Harmeling-Ziehe",
            "title": {
                "fragments": [],
                "text": "Kernel Feature Spaces and Nonlinear Blind Souce Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new mathematical construction is proposed that permits to adapt to the intrinsic dimension of the data and to find an orthonormal basis of this submanifold and allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2427083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49",
            "isKey": false,
            "numCitedBy": 8601,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning."
            },
            "slug": "Text-Categorization-with-Support-Vector-Machines:-Joachims",
            "title": {
                "fragments": [],
                "text": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper explores the use of Support Vector Machines for learning text classifiers from examples and analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 41
                            }
                        ],
                        "text": ", 84] \u2013 complex kernels from simple ones [30, 2], global kernels from local ones [38] \u2013 functional calculus for kernel matrices [60] \u2013 model selection, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 328
                            }
                        ],
                        "text": "are pd kernels, then so are \u2022 \u03b1k1, provided \u03b1 \u2265 0 \u2022 k1 + k2 \u2022 k1 \u00b7 k2 \u2022 k(x, x\u2032) := limn\u2192\u221e kn(x, x\u2032), provided it exists \u2022 k(A,B) := \u2211x\u2208A,x\u2032\u2208B k1(x, x\u2032), where A,B are finite subsets of X (using the feature map \u03a6\u0303(A) := \u2211 x\u2208A\u03a6(x)) Further operations to construct kernels from kernels: tensor products, direct sums, convolutions [30]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17702358,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac",
            "isKey": false,
            "numCitedBy": 1371,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs. The method can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set. The family of kernels generated generalizes the family of radial basis kernels. It can also be used to deene kernels in the form of joint Gibbs probability distributions. Kernels can be built from hidden Markov random elds, generalized regular expressions, pair-HMMs, or ANOVA de-compositions. Uses of the method lead to open problems involving the theory of innnitely divisible positive deenite functions. Fundamentals of this theory and the theory of reproducing kernel Hilbert spaces are reviewed and applied in establishing the validity of the method."
            },
            "slug": "Convolution-kernels-on-discrete-structures-Haussler",
            "title": {
                "fragments": [],
                "text": "Convolution kernels on discrete structures"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs is introduced, which can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61480753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb62d05f8eeaa7e1921c6c25c544935a2b6b131",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Pattern-Recognition-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102944324"
                        ],
                        "name": "Vapnik",
                        "slug": "Vapnik",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Vapnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17719593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3473ce22ada00842b355883a5ddde5a8c7c76ba6",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional classification approaches generalize poorly on image classification tasks, because of the high dimensionality of the feature space. This paper shows that Support Vector Machines (SVM) can generalize well on difficult image classification problems where the only features are high dimensional histograms. Heavy-tailed RBF kernels of the form K(x,y) = e\u2212\u03c1 P i |x i \u2212y i | with a \u2264 1 and b \u2264 2 are evaluated on the classification of images extracted from the Corel Stock Photo Collection and shown to far outperform traditional polynomial or Gaussian RBF kernels. Moreover, we observed that a simple remapping of the input xi \u2192 x a i improves the performance of linear SVMs to such an extend that it makes them, for this problem, a valid alternative to RBF kernels. keywords: Support Vector Machines, Radial Basis Functions, Image Histogram, Image Classification, Corel."
            },
            "slug": "SVMs-for-Histogram-Based-Image-Classification-Chapelle-Haffner",
            "title": {
                "fragments": [],
                "text": "SVMs for Histogram Based Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper shows that Support Vector Machines (SVM) can generalize well on difficult image classification problems where the only features are high dimensional histograms and observes that a simple remapping of the input xi \u2192 x a i improves the performance of linear SVMs to such an extend that it makes them a valid alternative to RBF kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "\u2022 in fact, one can use chunks which do not even contain all SVs [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15140283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c4749d9d3f1724aa01778d69a3774c732ca44c",
            "isKey": false,
            "numCitedBy": 844,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector Machine (SVM) is a new and very promising classification technique developed by Vapnik and his group at AT\\&T Bell Labs. This new learning algorithm can be seen as an alternative training technique for Polynomial, Radial Basis Function and Multi-Layer Perceptron classifiers. An interesting property of this approach is that it is an approximate implementation of the Structural Risk Minimization (SRM) induction principle. The derivation of Support Vector Machines, its relationship with SRM, and its geometrical insight, are discussed in this paper. Training a SVM is equivalent to solve a quadratic programming problem with linear and box constraints in a number of variables equal to the number of data points. When the number of data points exceeds few thousands the problem is very challenging, because the quadratic form is completely dense, so the memory needed to store the problem grows with the square of the number of data points. Therefore, training problems arising in some real applications with large data sets are impossible to load into memory, and cannot be solved using standard non-linear constrained optimization algorithms. We present a decomposition algorithm that can be used to train SVM''s over large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm. We present previous approaches, as well as results and important details of our implementation of the algorithm using a second-order variant of the Reduced Gradient Method as the solver of the sub-problems. As an application of SVM''s, we present preliminary results we obtained applying SVM to the problem of detecting frontal human faces in real images."
            },
            "slug": "Support-Vector-Machines:-Training-and-Applications-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Support Vector Machines: Training and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Preliminary results are presented obtained applying SVM to the problem of detecting frontal human faces in real images, and the main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 235
                            }
                        ],
                        "text": "Result: \u2022 the decision function f (x) = sgn(\u3008w,\u03a6(x)\u3009\u2212\u03c1) will be positive for \u201cmost\u201d examples xi contained in the training set \u2022 \u2016w\u2016 will be small, hence the separation from the origin large Related approaches: enclose data in a sphere [52, 65] B."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6636078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec8029e5855b6efbac161488a2e68f83298091c",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (\u2248 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. \n \nIn addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited."
            },
            "slug": "Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Extracting Support Data for a Given Task"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is observed that three different types of handwritten digit classifiers construct their decision surface from strongly overlapping small subsets of the data base, which opens up the possibility of compressing data bases significantly by disposing of theData which is not important for the solution of a given task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463572"
                        ],
                        "name": "M. O. Stitson",
                        "slug": "M.-O.-Stitson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stitson",
                            "middleNames": [
                                "Oliver"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O. Stitson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793317"
                        ],
                        "name": "A. Gammerman",
                        "slug": "A.-Gammerman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gammerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gammerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675281"
                        ],
                        "name": "V. Vovk",
                        "slug": "V.-Vovk",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vovk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vovk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "6 [64] \u2022 100 runs, with 25 randomly selected test points."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118468304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "221663c3ec94babfaa0754a00d92ff69e2a4424a",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) [Vapng] are a way of imposing a structure on multi-dimensional kernels which are generated as the tensor product of one-dimensional kernels. This gives more accurate control over the capacity of the learning machine (VCdimension). SVAD uses ideas from ANOVA decomposition methods and extends them to generate kernels which directly implement these ideas. SVAD is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel. The Boston housing data set from UCI has been tested on Bagging [Bre94] and Support Vector methods before [DBK97] and these results are compared to the SVAD method."
            },
            "slug": "Support-vector-regression-with-ANOVA-decomposition-Stitson-Gammerman",
            "title": {
                "fragments": [],
                "text": "Support vector regression with ANOVA decomposition kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Support Vector Machines using ANOVA Decomposition Kernels (SVAD) is used with spline kernels and results show that SVAD performs better than the respective non ANOVA decomposition kernel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17875902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dd9743183f07b7653cc0335fcc1042aa71032c6",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "There is much current interest in kernel methods for classi cation re gression PCA and other linear methods of data analysis Kernel methods may be particularly valuable for problems in which the input data is not readily described by explicit feature vectors One such problem is where input data consists of symbol sequences of di erent lengths and the re lationships between sequences are best captured by dynamic alignment scores This paper shows that the scores produced by certain dynamic align ment algorithms for sequences are in fact valid kernel functions This is proved by expressing the alignment scores explicitly as dot products Alignment kernels are potentially applicable to biological sequence data speech data and time series data The kernel construction may be extended from pair HMMs to pair probabilistic context free grammars Introduction Linear Methods using Kernel Functions Introduction Linear Methods using Kernel Functions In many types of machine learning the learner is given a training set of cases or examples a al A A denotes the set of all possible cases cases may be vectors pieces of text biological sequences sentences etc For supervised learning the cases are accompanied by a set of corresponding labels or values y yl The cases are mapped to feature vectors x xl X where the X is a real vector space termed the feature space The mapping from A to X is denoted by so that xi ai Sometimes the cases are given as feature vectors to start with in which case may be the identity mapping otherwise denotes the method of assigning numeric feature values to a case Once a feature vector xi has been de ned for each case ai it becomes pos sible to apply a wide range of linear methods such as support vector machines linear regression principal components analysis PCA and k means cluster analysis As shown in Vap for SV machines in for example Wah for linear re gression and in SSM for PCA and k means cluster analysis the calculations for all of these linear methods may be carried out using a dual rather than a primal formulation of the problem For example in linear least squares regression the primal formulation is to nd a coe cient vector that minimises kX yk whereX is the design matrix an l by d matrix in which the ith row is xi and each xi has d elements If l is larger than d the usual method of nding is to solve the normal equations XX Xy This requires the solution of a set of linear equations with coe cients given by the d d matrix XX The dual formulation is to nd a coe cient vector that minimises kXX yk so that one coe cient i is found for each case vector xi This requires the solution of a set of linear equations with coe cients given by the l l matrix XX Both methods lead to the same predicted value y for a new case x If there are more cases than features that is if l d the primal method is more economical because the d d matrix XX is smaller than the l l matrix XX For example if there are cases each described by a vector of measurements then the primal method requires solving a by system of linear equations while the dual method requires solving a by system which will have rank at most For such a problem the dual method has no advantage The potential advantage of the dual method for regression is that it can be applied to very large feature vectors The coe cient matrix XX contains the dot products of pairs of feature vectors the ijth element of XX is xi xj In the dual calculation it is only dot products of feature vectors that are used feature vectors never appear on their own As the feature vectors xi ai appear only in dot products it is often possible to avoid computing the feature vectors and to compute dot products directly in some economical fashion from the case descriptions ai instead A kernel is a function k that computes a dot product of feature vectors from the corresponding cases Applying Linear Methods to Structured Objects De nition A kernel is a function k such that for all a b A"
            },
            "slug": "Dynamic-Alignment-Kernels-Watkins",
            "title": {
                "fragments": [],
                "text": "Dynamic Alignment Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the scores produced by certain dynamic align ment algorithms for sequences are in fact valid kernel functions, proved by expressing the alignment scores explicitly as dot products."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 513,
                                "start": 497
                            }
                        ],
                        "text": "The Kernel Trick \u2014 Summary \u2022 any algorithm that only depends on dot products can benefit from the kernel trick \u2022 this way, we can apply linear methods to vectorial as well as non-vectorial data \u2022 think of the kernel as a nonlinear similarity measure \u2022 examples of common kernels: Polynomial k(x, x\u2032) = ( \u3008 x, x\u2032 \u3009 + c)d Sigmoid k(x, x\u2032) = tanh(\u03ba \u3008 x, x\u2032 \u3009 + \u0398) Gaussian k(x, x\u2032) = exp(\u2212\u2016x\u2212 x\u2032\u20162/(2\u03c32)) \u2022 Kernels are studied also in the Gaussian Process prediction community (covariance functions) [79, 76, 81, 41]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": ", \u03a9(\u2016f\u2016) = \u03bb\u2016f\u20162 \u2014 Gaussian process prior [81] with covariance function k \u2022 minimizer of (3) = MAP estimate Kernel PCA (see below) can be shown to correspond to the case of c((xi, yi, f (xi))i=1,."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6884486,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "28667c276ba78ab1d855064d5456d50d9932b775",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems."
            },
            "slug": "Prediction-with-Gaussian-Processes:-From-Linear-to-Williams",
            "title": {
                "fragments": [],
                "text": "Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The main aim of this paper is to provide a tutorial on regression with Gaussian processes, starting from Bayesian linear regression, and showing how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on prior over parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98498212"
                        ],
                        "name": "Sheng L. Chen",
                        "slug": "Sheng-L.-Chen",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Chen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng L. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300338"
                        ],
                        "name": "C. Harris",
                        "slug": "C.-Harris",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harris",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Harris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7723436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d19ed5baef9cc25eb381c9525dc868fb964ef4",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The conventional decision feedback equalizer (DFE) separates the different signal classes using a single hyperplane. It is well known that the popular minimum mean square error (MMSE) design is generally not the optimal minimum bit error rate (MBER) solution. We propose a method of designing the separating hyperplane for the conventional DFE based on support vector machines (SVMs). The SVM design achieves asymptotically the MBER solution and can be computed efficiently."
            },
            "slug": "Design-of-the-optimal-separating-hyperplane-for-the-Chen-Harris",
            "title": {
                "fragments": [],
                "text": "Design of the optimal separating hyperplane for the decision feedback equalizer using support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a method of designing the separating hyperplane for the conventional DFE based on support vector machines (SVMs) that achieves asymptotically the MBER solution and can be computed efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727076"
                        ],
                        "name": "H. Lodhi",
                        "slug": "H.-Lodhi",
                        "structuredName": {
                            "firstName": "Huma",
                            "lastName": "Lodhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lodhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 669209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f330f1f472f860212b980bb9be81eff884f7f0e1",
            "isKey": false,
            "numCitedBy": 1643,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results."
            },
            "slug": "Text-Classification-using-String-Kernels-Lodhi-Saunders",
            "title": {
                "fragments": [],
                "text": "Text Classification using String Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel kernel is introduced for comparing two text documents consisting of an inner product in the feature space consisting of all subsequences of length k, which can be efficiently evaluated by a dynamic programming technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47328136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e",
            "isKey": false,
            "numCitedBy": 15183,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 70866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2d13bc44e15fd93480e16305d37c025bc0818c2",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples \u2013 in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case."
            },
            "slug": "Regularization-Networks-and-Support-Vector-Machines-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularization Networks and Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Both formulations of regularization and Support Vector Machines are reviewed in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput. Math."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": ", 84] \u2013 complex kernels from simple ones [30, 2], global kernels from local ones [38] \u2013 functional calculus for kernel matrices [60] \u2013 model selection, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8606662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9e51a86d106e73e60650f2c4784c5271e8c9735",
            "isKey": false,
            "numCitedBy": 509,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels, based on the heat equation, called diffusion kernels, and show that these can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "slug": "Diffusion-kernels-on-graphs-and-other-discrete-Kondor",
            "title": {
                "fragments": [],
                "text": "Diffusion kernels on graphs and other discrete structures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper focuses on generating kernels on graphs, for which a special class of exponential kernels, based on the heat equation, are proposed, called diffusion kernels, and shows that these can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2002"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": ", via alignment [17] \u2013 kernels for dimensionality reduction [27]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On optimizing kernel alignment"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 2001-087, NeuroCOLT"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledgebased analysis of microarray gene expression data using support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences, 97(1):262\u2013267"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "Kernel PCA Denoising Idea: in feature space, discard higher-order principal components, and compute approximate pre-images [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input space vs"
            },
            "venue": {
                "fragments": [],
                "text": "feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000\u20131017"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support Vector Learning. R. Oldenbourg Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "Support Vector Learning. R. Oldenbourg Verlag"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Has'minskii. Statistical Estimation \u2014 Asymptotic Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Has'minskii. Statistical Estimation \u2014 Asymptotic Theory"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method. Automation and Remote Control"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern recognition using generalized portrait method. Automation and Remote Control"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trends and controversies \u2014 support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intelligent Systems, 13:18\u201328"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Theorem [70]: Denote #SV(m) the number of SVs obtained by training on m examples randomly drawn from P(x, y), and E the expectation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of Pattern Recognition [in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Nauka, Moscow, 1974. "
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principal Component Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive and Learning Systems for Signal Processing, Communications, and Control. John Wiley & Sons, New York"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reducing run-time complexity in SVMs"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 14th Int\u2019l Conf. on Pattern Recognition, Brisbane, Australia"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "\u2022 kernel design \u2013 transformation invariances [13] \u2013 kernels for discrete objects [30, 78, 40, 18, 74] \u2013 kernels based on generative models [34, 61, 68] \u2013 local kernels [e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": ": the directions of main variance of \u00b1 \u2202 \u2202t|t=0Ltx are scaled back \u2022 in practice, use C\u03bb := (1 \u2212 \u03bb)C + \u03bbI \u2022 for the nonlinear case, use the kernel PCA map [13]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "USPS Digit Recognition Application [13]"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incorporating invariances in nonlinear SVMs"
            },
            "venue": {
                "fragments": [],
                "text": "T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 235
                            }
                        ],
                        "text": "Result: \u2022 the decision function f (x) = sgn(\u3008w,\u03a6(x)\u3009\u2212\u03c1) will be positive for \u201cmost\u201d examples xi contained in the training set \u2022 \u2016w\u2016 will be small, hence the separation from the origin large Related approaches: enclose data in a sphere [52, 65] B."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Data domain description by support vectors"
            },
            "venue": {
                "fragments": [],
                "text": "M. Verleysen, editor, Proceedings ESANN, pages 251\u2013256, Brussels"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "\u2022 need to evaluate on average 2 \u2013 3 kernels per image location [49]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast face detection"
            },
            "venue": {
                "fragments": [],
                "text": "using a sequential reduced support vector evaluation. TR 73, Microsoft Research, Redmond, WA"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast face detection, using a sequential reduced support vector evaluation Published as: Computationally efficient face detection"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computer Vision"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 83,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-with-kernels-Sch\u00f6lkopf/96cff156cdc8a479aa994eae875a0860d663d317?sort=total-citations"
}