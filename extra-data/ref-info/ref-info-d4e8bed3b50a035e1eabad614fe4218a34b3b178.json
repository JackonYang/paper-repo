{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 147
                            }
                        ],
                        "text": "While smoothing is one technique for addressing sparse data issues, there are numerous other techniques that can be applied, such as word classing (Brown et al., 1992b) or decision-tree models (Bahl et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "They present a class of smoothing models that involve linear interpolation, e.g., Brown et al. (1992) take\nPinterp(wi|w i\u22121 i\u2212n+1) =\n\u03bbwi\u22121 i\u2212n+1\nPML(wi|w i\u22121 i\u2212n+1) +\n(1 \u2212 \u03bbwi\u22121 i\u2212n+1\n) Pinterp(wi|w i\u22121 i\u2212n+2) (3)\nThat is, the maximum likelihood estimate is interpolated with the smoothed\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2202717"
                        ],
                        "name": "Linda C. Bauman Peto",
                        "slug": "Linda-C.-Bauman-Peto",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Peto",
                            "middleNames": [
                                "C.",
                                "Bauman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda C. Bauman Peto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11426560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fa57bd91f731522c861404d29e4604ba6ac6d3",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as 'smoothing'. A number of interesting differences from smoothing emerge. The insights gained from a probabilistic view of this problem point towards new directions for language modelling. The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech, and DNA and protein sequences in molecular biology. The new algorithm is compared with smoothing on a two million word corpus. The methods prove to be about equally accurate, with the hierarchical model using fewer computational resources."
            },
            "slug": "A-hierarchical-Dirichlet-language-model-Mackay-Peto",
            "title": {
                "fragments": [],
                "text": "A hierarchical Dirichlet language model"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as 'smoothing' is discussed, and the methods prove to be about equally accurate, with the hierarchical model using fewer computational resources."
            },
            "venue": {
                "fragments": [],
                "text": "Nat. Lang. Eng."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122661322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "226a6dff9ccc1c2db9f09db644b13eb9d04322e7",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The language model probabilities are estimated by an empirical Bayes approach in which a prior distribution for the unknown probabilities is itself estimated through a novel choice of data. The predictive power of the model thus fitted is compared by means of its experimental perplexity [1] to the model as fitted by the Jelinek-Mercer deleted estimator and as fitted by the Turing-Good formulas for probabilities of unseen or rarely seen events."
            },
            "slug": "Estimation-of-probabilities-in-the-language-model-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities in the language model of the IBM speech recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The predictive power of the model thus fitted is compared by means of its experimental perplexity to the model as fitted by the Jelinek-Mercer deleted estimator and by the Turing-Good formulas for probabilities of unseen or rarely seen events."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069902"
                        ],
                        "name": "P. Roossin",
                        "slug": "P.-Roossin",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Roossin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roossin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 838,
                                "start": 142
                            }
                        ],
                        "text": "Most previous studies that have compared smoothing algorithms ( N\u00e1das, 1984 ; Katz, 1987; Church & Gale, 1991 ; Kneser & Ney, 1995 ; MacKay & Peto, 1995 ) have only done so with a small number of methods (typically two) on one or two corpora and using a single training set size. Perhaps the most complete previous comparison is that of Ney, Martin and Wessel (1997 ), which compared a variety of smoothing algorithms on three different training set sizes. However, this work did not consider all popular algorithms, and only considered data from a single source. Thus, it is currently difficult for a researcher to intelligently choose among smoothing schemes. In this work, we carry out an extensive empirical comparison of the most widely-used smoothing techniques, including those described by Jelinek and Mercer (1980 ); Katz (1987); Bell et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 858,
                                "start": 142
                            }
                        ],
                        "text": "Most previous studies that have compared smoothing algorithms ( N\u00e1das, 1984 ; Katz, 1987; Church & Gale, 1991 ; Kneser & Ney, 1995 ; MacKay & Peto, 1995 ) have only done so with a small number of methods (typically two) on one or two corpora and using a single training set size. Perhaps the most complete previous comparison is that of Ney, Martin and Wessel (1997 ), which compared a variety of smoothing algorithms on three different training set sizes. However, this work did not consider all popular algorithms, and only considered data from a single source. Thus, it is currently difficult for a researcher to intelligently choose among smoothing schemes. In this work, we carry out an extensive empirical comparison of the most widely-used smoothing techniques, including those described by Jelinek and Mercer (1980 ); Katz (1987); Bell et al. (1990); Ney et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 48
                            }
                        ],
                        "text": "We used the 50 000 word vocabulary developed by Placewayet al. (1997). For all corpora, any out-of-vocabulary words were mapped to a distinguished token and otherwise treated in the same fashion as all other words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1650,
                                "start": 142
                            }
                        ],
                        "text": "Most previous studies that have compared smoothing algorithms ( N\u00e1das, 1984 ; Katz, 1987; Church & Gale, 1991 ; Kneser & Ney, 1995 ; MacKay & Peto, 1995 ) have only done so with a small number of methods (typically two) on one or two corpora and using a single training set size. Perhaps the most complete previous comparison is that of Ney, Martin and Wessel (1997 ), which compared a variety of smoothing algorithms on three different training set sizes. However, this work did not consider all popular algorithms, and only considered data from a single source. Thus, it is currently difficult for a researcher to intelligently choose among smoothing schemes. In this work, we carry out an extensive empirical comparison of the most widely-used smoothing techniques, including those described by Jelinek and Mercer (1980 ); Katz (1987); Bell et al. (1990); Ney et al. (1994), andKneser and Ney (1995 ). We carry out experiments over many training set sizes on varied corpora using n- rams of various order (different n), and show how these factors affect the relative performance of smoothing techniques. For the methods with parameters that can be tuned to improve performance, we perform an automated search for optimal values and show that sub-optimal parameter selection can significantly decrease performance. To our knowledge, this is the first smoothing work that systematically investigates any of these issues. Our results make it apparent that previous evaluations of smoothing techniques have not been thorough enough to provide an adequate characterization of the relative performance of different algorithms. For instance, Katz (1987) compares his algorithm with an unspecified version of Jelinek\u2013Mercer deleted estimation and with N\u00e1dassmoothing ( 1984) using a single training corpus and a single test set of 100 sentences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 877,
                                "start": 142
                            }
                        ],
                        "text": "Most previous studies that have compared smoothing algorithms ( N\u00e1das, 1984 ; Katz, 1987; Church & Gale, 1991 ; Kneser & Ney, 1995 ; MacKay & Peto, 1995 ) have only done so with a small number of methods (typically two) on one or two corpora and using a single training set size. Perhaps the most complete previous comparison is that of Ney, Martin and Wessel (1997 ), which compared a variety of smoothing algorithms on three different training set sizes. However, this work did not consider all popular algorithms, and only considered data from a single source. Thus, it is currently difficult for a researcher to intelligently choose among smoothing schemes. In this work, we carry out an extensive empirical comparison of the most widely-used smoothing techniques, including those described by Jelinek and Mercer (1980 ); Katz (1987); Bell et al. (1990); Ney et al. (1994), andKneser and Ney (1995 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14386564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1066659ec1afee9dce586f6f49b7d44527827e1",
            "isKey": true,
            "numCitedBy": 1940,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results."
            },
            "slug": "A-Statistical-Approach-to-Machine-Translation-Brown-Cocke",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The application of the statistical approach to translation from French to English and preliminary results are described and the results are given."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 89
                            }
                        ],
                        "text": "Recently, several more sophisticated n-gram model pruning techniques have been developed (Kneser, 1996 ; Seymore & Rosenfeld, 1996 ; Stolcke, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Ney et al. (1994) suggest settingD through deleted estimation on the training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16084362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae7f2177b485737883235b9cc4233e7fd98e1365",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate statistical language models with a variable context length. For such models the number of relevant words in a context is not fixed as in conventional M-gram models but depends on the context itself. We develop a measure for the quality of variable-length models and present a pruning algorithm for the creation of such models, based on this measure. Further we address the question how the use of a special backing-off distribution can improve the language models. Experiments were performed on two data bases, the ARPANAB corpus and the German Verbmobil corpus, respectively. The results show that variable-length models outperform conventional models of the same size. Furthermore it can be seen that if a moderate loss in performance is acceptable, the size of a language model can be reduced drastically by using the presented pruning algorithm."
            },
            "slug": "Statistical-language-modeling-using-a-variable-Kneser",
            "title": {
                "fragments": [],
                "text": "Statistical language modeling using a variable context length"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A measure for the quality of variable-length models is developed and a pruning algorithm for the creation of such models is presented, based on this measure, to address the question how the use of a special backing-off distribution can improve the language models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62540359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa9b9743a55aa43b59d82f756a2bd370cf59e3d9",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors study various problems related to smoothing bigram probabilities for natural language modeling: the type of interpolation, i.e. linear vs. nonlinear, the optimal estimation of interpolation parameters, and the use of word equivalence classes (parts of speech). A nonlinear interpolation method that results in significant improvements over linear interpolation in the experimental tests is proposed. It is shown that the leaving-one-out method in combination with the maximum likelihood criterion can be efficiently used for the optimal estimation of interpolation parameters. In addition, an automatic clustering procedure is developed for finding word equivalence classes using a maximum likelihood criterion. Experimental results are presented for two text databases: a German database with 100000 words and an English database with 1.1 million words.<<ETX>>"
            },
            "slug": "On-smoothing-techniques-for-bigram-based-natural-Ney-Essen",
            "title": {
                "fragments": [],
                "text": "On smoothing techniques for bigram-based natural language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the leaving-one-out method in combination with the maximum likelihood criterion can be efficiently used for the optimal estimation of interpolation parameters."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9685476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate."
            },
            "slug": "Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney",
            "title": {
                "fragments": [],
                "text": "Improved backing-off for M-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to use distributions which are especially optimized for the task of back-off, which are quite different from the probability distributions that are usually used for backing-off."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206560877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3e53b9c0e3a7a60e7a5295e9b08af74d6fb3dbf",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database."
            },
            "slug": "On-structuring-probabilistic-dependences-in-Ney-Essen",
            "title": {
                "fragments": [],
                "text": "On structuring probabilistic dependences in stochastic language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The problem of stochastic language modelling is studied from the viewpoint of introducing suitable structures into the conditional probability distributions, and nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations are considered."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222167"
                        ],
                        "name": "E. Ristad",
                        "slug": "E.-Ristad",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ristad",
                            "middleNames": [
                                "Sven"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ristad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17051446,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d4af62eb3f8fa99850df061ebaf40f2b110c5ed7",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new solution to multinomial estimation and demonstrate that our solution outperforms standard solutions both in theory and in practice. The novelty of our approach lies in our use of combinatorial priors on strings."
            },
            "slug": "A-natural-law-of-succession-Ristad",
            "title": {
                "fragments": [],
                "text": "A natural law of succession"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new solution to multinomial estimation using combinatorial priors on strings is presented and it is demonstrated that this solution outperforms standard solutions both in theory and in practice."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE International Symposium on Information Theory (Cat. No.98CH36252)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39705397,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3888028ecc976f5a5786152ebc418559896da53c",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A. M. Turing, in a 1941 personal communication to I. J. Good, suggested a formula for estimating probabilities of words in text and, more generally, of species in a mixed population of various species. It is remarkable that Turing's formula can be obtained by significantly different statistical methods; we compare three ways to obtain it."
            },
            "slug": "On-Turing's-formula-for-word-probabilities-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "On Turing's formula for word probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is remarkable that Turing's formula can be obtained by significantly different statistical methods; it is compared three ways to obtain it."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066017394"
                        ],
                        "name": "P. Clarkson",
                        "slug": "P.-Clarkson",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Clarkson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Clarkson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 63
                            }
                        ],
                        "text": "It should be noted that there exist language modeling toolkits (Rosenfeld, 1995; Clarkson and Rosenfeld, 1997) which can be used to build smoothed n-gram models using a variety of smoothing algorithms, including Katz smoothing, Jelinek-Mercer smoothing, absolute discounting, and Witten-Bell smoothing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13988648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87",
            "isKey": false,
            "numCitedBy": 707,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The CMU Statistical Language Modeling toolkit was re leased in in order to facilitate the construction and testing of bigram and trigram language models It is currently in use in over academic government and industrial laboratories in over countries This paper presents a new version of the toolkit We outline the con ventional language modeling technology as implemented in the toolkit and describe the extra e ciency and func tionality that the new toolkit provides as compared to previous software for this task Finally we give an exam ple of the use of the toolkit in constructing and testing a simple language model"
            },
            "slug": "Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Statistical language modeling using the CMU-cambridge toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The CMU Statistical Language Modeling toolkit was re leased in in order to facilitate the construction and testing of bigram and trigram language models and the technology as implemented in the toolkit is outlined."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10618934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d609fd5e328a06a67f883c08e609bc57583100f0",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. \nIn this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure."
            },
            "slug": "Building-Probabilistic-Models-for-Natural-Language-Chen",
            "title": {
                "fragments": [],
                "text": "Building Probabilistic Models for Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis investigates three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment, and relates each of these frameworks to the Bayesian paradigm, and shows why each framework was appropriate for the given problem."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544946"
                        ],
                        "name": "K. Seymore",
                        "slug": "K.-Seymore",
                        "structuredName": {
                            "firstName": "Kristie",
                            "lastName": "Seymore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Seymore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716325"
                        ],
                        "name": "M. Esk\u00e9nazi",
                        "slug": "M.-Esk\u00e9nazi",
                        "structuredName": {
                            "firstName": "Maxine",
                            "lastName": "Esk\u00e9nazi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Esk\u00e9nazi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 178
                            }
                        ],
                        "text": "Due to the increasing speed and memory of computers, there has been some use of higher-order n-gram models such as 4-gram and 5-gram models in speech recognition in recent years (Seymore et al., 1997; Weng, Stolcke, and Sankar, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1755585,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "24dadc3a3b3872ffbebb7cc9f57bc52a2ffda58a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several language and pronunciation modeling techniques that were applied to the 1996 Hub 4 Broadcast News transcription task. These include topic adaptation, the use of remote corpora, vocabulary size optimization, n-gram cutoff optimization, modeling of spontaneous speech, handling of unknown linguistic boundaries, higher order n-grams, weight optimization in rescoring, and lexical modeling of phrases and acronyms."
            },
            "slug": "Language-and-Pronunciation-Modeling-in-the-CMU-1996-Seymore-Chen",
            "title": {
                "fragments": [],
                "text": "Language and Pronunciation Modeling in the CMU 1996 Hub 4 Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Several language and pronunciation modeling techniques that were applied to the 1996 Hub 4 Broadcast News transcription task are described, including topic adaptation, the use of remote corpora, vocabulary size optimization, n-gram cutoff optimization, modeling of spontaneous speech, and lexical modeling of phrases and acronyms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 133
                            }
                        ],
                        "text": "We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 133
                            }
                        ],
                        "text": "We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We describe experiments that systematically vary a wide range of variables, including training data size, corpus, count cuto s, and n-gram order, and show that most of these variables signi cantly a ect the relative performance of algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 133
                            }
                        ],
                        "text": "We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18511291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9463e3eca9f3b053fca7ca64abb157aaeac35f4f",
            "isKey": true,
            "numCitedBy": 398,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text. We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds."
            },
            "slug": "An-Estimate-of-an-Upper-Bound-for-the-Entropy-of-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "An Estimate of an Upper Bound for the Entropy of English"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An estimate of an upper bound of 1.75 bits for the entropy of characters in printed English is presented by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3131427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e09439f009dd0d4618949e4e01964df2e1f1d2d",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "One could estimate the probability of a word (or n-gram) in English by collecting a large corpus of English text, counting the number of times the word appears in the corpus, and normalizing by the size of the corpus. Unfortunately this method (known as MLE) produces a poor estimate when the count is small and an unacceptable estimate when the count is zero. This is particularly problematic in many applications because counts are often small and zero is often the most frequent count."
            },
            "slug": "1-What-\u2019-s-Wrong-with-Adding-One-Gale-Church",
            "title": {
                "fragments": [],
                "text": "- 1-What \u2019 s Wrong with Adding One ?"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "One could estimate the probability of a word (or n-gram) in English by collecting a large corpus of English text, counting the number of times the word appears in the corpus, and normalizing by the size of the corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17052790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance."
            },
            "slug": "A-Gaussian-Prior-for-Smoothing-Maximum-Entropy-Chen-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A Gaussian Prior for Smoothing Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Over a large number of data sets, it is found that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121808836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef9190e7669ea5523c3ef61180b35385b0ea345f",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparison-of-the-enhanced-Good-Turing-and-for-of-Church-Gale",
            "title": {
                "fragments": [],
                "text": "A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Gale and Church (1990; 1994) have argued that this method generally performs poorly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118316152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0132fb9fb941183cc451c3df02f5e4bffb6aa8c",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "It is difficult to estimate the probability of a word\u2019s context because of sparse data problems. If appropriate care is taken, we find that it is possible to make useful estimates of contextual probabilities that improve performance in a spelling correction application. In contrast, less careful estimates are found to be useless. Specifically, we will show that the Good-Turing method makes the use of contextual information practical for a spelling corrector, while attempts to use the maximum likelihood estimator (MLE) or expected likelihood estimator (ELE) fail. Spelling correction was selected as an application domain because it is analogous to many important recognition applications based on a noisy channel model (such as speech recognition), though somewhat simpler and therefore possibly more amenable to detailed statistical analysis."
            },
            "slug": "Estimation-Procedures-for-Language-Context:-Poor-Gale-Church",
            "title": {
                "fragments": [],
                "text": "Estimation Procedures for Language Context: Poor Estimates are Worse than None"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is found that it is possible to make useful estimates of contextual probabilities that improve performance in a spelling correction application, and it is shown that the Good-Turing method makes the use of contextual information practical for a spelling corrector."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "Ronald Rosenfeld",
                        "slug": "Ronald-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61832335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da0f0015ea687fd7285796f8f806a3c49033163f",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The Carnegie Mellon Statistical Language Modeling (CMU SLM) Toolkit is a set of Unix software tools designed to facilitate language modeling work in the research community. The package, including source code, is freely available for research purposes. As of December 1994, the toolkit is in active use by 23 research groups in 8 countries. It was recently used to process the 2.5 GB NAB corpus for the ARPA CSR community. In this paper, I firstdiscuss the design principles and features of the toolkit. Then, I describe the composition of the NAB corpus, and report on the ngram statistics, standard vocabulary and language models created using the SLM tools."
            },
            "slug": "The-CMU-Statistical-Language-Modeling-Toolkit-and-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "The CMU Statistical Language Modeling Toolkit and its use in the 1994 ARPA CSR Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The design principles and features of the CMU SLM toolkit are discussed, the composition of the NAB corpus is described, and reports are reported on the ngram statistics, standard vocabulary and language models created using the SLM tools."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": false,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16602008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb38cf21b2330068e47fe91c8dcbdcd9d6fbb00d",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of a hidden Markov model (HMM) for the assignment of part-of-speech (POS) tags to improve the performance of a text recognition algorithm is dis- cussed. Syntactic constraints are described by the tran- sition probabilities between POS tags. The confusion between the feature string for a word and the various tags is also described probabilislically. A modification of the Viterbi algorithm is also presented that finds a fixed number of sequences of tags for a given sentence that have the highest probabilities of occurrence, given the feature strings for the words. An experimental application of this approach is demonstrated with a word hypothesizalion algorithm that produces a number of guesses about the identity of each word in a running text. The use of first and second order transition probabili- lies is explored. Overall performance of between 65 and 80 percent reduction in the average number of words that can match a given image is achieved. A computational theory for word recognition has been proposed that overcomes some of the constraints of other methodologies (6). The design of this tech- nique is based on human performance in reading which suggests that the feature analysis of word images includes a wholistic analysis of word shape. Also, feature extraction from a word image is only; one part of a complex process of developing an understanding of a text. Furthermore, the model suggests that to achieve high levels of performance in text recognition for a range of input qualities it may be necessary to understand the text as well. One part of the understanding process that underlies word recognition is an analysis of the syn- tax of the text. This paper discusses aspects of a Markov model for POS tagging where the probability of observing any POS tag is dependent on the tag of the previous word (8, 9). This model is applied to text recognition by first using a word recognition algorithm to supply a number of alternatives for the identity of each word. The tags of the alternatives for the words in a sentence are then input to a modilied Viterbi algorithm that determines sequences of syntactic classes that include each word. An alternative for a word decision is out- put only if its part of speech tag is included in at least one of these sequences. The Markov model improves word recognition performance if the number of altema- lives for a word are reduced without removing the correct choice. The rest of this paper briefly introduces the com- putational model for word recognition. This is followed by a description of how a Markov model for language syntax is incorporated in the model. The modified Viterbi algorithm proposed in this paper is then described. The performance of this technique in reduc- ing the number of alternatives for words in a sample of text is then discussed. The effect of employing the first and second order Markov assumptions and different methods of estimating the probabilities are explored."
            },
            "slug": "Combining-Syntactic-Knowledge-and-Visual-Text-A-for-Hull",
            "title": {
                "fragments": [],
                "text": "Combining Syntactic Knowledge and Visual Text Recognition: A Hidden Markov Model for Part of Speech Tagging In a Word Recognition Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper discusses aspects of a Markov model for POS tagging where the probability of observing any POS tag is dependent on the tag of the previous word and this model is applied to text recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49014513"
                        ],
                        "name": "T. Bell",
                        "slug": "T.-Bell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10314497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5d21625f8264f455591b3c7cbdac18b983b3c0",
            "isKey": false,
            "numCitedBy": 848,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known. >"
            },
            "slug": "The-zero-frequency-problem:-Estimating-the-of-novel-Witten-Bell",
            "title": {
                "fragments": [],
                "text": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors propose the application of a Poisson process model of novelty, which ability to predict novel tokens is evaluated, and it consistently outperforms existing methods and offers a small improvement in the coding efficiency of text compression over the best method previously known."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 626195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. \nIn this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules. \nIn experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."
            },
            "slug": "Natural-Language-Parsing-as-Statistical-Pattern-Magerman",
            "title": {
                "fragments": [],
                "text": "Natural Language Parsing as Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3215185"
                        ],
                        "name": "G. Sampson",
                        "slug": "G.-Sampson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sampson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46217277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cddeb5149f4de157d4daeb609a8b1432a8126e7b",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Linguists and speech researchers who use statistical methods often need to estimate the frequency of some type of item in a population containing items of various types. A common approach is to divide the number of cases observed in a sample by the size of the sample; sometimes small positive quantities are added to divisor and dividend in order to avoid zero estimates for types missing from the sample. These approaches are obvious and simple, but they lack principled justification, and yield estimates that can be wildly inaccurate. I.J. Good and Alan Turing developed a family of theoretically well-founded techniques appropriate to this domain. Some versions of the Good\u2013Turing approach are very demanding computationally, but we define a version, the Simple Good\u2013Turing estimator, which is straightforward to use. Tested on a variety of natural-language-related data sets, the Simple Good\u2013Turing estimator performs well, absolutely and relative both to the approaches just discussed and to other, more sophisticated techniques."
            },
            "slug": "Good-Turing-Frequency-Estimation-Without-Tears-Gale-Sampson",
            "title": {
                "fragments": [],
                "text": "Good-Turing Frequency Estimation Without Tears"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The Simple Good\u2013Turing estimator is defined, which is straightforward to use and performs well, absolutely and relative both to the approaches just discussed and to other, more sophisticated techniques."
            },
            "venue": {
                "fragments": [],
                "text": "J. Quant. Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "In jelinek-mercer, we bucket the\u03bb wi\u22121 i\u2212n+1 according to \u2211 wi c(wi i\u2212n+1) as suggested by Bahl et al. (1983) We choose bucket boundaries by requiring that at least cmin counts in the held-out data fall in each bucket, where cmin is an adjustable parameter."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40552549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af386a4e0f2615ed929fdc64a86df8e383bd6121",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of predicting the next word a speaker will say, given the words already spoken; is discussed. Specifically, the problem is to estimate the probability that a given word will be the next word uttered. Algorithms are presented for automatically constructing a binary decision tree designed to estimate these probabilities. At each node of the tree there is a yes/no question relating to the words already spoken, and at each leaf there is a probability distribution over the allowable vocabulary. Ideally, these nodal questions can take the form of arbitrarily complex Boolean expressions, but computationally cheaper alternatives are also discussed. Some results obtained on a 5000-word vocabulary with a tree designed to predict the next word spoken from the preceding 20 words are included. The tree is compared to an equivalent trigram model and shown to be superior. >"
            },
            "slug": "A-tree-based-statistical-language-model-for-natural-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "A tree-based statistical language model for natural language speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Algorithms are presented for automatically constructing a binary decision tree designed to estimate the probability that a given word will be the next word uttered, which is compared to an equivalent trigram model and shown to be superior."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544946"
                        ],
                        "name": "K. Seymore",
                        "slug": "K.-Seymore",
                        "structuredName": {
                            "firstName": "Kristie",
                            "lastName": "Seymore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Seymore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "Ronald Rosenfeld",
                        "slug": "Ronald-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9379111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2c182f105d8ba97a7f26364055cdc4fb65b5a7f",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone."
            },
            "slug": "Scalable-backoff-language-models-Seymore-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Scalable backoff language models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This project investigates the degradation of a trigram backoff model's perplexity and word error rates as bigram and trigram cutoffs are increased and shows that excluding trigrams and bigrams based on a weighted version of this difference method results in better perplexityand word error rate performance than excluding trigram and bigram based on counts alone."
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745366"
                        ],
                        "name": "Doug Beeferman",
                        "slug": "Doug-Beeferman",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Beeferman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Beeferman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88507334"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Roni",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1995619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "469c4f553ddf7b5195530d4cf10adbbcc9b18df8",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition."
            },
            "slug": "Evaluation-Metrics-For-Language-Models-Chen-Beeferman",
            "title": {
                "fragments": [],
                "text": "Evaluation Metrics For Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This research attempts to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance and finds that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873196"
                        ],
                        "name": "Sven C. Martin",
                        "slug": "Sven-C.-Martin",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Martin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven C. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782283"
                        ],
                        "name": "F. Wessel",
                        "slug": "F.-Wessel",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Wessel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wessel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118943002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "382c5a5f938d426fb7734994933804b5b8b3fddd",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The need for a stochastic language model in speech recognition arises from Bayes\u2019 decision rule for minimum error rate (Bahl et al., 1983). The word sequence w1 ... w N to be recognized from the sequence of acoustic observations x 1 ... x T is determined as that word sequence w 1 ... w N for which the posterior probability Pr(w 1 ... w N |x 1 ... x T ) attains its maximum. This rule can be rewritten in the form: \n \n$$ \\mathop {\\arg \\,\\max }\\limits_{{\\omega _1} \\cdot \\cdot \\cdot {\\omega _N}} \\{ \\Pr ({\\omega _1} \\cdot \\cdot \\cdot {\\omega _N}) \\cdot \\Pr ({x_1} \\cdot \\cdot \\cdot {x_T}\\left| {{\\omega _1} \\cdot \\cdot \\cdot {\\omega _N}} \\right.)\\} , $$ \n \n, where Pr(x 1 ... x T |w 1 ... w N ) is the conditional probability of, given the word sequence w 1 ... w N , observing the sequence of acoustic measurements x 1 ... x T and where Pr(w 1 ... w N ) is the prior probability of producing the word sequence w 1 ... w N ."
            },
            "slug": "Statistical-Language-Modeling-Using-Leaving-One-Out-Ney-Martin",
            "title": {
                "fragments": [],
                "text": "Statistical Language Modeling Using Leaving-One-Out"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The need for a stochastic language model in speech recognition arises from Bayes\u2019 decision rule for minimum error rate and the conditional probability of the word sequence w 1 ... w N is determined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18774233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2588593c42126e059fb8aad7673fa1736755f1e1",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words."
            },
            "slug": "Probabilistic-Feature-Grammars-Goodman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Feature Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Probabilistic feature grammar combines most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067634566"
                        ],
                        "name": "James Brooks",
                        "slug": "James-Brooks",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Brooks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc9e5bf851dc95369e26f1869c2637b1d8919e6c",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events \u2014 ignoring events which occur less than 5 times in training data reduces performance to 81.6%."
            },
            "slug": "Prepositional-Phrase-Attachment-through-a-Model-Collins-Brooks",
            "title": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment through a Backed-off Model"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper shows that the problem of prepositional phrase attachment ambiguity is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490506"
                        ],
                        "name": "Mark D. Kernighan",
                        "slug": "Mark-D.-Kernighan",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Kernighan",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Kernighan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 212
                            }
                        ],
                        "text": "Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition (Bahl, Jelinek, and Mercer, 1983) as well as many other domains (Church, 1988; Brown et al., 1990; Kernighan, Church, and Gale, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32954707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31ef8b3ebc50d682cbd6ce95505ab49802be6bd7",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new program, correct, which takes words rejected by the Unix\u00ae spell program, proposes a list of candidate corrections, and sorts them by probability. The probability scores are the novel contribution of this work. Probabilities are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in the speech recognition literature (Jelinek, 1985), one can often recover the intended correction, c, from a typo, t, by finding the correction c that maximizes Pr(c) Pr(t/c). The first factor, Pr(c), is a prior model of word probabilities; the second factor, Pr(t/c), is a model of the noisy channel that accounts for spelling transformations on letter sequences (e.g., insertions, delections, substitutions and reversals). Both sets of probabilities were trained on data collected from the Associated Press (AP) newswire. This text is ideally suited for this purpose since it contains a large number of typos (about two thousand per month)."
            },
            "slug": "A-Spelling-Correction-Program-Based-on-a-Noisy-Kernighan-Church",
            "title": {
                "fragments": [],
                "text": "A Spelling Correction Program Based on a Noisy Channel Model"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new program is described, correct, which takes words rejected by the Unix\u00ae spell program, proposes a list of candidate corrections, and sorts them by probability, and the probability scores are the novel contribution of this work."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830767"
                        ],
                        "name": "P. Placeway",
                        "slug": "P.-Placeway",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Placeway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Placeway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108453673"
                        ],
                        "name": "Scotte Chen",
                        "slug": "Scotte-Chen",
                        "structuredName": {
                            "firstName": "Scotte",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scotte Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716325"
                        ],
                        "name": "M. Esk\u00e9nazi",
                        "slug": "M.-Esk\u00e9nazi",
                        "structuredName": {
                            "firstName": "Maxine",
                            "lastName": "Esk\u00e9nazi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Esk\u00e9nazi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055088150"
                        ],
                        "name": "Uday Jain",
                        "slug": "Uday-Jain",
                        "structuredName": {
                            "firstName": "Uday",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uday Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144176082"
                        ],
                        "name": "V. Parikh",
                        "slug": "V.-Parikh",
                        "structuredName": {
                            "firstName": "Vipul",
                            "lastName": "Parikh",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681921"
                        ],
                        "name": "B. Raj",
                        "slug": "B.-Raj",
                        "structuredName": {
                            "firstName": "Bhiksha",
                            "lastName": "Raj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Raj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34546277"
                        ],
                        "name": "M. Ravishankar",
                        "slug": "M.-Ravishankar",
                        "structuredName": {
                            "firstName": "Mosur",
                            "lastName": "Ravishankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ravishankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053916979"
                        ],
                        "name": "Rog\u00e9rio Rosenfeld",
                        "slug": "Rog\u00e9rio-Rosenfeld",
                        "structuredName": {
                            "firstName": "Rog\u00e9rio",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rog\u00e9rio Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544946"
                        ],
                        "name": "K. Seymore",
                        "slug": "K.-Seymore",
                        "structuredName": {
                            "firstName": "Kristie",
                            "lastName": "Seymore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Seymore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48263917"
                        ],
                        "name": "M. Siegler",
                        "slug": "M.-Siegler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Siegler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Siegler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697819"
                        ],
                        "name": "R. Stern",
                        "slug": "R.-Stern",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Stern",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235358"
                        ],
                        "name": "Eric H. Thayer",
                        "slug": "Eric-H.-Thayer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Thayer",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric H. Thayer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 207
                            }
                        ],
                        "text": "1 Higher Order n-Gram Models Due to the increasing speed and memory of computers, there has been some use of higher-order n-gram models such as 4-gram and 5-gram models in speech recognition in recent years (Seymore et al., 1997; Weng, Stolcke, and Sankar, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1618516,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ad7c2ba91f75032402795e43e88104688abfc16b",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the CMU Sphinx-3 system, and the configuration we used for the 1996 DARPA (Hub-4) evaluation. The model structure, acoustic modeling, language modeling, lexical modeling, and system structure are summarized. We also discuss the experimental results obtained with this system on the most recent DARPA evaluation, and some subsequent results are also discussed."
            },
            "slug": "The-1996-Hub-4-Sphinx-3-System-Placeway-Chen",
            "title": {
                "fragments": [],
                "text": "The 1996 Hub-4 Sphinx-3 System"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The model structure, acoustic modeling, language modeling, lexical modeling, and system structure are summarized and the experimental results obtained with this system on the most recent DARPA evaluation are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2648137"
                        ],
                        "name": "Charlotte M. Baltus",
                        "slug": "Charlotte-M.-Baltus",
                        "structuredName": {
                            "firstName": "Charlotte",
                            "lastName": "Baltus",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charlotte M. Baltus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9173685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d32b89707a521fbc6e45d8156531f83fc8ce42ce",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The output of handwritten word recognizers tends to be very noisy due to factors such as variable handwriting styles, distortions in the image data, etc. In order to compensate for this behaviour, several choices of the word recognizer are initially considered but eventually reduced to a single choice based on constraints posed by the particular domain. In the case of handwritten sentence/phrase recognition, linguistic constraints may be applied in order to improve the results of the word recognizer. Linguistic constraints can be applied as (i) a purely post-processing operation or (ii) in a feedback loop to the word recognizer. This paper discusses two statistical methods of applying syntactic constraints to the output of a handwritten word recognizer on input consisting of sentences/phrases. Both methods are based on syntactic categories (tags) associated with words. The first is a purely statistical method, the second is a hybrid method which combines higherlevel syntactic information (hypertags) with statistical information regarding transitions between hypertags. We show the utility of both these approaches in the problem of handwritten sentence/phrase recognition."
            },
            "slug": "Combining-Statistical-and-Syntactic-Methods-in-Srihari-Baltus",
            "title": {
                "fragments": [],
                "text": "Combining Statistical and Syntactic Methods in Recognizing Handwritten Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two statistical methods of applying syntactic constraints to the output of a handwritten word recognizer on input consisting of sentences/phrases based on syntactic categories associated with words are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3281101"
                        ],
                        "name": "I. Rogina",
                        "slug": "I.-Rogina",
                        "structuredName": {
                            "firstName": "Ivica",
                            "lastName": "Rogina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rogina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16519468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fd21af42c7f7032e7ec28138b1e0752329e6da4",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "a new training which included gender dependent models. At the training stage where the evaluation system had a 16.8% error on the 1992 WSJ development set si-dev-05, the improved system had a word error rate of 14.7%, which is a reduction by about 13%. The JANUS speech recognizer has proven to give good recognition results as shown on the 1994 Verbmobil evaluation. However, one week with the development data was not enough to tune JANUS to the NAB task. We expect great improvements from successfully applying gender dependent acoustic modeling, optimizing the architecture (context decision trees, number of models, size of mixtures), and channel normalization. The adaptation to a new task is always hard and tricky work. A large amount of ne-tuning work has to be done to reach good performance. Although JANUS scored worst in the 1994 CSR evaluation, we feel optimistic that with some more tuning and the above mentionned techniqes, JANUS will soon compare more favorably. [8] Maier M.: \"Dimensionalit\u007f atsreduktion von Sprachsig-nalen mit statistischen und neuronalen Methoden\","
            },
            "slug": "The-Janus-Speech-Recognizer-Rogina-Waibel",
            "title": {
                "fragments": [],
                "text": "The Janus Speech Recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The JANUS speech recognizer has proven to give good recognition results, but a large amount of ne-tuning work has to be done to reach good performance, and it is optimistic that with some more tuning and the above mentionned techniqes, JANus will soon compare more favorably."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34972608"
                        ],
                        "name": "J. Godfrey",
                        "slug": "J.-Godfrey",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Godfrey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Godfrey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2921734"
                        ],
                        "name": "E. Holliman",
                        "slug": "E.-Holliman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Holliman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Holliman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068950219"
                        ],
                        "name": "J. McDaniel",
                        "slug": "J.-McDaniel",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "McDaniel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDaniel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61412708,
            "fieldsOfStudy": [
                "Physics",
                "Linguistics"
            ],
            "id": "d80000d84223e177d070a01a734dba56d5f5c069",
            "isKey": false,
            "numCitedBy": 1965,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording.<<ETX>>"
            },
            "slug": "SWITCHBOARD:-telephone-speech-corpus-for-research-Godfrey-Holliman",
            "title": {
                "fragments": [],
                "text": "SWITCHBOARD: telephone speech corpus for research and development"
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42793,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1459,
                                "start": 72
                            }
                        ],
                        "text": "Then, each bucket is treated as a distinct probability distribution and Good-Turing estimation is performed within each. For a bigram in bucket b with rb counts, we calculate its corrected count r b as r b = (rb + 1)nb;r+1 nb;r where the counts nb;r include only those bigrams within bucket b. Church and Gale partition the range of possible pML(wi 1)pML(wi) values into about 35 buckets, with three buckets in each factor of 10. To smooth the nb;r for the Good-Turing estimate, they use a smoother by Shirey and Hastie (1988). While extensive empirical analysis is reported, they present only a single entropy result, comparing the above smoothing technique with another smoothing method introduced in their paper, extended deleted estimation. In our previous work (Chen, 1996), we present further results, indicating that this smoothing works well for bigram language models. When extending this method to trigram models, there are two options for implementation. Unfortunately, one of these methods is computationally intractable, and we have demonstrated that the other performs poorly. 2.9.2 Bayesian Smoothing Several smoothing techniques are motivated within a Bayesian framework. A prior distribution over smoothed distributions is selected, and this prior is used to somehow arrive at a nal smoothed distribution. For example, Nadas (1984) selects smoothed probabilities to be their mean a posteriori value given the prior distribution. Nadas (1984) hypothesizes a prior distribution from the family of beta functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 4
                            }
                        ],
                        "text": "The Good-Turing estimate cannot be used when nr = 0; it is generally necessary to \\smooth\" the nr, e.g., to adjust the nr so that they are all above zero. Recently, Gale and Sampson (1995) have proposed a simple and e ective algorithm for smoothing these values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1349,
                                "start": 72
                            }
                        ],
                        "text": "Then, each bucket is treated as a distinct probability distribution and Good-Turing estimation is performed within each. For a bigram in bucket b with rb counts, we calculate its corrected count r b as r b = (rb + 1)nb;r+1 nb;r where the counts nb;r include only those bigrams within bucket b. Church and Gale partition the range of possible pML(wi 1)pML(wi) values into about 35 buckets, with three buckets in each factor of 10. To smooth the nb;r for the Good-Turing estimate, they use a smoother by Shirey and Hastie (1988). While extensive empirical analysis is reported, they present only a single entropy result, comparing the above smoothing technique with another smoothing method introduced in their paper, extended deleted estimation. In our previous work (Chen, 1996), we present further results, indicating that this smoothing works well for bigram language models. When extending this method to trigram models, there are two options for implementation. Unfortunately, one of these methods is computationally intractable, and we have demonstrated that the other performs poorly. 2.9.2 Bayesian Smoothing Several smoothing techniques are motivated within a Bayesian framework. A prior distribution over smoothed distributions is selected, and this prior is used to somehow arrive at a nal smoothed distribution. For example, Nadas (1984) selects smoothed probabilities to be their mean a posteriori value given the prior distribution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 527,
                                "start": 72
                            }
                        ],
                        "text": "Then, each bucket is treated as a distinct probability distribution and Good-Turing estimation is performed within each. For a bigram in bucket b with rb counts, we calculate its corrected count r b as r b = (rb + 1)nb;r+1 nb;r where the counts nb;r include only those bigrams within bucket b. Church and Gale partition the range of possible pML(wi 1)pML(wi) values into about 35 buckets, with three buckets in each factor of 10. To smooth the nb;r for the Good-Turing estimate, they use a smoother by Shirey and Hastie (1988). While extensive empirical analysis is reported, they present only a single entropy result, comparing the above smoothing technique with another smoothing method introduced in their paper, extended deleted estimation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SWITCHBOARD: Telephone speech corpus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 499,
                                "start": 4
                            }
                        ],
                        "text": "The Good-Turing estimate (Good, 1953) is central to many smoothing techniques. It is not used directly for n-gram smoothing because, like additive smoothing, it does not perform the interpolation of lower- and higher-order models essential for good performance. Good-Turing states that an n-gram that occurs r times should be treated as if it had occurred r times, where r = (r + 1)nr+1 nr and where nr is the number of n-grams that occur exactly r times in the training data. Katz smoothing (1987) extends the intuitions of Good-Turing by adding the interpolation of higherorder models with lower-order models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2088,
                                "start": 11
                            }
                        ],
                        "text": "edu Joshua Goodman Harvard University Aiken Computation Laboratory 33 Oxford St. Cambridge, MA 02138 goodman@eecs.harvard.edu Abstract We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the rst time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) a ect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1 Introduction Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition (Bahl, Jelinek, and Mercer, 1983) as well as many other domains (Church, 1988; Brown et al., 1990; Kernighan, Church, and Gale, 1990). A language model is a probability distribution over strings P (s) that attempts to re ect the frequency with which each string s occurs as a sentence in natural text. Language models are used in speech recognition to resolve acoustically ambiguous utterances. For example, if we have that P (it takes two) P (it takes too), then we know ceteris paribus to prefer the former transcription over the latter. While smoothing is a central issue in language modeling, the literature lacks a de nitive comparison between the many existing techniques. Previous studies (Nadas, 1984; Katz, 1987; Church and Gale, 1991; MacKay and Peto, 1995) only compare a small number of methods (typically two) on a single corpus and using a single training data size. As a result, it is currently di cult for a researcher to intelligently choose between smoothing schemes. In this work, we carry out an extensive empirical comparison of the most widely used smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2164,
                                "start": 14
                            }
                        ],
                        "text": "Secondly, the Good-Turing estimate can be interpreted as stating that the number of these extra counts should be proportional to the number of words with exactly one count in the given distribution. We have found that taking = [n1(wi 1 i n+1) + ] (4) works well, where n1(wi 1 i n+1) = jwi : c(wi i n+1) = 1j is the number of words with one count, and where and are constants. 4 Experimental Methodology 4.1 Data We used the Penn treebank and TIPSTER corpora distributed by the Linguistic Data Consortium. From the treebank, we extracted text from the tagged Brown corpus, yielding about one million words. From TIPSTER, we used the Associated Press (AP), Wall Street Journal (WSJ), and San Jose Mercury News (SJM) data, yielding 123, 84, and 43 million words respectively. We created two distinct vocabularies, one for the Brown corpus and one for the TIPSTER data. The former vocabulary contains all 53,850 words occurring in Brown; the latter vocabulary consists of the 65,173 words occurring at least 70 times in TIPSTER. For each experiment, we selected three segments of held-out data along with the segment of training data. One held-out segment was used as the test data for performance evaluation, and the other two were used as development test data for optimizing the parameters of each smoothing method. Each piece of held-out data was chosen to be roughly 50,000 words. This decision does not re ect practice very well, as when the training data size is less than 50,000 words it is not realistic to have so much development test data available. However, we made this decision to prevent us having to optimize the training versus held-out data tradeo for each data size. In addition, the development test data is used to optimize typically very few parameters, so in practice small held-out sets are generally adequate, and perhaps can be avoided altogether with techniques such as deleted estimation. 4.2 Smoothing Implementations In this section, we discuss the details of our implementations of various smoothing techniques. Due to space limitations, these descriptions are not comprehensive; a more complete discussion is presented in Chen (1996). The titles of the following sections include the mnemonic we use to refer to the implementations in later sections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 11
                            }
                        ],
                        "text": "edu Joshua Goodman Harvard University Aiken Computation Laboratory 33 Oxford St. Cambridge, MA 02138 goodman@eecs.harvard.edu Abstract We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 713,
                                "start": 4
                            }
                        ],
                        "text": "The Good-Turing estimate (Good, 1953) is central to many smoothing techniques. It is not used directly for n-gram smoothing because, like additive smoothing, it does not perform the interpolation of lower- and higher-order models essential for good performance. Good-Turing states that an n-gram that occurs r times should be treated as if it had occurred r times, where r = (r + 1)nr+1 nr and where nr is the number of n-grams that occur exactly r times in the training data. Katz smoothing (1987) extends the intuitions of Good-Turing by adding the interpolation of higherorder models with lower-order models. It is perhaps the most widely used smoothing technique in speech recognition. Church and Gale (1991) describe a smoothing method that combines the Good-Turing estimate with bucketing, the technique of partitioning a set of n-grams into disjoint groups, where each group is characterized independently through a set of parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 26
                            }
                        ],
                        "text": "The Good-Turing estimate (Good, 1953) is central to many smoothing techniques."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2129,
                                "start": 11
                            }
                        ],
                        "text": "edu Joshua Goodman Harvard University Aiken Computation Laboratory 33 Oxford St. Cambridge, MA 02138 goodman@eecs.harvard.edu Abstract We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the rst time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) a ect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1 Introduction Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition (Bahl, Jelinek, and Mercer, 1983) as well as many other domains (Church, 1988; Brown et al., 1990; Kernighan, Church, and Gale, 1990). A language model is a probability distribution over strings P (s) that attempts to re ect the frequency with which each string s occurs as a sentence in natural text. Language models are used in speech recognition to resolve acoustically ambiguous utterances. For example, if we have that P (it takes two) P (it takes too), then we know ceteris paribus to prefer the former transcription over the latter. While smoothing is a central issue in language modeling, the literature lacks a de nitive comparison between the many existing techniques. Previous studies (Nadas, 1984; Katz, 1987; Church and Gale, 1991; MacKay and Peto, 1995) only compare a small number of methods (typically two) on a single corpus and using a single training data size. As a result, it is currently di cult for a researcher to intelligently choose between smoothing schemes. In this work, we carry out an extensive empirical comparison of the most widely used smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We carry out experiments over many training data sizes on varied corpora using both bigram and trigram models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 11
                            }
                        ],
                        "text": "edu Joshua Goodman Harvard University Aiken Computation Laboratory 33 Oxford St. Cambridge, MA 02138 goodman@eecs.harvard.edu Abstract We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the rst time how factors such as training data size, corpus (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The population frequencies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 158
                            }
                        ],
                        "text": "For example, we can take\nPinterp(wi|wi\u22121) = \u03bbPML(wi|wi\u22121)+(1\u2212\u03bb)PML(wi)\ngetting the behavior that bigrams involving common words are assigned higher probabilities (Jelinek and Mercer, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 136
                            }
                        ],
                        "text": "In this work, we carry out an extensive empirical comparison of the most widely used smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 0
                            }
                        ],
                        "text": "We refer to this data as the WSJ/NAB corpus. The Switchboard data is three million words of telephone conversation transcriptions ( Godfrey, Holliman & McDaniel, 1992 ). We used the 9800 word vocabulary created by Finkeet al. (1997). The Broadcast News text ( Rudnicky, 1996) consists of 130 million words of transcriptions of television and radio news shows from 1992\u20131996."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 97
                            }
                        ],
                        "text": "The other smoothing technique besides Katz smoothing widely used in speech recognition is due to Jelinek and Mercer (1980)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 189
                            }
                        ],
                        "text": "ar X\niv :c\nm p-\nlg /9\n60 60\n11 v1\n1 1\nJu n\n19 96\nWe present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": true,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1754,
                                "start": 322
                            }
                        ],
                        "text": "To end the recursion, we can take the smoothed 1st-order model to be the maximum likelihood distribution, or we can take the smoothed 0th-order model to be the uniform distribution punif(wi) = 1 jV j Given xed pML, it is possible to search e ciently for the wi 1 i n+1 that maximize the probability of some data using the Baum-Welch algorithm (Baum, 1972). To yield meaningful results, the data used to estimate the wi 1 i n+1 need to be di erent from the data used to calculate the pML.5 In held-out interpolation, one reserves a section of the training data for this purpose, where this heldout data is not used in calculating the pML. Alternatively, Jelinek and Mercer describe a technique known as deleted interpolation or deleted estimation where di erent parts of the training data rotate in training either the pML or the wi 1 i n+1 ; the results are then averaged. Notice that the optimal wi 1 i n+1 will be di erent for di erent histories wi 1 i n+1. For example, for a context we have seen thousands of times, a high will be suitable since the higher-order distribution will be very reliable; for a history that has occurred only once, a lower will be appropriate. Training each parameter wi 1 i n+1 independently is not generally felicitous; we would need an enormous amount of data to train so many independent parameters accurately. Instead, Jelinek and Mercer suggest dividing the wi 1 i n+1 into a moderate number of partitions or buckets, and constraining all wi 1 i n+1 in the same bucket to have the same value, thereby reducing the number of independent parameters to be estimated. Ideally, we should tie together those wi 1 i n+1 that we have an a priori reason to believe should have similar values. Bahl, Jelinek, and Mercer (1983) suggest choosing these sets of wi 1 i n+1 according to Pwi c(wi i n+1), the total number of counts in the higher-order distribution being interpolated (which is equal to the number of counts of the corresponding history)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 355,
                                "start": 343
                            }
                        ],
                        "text": "To end the recursion, we can take the smoothed 1st-order model to be the maximum likelihood distribution, or we can take the smoothed 0th-order model to be the uniform distribution punif(wi) = 1 jV j Given xed pML, it is possible to search e ciently for the wi 1 i n+1 that maximize the probability of some data using the Baum-Welch algorithm (Baum, 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1361,
                                "start": 73
                            }
                        ],
                        "text": "For instances of Jelinek-Mercer smoothing, the 's were trained using the Baum-Welch algorithm on the second of the three held-out sets; all other parameters were optimized using Powell's algorithm on the rst set. In particular, to evaluate the entropy associated with a given set of (non- ) parameters in Powell's search, we rst optimize the 's on the second held-out set. To constrain the parameter search in our main experiments, we searched only those parameters that were found to a ect performance signi cantly, as indicated through preliminary experiments over several data sizes. In each run of these preliminary experiments, we xed all (non- ) parameters but one to some reasonable value, and used Powell's algorithm to search on the single free parameter. If the range of test data entropies over all parameter values considered by Powell's algorithm was much smaller than the typical di erence in entropies between di erent algorithms (i.e., 0.005 bits), we chose not to perform the search over this parameter in the later experiments, and simply assign an arbitrary reasonable value to the parameter. For each parameter, we tried three di erent training sets: 20,000 words from the WSJ corpus, one million words from the Brown corpus, and three million words from the WSJ corpus. We summarize the results of these experiments in Table 3; Chen (1996) gives more details."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": true,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 62
                            }
                        ],
                        "text": "This modification is motivated by evidence to be presented in Section 5.2.1that the ideal average discount for n-grams with one or two counts is substantially different from the ideal average discount for n-grams with higher counts. Indeed, we will see later that modified Kneser\u2013Ney smoothing significantly outperforms regular Kneser\u2013Ney smoothing. Just asNey et al. (1994) have developed an estimate for the optimal D for absolute discounting and Kneser\u2013Ney smoothing as a function of training data counts (as given in Equation (11)), it is possible to create analogous equations to estimate the optimal values for D1, D2, andD3 (Ries, 1997)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Esitmation of probabilities from sparse data for the language model component of a speech recognizer.IEEE Transactions on Acoustics, Speech and Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Mind,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 99
                            }
                        ],
                        "text": "1 Additive Smoothing One of the simplest types of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Je reys, 1948), which is just a generalization of the method given in equation (5)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "The simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), where we take\nPadd(wi|w i\u22121 i\u2212n+1) =\nc(wii\u2212n+1) + \u03b4\nc(wi\u22121i\u2212n+1) + \u03b4|V | (2)\nand where Lidstone and Jeffreys advocate \u03b4 = 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 116
                            }
                        ],
                        "text": "To give an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually does (Lidstone, 1920; Johnson, 1932; Je reys, 1948), yielding p(wijwi 1) = 1 + c(wi 1wi) Pwi [1 + c(wi 1wi)] = 1 + c(wi 1wi) jV j+Pwi c(wi 1wi) (5) 6"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "As an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually did (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), yielding\nP+1(wi|wi\u22121) = c(wi\u22121wi) + 1\nc(wi\u22121) + |V |\nwhere V is the vocabulary, the set of all words being considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note on the general case of the Bayes-Laplace formula for inductive or"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1920
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 78
                            }
                        ],
                        "text": "One of the simplest types of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Je reys, 1948), which is just a generalization of the method given in equation (5)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "The simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), where we take\nPadd(wi|w i\u22121 i\u2212n+1) =\nc(wii\u2212n+1) + \u03b4\nc(wi\u22121i\u2212n+1) + \u03b4|V | (2)\nand where Lidstone and Jeffreys advocate \u03b4 = 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "As an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually did (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), yielding\nP+1(wi|wi\u22121) = c(wi\u22121wi) + 1\nc(wi\u22121) + |V |\nwhere V is the vocabulary, the set of all words being considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 116
                            }
                        ],
                        "text": "To give an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually does (Lidstone, 1920; Johnson, 1932; Je reys, 1948), yielding"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Faculty of Actuaries,"
            },
            "year": 1920
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 110
                            }
                        ],
                        "text": "As an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually did (Lidstone, 1920; Johnson, 1932; Je reys, 1948), yielding P+1(wijwi 1) = c(wi 1wi) + 1 c(wi 1) + jV j where V is the vocabulary, the set of all words being considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "The simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), where we take\nPadd(wi|w i\u22121 i\u2212n+1) =\nc(wii\u2212n+1) + \u03b4\nc(wi\u22121i\u2212n+1) + \u03b4|V | (2)\nand where Lidstone and Jeffreys advocate \u03b4 = 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "As an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually did (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), yielding\nP+1(wi|wi\u22121) = c(wi\u22121wi) + 1\nc(wi\u22121) + |V |\nwhere V is the vocabulary, the set of all words being considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 86
                            }
                        ],
                        "text": "2 Previous Work The simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Je reys, 1948), where we take Padd(wijwi 1 i n+1) = c(wi i n+1) + c(wi 1 i n+1) + jV j (2) and where Lidstone and Je reys advocate = 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note on the general case"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1920
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143972283"
                        ],
                        "name": "W. Stewart",
                        "slug": "W.-Stewart",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Stewart",
                            "middleNames": [
                                "b."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stewart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 220318587,
            "fieldsOfStudy": [],
            "id": "e3cbcd4a12639b3f3af1a045afe74df836a35adb",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Church-Stewart",
            "title": {
                "fragments": [],
                "text": "Church"
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedic Dictionary of Archaeology"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96961193"
                        ],
                        "name": "W. E. Johnson",
                        "slug": "W.-E.-Johnson",
                        "structuredName": {
                            "firstName": "Willis",
                            "lastName": "Johnson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. E. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 2
                            }
                        ],
                        "text": ") We use this formulation because it leads to a cleaner derivation of essentially the same formula; no approximations are required, unlike in the original derivation. In addition, as will be shown later in this paper, and as has been independently observed byNeyet al. (1997), the former formulation generally yields better performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 170852080,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "25035fe0d23f9ffa62c6c1d2c9172e03aa5b4801",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "I.\u2014PROBABILITY:-THE-DEDUCTIVE-AND-INDUCTIVE-Johnson",
            "title": {
                "fragments": [],
                "text": "I.\u2014PROBABILITY: THE DEDUCTIVE AND INDUCTIVE PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1932
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16318570"
                        ],
                        "name": "H. Kucera",
                        "slug": "H.-Kucera",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kucera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kucera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35097577"
                        ],
                        "name": "W. Francis",
                        "slug": "W.-Francis",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Francis",
                            "middleNames": [
                                "Nelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Francis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "75103111"
                        ],
                        "name": "W. Twaddell",
                        "slug": "W.-Twaddell",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Twaddell",
                            "middleNames": [
                                "Freeman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Twaddell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3576908"
                        ],
                        "name": "M. L. Marckworth",
                        "slug": "M.-L.-Marckworth",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marckworth",
                            "middleNames": [
                                "Lois"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Marckworth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054518662"
                        ],
                        "name": "Laura M. Bell",
                        "slug": "Laura-M.-Bell",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Bell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura M. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40418506"
                        ],
                        "name": "J. Carroll",
                        "slug": "J.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carroll"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 29
                            }
                        ],
                        "text": "The text of the Brown corpus (Kucera and Francis, 1967) was extracted from the tagged text in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) and amounted to about one million words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143602821,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b7780f292c48e504e3a2724e54c205e6c6221932",
            "isKey": false,
            "numCitedBy": 6758,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computational-analysis-of-present-day-American-Kucera-Francis",
            "title": {
                "fragments": [],
                "text": "Computational analysis of present-day American English"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116312510"
                        ],
                        "name": "M. McCarthy",
                        "slug": "M.-McCarthy",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "McCarthy",
                            "middleNames": [
                                "Dianne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McCarthy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 67
                            }
                        ],
                        "text": "A general class of interpolated models is described by Jelinek and Mercer (1980). An elegant way of performing this interpolation is given by Brown et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 67
                            }
                        ],
                        "text": "A general class of interpolated models is described by Jelinek and Mercer (1980). An elegant way of performing this interpolation is given by Brown et al. (1992a) as follows pinterp(wijwi 1 i n+1) = wi 1 i n+1 pML(wijwi 1 i n+1) + (1 wi 1 i n+1 ) pinterp(wijwi 1 i n+2) (12) That is, the nth-order smoothed model is de ned recursively as a linear interpolation between the nth-order maximum likelihood model and the (n 1)th-order smoothed model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 70631861,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "0ae47348a378307514411560e6db99df8bbf6222",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-statistical-approach-McCarthy",
            "title": {
                "fragments": [],
                "text": "The statistical approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221583858,
            "fieldsOfStudy": [],
            "id": "769dbbe88801b57a9b44f89c5516264f16cbed60",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123729051"
                        ],
                        "name": "L. M. M.-T.",
                        "slug": "L.-M.-M.-T.",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "M.-T.",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. M.-T."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4036480,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f1f4386524be3ed96caaf05f661aacb94db1e566",
            "isKey": false,
            "numCitedBy": 5288,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Probability-M.-T.",
            "title": {
                "fragments": [],
                "text": "Theory of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1929
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33088764"
                        ],
                        "name": "V. Rich",
                        "slug": "V.-Rich",
                        "structuredName": {
                            "firstName": "Vera",
                            "lastName": "Rich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4270736,
            "fieldsOfStudy": [],
            "id": "7f1d0cc5b6a1e23c6756454a8bb6a756a3f8ffde",
            "isKey": false,
            "numCitedBy": 12561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Personal-communication-Rich",
            "title": {
                "fragments": [],
                "text": "Personal communication"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 0
                            }
                        ],
                        "text": "Goodman 371 whereY = n1 n1+2n2 . Note thatNey et al. (1997) independently introduced the idea of multiple discounts, suggesting two discounts instead of three, and giving estimates for the discounts based on the Good\u2013Turing estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 26
                            }
                        ],
                        "text": "The Good-Turing estimate (Good, 1953) is central to many smoothing techniques."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 46
                            }
                        ],
                        "text": "Good\u2013Turing estimate The Good\u2013Turing estimate ( Good, 1953) is central to many smoothing techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11945361,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2986b25f50babd536dd0ecf2237d9eabf5843c2",
            "isKey": false,
            "numCitedBy": 3274,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-POPULATION-FREQUENCIES-OF-SPECIES-AND-THE-OF-Good",
            "title": {
                "fragments": [],
                "text": "THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "This terminology comes from the area of Markov models (Markov, 1913), of which n-gram models are an instance."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of statistical investigation in the text of `Eugene Onyegin' illustrating coupling of tests in chains"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Academy of Science, St. Petersburg, 7:153{162. 62"
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What's wrong with adding one? In N"
            },
            "venue": {
                "fragments": [],
                "text": "Oostdijk"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Natural Language Pars"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved backing-o for m-gram language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Kenneth W"
            },
            "venue": {
                "fragments": [],
                "text": "Church."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Kenneth W"
            },
            "venue": {
                "fragments": [],
                "text": "Church."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation procedures for language context"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hub4 language modeling using domain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speci cation of the 1995 ARPA hub 3 evaluation: Unlimited vocabulary NAB news baseline"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the DARPA Speech Recognition Workshop, pages 5{7."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Linda C"
            },
            "venue": {
                "fragments": [],
                "text": "Peto."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved backingoff for mgram language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE International Conference on Acoustics , Speech and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building Probabilistic Mod"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 192
                            }
                        ],
                        "text": "Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition (Bahl, Jelinek, and Mercer, 1983) as well as many other domains (Church, 1988; Brown et al., 1990; Kernighan, Church, and Gale, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Paul S"
            },
            "venue": {
                "fragments": [],
                "text": "Roossin."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Robert L"
            },
            "venue": {
                "fragments": [],
                "text": "Mercer."
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and William A"
            },
            "venue": {
                "fragments": [],
                "text": "Gale."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building a large annotated corpus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic parts program"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "They present a class of smoothing models that involve linear interpolation, e.g., Brown et al. (1992) take\nPinterp(wi|w i\u22121 i\u2212n+1) =\n\u03bbwi\u22121 i\u2212n+1\nPML(wi|w i\u22121 i\u2212n+1) +\n(1 \u2212 \u03bbwi\u22121 i\u2212n+1\n) Pinterp(wi|w i\u22121 i\u2212n+2) (3)\nThat is, the maximum likelihood estimate is interpolated with the smoothed\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Robert L"
            },
            "venue": {
                "fragments": [],
                "text": "Mercer."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On structuring probabilistic dependences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Specification of the 1995 ARPA Hub 3 evaluation : Unlimited vocabulary NAB news baseline"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speci cation of the 1995 ARPA hub 3 evaluation: Unlimited vocabulary"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 180
                            }
                        ],
                        "text": "In each run except as noted below, optimal values for the parameters of the given technique were searched for using Powell\u2019s search algorithm as realized in Numerical Recipes in C (Press et al., 1988, pp. 309\u2013317)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Teukolsky"
            },
            "venue": {
                "fragments": [],
                "text": "and W.T. Vetterling."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "This terminology comes from the area of Markov models (Markov, 1913), of which n-gram models are an instance."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of statistical investigation in the text of `Eugene Onyegin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining statistical and syntactic methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluation metrics for lan"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 32
                            }
                        ],
                        "text": "12 The text of the Brown corpus (Kucera and Francis, 1967) was extracted from the tagged text in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) and amounted to about one million words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational Analysis of Present-Day"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note on the general case of the Bayes \u2013 Laplace formula for inductive or a posteriori probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Faculty of Actuaries"
            },
            "year": 1920
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1284,
                                "start": 145
                            }
                        ],
                        "text": "As an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually did (Lidstone, 1920; Johnson, 1932; Je reys, 1948), yielding P+1(wijwi 1) = c(wi 1wi) + 1 c(wi 1) + jV j where V is the vocabulary, the set of all di erent words that occur in the training data. This has the desirable quality of preventing zero bigram probabilities. However, this scheme has the aw of assigning the same probability to say, burnish the and burnish thou (assuming neither occurred in the training data), even though intuitively the former seems more likely because the word the is much more common than thou. To address this, another smoothing technique is to interpolate the bigram model with a unigram model PML(wi) = c(wi)=NS , a model that re ects how often each word occurs in the training data. For example, we can take Pinterp(wijwi 1) = PML(wijwi 1)+(1 )PML(wi) getting the behavior that bigrams involving common words are assigned higher probabilities (Jelinek and Mercer, 1980). 2 Previous Work The simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920; Johnson, 1932; Je reys, 1948), where we take Padd(wijwi 1 i n+1) = c(wi i n+1) + c(wi 1 i n+1) + jV j and where Lidstone and Je reys advocate = 1. Gale and Church (1994) have argued that this method generally performs poorly."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining syntactic knowl"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 144
                            }
                        ],
                        "text": "The Broadcast News text was taken from the language modeling data distributed for the 1996 DARPA Hub 4 continuous speech recognition evaluation (Rudnicky, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hub 4: Business Broadcast News"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the DARPA Speech"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 23
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 88,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua/d4e8bed3b50a035e1eabad614fe4218a34b3b178?sort=total-citations"
}