{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Currently, we have included work from Gargi et al [3], Chaddha et al [1], Winger et al [14], LeBourgeois [7] and Mitrea and deWith [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Figure 2 shows the results of localization by two algorithms ([3] and [9]), indicating the need for data fusion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Detected text in video can provide a useful index with or with recognition [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30086203,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "7e3ed958f25974dab09b445fbd6f297852b761fc",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Like shot changes, the presence of text in digital video is an important event that can be used to index digital video and provide extremely useful semantic information about the scene content. The special characteristics of digital video compared to document images both require and allow new robust approaches to recognition of text in video. We discuss the characteristics and special challenges of text in video and present a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem. Preliminary results from our approach are presented."
            },
            "slug": "Indexing-text-events-in-digital-video-databases-Gargi-Antani",
            "title": {
                "fragments": [],
                "text": "Indexing text events in digital video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The characteristics and special challenges of text in video are discussed and a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Shim et al [13] detect artificial text from MPEG video by identifying homogeneous regions in intensity images, perform segmentation by interactive threshold selection, and apply heuristics to eliminate non-text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "those proposed in [4] and [7]) assume that the background is the largest component of the histogram."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "Jain and Yu [4] extract text from video frames and compressed images by background color separation and applying the heuristic that the text has high contrast with the background and is of the foreground color."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Lienhart and Stuber [8] have developed a system for automatic text recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34602043"
                        ],
                        "name": "L. Winger",
                        "slug": "L.-Winger",
                        "structuredName": {
                            "firstName": "Lowell",
                            "lastName": "Winger",
                            "middleNames": [
                                "LeRoy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Winger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143794424"
                        ],
                        "name": "M. Jernigan",
                        "slug": "M.-Jernigan",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jernigan",
                            "middleNames": [
                                "Ed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jernigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115466358"
                        ],
                        "name": "J. Robinson",
                        "slug": "J.-Robinson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Robinson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Currently, we have included work from Gargi et al [3], Chaddha et al [1], Winger et al [14], LeBourgeois [7] and Mitrea and deWith [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5218832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8283a9fefa9e3690cc2d64308c6bb61f55623371",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We are developing a portable text-to-speech system for the vision impaired. The input image is acquired with a lightweight CCD camera that may be poorly focused and aimed, and perhaps taken under inadequate and uneven illumination. We therefore require efficient and effective thresholding and segmentation methods which are robust with respect to character contrast, font, size, and format. In this paper, we present a fast thresholding scheme which combines a local variance measure with a logical stroke-width method. An efficient post- thresholding segmentation scheme utilizing Fisher's linear discriminant to distinguish noise and character components functions as an effective pre-processing step for the application of commercial segmentation and character recognition methods. The performance of this fast new method compared favorably with other methods for the extraction of characters from uncontrolled illumination, omnifont scene images. We demonstrate the suitability of this method for use in an automated portable reader through a software implementation running on a laptop 486 computer in our prototype device."
            },
            "slug": "Character-segmentation-and-thresholding-in-scene-Winger-Jernigan",
            "title": {
                "fragments": [],
                "text": "Character segmentation and thresholding in low-contrast scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a fast thresholding scheme which combines a local variance measure with a logical stroke-width method and demonstrates the suitability of this method for use in an automated portable reader through a software implementation running on a laptop 486 computer in the prototype device."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767238"
                        ],
                        "name": "Y. Nakajima",
                        "slug": "Y.-Nakajima",
                        "structuredName": {
                            "firstName": "Yasuyuki",
                            "lastName": "Nakajima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakajima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144415144"
                        ],
                        "name": "A. Yoneyama",
                        "slug": "A.-Yoneyama",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Yoneyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yoneyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796620"
                        ],
                        "name": "H. Yanagihara",
                        "slug": "H.-Yanagihara",
                        "structuredName": {
                            "firstName": "Hiromasa",
                            "lastName": "Yanagihara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yanagihara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769639"
                        ],
                        "name": "M. Sugano",
                        "slug": "M.-Sugano",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Sugano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sugano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Adjacent motion vectors are compared to reject those which are spatially inconsistent (high angular difference [10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58750105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcbac7ea4875d3e56886d4f1a4423bd3af30b2ef",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method of moving object detection directly from MPEG coded data. Since motion information in MPEG coded data is determined in terms of coding efficiency point of view, it does not always provide real motion information of objects. We use a wide variety of coding information including motion vectors and DCT coefficients to estimate real object motion. Since such information can be directly obtained from coded bitstream, very fast operation can be expected. Moving objects are detected basically analyzing motion vectors and spatio-temporal correlation of motion in P-, and B-pictures. Moving objects are also detected in intra macroblocks by analyzing coding characteristics of intra macroblocks in P- and B-pictures and by investigating temporal motion continuity in I-pictures. The simulation results show that successful moving object detection has been performed on macroblock level using several test sequences. Since proposed method is very simple and requires much less computational power than the conventional object detection methods, it has a significant advantage as motion analysis tool."
            },
            "slug": "Moving-object-detection-from-MPEG-coded-data-Nakajima-Yoneyama",
            "title": {
                "fragments": [],
                "text": "Moving-object detection from MPEG coded data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation results show that successful moving object detection has been performed on macroblock level using several test sequences and proposed method has a significant advantage as motion analysis tool."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Ohya et al [11] operate on gray scale images to perform character segmentation by local thresholding followed by pairing of nearby regions with similar gray levels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144851973"
                        ],
                        "name": "M. Kamel",
                        "slug": "M.-Kamel",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Kamel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500387699"
                        ],
                        "name": "A. Zhao",
                        "slug": "A.-Zhao",
                        "structuredName": {
                            "firstName": "Aiguo",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "Using the logical level method of Kamel and Zhao [5] which was proposed for low-contrast check images, we obtained good results by using it on both original and inverted images and choosing one of the results based on the size and distribution of the connected components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33047782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b786be0f6e167d58a9da67eca864113efbf6b01",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The extraction of binary character/graphics images from gray-scale document images with background pictures, shadows, highlight, smear, and smudge is a common critical image processing operation, particularly for document image analysis, optical character recognition, check image processing, image transmission, and videoconferencing. After a brief review of previous work with emphasis on five published extraction techniques, viz., a global thresholding technique, YDH technique, a nonlinear adaptive technique, an integrated function technique, and a local contrast technique, this paper presents two new extraction techniques: a logical level technique and a mask-based subtraction technique. With experiments on images of a typical check and a poor-quality text document, this paper systematically evaluates and analyses both new and published techniques with respect to six aspects, viz., speed, memory requirement, stroke width restriction, parameter number, parameter setting, and human subjective evaluation of result images. Experiments and evaluations have shown that one new technique is superior to the rest, suggesting its suitability for high-speed low-cost applications."
            },
            "slug": "Extraction-of-Binary-Character/Graphics-Images-from-Kamel-Zhao",
            "title": {
                "fragments": [],
                "text": "Extraction of Binary Character/Graphics Images from Grayscale Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents two new extraction techniques: a logical level technique and a mask-based subtraction technique, suggesting its suitability for high-speed low-cost applications."
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693680"
                        ],
                        "name": "N. Chaddha",
                        "slug": "N.-Chaddha",
                        "structuredName": {
                            "firstName": "Navin",
                            "lastName": "Chaddha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chaddha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153216710"
                        ],
                        "name": "R. Sharma",
                        "slug": "R.-Sharma",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075308385"
                        ],
                        "name": "A. Agrawal",
                        "slug": "A.-Agrawal",
                        "structuredName": {
                            "firstName": "Avneesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041740460"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Currently, we have included work from Gargi et al [3], Chaddha et al [1], Winger et al [14], LeBourgeois [7] and Mitrea and deWith [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Methods described in [1] and [9] have been modified as described below resulting in improved performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "The original method described in [1] computes an energy measure of a set of DCT coefficients of intra-coded blocks as a texture measure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58358347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9754dba62f5a329c9dd64b7d091c7900f489f5f0",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Block based algorithms have found widespread use in image and video compression. However, popular algorithms such as JPEG, which are very effective in compressing continuous tone images, do not perform well with mixed-mode images which have a substantial text component. With a growing number of applications where such images occur, e.g., color facsimile, digital libraries and educational videos, there are advantages in being able to classify each block as being text or continuous tone. With such a classification, different compression parameters or even algorithms may be employed for the two kinds of data to obtain high compression with minimal loss in visual quality. In this paper we analyze and compare four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods. Our evaluation of each scheme is based on the accuracy of segmentation, robustness across different types of images and sensitivity to the threshold used for segmentation. Our results show that DCT based segmentation offers the best accuracy and robustness. Another advantage of DCT is that it is compatible with standards like JPEG, MPEG and H.261.<<ETX>>"
            },
            "slug": "Text-segmentation-in-mixed-mode-images-Chaddha-Sharma",
            "title": {
                "fragments": [],
                "text": "Text segmentation in mixed-mode images"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper analyzes and compares four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods, and shows that DCTbased segmentation offers the best accuracy and robustness."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "All motion vectors that are labeled \u201cflat\u201d using an \u201cedginess\u201d criterion [12], are ignored."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1543016,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "69a84b03133a766cf0f60f6bab2d76b2fb0c93d0",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple and effective method to determine global camera motion using raw MPEG-1 motion vectors information obtained straight form real MPEG-1 streams such as those of the new HITACHI MP-EG1A digital camcorder. The simple approach we have experimented with robustly fits a global affine optic flow model to the motion vectors. Other more robust methods are also proposed. In order to cope with the group-of-frames (GOF) discontinuity of the MPEG stream, B frames are used backward to determine the 'missing link' to a previous GOF thereby ensuring continuity of the motion estimation across a reasonable number of frames. As a tested, we have applied the method to the image mosaicing problem, for which interesting results have been obtained. Although several other methods exists to perform camera motion estimation, the approach presented here is particularly interesting because exploits 'free' information present in MPEG streams and bypass the highly expensive correlation process."
            },
            "slug": "Using-raw-MPEG-motion-vectors-to-determine-global-Pilu",
            "title": {
                "fragments": [],
                "text": "Using raw MPEG motion vectors to determine global camera motion"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a simple and effective method to determine global camera motion using raw MPEG-1 motion vectors information obtained straight form real MPEG- 1 streams such as those of the new HITACHI MP-EG1A digital camcorder."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145573734"
                        ],
                        "name": "F. Lebourgeois",
                        "slug": "F.-Lebourgeois",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Lebourgeois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lebourgeois"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "those proposed in [4] and [7]) assume that the background is the largest component of the histogram."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Currently, we have included work from Gargi et al [3], Chaddha et al [1], Winger et al [14], LeBourgeois [7] and Mitrea and deWith [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "We have tried a few approaches at segmentation; LeBourgeois\u2019 method [7] for inter-character segmentation fails for overlapping or italic characters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29939979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8754ab68589b8e893d6962eb92c56300ecb764",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera. The system works with minimum assumptions on font, text location, size, color and the background scene. The text blocks localization in complex scenes using a specific filter which enhances any text from the background without binarization. A special stage is designed to separate characters, even touched by using gray-level information. The authors also extract gray-level features which make the algorithm more reliable, in particular under poor printing conditions or bad contrast digitization."
            },
            "slug": "Robust-multifont-OCR-system-from-gray-level-images-Lebourgeois",
            "title": {
                "fragments": [],
                "text": "Robust multifont OCR system from gray level images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera, with minimum assumptions on font, text location, size, color and the background scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405374270"
                        ],
                        "name": "M. Schaar-Mitrea",
                        "slug": "M.-Schaar-Mitrea",
                        "structuredName": {
                            "firstName": "Mihaela",
                            "lastName": "Schaar-Mitrea",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schaar-Mitrea"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "With complex backgrounds this assumption is invalid."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "Figure 2 shows the results of localization by two algorithms ([3] and [9]), indicating the need for data fusion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Mitrea and de With [9] proposed a simple algorithm to classify video frame blocks into graphics or video based on the dynamic range and variation of gray levels within the block."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Methods described in [1] and [9] have been modified as described below resulting in improved performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "Currently, we have included work from Gargi et al [3], Chaddha et al [1], Winger et al [14], LeBourgeois [7] and Mitrea and deWith [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62607491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4155a9423e46ed278ee1acde33c1a0e3054170a5",
            "isKey": true,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The diversity in TV images has augmented with the increased application of computer graphics. In this paper we study z coding system that supports both the lossless coding of such graphics data and regular lossy video compression. The lossless coding techniques are based on runlength and arithmetical coding. For video compression, we introduce a simple block predictive coding technique featuring individual pixel access, so that it enables a gradual shift from lossless coding of graphics to the lossy coding of video. An overall bit rate control completes the system. Computer simulations show a very high quality with a compression factor between 2-3.\u00a9 (1998) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."
            },
            "slug": "Compression-of-mixed-video-and-graphics-images-for-Schaar-Mitrea",
            "title": {
                "fragments": [],
                "text": "Compression of mixed video and graphics images for TV systems"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Z coding system that supports both the lossless coding of such graphics data and regular lossy video compression is studied and a simple block predictive coding technique featuring individual pixel access is introduced, so that it enables a gradual shift from lossed coding of graphics to the lossy coding of video."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228538"
                        ],
                        "name": "S. Devadiga",
                        "slug": "S.-Devadiga",
                        "structuredName": {
                            "firstName": "Sadashiva",
                            "lastName": "Devadiga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Devadiga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "Devadiga [2] has developed a recursive motion-based segmentation algorithm to segment image into regions corresponding to planar and other objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 65002183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfc9aadcc9db8c0afc1bd0e02629b6b7d96641ca",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to detect and locate runways/taxiways and obstacles in images captured using on-board sensors is an essential first step in the automation of low-altitude flight, landing, takeoff, and taxiing phase of aircraft navigation. Automation of these functions under different weather and lighting situations, can be facilitated by using sensors of different modalities. An aircraft-based Synthetic Vision System (SVS), with sensors of different modalities mounted on-board, complements the current ground-based systems in functions such as detection and prevention of potential runway collisions, airport surface navigation, and landing and takeoff in all weather conditions. In this report, we address the problem of detection of objects in monocular image sequences obtained from two types of sensors, a Passive Millimeter Wave (PMMW) sensor and a video camera mounted on-board a landing aircraft. Since the sensors differ in their spatial resolution, and the quality of the images obtained using these sensors is not the same, different approaches are used for detecting obstacles depending on the sensor type. These approaches are described separately in two parts of this report. The goal of the first part of the report is to develop a method for detecting runways/taxiways and objects on the runway in a sequence of images obtained from a moving PMMW sensor. Since the sensor resolution is low and the image quality is very poor, we propose a model-based approach for detecting runways/taxiways. We use the approximate runway model and the position information of the camera provided by the Global Positioning System (GPS) to define regions of interest in the image plane to search for the image features corresponding to the runway markers. Once the runway region is identified, we use histogram-based thresholding to detect obstacles on the runway and regions outside the runway. This algorithm is tested using image sequences simulated from a single real PMMW image. The camera position information provided by the GPS is not accurate. An alternative is to estimate the camera position using image-based features, such as points"
            },
            "slug": "Detection-of-obstacles-in-monocular-image-sequences-Devadiga",
            "title": {
                "fragments": [],
                "text": "Detection of obstacles in monocular image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This report addresses the problem of detection of objects in monocular image sequences obtained from two types of sensors, a Passive Millimeter Wave (PMMW) sensor and a video camera mounted on-board a landing aircraft."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682208"
                        ],
                        "name": "K. Kanatani",
                        "slug": "K.-Kanatani",
                        "structuredName": {
                            "firstName": "Kenichi",
                            "lastName": "Kanatani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kanatani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122997301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab1644cb9cf5cf4af70044d131e1c8be44f1b53f",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The three-dimensional shape of a polyhedron is reconstructed from its perspectively projected image by finding rectangular corners. If a corner is known to be rectangular, its three-dimensional configuration is analytically computed, and hence the surface gradients of the faces incident to it are also determined. In order to decide which corners are rectangular, the rectangularity heuristics are introduced, i.e. corners are assumed to be rectangular as long as no inconsistency arises as a result. An optimization technique is presented in order to cope with the problem of insufficiency of information and inconsistency due to error and noise.<<ETX>>"
            },
            "slug": "3D-recovery-of-polyhedra-by-rectangularity-Kanatani",
            "title": {
                "fragments": [],
                "text": "3D recovery of polyhedra by rectangularity heuristics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An optimization technique is presented in order to cope with the problem of insufficiency of information and inconsistency due to error and noise."
            },
            "venue": {
                "fragments": [],
                "text": "International Workshop on Industrial Applications of Machine Intelligence and Vision,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682208"
                        ],
                        "name": "K. Kanatani",
                        "slug": "K.-Kanatani",
                        "structuredName": {
                            "firstName": "Kenichi",
                            "lastName": "Kanatani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kanatani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 182
                            }
                        ],
                        "text": "The planar motion parameters can be used to compute the equation of the plane as well as the camera motion parameters, giving two solutions (except for a scale factor) in most cases [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60697397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efe44a5cadf1028723ba07681d49c406a7dea1b9",
            "isKey": false,
            "numCitedBy": 482,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Geometric-computation-for-machine-vision-Kanatani",
            "title": {
                "fragments": [],
                "text": "Geometric computation for machine vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-system-for-automatic-text-detection-in-video-Gargi-Crandall/1d784187382befa5cb50b62f10ddb87feb487102?sort=total-citations"
}