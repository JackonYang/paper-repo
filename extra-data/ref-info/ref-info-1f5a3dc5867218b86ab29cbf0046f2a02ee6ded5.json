{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35567771"
                        ],
                        "name": "K. Buescher",
                        "slug": "K.-Buescher",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Buescher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Buescher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151478680"
                        ],
                        "name": "P. R. Kumar",
                        "slug": "P.-R.-Kumar",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. R. Kumar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120177636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0f6f9caeba0bb5a15e29012b025e380c5b12d4f",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the problem of learning from examples in a framework that is based on, but more general than, Valiant's probably approximately correct (PAC) model for learning. In our framework, the learner observes examples that consist of sample points drawn and labeled according to a fixed, unknown probability distribution. Based on this empirical data, the learner must select, from a set of candidate functions, a particular function or \"hypothesis\" that will accurately predict the labels of future sample points. The expected mismatch between a hypothesis prediction and the label of a new sample point is called the hypothesis \"generalization error\". Following the pioneering work of Vapnik and Chervonenkis, others have attacked this sort of learning problem by finding hypotheses that minimize the relative frequency-based empirical error estimate. We generalize this approach by examining the \"simultaneous estimation\" problem. We demonstrate how one can learn from such a simultaneous error estimate and propose a new class of estimators called \"smooth estimators\". We characterize the class of simultaneous estimation problems solvable by a smooth estimator."
            },
            "slug": "Learning-by-canonical-smooth-estimation.-I.-Buescher-Kumar",
            "title": {
                "fragments": [],
                "text": "Learning by canonical smooth estimation. I. Simultaneous estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper examines the problem of learning from examples in a framework that is based on, but more general than, Valiant's probably approximately correct (PAC) model for learning and proposes a new class of estimators called \"smooth estimators\"."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Autom. Control."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "The results of section 4 show that it is possible to decompose the hypothesis class on the basis of theobserv d data in some cases: there we did it in terms of the margin attained."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11902833,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a6415df16c642be588aa1ab051a0a108bab7a482",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new general-purpose algorithm for learning classes of 0, 1-valued functions in a generalization of the prediction model and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi, and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each?>0, we establish weaker sufficient and stronger necessary conditions for a class of 0, 1-valued functions to be agnostically learnable to within?and to be an?-uniform Glivenko?Cantelli class."
            },
            "slug": "Prediction,-Learning,-Uniform-Convergence,-and-Bartlett-Long",
            "title": {
                "fragments": [],
                "text": "Prediction, Learning, Uniform Convergence, and Scale-Sensitive Dimensions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi, and Haussler is proved."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35567771"
                        ],
                        "name": "K. Buescher",
                        "slug": "K.-Buescher",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Buescher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Buescher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151478680"
                        ],
                        "name": "P. R. Kumar",
                        "slug": "P.-R.-Kumar",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. R. Kumar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6540475,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "996ada3d17d3a7b184cf96f6d82ed1f46dec30d1",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the problem of learning from examples in a framework that is based on, but more general than, Valiant's probably approximately correct (PAC) model for learning. In our framework, the Learner observes examples that consist of sample points drawn and labeled according to a fixed, unknown probability distribution. Based on this empirical data, the learner must select, from a set of candidate functions, a par- ticular function or \"hypothesis\" that will accurately predict the labels of future sample points. The expected mismatch between a hypothesis' prediction and the label of a new sample point is called the hypothesis' \"generalization error.\" Following the pioneering work of Vapnik and Chervonenkis, others have attacked this sort of learning problem by finding hypotheses that minimize the relative frequency-based empirical error estimate. We generalize this approach by examining the \"simultaneous estimation\" problem: When does some procedure exist for estimating the generalization error of all of the candidate hypotheses, simultaneously, from the same labeled sample? We demonstrate how one can learn from such a simultaneous error estimate and propose a new class of estimators called \"smooth estimators\" that, in many cases of interest, contains the empirical estimator. We characterize the class of simultaneous estimation problems solvable by a smooth estimator and give a canonical form for the smooth simultaneous estimator."
            },
            "slug": "Learning-by-Canonical-Smooth-Es-timation-Part-I:-Buescher-Kumar",
            "title": {
                "fragments": [],
                "text": "Learning by Canonical Smooth Es timation-Part I: Simultaneous Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35567771"
                        ],
                        "name": "K. Buescher",
                        "slug": "K.-Buescher",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Buescher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Buescher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151478680"
                        ],
                        "name": "P. R. Kumar",
                        "slug": "P.-R.-Kumar",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. R. Kumar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 48201742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7a198e168ce5fb3cab1d56763885f3185d36b2",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the properties of a procedure for learning from examples. This \"canonical learner\" is based on a canonical error estimator developed in Part I. In learning problems one can observe data that consists of labeled sample points, and the goal is to find a model or \"hypothesis\" from a set of candidates that will accurately predict the labels of new sample points. The expected mismatch between a hypothesis prediction and the actual label of a new sample point is called the hypothesis \"generalization error\". We compare the canonical learner with the traditional technique of finding hypotheses that minimize the relative frequency-based empirical error estimate. It is shown that for a broad class of learning problems, the set of cases for which such empirical error minimization works is a proper subset of the cases for which the canonical learner works. We derive bounds to show that the number of samples required by these two methods is comparable. We also address the issue of how to determine the appropriate complexity for the class of candidate hypotheses."
            },
            "slug": "Learning-by-canonical-smooth-estimation.-II.-and-of-Buescher-Kumar",
            "title": {
                "fragments": [],
                "text": "Learning by canonical smooth estimation. II. Learning and choice of model complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper analyzes the properties of a procedure for learning from examples based on a canonical error estimator developed in Part I, and shows that for a broad class of learning problems, the set of cases for which such empirical error minimization works is a proper subset of the Cases for which the canonical learner works."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Autom. Control."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734327"
                        ],
                        "name": "N. Alon",
                        "slug": "N.-Alon",
                        "structuredName": {
                            "firstName": "Noga",
                            "lastName": "Alon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409749316"
                        ],
                        "name": "S. Ben-David",
                        "slug": "S.-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "The results of section 4 show that it is possible to decompose the hypothesis class on the basis of theobserv d data in some cases: there we did it in terms of the margin attained."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Ifx;y 2 Xm, we denote byxy their concatenation(x1; : : : ; xm; y1; : : : ; ym)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8347198,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a5a86656448540a91ec06dbe017231d16862a502",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Gliveako-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to characterize PAC learnability in the statistical regression framework of probabilistic concepts, solving an open problem posed by Kearns and Schapire. Our characterization shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.<<ETX>>"
            },
            "slug": "Scale-sensitive-dimensions,-uniform-convergence,-Alon-Ben-David",
            "title": {
                "fragments": [],
                "text": "Scale-sensitive dimensions, uniform convergence, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A characterization of learnability in the probabilistic concept model, solving an open problem posed by Kearns and Schapire, and shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Linial, Mansour and Rivest [29] studied learning in a framework as above by allowing the learner to seek a consistent hypothesis in each subclass Hd in turn, drawing enough extra examples at each stage to ensure the correct level of accuracy and confidence should a consistent hypothesis be found."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[29]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28816563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf8f8af590f63549c17df967aede65bbab80f2a0",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Results-on-learnability-and-the-dimension-Linial-Mansour",
            "title": {
                "fragments": [],
                "text": "Results on learnability and the Vapnick-Chervonenkis dimension"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 [43] Let Hi, i be a sequence of hypothesis classes mapping X to f g such that VCdim Hi i, and let P be a probability distribution on X ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some of the results of this paper appeared in [43]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The following theorem, which appears in [43], covers the case where there are no errors on the training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5280896,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "67f9e3de2fb39f051ef23b8fbed6d72de7b02900",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces a framework for studying structural risk minimisation. The model views structural risk minimisation in a PAC context. It then considers the more general case when the hierarchy of classes is chosen in response to the data. This theoretically explains the impressive performance of the maximal margin hyperplane algorithm of Vapnik. It may also provide a general technique for exploitingserendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "slug": "A-framework-for-structural-risk-minimisation-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "A framework for structural risk minimisation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The paper introduces a framework for studying structural risk minimisation in a PAC context and considers the more general case when the hierarchy of classes is chosen in response to the data."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "The margin occurs even more explictly in the Winnow algorithms and their varia nts developed by Littlestone and others [30, 31, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dace286582d91916fe470d08f30381cf453f20",
            "isKey": false,
            "numCitedBy": 1612,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space."
            },
            "slug": "Learning-Quickly-When-Irrelevant-Attributes-Abound:-Littlestone",
            "title": {
                "fragments": [],
                "text": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": false,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Linial, Mansour and Rivest [29] studied learning in a framework as above by allowing the learner to seek a consistent hypothesis in each subclassHd in turn, drawing enough extra examples at each stage to ensure the correct level of accuracy and confidence should a consistent hypothesis be found."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Linial, Mansour and Rivest [29] studied learning in a framework as above by al lowing the learner to seek a consistent hypothesis in each subclass Hd in turn, drawing enough extra examples at each stage to ensure the correct level of accuracy and confidence shoul d a consistent hypothesis be found."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12405514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "907f58a77ef4909d0e96c352cd5c7379eb22d5f5",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a concept from examples in a distribution-free model is considered. The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension. An important variation on the problem of learning from examples, called approximating from examples, is also discussed. The problem of computing the VC dimension of a finite concept set defined on a finite domain is considered.<<ETX>>"
            },
            "slug": "Results-on-learnability-and-the-Vapnik-Chervonenkis-Linial-Mansour",
            "title": {
                "fragments": [],
                "text": "Results on learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced and is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48491389"
                        ],
                        "name": "L. Gurvits",
                        "slug": "L.-Gurvits",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Gurvits",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gurvits"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710207"
                        ],
                        "name": "P. Koiran",
                        "slug": "P.-Koiran",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Koiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koiran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2100523,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "f57e37b749dacb987e2e14d1e0e4ee13db599798",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a fairly general method for constructing classes of functions of finite scale-sensitive dimension (the scale-sensitive dimension is a generalization of the Vapnik-Chervonenkis dimension to real-valued functions). The construction is as follows: start from a class F of functions of finite VC dimension, take the convex hull coF of F, and then take the closure \\({\\cal L}\\) of coF in an appropriate sense. As an example, we study in more detail the case where F is the class of threshold functions. It is shown that \\({\\cal L}\\) includes two important classes of functions: \n \n \nneural networks with one hidden layer and bounded output weights; \n \n \nthe so-called \u0393 class of Barron, which was shown to satisfy a number of interesting approximation and closure properties."
            },
            "slug": "Approximation-and-Learning-of-Convex-Superpositions-Gurvits-Koiran",
            "title": {
                "fragments": [],
                "text": "Approximation and Learning of Convex Superpositions"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A fairly general method for constructing classes of functions of finite scale-sensitive dimension, which includes the so-called \u0393 class of Barron, which was shown to satisfy a number of interesting approximation and closure properties."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751563"
                        ],
                        "name": "K. Zeger",
                        "slug": "K.-Zeger",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Zeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Zeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5683813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f78a09f13c2fcb7168e0533156fa9700d577af",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply the method of complexity regularization to learn concepts from large concept classes. The method is shown to automatically find the best balance between the approximation error and the estimation error. In particular, the error probability of the obtained classifier is shown to decrease as 0(/spl radic/(log n/n)) to the achievable optimum, for large nonparametric classes of distributions, as the sample size n grows. In pattern recognition, or concept learning, the value of a {0,1}-valued random variable Y is to be predicted based upon observing an R/sup d/-valued random variable X."
            },
            "slug": "Concept-learning-using-complexity-regularization-Lugosi-Zeger",
            "title": {
                "fragments": [],
                "text": "Concept learning using complexity regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The method of complexity regularization is shown to automatically find the best balance between the approximation error and the estimation error, for large nonparametric classes of distributions, as the sample size n grows."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Symposium on Information Theory"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794435"
                        ],
                        "name": "A. Nobel",
                        "slug": "A.-Nobel",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Nobel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16950903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f66bfcf59f56e10d7d0cc0d50168e257cb5a094",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Given n independent replicates of a jointly distributed pair (X,Y) in Rd times R, we wish to select from a fixed sequence of model classes F1,F2... a deterministic prediction rule f: Rd to R whose risk is small. We investigate the possibility of empirically assessing the complexity of each model class, that is, the actual difficulty of the estimation problem within each class. The estimated complexities are in turn used to define an adaptive model selection procedure, which is based on complexity penalized empirical risk. The available data are divided into two parts. The first is used to form an empirical cover of each model class, and the second is used to select a candidate rule from each cover based on empirical risk. The covering radii are determined empirically to optimize a tight upper bound on the estimation error. An estimate is chosen from the list of candidates in order to minimize the sum of class complexity and empirical risk. A distinguishing feature of the approach is that the complexity of each model class is assessed empirically, based on the size of its empirical cover. Finite sample performance bounds are established for the estimates, and these bounds are applied to several non-parametric estimation problems. The estimates are shown to achieve a favorable tradeoff between approximation and estimation error, and to perform as well as if the distribution-dependent complexities of the model classes were known beforehand. In addition, it is shown that the estimate can be consistent, and even possess near optimal rates of convergence, when each model class has an infinite VC or pseudo dimension. For regression estimation with squared loss we modify our estimate to achieve a faster rate of convergence."
            },
            "slug": "Adaptive-Model-Selection-Using-Empirical-Lugosi-Nobel",
            "title": {
                "fragments": [],
                "text": "Adaptive Model Selection Using Empirical Complexities"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The estimates are shown to achieve a favorable tradeoff between approximation and estimation error, and to perform as well as if the distribution-dependent complexities of the model classes were known beforehand, when each model class has an infinite VC or pseudo dimension."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14313855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04641b38cdab7b5b6c7d0d09193d3220ef40efc5",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of Structural Risk Minimization refers to tuning the capacity of the classifier to the available amount of training data. This capacity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) learning algorithm. Actions based on these three factors are combined here to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition."
            },
            "slug": "Structural-Risk-Minimization-for-Character-Guyon-Vapnik",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization for Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The method of Structural Risk Minimization is used to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "The results of section 4 show that it is possible to decompose the hypothesis class on the basis of theobserv d data in some cases: there we did it in terms of the margin attained."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2551295,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fc48a9403b5d01a2d1724d4e04218a4a9b78cb3a",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning real-valued functions from random examples when the function values are corrupted with noise. With mild conditions on independent observation noise, we provide characterizations of the learnability of a real-valued function class in terms of a generalization of the Vapnik-Chervonenkis dimension, the fat shattering function, introduced by Kearns and Schapire. We show that, given some restrictions on the noise, a function class is learnable in our model if and only if its fat-shattering function is finite. With different (also quite mild) restrictions, satisfied for example by gaussian noise, we show that a function class is learnable from polynomially many examples if and only if its fat-shattering function grows polynomially. We prove analogous results in an agnostic setting, where there is no assumption of an underlying function class."
            },
            "slug": "Fat-shattering-and-the-learnability-of-real-valued-Bartlett-Long",
            "title": {
                "fragments": [],
                "text": "Fat-shattering and the learnability of real-valued functions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, given some restrictions on the noise, a function class is learnable in this model if and only if its fat-shattering function is finite, and analogous results in an agnostic setting, where there is no assumption of an underlying function class."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157435397"
                        ],
                        "name": "M. Jones",
                        "slug": "M.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098093944"
                        ],
                        "name": "Tomaso PoggioCenter",
                        "slug": "Tomaso-PoggioCenter",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "PoggioCenter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomaso PoggioCenter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12132966,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "96af5da062cee7ce50fc0624654c61363506772b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units, called Regularization Networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known Radial Basis Functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to diierent classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends Radial Basis Functions (RBF) to Hyper Basis Functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of Projection Pursuit Regression and several types of neural networks. We propose to use the term Generalized Regularization Networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the diierent classes of basis functions correspond to diierent classes of prior probabilities on the approximating function spaces, and therefore to diierent types of smoothness assumptions. In summary, diierent multilayer networks with one hidden layer, which we collectively call Generalized Regularization Networks, correspond to diierent classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are a) Radial Basis Functions that can be generalized to Hyper Basis Functions, b) some tensor product splines, and c) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions and several perceptron-like neural networks with one-hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Diierent multilayer networks with one hidden layer, which are collectively called Generalized Regularization Networks, correspond to diierent classes of priors and associated smoothness functionals in a classical regularization principle."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118425960,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8b7d60f49e7a5368920457c885c813a912c34997",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts of universal data compression lead to minimum description-length criteria for parsimonious statistical model selection for the estimation of functions. In this paper we define general complexity regularization criteria and establish bounds on the statistical risk of the estimated functions. These bounds establish consistency, yield rates of convergence, and demonstrate the near asymptotic optimality of the model selection criterion in both parametric and nonparametric cases. A fundamental role is played by an index of resolvability that quantifies the tradeoff between complexity and accuracy of candidate models. Applications are given to polynomial regression and artificial neural networks."
            },
            "slug": "Complexity-Regularization-with-Application-to-Barron",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Application to Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper defines general complexity regularization criteria and establishes bounds on the statistical risk of the estimated functions and establishes consistency, yield rates of convergence, and the near asymptotic optimality of the model selection criterion in both parametric and nonparametric cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436284"
                        ],
                        "name": "Chris Mesterharm",
                        "slug": "Chris-Mesterharm",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Mesterharm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Mesterharm"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The margin occurs even more explictly in the Winnow algorithms and their variants developed by Littlestone and others [30, 31, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12776172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "561b14f61beab6ec7bca222cb8e05e4e7d303d97",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a mistake-driven variant of an on-line Bayesian learning algorithm (similar to one studied by Cesa-Bianchi, Helmbold, and Panizza [CHP96]). This variant only updates its state (learns) on trials in which it makes a mistake. The algorithm makes binary classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner. We have been able to show, theoretically and in simulations, that this algorithm performs well under assumptions quite different from those embodied in the prior of the original Bayesian algorithm. It can handle situations that we do not know how to handle in linear time with Bayesian algorithms. We expect our techniques to be useful in deriving and analyzing other apobayesian algorithms."
            },
            "slug": "An-Apobayesian-Relative-of-Winnow-Littlestone-Mesterharm",
            "title": {
                "fragments": [],
                "text": "An Apobayesian Relative of Winnow"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work studies a mistake-driven variant of an on-line Bayesian learning algorithm that makes binary classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1365,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710207"
                        ],
                        "name": "P. Koiran",
                        "slug": "P.-Koiran",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Koiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koiran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2155092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7363600ced1e7d64c1fe54f42e0998cfe72a4759",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that neural networks which use continuous activation functions have VC dimension at least as large as the square of the number of weightsw. This results settles a long-standing open question, namely whether the well-knownO(wlogw) bound, known for hard-threshold nets, also held for more general sigmoidal nets. Implications for the number of samples needed for valid generalization are discussed."
            },
            "slug": "Neural-Networks-with-Quadratic-VC-Dimension-Koiran-Sontag",
            "title": {
                "fragments": [],
                "text": "Neural Networks with Quadratic VC Dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "It is shown that neural networks which use continuous activation functions have VC dimension at least as large as the square of the number of weightsw, which settles a long-standing open question."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "This result is a special case of Corollary 6 in [2], which applied more generally to arbitrary real-valued target functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "As in [2], we will use thel distance over a finite sample x x xm for the pseudometric in the space of functions, dx f g max i jf xi g xi j We writeN F x for the -covering number of F with respect to the pseudo-metric dx."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 141
                            }
                        ],
                        "text": "The main tool we use is the fat-shattering dimension, which was introduced in [26], and has been used for several problems in learning since [1, 11, 2, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "(This application to classification problems was not described in [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10454061,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07e5be1e9339af65685b08a84a9d0fc9153507af",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study a statistical property of classes of real-valued functions that we call approximation from interpolated examples. We derive a characterization of function classes that have this property, in terms of their \u2018fat-shattering function\u2019, a notion that has proved useful in computational learning theory. The property is central to a problem of learning real-valued functions from random examples in which we require satisfactory performance from every algorithm that returns a function which approximately interpolates the training examples."
            },
            "slug": "Function-Learning-from-Interpolation-Anthony-Bartlett",
            "title": {
                "fragments": [],
                "text": "Function Learning from Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper requires that, for most training samples, with high probability the absolute difference between the values of the learner's hypothesis and the target function on a random point is small."
            },
            "venue": {
                "fragments": [],
                "text": "Combinatorics, Probability and Computing"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14332165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks."
            },
            "slug": "Probable-networks-and-plausible-predictions-a-of-Mackay",
            "title": {
                "fragments": [],
                "text": "Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15348764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9642a175637a400b425f0ac0cb6a2b067cc8fe6b",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning is posed as a problem of function estimation, for which two principles of solution are considered: empirical risk minimization and structural risk minimization. These two principles are applied to two different statements of the function estimation problem: global and local. Systematic improvements in prediction power are illustrated in application to zip-code recognition."
            },
            "slug": "Principles-of-Risk-Minimization-for-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Principles of Risk Minimization for Learning Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Systematic improvements in prediction power and empirical risk minimization are illustrated in application to zip-code recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10664055,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c677b8adf2899169f21faec274e17b1c18180cc7",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "For classes of concepts defined by certain classes of analytic functions depending on n parameters, there are nonempty open sets of samples of length 2n + 2 that cannot be shattered. A slighly weaker result is also proved for piecewise-analytic functions. The special case of neural networks is discussed."
            },
            "slug": "Shattering-All-Sets-of-k-Points-in-General-Position-Sontag",
            "title": {
                "fragments": [],
                "text": "Shattering All Sets of k Points in General Position Requires (k 1)/2 Parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "For classes of concepts defined by certain classes of analytic functions depending on n parameters, there are nonempty open sets of samples of length 2n + 2 that cannot be shattered."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071092667"
                        ],
                        "name": "M\u00e1rta Pint\u00e9r",
                        "slug": "M\u00e1rta-Pint\u00e9r",
                        "structuredName": {
                            "firstName": "M\u00e1rta",
                            "lastName": "Pint\u00e9r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rta Pint\u00e9r"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "In addition to the above two motivations, the approach mirrors closely that ta ken in a recent paper by Lugosi and Pint\u00e9r [36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 103
                            }
                        ],
                        "text": "In addition to the above two motivations, the approach mirrors closely that taken in a recent paper by Lugosi and Pinte\u0301r [36]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3027744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10e59b273d8fc2866df6d4655e4bf65d1d05666e",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and analyze a method for learning based on minimizing the empirical risk over a carefully selected \u201cskeleton\u201d of a class of regression functions (concepts). The skeleton is a covering of the class based on a datadependent metric, especially fitted for classification. The method is shown to outperform empirical risk minimization and other non-datadependent skeleton-based estimates."
            },
            "slug": "A-data-dependent-skeleton-estimate-for-learning-Lugosi-Pint\u00e9r",
            "title": {
                "fragments": [],
                "text": "A data-dependent skeleton estimate for learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A method for learning based on minimizing the empirical risk over a carefully selected \u201cskeleton\u201d of a class of regression functions (concepts) is introduced and shown to outperform empirical risk minimization and other non-datadependent skeleton-based estimates."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706352"
                        ],
                        "name": "N. Biggs",
                        "slug": "N.-Biggs",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Biggs",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Biggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Using Lemma 3.2 from [ 42 ] with , the above holds provided"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15276148,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "03d0b7a3547ec946e89129bc4832af50d66836af",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bounding-Sample-Size-with-the-Vapnik-Chervonenkis-Shawe-Taylor-Anthony",
            "title": {
                "fragments": [],
                "text": "Bounding Sample Size with the Vapnik-Chervonenkis Dimension"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14460436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fca98082fa9ff8e9dbae9922491ae54976a0ccef",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an index of resolvability that is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length. The results on the index of resolvability demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference. The minimum complexity estimator converges to true density nearly as fast as an estimator based on prior knowledge of the true subclass of densities. Interpretations and basic properties of minimum complexity estimators are discussed. Some regression and classification problems that can be examined from the minimum description-length framework are considered. >"
            },
            "slug": "Minimum-complexity-density-estimation-Barron-Cover",
            "title": {
                "fragments": [],
                "text": "Minimum complexity density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An index of resolvability is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length to demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2679783"
                        ],
                        "name": "M. Hassoun",
                        "slug": "M.-Hassoun",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hassoun",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hassoun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21254772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "049aca6228fb68a263369380eda6d9a4fcbdb382",
            "isKey": false,
            "numCitedBy": 2176,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nAs book review editor of the IEEE Transactions on Neural Networks, Mohamad Hassoun has had the opportunity to assess the multitude of books on artificial neural networks that have appeared in recent years. Now, in Fundamentals of Artificial Neural Networks, he provides the first systematic account of artificial neural network paradigms by identifying clearly the fundamental concepts and major methodologies underlying most of the current theory and practice employed by neural network researchers. \nSuch a systematic and unified treatment, although sadly lacking in most recent texts on neural networks, makes the subject more accessible to students and practitioners. Here, important results are integrated in order to more fully explain a wide range of existing empirical observations and commonly used heuristics. There are numerous illustrative examples, over 200 end-of-chapter analytical and computer-based problems that will aid in the development of neural network analysis and design skills, and a bibliography of nearly 700 references. \nProceeding in a clear and logical fashion, the first two chapters present the basic building blocks and concepts of artificial neural networks and analyze the computational capabilities of the basic network architectures involved. Supervised, reinforcement, and unsupervised learning rules in simple nets are brought together in a common framework in chapter three. The convergence and solution properties of these learning rules are then treated mathematically in chapter four, using the \"average learning equation\" analysis approach. This organization of material makes it natural to switch into learning multilayer nets using backpropand its variants, described in chapter five. Chapter six covers most of the major neural network paradigms, while associative memories and energy minimizing nets are given detailed coverage in the next chapter. The final chapter takes up Boltzmann machines and Boltzmann learning along with other global search/optimization algorithms such as stochastic gradient search, simulated annealing, and genetic algorithms."
            },
            "slug": "Fundamentals-of-Artificial-Neural-Networks-Hassoun",
            "title": {
                "fragments": [],
                "text": "Fundamentals of Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Fundamentals of Artificial Neural Networks provides the first systematic account of artificial neural network paradigms by identifying clearly the fundamental concepts and major methodologies underlying most of the current theory and practice employed by neural network researchers."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14670844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df99a5e005c95ab367e45666ea4a36288ba88003",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian model comparison framework is reviewed, and the Bayesian Occam's razor is explained. This framework can be applied to feedforward networks, making possible (1) objective comparisons between solutions using alternative network architectures; (2) objective choice of magnitude and type of weight decay terms; (3) quantified estimates of the error bars on network parameters and on network output. The framework also generates a measure of the effective number of parameters determined by the data. \n \nThe relationship of Bayesian model comparison to recent work on prediction of generalisation ability (Guyon et al., 1992, Moody, 1992) is discussed."
            },
            "slug": "Bayesian-Model-Comparison-and-Backprop-Nets-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Model Comparison and Backprop Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The Bayesian model comparison framework is reviewed, and the Bayesian Occam's razor is explained, making possible objective comparisons between solutions using alternative network architectures."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751563"
                        ],
                        "name": "K. Zeger",
                        "slug": "K.-Zeger",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Zeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Zeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5860393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "033b5755745d75f595ab1ecbf28a8c1cd97d25bb",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "A general notion of universal consistency of nonparametric estimators is introduced that applies to regression estimation, conditional median estimation, curve fitting, pattern recognition, and learning concepts. General methods for proving consistency of estimators based on minimizing the empirical error are shown. In particular, distribution-free almost sure consistency of neural network estimates and generalized linear estimators is established. >"
            },
            "slug": "Nonparametric-estimation-via-empirical-risk-Lugosi-Zeger",
            "title": {
                "fragments": [],
                "text": "Nonparametric estimation via empirical risk minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A general notion of universal consistency of nonparametric estimators is introduced that applies to regression estimation, conditional median estimation, curve fitting, pattern recognition, and learning concepts and distribution-free almost sure consistency of neural network estimates and generalized linear estimators are established."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2508221"
                        ],
                        "name": "A. V. D. Vaart",
                        "slug": "A.-V.-D.-Vaart",
                        "structuredName": {
                            "firstName": "Aad",
                            "lastName": "Vaart",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. D. Vaart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144952368"
                        ],
                        "name": "J. Wellner",
                        "slug": "J.-Wellner",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Wellner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wellner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We assume with no further discussion \u201cpermissibility\u201d of the function classes involved (see Appendix C of [41] and section 2.3 of [ 45 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117819015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "45ee7447b9dd406496c4a5d9d8fb6556366a01c6",
            "isKey": true,
            "numCitedBy": 5375,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1.1. Introduction.- 1.2. Outer Integrals and Measurable Majorants.- 1.3. Weak Convergence.- 1.4. Product Spaces.- 1.5. Spaces of Bounded Functions.- 1.6. Spaces of Locally Bounded Functions.- 1.7. The Ball Sigma-Field and Measurability of Suprema.- 1.8. Hilbert Spaces.- 1.9. Convergence: Almost Surely and in Probability.- 1.10. Convergence: Weak, Almost Uniform, and in Probability.- 1.11. Refinements.- 1.12. Uniformity and Metrization.- 2.1. Introduction.- 2.2. Maximal Inequalities and Covering Numbers.- 2.3. Symmetrization and Measurability.- 2.4. Glivenko-Cantelli Theorems.- 2.5. Donsker Theorems.- 2.6. Uniform Entropy Numbers.- 2.7. Bracketing Numbers.- 2.8. Uniformity in the Underlying Distribution.- 2.9. Multiplier Central Limit Theorems.- 2.10. Permanence of the Donsker Property.- 2.11. The Central Limit Theorem for Processes.- 2.12. Partial-Sum Processes.- 2.13. Other Donsker Classes.- 2.14. Tail Bounds.- 3.1. Introduction.- 3.2. M-Estimators.- 3.3. Z-Estimators.- 3.4. Rates of Convergence.- 3.5. Random Sample Size, Poissonization and Kac Processes.- 3.6. The Bootstrap.- 3.7. The Two-Sample Problem.- 3.8. Independence Empirical Processes.- 3.9. The Delta-Method.- 3.10. Contiguity.- 3.11. Convolution and Minimax Theorems.- A. Appendix.- A.1. Inequalities.- A.2. Gaussian Processes.- A.2.1. Inequalities and Gaussian Comparison.- A.2.2. Exponential Bounds.- A.2.3. Majorizing Measures.- A.2.4. Further Results.- A.3. Rademacher Processes.- A.4. Isoperimetric Inequalities for Product Measures.- A.5. Some Limit Theorems.- A.6. More Inequalities.- A.6.1. Binomial Random Variables.- A.6.2. Multinomial Random Vectors.- A.6.3. Rademacher Sums.- Notes.- References.- Author Index.- List of Symbols."
            },
            "slug": "Weak-Convergence-and-Empirical-Processes:-With-to-Vaart-Wellner",
            "title": {
                "fragments": [],
                "text": "Weak Convergence and Empirical Processes: With Applications to Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter discusses Convergence: Weak, Almost Uniform, and in Probability, which focuses on the part of Convergence of the Donsker Property which is concerned with Uniformity and Metrization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "A further motivation can be seen from the distribution dependent learning descri b d in [5], where it is shown that classes which have infinite VC dimension may still be learnable provided that the distribution is sufficiently concentrated on regions of the input space wher e the set of hypotheses has low VC dimension."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17376135,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "90161ef93261603d07458a5079d01f1c87e392e0",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Sufficient-Condition-for-Polynomial-Learnability-Anthony-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "A Sufficient Condition for Polynomial Distribution-dependent Learnability"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143689789"
                        ],
                        "name": "D. Pollard",
                        "slug": "D.-Pollard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We assume with no further discussion \u201cpermissibility\u201d of the function classes involved (see Appendix C of [41] and section 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122104450,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0fca96e6ad7adf6b79ef5eb60e0360c20f8b3311",
            "isKey": false,
            "numCitedBy": 1852,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I Functional on Stochastic Processes.- 1. Stochastic Processes as Random Functions.- Notes.- Problems.- II Uniform Convergence of Empirical Measures.- 1. Uniformity and Consistency.- 2. Direct Approximation.- 3. The Combinatorial Method.- 4. Classes of Sets with Polynomial Discrimination.- 5. Classes of Functions.- 6. Rates of Convergence.- Notes.- Problems.- III Convergence in Distribution in Euclidean Spaces.- 1. The Definition.- 2. The Continuous Mapping Theorem.- 3. Expectations of Smooth Functions.- 4. The Central Limit Theorem.- 5. Characteristic Functions.- 6. Quantile Transformations and Almost Sure Representations.- Notes.- Problems.- IV Convergence in Distribution in Metric Spaces.- 1. Measurability.- 2. The Continuous Mapping Theorem.- 3. Representation by Almost Surely Convergent Sequences.- 4. Coupling.- 5. Weakly Convergent Subsequences.- Notes.- Problems.- V The Uniform Metric on Spaces of Cadlag Functions.- 1. Approximation of Stochastic Processes.- 2. Empirical Processes.- 3. Existence of Brownian Bridge and Brownian Motion.- 4. Processes with Independent Increments.- 5. Infinite Time Scales.- 6. Functional of Brownian Motion and Brownian Bridge.- Notes.- Problems.- VI The Skorohod Metric on D(0, ?).- 1. Properties of the Metric.- 2. Convergence in Distribution.- Notes.- Problems.- VII Central Limit Theorems.- 1. Stochastic Equicontinuity.- 2. Chaining.- 3. Gaussian Processes.- 4. Random Covering Numbers.- 5. Empirical Central Limit Theorems.- 6. Restricted Chaining.- Notes.- Problems.- VIII Martingales.- 1. A Central Limit Theorem for Martingale-Difference Arrays.- 2. Continuous Time Martingales.- 3. Estimation from Censored Data.- Notes.- Problems.- Appendix A Stochastic-Order Symbols.- Appendix B Exponential Inequalities.- Notes.- Problems.- Appendix C Measurability.- Notes.- Problems.- References.- Author Index."
            },
            "slug": "Convergence-of-stochastic-processes-Pollard",
            "title": {
                "fragments": [],
                "text": "Convergence of stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079246689"
                        ],
                        "name": "Axthonv G. Oettinger",
                        "slug": "Axthonv-G.-Oettinger",
                        "structuredName": {
                            "firstName": "Axthonv",
                            "lastName": "Oettinger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axthonv G. Oettinger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207762321,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "04a5241e96d0e7ac63eb44ed8cfbcdcda9df1583",
            "isKey": false,
            "numCitedBy": 1598,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since at most T of these equations can come from (7), at least (m T) of them come from (8\u2019). Thus, for the distribution (PI\u2019, . . . , p,\u2018), at most T of the probabilities are nonzero. The proof of our contention now follows by letting plo, . . . , 10~0 in the above argument be a distribution for which channel capacity is attained. It should be remarked that this proof provides (via the Simplex Method of linear programming) a means for reducing any transmitter with more than r symbols, to another, equally good or better, with at most T symbols. For a discussion of the possibility of reducing the number of receiver symbols, see Feinstein.5"
            },
            "slug": "IEEE-TRANSACTIONS-ON-INFORMATION-THEORY-Oettinger",
            "title": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON INFORMATION THEORY"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42792,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "This work was carried out in part whilst John Shawe-Taylor and Martin Anthony were visiting the Australian National University, and whilst Robert Williamson was visiting Royal Holloway and Bedford New College, University of London."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved ver sion due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 89
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved version due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10570532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306c40863d0c9d57b238b980bb288b008425b475",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Result-of-Vapnik-with-Applications-Anthony-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "A Result of Vapnik with Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Indeed, the results reported in Sections 2 and 3 of this paper are implicit inthe cited references, but our treatment serves to introduce the main results of the paper in lat r sections, and we make explicit some of the assumptions implicit in the presentations in [48,50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Example 5.1 Consider the hierarchy of classes introduced in Section 2 and defineU(x; h) = minfd : h 2 Hdg: Then it follows from Sauer\u2019s lemma that for anyx we can bound\u0300(x; h) by`(x; h) emd d ; whered = U(x; h)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In Section 5 we apply this to the case con idered by Vapnik: separating hyperplanes with a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Second Example\nThe second example we consider involves examples lying on hyperplanes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In Section 6 we introduce a more general framework which allows a rather large class of methods of measuring the luckiness of a sample, in the sense that the large margin is \u201clucky\u201d."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 258
                            }
                        ],
                        "text": "The theory characterises when a target function fromH can be learned from examples in terms of the VapnikChervonenkis dimension, a measure of the flexibility of the classH and specifies sample sizes required to deliver the required accuracy with the allowed confidence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "In thissection, we introduce a more general framework which subsumes the standard PAC model, the frameworkdesc ibed in Section 2 and can recover (in a slightly weaker form) the results of Section 4 as a special case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In Section 4 we prove a result which shows that if one achieves corr ct classification of some training data with a class off0; 1g-valued functions which are thresholded, and if the values of the real-valued functions on the training points are all well away from zero, then there is a bound on the generalization error which can be much better than the one obtained from the VC-dimension of the thresholded class."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In Section 7 we explictly show how Vapnik\u2019s maximum margin hyperplanes fit into this general framework, which then also allows the radius of the set of points to be estimated from the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Decomposing the hypothesis class, as described in Section 2, all ws us to bias our generalization error bounds in favour of certain target functions and distributions: those for which some hypothesis low in the hierarchy is an accurate approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "We show in Section 6 that the hyperplane margin of Section 5 is a luckiness functionwh ch satisfies the technical restrictions we introduce below."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "From Section 4 onwards we address a shortcoming of the SRM method which Vapnik [48, page 161] highlights:according to the SRM principle the structure has to be defineda priori before the training data appear."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": true,
            "numCitedBy": 3709,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60565534,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69d7086300e7f5322c06f2f242a565b3a182efb5",
            "isKey": false,
            "numCitedBy": 4649,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Bill Baird { Publications References 1] B. Baird. Bifurcation analysis of oscillating neural network model of pattern recognition in the rabbit olfactory bulb. In D. 3] B. Baird. Bifurcation analysis of a network model of the rabbit olfactory bulb with periodic attractors stored by a sequence learning algorithm. 5] B. Baird. Bifurcation theory methods for programming static or periodic attractors and their bifurcations in dynamic neural networks."
            },
            "slug": "In-Advances-in-Neural-Information-Processing-Hanson",
            "title": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The level fat-shattering dimension is a scale sensitive version of a dimension introduced by Vapnik [46]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 160
                            }
                        ],
                        "text": "5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-v ec or machines [16, 48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved version due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "In Section 5 we apply this to the case con idered by Vapnik: separating hyperplanes with a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 180
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes nd hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i.e. the hyperplane which maximises the minimal distance of points \u2014 assuming a correct classific tion can be made) will improve the generalization of the resulting classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik [48, Theorem 5.2] gives a bound on the expect d generalization error in terms of the number of support vectors as well as giving examples of classifiers [48, Table 5.2] for which the number of support vectors was very much less than the number of training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 189
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different cla sses nd hence a reliable way of applying the structural risk minimisation principle introduce d by Vapnik [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 267
                            }
                        ],
                        "text": "Indeed, the results reported in Sections 2 and 3 of this paper are implicit in the cited references, but our treatment serves to introduce the main results of the pape r in lat r sections, and we make explicit some of the assumptions implicit in the presentations in [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "The paper introduces some generalizations of Vapnik\u2019s method of structural risk minimisation (SRM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "The theory characterises when a target function fromH can be learned from examples in terms of the VapnikChervonenkis dimension, a measure of the flexibility of the classH and specifies sample sizes required to deliver the required accuracy with the allowed confidence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Theorem 4.1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 133
                            }
                        ],
                        "text": "The fact that the bound in Theorem 4.5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vec or machines [16, 48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We would like to thank Vladimir Vapnik for useful discussions at a Workshop on Artificial Neural Networks: Learning, Generalization and Statistics at the Centre de Rcherches Mathe\u0301matiques, Universite\u0301 de Montre\u0301al, where some of these results were presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "An algorithm using maximally separating hyperplanes proposed by Vapnik [46] and co-workers [14, 16] violates this principle in that the hierarchydefined depends on the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik has used precisely the expression for this unluckiness function (given in Definition 5.2) as an estimate of the effective VC dimension of the Support Vector Machine [48, p.139]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "We have shown that Vapnik\u2019s maxi al margin hyperplane algorithm is an example of implementing this strategy where the luckiness function is the ratio of the maximum size of the input vectors to the maximal margin observed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 363
                            }
                        ],
                        "text": "Then for all 0, fatF\u0302( ) = fatF( ):\nProof : For anyc 2 f0; 1gm, we have thatfb realises dichotomyb onx = (x1; : : : ; xm) with margin about output valuesri if and only if f\u0302b realises dichotomyb c onx\u0302 = ((x1; c1); : : : ; (xm; cm)); with margin about output valuesr\u0302i = ri(1 ci) + (2 ri)ci: We will make use of the following lemma, which in the form below is due to Vapnik [46, page 168]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 129
                            }
                        ],
                        "text": "It is important to note that our explanation of the good performance of maximum margin hyperplanes is different to that given by Vapnik in [48, page 135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 18
                            }
                        ],
                        "text": "Vapnik and others [46, 48 , 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "In Section 7 we explictly show how Vapnik\u2019s maximum margin hyperplanes fit into this general framework, which then also allows the radius of the set of points to be estimated from the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "From Section 4 onwards we address a shortcoming of the SRM method which Vapnik [48, page 161] highlights:according to the SRM principle the structure has to be defineda priori before the training data appear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 5
                            }
                        ],
                        "text": "Thus Vapnik\u2019s algorithm along with the bound of Theorem 4.5 should allow gooda posterioribounds on the generalization error in a range of applications."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699388"
                        ],
                        "name": "L. Ljung",
                        "slug": "L.-Ljung",
                        "structuredName": {
                            "firstName": "Lennart",
                            "lastName": "Ljung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ljung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A commonly studied related problem is that of model order selection (see, for example, [ 34 ]), and we here briefly make some remarks on the relationship with the work presented in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56542330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a17ebeeb80cd696bc83a288f1a77ddfc1467079",
            "isKey": false,
            "numCitedBy": 20317,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Das Buch behandelt die Systemidentifizierung in dem theoretischen Bereich, der direkte Auswirkungen auf Verstaendnis und praktische Anwendung der verschiedenen Verfahren zur Identifizierung hat. Da ..."
            },
            "slug": "System-Identification:-Theory-for-the-User-Ljung",
            "title": {
                "fragments": [],
                "text": "System Identification: Theory for the User"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Das Buch behandelt die Systemidentifizierung in dem theoretischen Bereich, der direkte Auswirkungen auf Verstaendnis and praktische Anwendung der verschiedenen Verfahren zur IdentifIZierung hat."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150334706"
                        ],
                        "name": "Vladimir Naumovich Vapni",
                        "slug": "Vladimir-Naumovich-Vapni",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapni",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vladimir Naumovich Vapni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The level fat-shattering dimension is a scale sensitive version of a dimension introduced by Vapnik [46]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved version due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "In Section 5 we apply this to the case con idered by Vapnik: separating hyperplanes with a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 180
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes nd hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i.e. the hyperplane which maximises the minimal distance of points \u2014 assuming a correct classific tion can be made) will improve the generalization of the resulting classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik [48, Theorem 5.2] gives a bound on the expect d generalization error in terms of the number of support vectors as well as giving examples of classifiers [48, Table 5.2] for which the number of support vectors was very much less than the number of training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "Indeed, the results reported in Sections 2 and 3 of this paper are implicit in the cited references, but our treatment serves to introduce the main results of the paper in later sections, and we make explicit some of the assumptions implicit in the presentations in [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "1 (Vapnik [48]) Suppose X is a subset of the input space contained in a ball of radius R about some point."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "The paper introduces some generalizations of Vapnik\u2019s method of structural risk minimisation (SRM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "The theory characterises when a target function fromH can be learned from examples in terms of the VapnikChervonenkis dimension, a measure of the flexibility of the classH and specifies sample sizes required to deliver the required accuracy with the allowed confidence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Theorem 4.1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 133
                            }
                        ],
                        "text": "The fact that the bound in Theorem 4.5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vec or machines [16, 48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We would like to thank Vladimir Vapnik for useful discussions at a Workshop on Artificial Neural Networks: Learning, Generalization and Statistics at the Centre de Rcherches Mathe\u0301matiques, Universite\u0301 de Montre\u0301al, where some of these results were presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 159
                            }
                        ],
                        "text": "5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vector machines [16, 48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "An algorithm using maximally separating hyperplanes proposed by Vapnik [46] and co-workers [14, 16] violates this principle in that the hierarchydefined depends on the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik has used precisely the expression for this unluckiness function (given in Definition 5.2) as an estimate of the effective VC dimension of the Support Vector Machine [48, p.139]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "We have shown that Vapnik\u2019s maxi al margin hyperplane algorithm is an example of implementing this strategy where the luckiness function is the ratio of the maximum size of the input vectors to the maximal margin observed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 363
                            }
                        ],
                        "text": "Then for all 0, fatF\u0302( ) = fatF( ):\nProof : For anyc 2 f0; 1gm, we have thatfb realises dichotomyb onx = (x1; : : : ; xm) with margin about output valuesri if and only if f\u0302b realises dichotomyb c onx\u0302 = ((x1; c1); : : : ; (xm; cm)); with margin about output valuesr\u0302i = ri(1 ci) + (2 ri)ci: We will make use of the following lemma, which in the form below is due to Vapnik [46, page 168]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 18
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 129
                            }
                        ],
                        "text": "It is important to note that our explanation of the good performance of maximum margin hyperplanes is different to that given by Vapnik in [48, page 135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 188
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes and hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "In Section 7 we explictly show how Vapnik\u2019s maximum margin hyperplanes fit into this general framework, which then also allows the radius of the set of points to be estimated from the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "From Section 4 onwards we address a shortcoming of the SRM method which Vapnik [48, page 161] highlights:according to the SRM principle the structure has to be defineda priori before the training data appear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 5
                            }
                        ],
                        "text": "Thus Vapnik\u2019s algorithm along with the bound of Theorem 4.5 should allow gooda posterioribounds on the generalization error in a range of applications."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195825425,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "6135a83a7ac7543fdaea830c90fe8bc08e7c8f80",
            "isKey": true,
            "numCitedBy": 2153,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapni",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The level fat-shattering dimension is a scale sensitive version of a dimension introduced by Vapnik [46]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved version due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "In Section 5 we apply this to the case con idered by Vapnik: separating hyperplanes with a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 180
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes nd hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i.e. the hyperplane which maximises the minimal distance of points \u2014 assuming a correct classific tion can be made) will improve the generalization of the resulting classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik [48, Theorem 5.2] gives a bound on the expect d generalization error in terms of the number of support vectors as well as giving examples of classifiers [48, Table 5.2] for which the number of support vectors was very much less than the number of training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "The paper introduces some generalizations of Vapnik\u2019s method of structural risk minimisation (SRM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "The theory characterises when a target function fromH can be learned from examples in terms of the VapnikChervonenkis dimension, a measure of the flexibility of the classH and specifies sample sizes required to deliver the required accuracy with the allowed confidence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Theorem 4.1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 133
                            }
                        ],
                        "text": "The fact that the bound in Theorem 4.5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vec or machines [16, 48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We would like to thank Vladimir Vapnik for useful discussions at a Workshop on Artificial Neural Networks: Learning, Generalization and Statistics at the Centre de Rcherches Mathe\u0301matiques, Universite\u0301 de Montre\u0301al, where some of these results were presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 159
                            }
                        ],
                        "text": "5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vector machines [16, 48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "An algorithm using maximally separating hyperplanes proposed by Vapnik [46] and co-workers [14, 16] violates this principle in that the hierarchydefined depends on the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik has used precisely the expression for this unluckiness function (given in Definition 5.2) as an estimate of the effective VC dimension of the Support Vector Machine [48, p.139]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 189
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different cla sses and hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "We have shown that Vapnik\u2019s maxi al margin hyperplane algorithm is an example of implementing this strategy where the luckiness function is the ratio of the maximum size of the input vectors to the maximal margin observed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 363
                            }
                        ],
                        "text": "Then for all 0, fatF\u0302( ) = fatF( ):\nProof : For anyc 2 f0; 1gm, we have thatfb realises dichotomyb onx = (x1; : : : ; xm) with margin about output valuesri if and only if f\u0302b realises dichotomyb c onx\u0302 = ((x1; c1); : : : ; (xm; cm)); with margin about output valuesr\u0302i = ri(1 ci) + (2 ri)ci: We will make use of the following lemma, which in the form below is due to Vapnik [46, page 168]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 18
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 129
                            }
                        ],
                        "text": "It is important to note that our explanation of the good performance of maximum margin hyperplanes is different to that given by Vapnik in [48, page 135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "In Section 7 we explictly show how Vapnik\u2019s maximum margin hyperplanes fit into this general framework, which then also allows the radius of the set of points to be estimated from the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "From Section 4 onwards we address a shortcoming of the SRM method which Vapnik [48, page 161] highlights:according to the SRM principle the structure has to be defineda priori before the training data appear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 267
                            }
                        ],
                        "text": "Indeed, the results reported in Sections 2 and 3 of this paper are implicit in the cited references, but our treatment serves to intr duce the main results of the paper in later sections, and we make explicit some of the assumpti ons implicit in the presentations in [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 5
                            }
                        ],
                        "text": "Thus Vapnik\u2019s algorithm along with the bound of Theorem 4.5 should allow gooda posterioribounds on the generalization error in a range of applications."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik,The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The level fat-shattering dimension is a scale sensitive version of a dimension introduced by Vapnik [46]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved version due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "In Section 5 we apply this to the case con idered by Vapnik: separating hyperplanes with a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 180
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes nd hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i.e. the hyperplane which maximises the minimal distance of points \u2014 assuming a correct classific tion can be made) will improve the generalization of the resulting classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik [48, Theorem 5.2] gives a bound on the expect d generalization error in terms of the number of support vectors as well as giving examples of classifiers [48, Table 5.2] for which the number of support vectors was very much less than the number of training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 189
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different cla sses nd hence a reliable way of applying the structural risk minimisation principle introduce d by Vapnik [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 267
                            }
                        ],
                        "text": "Indeed, the results reported in Sections 2 and 3 of this paper are implicit in the cited references, but our treatment serves to introduce the main results of the pape r in lat r sections, and we make explicit some of the assumptions implicit in the presentations in [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "The paper introduces some generalizations of Vapnik\u2019s method of structural risk minimisation (SRM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "The theory characterises when a target function fromH can be learned from examples in terms of the VapnikChervonenkis dimension, a measure of the flexibility of the classH and specifies sample sizes required to deliver the required accuracy with the allowed confidence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Theorem 4.1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 133
                            }
                        ],
                        "text": "The fact that the bound in Theorem 4.5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vec or machines [16, 48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We would like to thank Vladimir Vapnik for useful discussions at a Workshop on Artificial Neural Networks: Learning, Generalization and Statistics at the Centre de Rcherches Mathe\u0301matiques, Universite\u0301 de Montre\u0301al, where some of these results were presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "An algorithm using maximally separating hyperplanes proposed by Vapnik [46] and co-workers [14, 16] violates this principle in that the hierarchydefined depends on the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik has used precisely the expression for this unluckiness function (given in Definition 5.2) as an estimate of the effective VC dimension of the Support Vector Machine [48, p.139]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "We have shown that Vapnik\u2019s maxi al margin hyperplane algorithm is an example of implementing this strategy where the luckiness function is the ratio of the maximum size of the input vectors to the maximal margin observed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 363
                            }
                        ],
                        "text": "Then for all 0, fatF\u0302( ) = fatF( ):\nProof : For anyc 2 f0; 1gm, we have thatfb realises dichotomyb onx = (x1; : : : ; xm) with margin about output valuesri if and only if f\u0302b realises dichotomyb c onx\u0302 = ((x1; c1); : : : ; (xm; cm)); with margin about output valuesr\u0302i = ri(1 ci) + (2 ri)ci: We will make use of the following lemma, which in the form below is due to Vapnik [46, page 168]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 129
                            }
                        ],
                        "text": "It is important to note that our explanation of the good performance of maximum margin hyperplanes is different to that given by Vapnik in [48, page 135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "In Section 7 we explictly show how Vapnik\u2019s maximum margin hyperplanes fit into this general framework, which then also allows the radius of the set of points to be estimated from the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "From Section 4 onwards we address a shortcoming of the SRM method which Vapnik [48, page 161] highlights:according to the SRM principle the structure has to be defineda priori before the training data appear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 5
                            }
                        ],
                        "text": "Thus Vapnik\u2019s algorithm along with the bound of Theorem 4.5 should allow gooda posterioribounds on the generalization error in a range of applications."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis, \u201cOrdered Risk Minimiz  ation (I and II)"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Control,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "The level fat-shattering dimension is a scale sensitive version of a dimension introduced by Vapnik [46]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "We will make use of the following result of Vapnik in a slightly improved version due to Anthony and Shawe-Taylor [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "In Section 5 we apply this to the case con idered by Vapnik: separating hyperplanes with a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 180
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes nd hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik and others [46, 48, 14, 16], [18, page 140] have suggested that choosing the maximal margin hyperplane (i.e. the hyperplane which maximises the minimal distance of points \u2014 assuming a correct classific tion can be made) will improve the generalization of the resulting classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik [48, Theorem 5.2] gives a bound on the expect d generalization error in terms of the number of support vectors as well as giving examples of classifiers [48, Table 5.2] for which the number of support vectors was very much less than the number of training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "Indeed, the results reported in Sections 2 and 3 of this paper are implicit in the cited references, but our treatment serves to introduce the main results of the paper in later sections, and we make explicit some of the assumptions implicit in the presentations in [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "The paper introduces some generalizations of Vapnik\u2019s method of structural risk minimisation (SRM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 99
                            }
                        ],
                        "text": "The theory characterises when a target function fromH can be learned from examples in terms of the VapnikChervonenkis dimension, a measure of the flexibility of the classH and specifies sample sizes required to deliver the required accuracy with the allowed confidence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Theorem 4.1 (Vapnik [48]) SupposeX0 is a subset of the input space contained in a ball of radiusR about some point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 133
                            }
                        ],
                        "text": "The fact that the bound in Theorem 4.5 does not depend on the dimension of the input space is particularly important in the light of Vapnik\u2019s ingenious construction of his support-vec or machines [16, 48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We would like to thank Vladimir Vapnik for useful discussions at a Workshop on Artificial Neural Networks: Learning, Generalization and Statistics at the Centre de Rcherches Mathe\u0301matiques, Universite\u0301 de Montre\u0301al, where some of these results were presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "An algorithm using maximally separating hyperplanes proposed by Vapnik [46] and co-workers [14, 16] violates this principle in that the hierarchydefined depends on the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Vapnik has used precisely the expression for this unluckiness function (given in Definition 5.2) as an estimate of the effective VC dimension of the Support Vector Machine [48, p.139]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "We have shown that Vapnik\u2019s maxi al margin hyperplane algorithm is an example of implementing this strategy where the luckiness function is the ratio of the maximum size of the input vectors to the maximal margin observed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 363
                            }
                        ],
                        "text": "Then for all 0, fatF\u0302( ) = fatF( ):\nProof : For anyc 2 f0; 1gm, we have thatfb realises dichotomyb onx = (x1; : : : ; xm) with margin about output valuesri if and only if f\u0302b realises dichotomyb c onx\u0302 = ((x1; c1); : : : ; (xm; cm)); with margin about output valuesr\u0302i = ri(1 ci) + (2 ri)ci: We will make use of the following lemma, which in the form below is due to Vapnik [46, page 168]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 129
                            }
                        ],
                        "text": "It is important to note that our explanation of the good performance of maximum margin hyperplanes is different to that given by Vapnik in [48, page 135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 188
                            }
                        ],
                        "text": "The model we consider allows a precise bound on the error arising in different classes and hence a reliable way of applying the structural risk minimisation principle introduced by Vapnik [48, 50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "In Section 7 we explictly show how Vapnik\u2019s maximum margin hyperplanes fit into this general framework, which then also allows the radius of the set of points to be estimated from the data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "From Section 4 onwards we address a shortcoming of the SRM method which Vapnik [48, page 161] highlights:according to the SRM principle the structure has to be defineda priori before the training data appear."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 5
                            }
                        ],
                        "text": "Thus Vapnik\u2019s algorithm along with the bound of Theorem 4.5 should allow gooda posterioribounds on the generalization error in a range of applications."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis, \u201cOrdered Risk Minimization (I and II)"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Control,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "This result is a special case of Corollary 6 in [2], which applied more generally to arbitrary real -valued target functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "The main tool we use is the fat-shattering dimension, which was introduced in [26], a nd h s been used for several problems in learning since [1, 11, 2, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "As in [2], we will use thel1 distance over a finite sample x = (x1; : : : ; xm) for the pseudometric in the space of functions, dx(f; g) = max i jf(xi) g(xi)j: We writeN ( ;F ;x) for the -covering number of F with respect to the pseudo-metric dx."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "(This application to classification problems was not described in [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Function learning from interpolati on"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "5 (Littlestone and Warmuth [33]) LetD be any probability distribution on a domainX, c be any concept onX."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 195
                            }
                        ],
                        "text": "We will omit the proof of the probable smoothness of the support vectors\u2019 unluckiness function since a more direct bound on the generalization error can be obtained using the results of Floyd and Warmuth [19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 29
                            }
                        ],
                        "text": "Theorem 6.5 (Littlestone and Warmuth [33]) LetD be any probability distribution on a domainX, c be any concept onX."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relating Data Compression and  Learnability\u201d, unpublished manuscript, University of California"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29723334"
                        ],
                        "name": "D. Horne",
                        "slug": "D.-Horne",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Horne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Horne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 161271742,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "9c2c80e2237cf0c89fa3e254d6b4933fc2be5982",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-lucky-country-:-Australia-in-the-sixties-Horne",
            "title": {
                "fragments": [],
                "text": "The lucky country : Australia in the sixties"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49737603"
                        ],
                        "name": "M. Biehl",
                        "slug": "M.-Biehl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Biehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Biehl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 204001901,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "04f2615aaa4e9ec2be624cdda1673f638ef8fa92",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perceptron-learning:-the-largest-version-space-Biehl-Opper",
            "title": {
                "fragments": [],
                "text": "Perceptron learning: the largest version space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821511"
                        ],
                        "name": "Gyora M. Benedek",
                        "slug": "Gyora-M.-Benedek",
                        "structuredName": {
                            "firstName": "Gyora",
                            "lastName": "Benedek",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyora M. Benedek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736744"
                        ],
                        "name": "A. Itai",
                        "slug": "A.-Itai",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Itai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Itai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "The learner may only be given a hierarchy of classesH1 H2 Hd and be told that the target will lie in one of the setsHd."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6815721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe9c43c67cd2bac32aeb0a7e9f44e5f2a5b444a6",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonuniform-Learnability-Benedek-Itai",
            "title": {
                "fragments": [],
                "text": "Nonuniform Learnability"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706352"
                        ],
                        "name": "N. Biggs",
                        "slug": "N.-Biggs",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Biggs",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Biggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2586565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "190064114fa5fa1e74ffa04b101c5976549bfebb",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-learnability-of-formal-concepts-Anthony-Biggs",
            "title": {
                "fragments": [],
                "text": "The learnability of formal concepts"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2352366"
                        ],
                        "name": "S. Ross",
                        "slug": "S.-Ross",
                        "structuredName": {
                            "firstName": "Sheldon",
                            "lastName": "Ross",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "We assume with no further discussion \u201cpermissibility\u201d of the function cl asses involved (see Appendix C of [41] and section 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7710371,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "49594776b8a1dce89e976c846266ccefafa948b7",
            "isKey": false,
            "numCitedBy": 6263,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Processes-Ross",
            "title": {
                "fragments": [],
                "text": "Stochastic Processes"
            },
            "venue": {
                "fragments": [],
                "text": "Gauge Integral Structures for Stochastic Calculus and Quantum Electrodynamics"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A F ramework for Structural Risk Minimization"
            },
            "venue": {
                "fragments": [],
                "text": "68{76 in Proceedings of the 9th Annual Conference on Computational Learning Theory"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "A commonly studied related problem is that of model order selection (see for example [34]), and we here briefly make some remarks on the relationship with the work presented in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identification: Theory for the User, Prentice-Hall PTR"
            },
            "venue": {
                "fragments": [],
                "text": "Upper Saddle River, New Jersey,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Benedek and Alon Itai , \u201c Dominating Distrubutions and Learnability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "The margin occurs even more explictly in the Winnow algorithms and their varia nts developed by Littlestone and others [30, 31, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Apobayesian Relative of Wi  nnow"
            },
            "venue": {
                "fragments": [],
                "text": "Preprint, NEC Research Center, New Jersey, (1996)."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Fatshattering and the learnability of Real-valued Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ordered Risk Minimization (I and II)"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Control"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nick Littlestone, \\Learning Quickly When Irrelevant A ttributes Abound: A New Linear Threshold Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Learning Machines, Maximal Margin, Support Vector Machines, Probable Smooth Luckiness, Uniform Convergence, Vapnik-Chervonenkis Dimension, Fat Shattering Dimension, Computational Learning Theory, Probably Approximately Correct Learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Applications to Artificial Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nonparametric Functional Estimation and Related Topics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Applications to Artific al Neural Networks , \u201d pages 561 \u2013 576 in"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis, \\Ordered Risk Minimization (I and II)"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Control"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "The learner may only be given a hierarchy of classesH1 H2 Hd and be told that the target will lie in one of the setsHd."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dominating Distrubutions and Learnability"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth Annual Workshop on Computational Learning Theory"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "A commonly studied related problem is that of model order selection(see for example [34]), and we here briefly make some remarks on the relationship with the work present ed in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identification: Theory for the User"
            },
            "venue": {
                "fragments": [],
                "text": "Prentice-Hall PTR, Upper Saddle River, New Jersey,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Learnability of Form  al Concepts,"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Third Annual Workshop on Computational Learning Theory"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long , and Robert C . Williamson , \u201c Fat - shattering and the learn - ability of Real - valued Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Benedek and Alon Itai , \u201c Nonuniform learnability"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long , and Robert C . Williamson , \u201c Fat - shattering and the learn - ability of Real - valued Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cover , \u201c Minimum Complexity Density Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Applications to Artifi  c al Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "pages 561\u2013576 in G. Roussas (Ed.)  Nonparametric Functional Estimation and Related TopicsKluwer Academic Publishers, 1991."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The results of section 4 show that it is possible to decompose the hypothesis class on the basis of theobserv d data in some cases: there we did it in terms of the margin attained."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": "The main idea is to fix in advance some assumption about the target function and distribution, and encode this assumption in a real-valued function defined on the space of trainings mples and hypotheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Function learning from interpolationAn extended abstract appeared in Computational Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2nd European Conference, EuroCOLT'95"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "(Recall that the VC-dimension of a class of f0; 1g-valued functions is the size of the largest subset of their domain for which the restriction of the clas s to that subset is the set of allf0; 1g-valued functions; see [49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Proof : Using the standard permutation argument (as in [49]), we may fix a sequence xy and bound the probability under the uniform distribution on swapping permutations that the permuted sequence satisfies the condition stated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the Uniform Convergenc  e of Relative Frequencies of Events to their Probabilities"
            },
            "venue": {
                "fragments": [],
                "text": " Theory of Probability and Applications  , 16, 264\u2013280 (1971)."
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Buescher , \u201c Learning by Canonical Smooth Estimation , Part 2 : learning and Choice of Model Complexity"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "System Identiication: Theory for the User"
            },
            "venue": {
                "fragments": [],
                "text": "System Identiication: Theory for the User"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Corinna Cortes and Vladimir Vapnik , \u201c Support - Vector Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Buescher and P . R . Kumar , \u201c Learning by Canonical Smooth Estimation , Part I : Simultaneous Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik Bernhard E . Boser , Leon Bottou and Sara A . Solla , \u201c Structural Risk Minimization for Character Recognition , \u201d pages 471 \u2013 479 in"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Complexity Regularization with Applications to Articial Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "561{576 in G. Roussas (Ed.) Nonparametric Functional Estimation and Related T opics Kluwer Academic Publishers"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "The main tool we use is the fat-shattering dimension, which was introduced in [26], a nd h s been used for several problems in learning since [1, 11, 2, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fat-sha ttering and the learnability of Real-valued Functions"
            },
            "venue": {
                "fragments": [],
                "text": " Journal of Computer and System Sciences, 52(3), 434- 452, (1996)."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Barron and Thomas M . Cover , \u201c Minimum Complexity Density Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "The margin occurs even more explictly in the Winnow algorithms and their variants developed by Littlestone and others [30, 31, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mistake-driven bayes Sports: Bounds for Symmetric Apobayesian Learning Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report, NEC Research Center, New Jersey, (1996)."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Preprint, NEC Research C e n ter"
            },
            "venue": {
                "fragments": [],
                "text": "Preprint, NEC Research C e n ter"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Tomaso Poggio , \u201c Regularization Theory and Neural Networks Architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Manfred Warmuth , \u201c Sample Compression , learnability , andthe Vapnik - Chervonenkis Dimension"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Classification and Scene Analysis"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "The margin occurs even more explictly in the Winnow algorithms and their varia nts developed by Littlestone and others [30, 31, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mistake-driven bayes Sports: Bounds for Symmetric A pobayesian Learning Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report, NEC Research Center, New Jers  ey, (1996)."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A Data-dependent S k eleton Estimate for Learning"
            },
            "venue": {
                "fragments": [],
                "text": "51{56 in Proceedings of the Ninth Annual Workshop on Computational Learning Theory"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire, \\EEcient Distribution-free Learning of Probabilistic Concepts"
            },
            "venue": {
                "fragments": [],
                "text": "382{391 in Proceedings of the 31st Symposium on the Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks Architecture Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks Architecture Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fundamentals of Artiicial Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Fundamentals of Artiicial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity Regularization with Applications to Artificial Neural Networks , \u201d pages 561 \u2013 576 in"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Related work is presented by Benedek and Itai [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dominating Distrubutions and Learnability"
            },
            "venue": {
                "fragments": [],
                "text": "pages 253\u2013264 inProceedings of the Fifth Annual Workshop on Computational Learning Theory, Pittsburgh ACM, (1992). 28"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nick Littlestone, \\Mistake-driven bayes Sports: Bounds for Symmetric Apobayesian Learning Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Nick Littlestone, \\Mistake-driven bayes Sports: Bounds for Symmetric Apobayesian Learning Algorithms"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik and Aleksei Ja . Chervonenkis , \u201c Ordered Risk Minimization ( I and II ) \u201d"
            },
            "venue": {
                "fragments": [],
                "text": "Automation and Remote Control"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architecture,\u201dNeural"
            },
            "venue": {
                "fragments": [],
                "text": "Computation  ,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Largest Version Space in Neural Networks: The Statistical Mechancis Perspective"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the CTP{PBSRI Workshop on Theoretical Physics"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 33,
            "methodology": 13,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 98,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett/1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5?sort=total-citations"
}