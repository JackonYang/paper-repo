{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60733663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdc5b6ddfeb5964db46f6823461701abdbead279",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the extension of our decoupled extended Kalman filter (DEKF) training algorithm to networks with internal recurrent (or feedback) connections; we call the resulting algorithm dynamic DEKF (or DDEKF for short). Analysis of DDEKF's computational complexity and empirical evidence suggest significant computational and performance advantages in comparison to training algorithms based exclusively upon gradient descent. We demonstrate DDEKF's effectiveness by training networks with recurrent connections for four different classes of problems. First, DDEKF is used to train a recurrent network that produces as its output a delayed copy of its input. Second, recurrent networks are trained by DDEKF to recognize sequences of events with arbitrarily long time delays between the events. Third, DDEKF is applied to the training of identification networks to act as models of the input-output behavior for nonlinear dynamical systems. We conclude the paper with a brief discussion of the extension of DDEKF to the training of neural controllers with internal feedback connections."
            },
            "slug": "Recurrent-network-training-with-the-algorithm-Puskorius-Feldkamp",
            "title": {
                "fragments": [],
                "text": "Recurrent network training with the decoupled-extended-Kalman-filter algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The extension of the decoupled extended Kalman filter (DEKF) training algorithm to networks with internal recurrent (or feedback) connections is described and the resulting algorithm is called dynamic DEKF (or DDEF for short)."
            },
            "venue": {
                "fragments": [],
                "text": "Defense, Security, and Sensing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61929249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "432263db51110fa5694f853a7f7b20f96e11b841",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Two fundamental extensions of the dynamic backpropagation (DBP) gradient descent procedure which generally result in faster convergence times and higher quality solutions are presented. The decoupled extended Kalman filter training algorithm (DEKF) for feedforward layered networks is extended to the training of neural controllers in a dynamic indirect adaptive control scheme; the resulting algorithm is called dynamic DEKF (or DDEKF). The DDEKF neural controller training algorithm is extended to include control network architectures with explicit internal feedback connections. It is demonstrated that the DDEKF algorithm has computational complexity and requirements that are similar to those of DBP for control networks with a large number of recurrent connections. The use of these extensions for a model reference adaptive control (MRAC) problem in which the example dynamical system is highly nonlinear and does not possess a unique inverse is presented.<<ETX>>"
            },
            "slug": "Model-reference-adaptive-control-with-recurrent-by-Puskorius-Feldkamp",
            "title": {
                "fragments": [],
                "text": "Model reference adaptive control with recurrent networks trained by the dynamic DEKF algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Two fundamental extensions of the dynamic backpropagation (DBP) gradient descent procedure which generally result in faster convergence times and higher quality solutions are presented and used for a model reference adaptive control (MRAC) problem in which the example dynamical system is highly nonlinear and does not possess a unique inverse."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10745035"
                        ],
                        "name": "M. Livstone",
                        "slug": "M.-Livstone",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Livstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Livstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50743502"
                        ],
                        "name": "J. Farrell",
                        "slug": "J.-Farrell",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Farrell",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Farrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31277108"
                        ],
                        "name": "W. Baker",
                        "slug": "W.-Baker",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Baker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Baker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27886925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08c9ca14c796c1b478fff198f8aec5157f1b3979",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary goal of this paper is to investigate the training of recurrent networks for control and signal processing applications. This paper first a characterizes a class of network architectures that are well suited for the incremental learning of nonlinear multivariable dynamic mappings, and then presents a general, computationally efficient algorithm for training this class of recurrent networks. The learning algorithm is a local modification of the Extended Kalman Filter that views the network as a parametric model of a nonlinear dynamic system. Computational efficiency of the learning scheme is achieved by exploiting local properties of the network architectures. The ability of this algorithm to train recurrent networks successfully is demonstrated by way of two examples."
            },
            "slug": "A-Computationally-Efficient-Algorithm-for-Training-Livstone-Farrell",
            "title": {
                "fragments": [],
                "text": "A Computationally Efficient Algorithm for Training Recurrent Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A class of network architectures that are well suited for the incremental learning of nonlinear multivariable dynamic mappings are characterized, and a general, computationally efficient algorithm for training this class of recurrent networks is presented."
            },
            "venue": {
                "fragments": [],
                "text": "1992 American Control Conference"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120531662"
                        ],
                        "name": "B. Fern\u00e1ndez",
                        "slug": "B.-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Benito",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838904"
                        ],
                        "name": "A. Parlos",
                        "slug": "A.-Parlos",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Parlos",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Parlos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728607"
                        ],
                        "name": "W. Tsai",
                        "slug": "W.-Tsai",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Tsai",
                            "middleNames": [
                                "Kang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Tsai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35941758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2faee5c2ca5aea2980fb609301dbd3e740f33ba6",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A recurrent multilayer perceptron (MLP) network topology is used in the identification of nonlinear dynamic systems from only the input/output measurements. This effort is part of a research program devoted to developing real-time diagnostics and predictive control techniques for large-scale complex nonlinear dynamic systems. The identification is performed in the discrete-time domain, with the learning algorithm being a modified form of the back-propagation (BP) rule. The recurrent dynamic network (RDN) developed is used for the identification of a simple power plant boiler with known nonlinear behavior. Results indicate that the RDN can reproduce the nonlinear response of the boiler while keeping the number of nodes roughly equal to the relative order of the system. A number of issues are identified regarding the behavior of the RDN which are unresolved and require further research. Use of the recurrent MLP structure with a variety of different learning algorithms may prove useful in utilizing artificial neural networks for recognition, classification, and prediction of dynamic patterns"
            },
            "slug": "Nonlinear-dynamic-system-identification-using-Fern\u00e1ndez-Parlos",
            "title": {
                "fragments": [],
                "text": "Nonlinear dynamic system identification using artificial neural networks (ANNs)"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The recurrent dynamic network (RDN) developed is used for the identification of a simple power plant boiler with known nonlinear behavior and results indicate that the RDN can reproduce the nonlinear response of the boiler while keeping the number of nodes roughly equal to the relative order of the system."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47565163"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Leighton",
                            "lastName": "Davis",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66009583"
                        ],
                        "name": "F. Yuan",
                        "slug": "F.-Yuan",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yuan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58251937,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "676fd9016c7a660f446ffada1c2fecf08c1a67ff",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of dynamic gradient-based training of neural controllers for automotive systems is illustrated. The authors use a recurrent structure that embeds an identification network and a neural controller and that properly treats both short- and long-term effects of controller weight changes. This results in an approximately optimal control strategy. Feedforward and hybrid feedforward-feedback neural controllers trained by dynamic backpropagation and a dynamic decoupled extended Kalman filter (DDEKF) are investigated. A quarter-car active suspension model is considered in both linear and nonlinear forms, and representative results are presented. Methods using higher-order information, e.g., DDEKF are very effective in comparison to methods based exclusively upon gradient descent, e.g., dynamic backpropagation (DBP). The use of a recurrent structure for obtaining derivatives for controller training is illustrated.<<ETX>>"
            },
            "slug": "Neural-control-systems-trained-by-dynamic-gradient-Feldkamp-Puskorius",
            "title": {
                "fragments": [],
                "text": "Neural control systems trained by dynamic gradient methods for automotive applications"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The authors use a recurrent structure that embeds an identification network and a neural controller and that properly treats both short- and long-term effects of controller weight changes to result in an approximately optimal control strategy."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61727082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "219591f66d5bcd03346e656cfcbc92972d060957",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a training algorithm for feedforward layered networks based on a decoupled extended Kalman filter (DEKF). The authors present an artificial process noise extension to DEKF that increases its convergence rate and assists in the avoidance of local minima. Computationally efficient formulations for two particularly natural and useful cases of DEKF are given. Through a series of pattern classification and function approximation experiments, three members of DEKF are compared with one another and with standard backpropagation (SBP). These studies demonstrate that the judicious grouping of weights along with the use of artificial process noise in DEKF result in input-output mapping performance that is comparable to the global extended Kalman algorithm, and is often superior to SBP, while requiring significantly fewer presentations of training data than SBP and less overall training time than either of these procedures.<<ETX>>"
            },
            "slug": "Decoupled-extended-Kalman-filter-training-of-Puskorius-Feldkamp",
            "title": {
                "fragments": [],
                "text": "Decoupled extended Kalman filter training of feedforward layered networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "These studies demonstrate that the judicious grouping of weights along with the use of artificial process noise in DEKF result in input-output mapping performance that is comparable to the global extended Kalman algorithm, and is often superior to SBP, while requiring significantly fewer presentations of training data than SBP and less overall training time than either of these procedures."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145245016"
                        ],
                        "name": "K. Parthasarathy",
                        "slug": "K.-Parthasarathy",
                        "structuredName": {
                            "firstName": "Kannan",
                            "lastName": "Parthasarathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Parthasarathy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10127893,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "10a9286df1d47b4a4bd91d0c0d41129edca6e622",
            "isKey": false,
            "numCitedBy": 6224,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis is on models for both identification and control. Static and dynamic backpropagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations, and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout, and theoretical questions that have to be addressed are also described."
            },
            "slug": "Identification-and-control-of-dynamical-systems-Narendra-Parthasarathy",
            "title": {
                "fragments": [],
                "text": "Identification and control of dynamical systems using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems and the models introduced are practically feasible."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110349627"
                        ],
                        "name": "R.J. Williams",
                        "slug": "R.J.-Williams",
                        "structuredName": {
                            "firstName": "R.J.",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R.J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7007887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e34d0053b71d78cec26e8c29f57a3b9e85de49",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The author describes some relationships between the extended Kalman filter (EKF) as applied to recurrent net learning and some simpler techniques that are more widely used. In particular, making certain simplifications to the EKF gives rise to an algorithm essentially identical to the real-time recurrent learning (RTRL) algorithm. Since the EKF involves adjusting unit activity in the network, it also provides a principled generalization of the teacher forcing technique. Preliminary simulation experiments on simple finite-state Boolean tasks indicated that the EKF can provide substantial speed-up in number of time steps required for training on such problems when compared with simpler online gradient algorithms. The computational requirements of the EKF are steep, but scale with network size at the same rate as RTRL.<<ETX>>"
            },
            "slug": "Training-recurrent-networks-using-the-extended-Williams",
            "title": {
                "fragments": [],
                "text": "Training recurrent networks using the extended Kalman filter"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The author describes some relationships between the extended Kalman filter (EKF) as applied to recurrent net learning and some simpler techniques that are more widely used, and gives rise to an algorithm essentially identical to the real-time recurrent learning (RTRL) algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145245016"
                        ],
                        "name": "K. Parthasarathy",
                        "slug": "K.-Parthasarathy",
                        "structuredName": {
                            "firstName": "Kannan",
                            "lastName": "Parthasarathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Parthasarathy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9267259,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be36852f284e19f2c512acb296dcb713ac5c01e7",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "An extension of the backpropagation method, termed dynamic backpropagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed. The method is based on the fact that gradient methods used in linear dynamical systems can be combined with backpropagation methods for neural networks to obtain the gradient of a performance index of nonlinear dynamical systems. The method can be applied to any complex system which can be expressed as the interconnection of linear dynamical systems and multilayer neural networks. To facilitate the practical implementation of the proposed method, emphasis is placed on the diagrammatic representation of the system which generates the gradient of the performance function."
            },
            "slug": "Gradient-methods-for-the-optimization-of-dynamical-Narendra-Parthasarathy",
            "title": {
                "fragments": [],
                "text": "Gradient methods for the optimization of dynamical systems containing neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An extension of the backpropagation method, termed dynamic back Propagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109171376"
                        ],
                        "name": "S. Shah",
                        "slug": "S.-Shah",
                        "structuredName": {
                            "firstName": "Samir",
                            "lastName": "Shah",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058020346"
                        ],
                        "name": "F. Palmieri",
                        "slug": "F.-Palmieri",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Palmieri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Palmieri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5769413"
                        ],
                        "name": "M. Datum",
                        "slug": "M.-Datum",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Datum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Datum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26260758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f3dd24c8112b9e1c4b44418b84fe39d051a4956",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-filtering-algorithms-for-fast-learning-in-Shah-Palmieri",
            "title": {
                "fragments": [],
                "text": "Optimal filtering algorithms for fast learning in feedforward neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24224372,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f7b4c69d63868e84e5240385ce975df7db29847b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the development of recurrent neural network controllers for an automotive engine idle speed control (ISC) problem. Engine ISC is a difficult problem because of troublesome characteristics such as severe process nonlinearities, variable time delays, time-varying process dynamics and unobservable system states and disturbances. We demonstrate that recurrent neural network controllers can be trained to handle these difficulties gracefully while achieving good regulator performance for a representative model of 4-cylinder, 1.6 liter engine. Empirical results clearly illustrate that neural network controllers with relatively large amounts of internal feedback provide more robust performance for the ISC problem than do neural network controllers that are static or contain limited internal recurrent connections."
            },
            "slug": "Automotive-Engine-Idle-Speed-Control-with-Recurrent-Puskorius-Feldkamp",
            "title": {
                "fragments": [],
                "text": "Automotive Engine Idle Speed Control with Recurrent Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "1993 American Control Conference"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47565163"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Leighton",
                            "lastName": "Davis",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974892"
                        ],
                        "name": "G. Puskorius",
                        "slug": "G.-Puskorius",
                        "structuredName": {
                            "firstName": "Gintaras",
                            "lastName": "Puskorius",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Puskorius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66009583"
                        ],
                        "name": "F. Yuan",
                        "slug": "F.-Yuan",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48041290"
                        ],
                        "name": "L. Feldkamp",
                        "slug": "L.-Feldkamp",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Feldkamp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feldkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110073276,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "5eb3c8abc9ca8d4fea4894c7c591a1bc58a0ec96",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors have previously described neural-network-based methods for modeling automotive systems and training near-optimal controllers. These methods are based on the premise that the physical system can be sufficiently instrumented during network training so that accurate evaluation of the effect of control actions is possible. In certain systems, such a automotive anti-lock braking (ABS), it may be costly to obtain the detailed data that would be required to exploit the full capabilities of neural methods. The present paper reports an initial simulation-based study to determine the performance potential of controllers designed with these methods. Such studies will help determine whether the cost of carrying out neural training methods on actual systems is justified.<<ETX>>"
            },
            "slug": "Neural-network-modeling-and-control-of-an-anti-lock-Davis-Puskorius",
            "title": {
                "fragments": [],
                "text": "Neural network modeling and control of an anti-lock brake system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Intelligent Vehicles `92 Symposium"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48988892"
                        ],
                        "name": "S. Douglas",
                        "slug": "S.-Douglas",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Douglas",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Douglas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3019131"
                        ],
                        "name": "T. Meng",
                        "slug": "T.-Meng",
                        "structuredName": {
                            "firstName": "Teresa",
                            "lastName": "Meng",
                            "middleNames": [
                                "H.",
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Meng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123436420,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "89d8d30a6add16b30797a1a6859434cc4b55c400",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors develop a linearized least-squares formulation for estimating the weight coefficients of a neural network. Linearization of the nonlinear network about the most recent weight estimates leads to a conditional least-squares criterion which may be solved recursively in time. The resulting coefficient update equations resemble those of the recursive least-squares solution in adaptive filtering, much as the update equations for linearized stochastic gradient descent (backpropagation) resemble those of the least mean squares solution in adaptive filtering. Simulations on small logic mapping problems indicate a three- to tenfold increase in training efficiency for this technique as compared to gradient descent.<<ETX>>"
            },
            "slug": "Linearized-least-squares-training-of-multilayer-Douglas-Meng",
            "title": {
                "fragments": [],
                "text": "Linearized least-squares training of multilayer feedforward neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bc0449360d7016f684eafae5b5d2feded32041",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time."
            },
            "slug": "An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng",
            "title": {
                "fragments": [],
                "text": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described, intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143860820"
                        ],
                        "name": "S. Singhal",
                        "slug": "S.-Singhal",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145722821"
                        ],
                        "name": "Lance Wu",
                        "slug": "Lance-Wu",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lance Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11946868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4ea370b7261f2a2bf3a9339cedb1ab1de348301",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples."
            },
            "slug": "Training-Multilayer-Perceptrons-with-the-Extende-Singhal-Wu",
            "title": {
                "fragments": [],
                "text": "Training Multilayer Perceptrons with the Extende Kalman Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1933935"
                        ],
                        "name": "D. Ruck",
                        "slug": "D.-Ruck",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Ruck",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30603673"
                        ],
                        "name": "S. Rogers",
                        "slug": "S.-Rogers",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Rogers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786615"
                        ],
                        "name": "M. Kabrisky",
                        "slug": "M.-Kabrisky",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kabrisky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kabrisky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615526"
                        ],
                        "name": "P. Maybeck",
                        "slug": "P.-Maybeck",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Maybeck",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maybeck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693682"
                        ],
                        "name": "M. Oxley",
                        "slug": "M.-Oxley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Oxley",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oxley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12943364,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "042ff2a35986067fc633dd5d015e810a7eee5a1d",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The relationship between backpropagation and extended Kalman filtering for training multilayer perceptrons is examined. These two techniques are compared theoretically and empirically using sensor imagery. Backpropagation is a technique from neural networks for assigning weights in a multilayer perceptron. An extended Kalman filter can also be used for this purpose. A brief review of the multilayer perceptron and these two training methods is provided. Then, it is shown that backpropagation is a degenerate form of the extended Kalman filter. The training rules are compared in two examples: an image classification problem using laser radar Doppler imagery and a target detection problem using absolute range images. In both examples, the backpropagation training algorithm is shown to be three orders of magnitude less costly than the extended Kalman filter algorithm in terms of a number of floating-point operations. >"
            },
            "slug": "Comparative-Analysis-of-Backpropagation-and-the-for-Ruck-Rogers",
            "title": {
                "fragments": [],
                "text": "Comparative Analysis of Backpropagation and the Extended Kalman Filter for Training Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The backpropagation training algorithm is shown to be three orders of magnitude less costly than the extended Kalman filter algorithm in terms of a number of floating-point operations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118445361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce6bd107e80f38288ca37e87e614f6902f081863",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "WHEN DISTINCT OUTPUTS OF AN ADAPTIVE SYSTEM HAVE EQUIVALENT EFFECTS ON THE ENVIRONMENT, THE PROBLEM OF FINDING APPROPRIATE ACTIONS GIVEN DESIRED RESULTS IS ILL-POSED. FOR SUPERVISED LEARNING ALGORITHMS, THE ILL-POSEDNESS OF SUCH \"INVERSE LEARNING PROBLEMS\" IMPLIES A CERTAIN FLEXIBILITY---DURING TRAINING, THERE ARE IN GENERAL MANY POSSIBLE TARGET VECTORS CORRESPONDING TO EACH INPUT VECTOR. TO ALLOW SUPERVISED LEARNING ALGORITHMS TO MAKE USE OF THIS FLEXIBILITY, THE CURRENT PAPER CONSIDERS HOW TO SPECIFY TARGETS BY SETS OF CONSTRAINTS, RATHER THAN AS PARTICULAR VECTORS. TWO CLASSES OF CONSTRAINTS ARE DISTINGUISHED---`CONFIGURATIONAL'' CONSTRAINTS, WHICH DEFINE REGIONS OF OUTPUT SPACE IN WHICH AN OUTPUT VECTOR MUST LIE, AND `TEMPORAL'' CONSTRAINTS, WHICH DEFINE RELATIONSHIPS BETWEEN OUTPUTS PRODUCED AT DIFFER- ENT POINTS IN TIME. LEARNING ALGORITHMS MINIMIZE A COST FUNCTION THAT CON- TAINS TERMS FOR BOTH KINDS OF CONSTRAINTS. THIS APPROACH TO INVERSE LEARN- ING IS ILLUSTRATED BY A ROBOTICS APPLICATION IN WHICH A NETWORK FINDS TRA- JECTORIES OF INVERSE KINEMATIC SOLUTIONS FOR MANIPULATORS WITH EXCESS DEGREES OF FREEDOM."
            },
            "slug": "Supervised-learning-and-systems-with-excess-degrees-Jordan",
            "title": {
                "fragments": [],
                "text": "Supervised learning and systems with excess degrees of freedom"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a novel approach to InversE LEARN- ING, a robotics application in which a network of networks develops TRA- JECTORIES of Inverse KINEMATIC SOLUTIONS for MANIPULATORS with EXCESS DEGREes of freedom."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18470994,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1a3d22599028a05669e884f3eaf19a342e190a87",
            "isKey": false,
            "numCitedBy": 4036,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for backpropagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, i t describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed."
            },
            "slug": "Backpropagation-Through-Time:-What-It-Does-and-How-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation Through Time: What It Does and How to Do It"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis, and describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1522994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a7acaf6469c06ae5876d92f013184db5897bb13",
            "isKey": false,
            "numCitedBy": 3237,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "slug": "Neuronlike-adaptive-elements-that-can-solve-control-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Neuronlike adaptive elements that can solve difficult learning control problems"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem and the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50472018"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Miller",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63862586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e4a1955779b9386c1d51c0bd3f74ae9d6e9ef6",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Recurrent Networks, Gradient-Based Learning Algorithms for Recurrent Networks, Relationship to Standard Engineering Approaches, Temporal Behavior: Three Connectionist Approaches, Significance of the Radical Approach, Conclusion, Acknowledgments, References"
            },
            "slug": "Adaptive-State-Representation-and-Estimation-Using-Miller-Sutton",
            "title": {
                "fragments": [],
                "text": "Adaptive State Representation and Estimation Using Recurrent Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This chapter contains sections titled: Introduction, Recurrent Networks, Gradient-Based Learning Algorithms for Recurrent networks, Relationship to Standard Engineering Approaches, Temporal Behavior, Significance of the Radical Approach, and Conclusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50409681"
                        ],
                        "name": "G. Vachtsevanos",
                        "slug": "G.-Vachtsevanos",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Vachtsevanos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vachtsevanos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65790760"
                        ],
                        "name": "S. Farinwata",
                        "slug": "S.-Farinwata",
                        "structuredName": {
                            "firstName": "Shehu",
                            "lastName": "Farinwata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Farinwata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98293013"
                        ],
                        "name": "D. Pirovolou",
                        "slug": "D.-Pirovolou",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Pirovolou",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pirovolou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22083828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50337f9c7d052e3de4997f7f0b5c6c18cf53a721",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A systematic fuzzy logic control design method for control of automotive engine idling speed is discussed. The method uses the direct intelligent control paradigm. The procedure is based on partitioning of the state space into small rectangles called cell groups, and quantization of the states and the available controls into finite levels or bins. Membership functions are then assigned for the state and controls. The transition from one conditional subspace to another is accomplished via a center-point mapping of the cell groups, under the applied action of each if-then rule. The systematic aspects of the procedure refer to the way transitional elements are clustered on the basis of an explicitly defined performance measure. Optimum transitions are selected using a search procedure. The fuzzy rules are generated automatically for transitioning from any initial cell group to the target cell group. A simplified engine model for idling speed control is used as the testbed for development and demonstration purposes.<<ETX>>"
            },
            "slug": "Fuzzy-logic-control-of-an-automotive-engine-Vachtsevanos-Farinwata",
            "title": {
                "fragments": [],
                "text": "Fuzzy logic control of an automotive engine"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A systematic fuzzy logic control design method for control of automotive engine idling speed is discussed, based on partitioning of the state space into small rectangles called cell groups, and quantization of the states and the available controls into finite levels or bins."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18686894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "909226ce00fbb74306da00911d48651383bf1ae8",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning.<<ETX>>"
            },
            "slug": "Generic-constraints-on-underspecified-target-Jordan",
            "title": {
                "fragments": [],
                "text": "Generic constraints on underspecified target trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations, which uses the notion of a forward model to give constraints a domain- specific interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31157975"
                        ],
                        "name": "J. Cook",
                        "slug": "J.-Cook",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Cook",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418581"
                        ],
                        "name": "B. Powell",
                        "slug": "B.-Powell",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Powell",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Powell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16554312,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6466eb37540355718e5904d6ff858782086a71e5",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent activity in nonthermodynamic modeling of automotive internal combustion engines with spark ignition, which are inherently nonlinear, is reviewed. A fundamental nonlinear model of the engine is presented, and a linear control-oriented model is derived from the nonlinear process. Techniques for experimental verification are examined, and a practical linear engine example incorporating multirate sampling is illustrated.<<ETX>>"
            },
            "slug": "Modeling-of-an-internal-combustion-engine-for-Cook-Powell",
            "title": {
                "fragments": [],
                "text": "Modeling of an internal combustion engine for control analysis"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems Magazine"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49774886"
                        ],
                        "name": "P. Agrawal",
                        "slug": "P.-Agrawal",
                        "structuredName": {
                            "firstName": "Pramod",
                            "lastName": "Agrawal",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2840120"
                        ],
                        "name": "C. Lee",
                        "slug": "C.-Lee",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977048677"
                        ],
                        "name": "Henry C. Lim",
                        "slug": "Henry-C.-Lim",
                        "structuredName": {
                            "firstName": "Henry C.",
                            "lastName": "Lim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry C. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002869"
                        ],
                        "name": "D. Ramkrishna",
                        "slug": "D.-Ramkrishna",
                        "structuredName": {
                            "firstName": "Doraiswami",
                            "lastName": "Ramkrishna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramkrishna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14662832,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "72fa0957b3f1c6f5beff377ff0fd4822d897fc10",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-investigations-of-dynamic-behavior-of-Agrawal-Lee",
            "title": {
                "fragments": [],
                "text": "Theoretical investigations of dynamic behavior of isothermal continuous stirred tank biological reactors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50472018"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Miller",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64160057,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "492f9d43bf1d971a74e71745832658ca239a4658",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Current Approaches to Adaptive Control, Motivations for Chemical Process Control, Comparison of Chemical and Robotic Process Control, Adaptive Control Benchmarks, A Benchmark Bioreactor Control Problem, Conclusions, References"
            },
            "slug": "A-Bioreactor-Benchmark-for-Adaptive-Network-based-Miller-Sutton",
            "title": {
                "fragments": [],
                "text": "A Bioreactor Benchmark for Adaptive Network-based Process Control"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, Current Approaches to Adaptive Control, Motivations for Chemical Process Control, Comparison of Chemical and Robotic process control, Adaptive control Benchmarks, A Benchmark Bioreactor Control Problem, Conclusions, References."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418581"
                        ],
                        "name": "B. Powell",
                        "slug": "B.-Powell",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Powell",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Powell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31157975"
                        ],
                        "name": "J. Cook",
                        "slug": "J.-Cook",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Cook",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cook"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30538286,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "2843c8a6dbfc0bf5ddba38061b81eb9fdb31c9a3",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper contains a bibliography and discussion of recent activity in nonthermodynamic (low frequency) internal combustion engine model development where spark advance, fuel, throttle, and exhaust gas recirculation are the elected control variables. Modeling considerations are delineated and the modeling relationship to experimental verification is discussed. Examples with explicit methods of parameter determination and dynamic validation are presented."
            },
            "slug": "Nonlinear-Low-Frequency-Phenomenological-Engine-and-Powell-Cook",
            "title": {
                "fragments": [],
                "text": "Nonlinear Low Frequency Phenomenological Engine Modeling and Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "1987 American Control Conference"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feldkamp received the B.S.E. degree in Electncal Engineering (1964) and the M.S. (1965) and Ph.D. (1969) degrees in Nuclear Engineering, all from the University of Michigan"
            },
            "venue": {
                "fragments": [],
                "text": "He IS currently a Staff Scientist with Ford Motor Company\u2019s"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronlike elements that can solve difficult leaming control problems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems , Man and Cybernetics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some observations on the use of the extended Kalman filter as a recurrent network Ieaming algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "University. He joined Ford Motor Company\u2019s Physics Department in 1982, and moved to the Manufacturing Systems Department"
            },
            "venue": {
                "fragments": [],
                "text": "Physics"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural network nonlinear adaptive filtering using the - - extended Kalman filter algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Infernational Neural Netn \u201d + . s Conference . Paris"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Neurocontrol-of-nonlinear-dynamical-systems-with-Puskorius-Feldkamp/0e63335010c6d3a56ffba62595118447ff9e8734?sort=total-citations"
}