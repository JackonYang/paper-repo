{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40200766"
                        ],
                        "name": "A. Sestito",
                        "slug": "A.-Sestito",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Sestito",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sestito"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26929971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71492e07eebe4696ebcc1c2355b7b5fe4e1e8ac1",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel simple approach to deal with fairly general graph structures by neural networks. Using a constructive approach, the model Neural Network for Graphs (NN4G) exploits the contextual information stored in the hidden units progressively added to the network, without introducing cycles in the definition of the state variables. In contrast to previous neural networks for structures, NN4G is not recursive but uses standard neurons (with no feedbacks) that traverse each graph without hierarchical assumptions on its topology, allowing the extension of structured domain to cyclic directed/undirected graphs. Initial experimental results, obtained on the prediction of the boiling point of alkanes and on the classification of artificial cyclic structures, show the effectiveness of this new approach."
            },
            "slug": "A-New-Neural-Network-Model-for-Contextual-of-Graphs-Micheli-Sestito",
            "title": {
                "fragments": [],
                "text": "A New Neural Network Model for Contextual Processing of Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Initial experimental results, obtained on the prediction of the boiling point of alkanes and on the classification of artificial cyclic structures, show the effectiveness of the model Neural Network for Graphs (NN4G)."
            },
            "venue": {
                "fragments": [],
                "text": "WIRN/NAIS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020416"
                        ],
                        "name": "M. Bianchini",
                        "slug": "M.-Bianchini",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Bianchini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bianchini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 437255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a062f11edf3270f9b702767dca5bacf8ac0bbe2",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing."
            },
            "slug": "Processing-directed-acyclic-graphs-with-recursive-Bianchini-Gori",
            "title": {
                "fragments": [],
                "text": "Processing directed acyclic graphs with recursive neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6197973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5eb96540ef53b49eac2246d6b13635fe6e54451",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist. The general framework proposed in this paper can be regarded as an extension of both recurrent neural networks and hidden Markov models to the case of acyclic graphs. In particular we study the supervised learning problem as the problem of learning transductions from an input structured space to an output structured space, where transductions are assumed to admit a recursive hidden statespace representation. We introduce a graphical formalism for representing this class of adaptive transductions by means of recursive networks, i.e., cyclic graphs where nodes are labeled by variables and edges are labeled by generalized delay elements. This representation makes it possible to incorporate the symbolic and subsymbolic nature of data. Structures are processed by unfolding the recursive network into an acyclic graph called encoding network. In so doing, inference and learning algorithms can be easily inherited from the corresponding algorithms for artificial neural networks or probabilistic graphical model."
            },
            "slug": "A-general-framework-for-adaptive-processing-of-data-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "A general framework for adaptive processing of data structures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information, where relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1593083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b7a0048801f9d43dc48a8f04367be813146b05a",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a general methodology for the design of large-scale recursive neural network architectures (DAG-RNNs) which comprises three fundamental steps: (1) representation of a given domain using suitable directed acyclic graphs (DAGs) to connect visible and hidden node variables; (2) parameterization of the relationship between each variable and its parent variables by feedforward neural networks; and (3) application of weight-sharing within appropriate subsets of DAG connections to capture stationarity and control model complexity. Here we use these principles to derive several specific classes of DAG-RNN architectures based on lattices, trees, and other structured graphs. These architectures can process a wide range of data structures with variable sizes and dimensions. While the overall resulting models remain probabilistic, the internal deterministic dynamics allows efficient propagation of information, as well as training by gradient descent, in order to tackle large-scale problems. These methods are used here to derive state-of-the-art predictors for protein structural features such as secondary structure (1D) and both fine- and coarse-grained contact maps (2D). Extensions, relationships to graphical models, and implications for the design of neural architectures are briefly discussed. The protein prediction servers are available over the Web at: www.igb.uci.edu/tools.htm ."
            },
            "slug": "The-Principled-Design-of-Large-Scale-Recursive-and-Baldi-Pollastri",
            "title": {
                "fragments": [],
                "text": "The Principled Design of Large-Scale Recursive Neural Network Architectures--DAG-RNNs and the Protein Structure Prediction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These methods are used to derive state-of-the-art predictors for protein structural features such as secondary structure and both fine- and coarse-grained contact maps and implications for the design of neural architectures are briefly discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34750387"
                        ],
                        "name": "V. Massa",
                        "slug": "V.-Massa",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Massa",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Massa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3073217"
                        ],
                        "name": "G. Monfardini",
                        "slug": "G.-Monfardini",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Monfardini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Monfardini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608269"
                        ],
                        "name": "L. Sarti",
                        "slug": "L.-Sarti",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Sarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35251916"
                        ],
                        "name": "Marco Maggini",
                        "slug": "Marco-Maggini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Maggini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Maggini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15143034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a725e8cf01b687a1d61ba8d9b760f4e0c0a3c50d",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive neural networks (RNNs) and graph neural networks (GNNs) are two connectionist models that can directly process graphs. RNNs and GNNs exploit a similar processing framework, but they can be applied to different input domains. RNNs require the input graphs to be directed and acyclic, whereas GNNs can process any kind of graphs. The aim of this paper consists in understanding whether such a difference affects the behaviour of the models on a real application. An experimental comparison on an image classification problem is presented, showing that GNNs outperforms RNNs. Moreover the main differences between the models are also discussed w.r.t. their input domains, their approximation capabilities and their learning algorithms."
            },
            "slug": "A-Comparison-between-Recursive-Neural-Networks-and-Massa-Monfardini",
            "title": {
                "fragments": [],
                "text": "A Comparison between Recursive Neural Networks and Graph Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An experimental comparison on an image classification problem is presented, showing that GNNs outperforms RNNs, and the main differences between the models are discussed w.r.t. their input domains, their approximation capabilities and their learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "The 2006 IEEE International Joint Conference on Neural Network Proceedings"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144020416"
                        ],
                        "name": "M. Bianchini",
                        "slug": "M.-Bianchini",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Bianchini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bianchini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608269"
                        ],
                        "name": "L. Sarti",
                        "slug": "L.-Sarti",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Sarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260481"
                        ],
                        "name": "F. Scarselli",
                        "slug": "F.-Scarselli",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Scarselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Scarselli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15633336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e943409eac3bd89f0155f28bae34fab011e877db",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive neural networks are a powerful tool for processing structured data. According to the recursive learning paradigm, the input information consists of directed positional acyclic graphs (DPAGs). In fact, recursive networks are fed following the partial order defined by the links of the graph. Unfortunately, the hypothesis of processing DPAGs is sometimes too restrictive, being the nature of some real-world problems intrinsically cyclic. In this paper, a methodology is proposed, which allows us to process any cyclic directed graph. Therefore, the computational power of recursive networks is definitely established, also clarifying the underlying limitations of the model."
            },
            "slug": "Recursive-processing-of-cyclic-graphs-Bianchini-Gori",
            "title": {
                "fragments": [],
                "text": "Recursive processing of cyclic graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A methodology is proposed, which allows us to process any cyclic directed graph, and the computational power of recursive networks is definitely established, also clarifying the underlying limitations of the model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727490"
                        ],
                        "name": "Diego Sona",
                        "slug": "Diego-Sona",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Sona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diego Sona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12370239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "443077eb859540108d3adb5f2fa42b394ddf2a06",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper propose a first approach to deal with contextual information in structured domains by recursive neural networks. The proposed model, i.e., contextual recursive cascade correlation (CRCC), a generalization of the recursive cascade correlation (RCC) model, is able to partially remove the causality assumption by exploiting contextual information stored in frozen units. We formally characterize the properties of CRCC showing that it is able to compute contextual transductions and also some causal supersource transductions that RCC cannot compute. Experimental results on controlled sequences and on a real-world task involving chemical structures confirm the computational limitations of RCC, while assessing the efficiency and efficacy of CRCC in dealing both with pure causal and contextual prediction tasks. Moreover, results obtained for the real-world task show the superiority of the proposed approach versus RCC when exploring a task for which it is not known whether the structural causality assumption holds."
            },
            "slug": "Contextual-processing-of-structured-data-by-cascade-Micheli-Sona",
            "title": {
                "fragments": [],
                "text": "Contextual processing of structured data by recursive cascade correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results obtained for the real-world task show the superiority of the proposed approach versus RCC when exploring a task for which it is not known whether the structural causality assumption holds, as well as assessing the efficiency and efficacy of CRCC in dealing both with pure causal and contextual prediction tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10845957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23f0423ff5a986a695d629fa2ffc6e094d122416",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade correlation (CC) constitutes a training method for neural networks that determines the weights as well as the neural architecture during training. Various extensions of CC to structured data have been proposed: recurrent cascade correlation (RCC) for sequences, recursive cascade correlation (RecCC) for tree structures with limited fan-out, and contextual recursive cascade correlation (CRecCC) for rooted directed positional acyclic graphs (DPAGs) with limited fan-in and fan-out. We show that these models possess the universal approximation property in the following sense: given a probability measure P on the input set, every measurable function from sequences into a real vector space can be approximated by a sigmoidal RCC up to any desired degree of accuracy up to inputs of arbitrary small probability. Every measurable function from tree structures with limited fan-out into a real vector space can be approximated by a sigmoidal RecCC with multiplicative neurons up to any desired degree of accuracy up to inputs of arbitrary small probability. For sigmoidal CRecCC networks with multiplicative neurons, we show the universal approximation capability for functions on an important subset of all DPAGs with limited fan-in and fan-out for which a specific linear representation yields unique codes. We give one sufficient structural condition for the latter property, which can easily be tested: the enumeration of ingoing and outgoing edges should becom patible. This property can be fulfilled for every DPAG with fan-in and fan-out two via reenumeration of children and parents, and for larger fan-in and fan-out via an expansion of the fan-in and fan-out and reenumeration of children and parents. In addition, the result can be generalized to the case of input-output isomorphic transductions of structures. Thus, CRecCC networks consti-tute the first neural models for which the universal approximation ca-pability of functions involving fairly general acyclic graph structures is proved."
            },
            "slug": "Universal-Approximation-Capability-of-Cascade-for-Hammer-Micheli",
            "title": {
                "fragments": [],
                "text": "Universal Approximation Capability of Cascade Correlation for Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "CRecCC networks are shown to be the first neural models for which the universal approximation capability of functions involving fairly general acyclic graph structures is proved."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5942593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e33eca03933caaec671e20692e79d1acc9527e1",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach. In fact, feature-based approaches usually fail to give satisfactory solutions because of the sensitivity of the approach to the a priori selection of the features, and the incapacity to represent any specific information on the relationships among the components of the structures. However, we show that neural networks can, in fact, represent and classify structured patterns. The key idea underpinning our approach is the use of the so called \"generalized recursive neuron\", which is essentially a generalization to structures of a recurrent neuron. By using generalized recursive neurons, all the supervised networks developed for the classification of sequences, such as backpropagation through time networks, real-time recurrent networks, simple recurrent networks, recurrent cascade correlation networks, and neural trees can, on the whole, be generalized to structures. The results obtained by some of the above networks (with generalized recursive neurons) on the classification of logic terms are presented."
            },
            "slug": "Supervised-neural-networks-for-the-classification-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Supervised neural networks for the classification of structures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that neural networks can, in fact, represent and classify structured patterns and all the supervised networks developed for the classification of sequences can, on the whole, be generalized to structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15529289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a0f2f018956189edf636d41672a47fa90530450",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis examines so-called folding neural networks as a mechanism for machine learning. Folding networks form a generalization of partial recurrent neural networks such that they are able to deal with tree structured inputs instead of simple linear lists. In particular, they can handle classical formulas { they were proposed originally for this purpose. After a short explanation of the neural architecture we show that folding networks are well suited as a learning mechanism in principle. This includes three parts: the proof of their universal approximation ability, the aspect of information theoretical learnability, and the examination of the complexity of training. Approximation ability: It is shown that any measurable function can be approximated in probability. Explicit bounds on the number of neurons result if only a nite number of points is dealt with. These bounds are new results in the case of simple recurrent networks, too. Several restrictions occur if a function is to be approximated in the maximum norm. Afterwards, we consider brie y the topic of computability. It is shown that a sigmoidal recurrent neural network can compute any mapping in exponential time. However, if the computation is subject to noise almost the capability of tree automata arises. Information theoretical learnability: This part contains several contributions to distribution dependent learnability: The notation of PAC and PUAC learnability, consistent PAC/ PUAC learnability, and scale sensitive versions are considered. We nd equivalent characterizations of these terms and examine their respective relation answering in particular an open question posed by Vidyasagar. It is shown at which level learnability only because of an encoding trick is possible. Two approaches from the literature which can guarantee distribution dependent learnability if the VC dimension of the concept class is in nite are generalized to function classes: The function class is strati ed according to the input space or according to a so-called luckiness function which depends on the output of the learning algorithm and the concrete training data. Afterwards, the VC, pseudo-, and fat shattering dimension of folding networks are estimated: We improve some lower bounds for recurrent networks and derive new lower bounds for the pseudodimension and lower and upper bounds for folding networks in general. As a consequence, folding architectures are not distribution independent learnable. Distribution dependent learnability can be guaranteed. Explicit bounds on the number of examples which guarantee valid generalization can be derived using the two approaches mentioned above. We examine in which cases these bounds are polynomial. Furthermore, we construct an explicit example for a learning scenario where an exponential number of examples is necessary. Complexity: It is shown that training a xed folding architecture with perceptron activation function is polynomial. Afterwards, a decision problem, the so-called loading problem, which is correlated to neural network training is examined. For standard multilayer feed-forward networks the following situations turn out to be NP-hard: Concerning the perceptron activation function, a classical result from the literature, the NP-hardness for varying input dimension, is generalized to arbitrary multilayer architectures. Additionally, NP-hardness can be found if the input dimension is xed but the number of neurons may vary in at least two hidden layers. Furthermore, the NP-hardness is examined if the number of patterns and number of hidden neurons are correlated. We nish with a generalization of the classical NP result as mentioned above to the sigmoidal activation function which is used in practical applications."
            },
            "slug": "Learning-with-recurrent-neural-networks-Hammer",
            "title": {
                "fragments": [],
                "text": "Learning with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This thesis examines so-called folding neural networks as a mechanism for machine learning such that they are able to deal with tree structured inputs instead of simple linear lists and shows that folding networks are well suited as a learning mechanism in principle."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056703"
                        ],
                        "name": "S. Menchetti",
                        "slug": "S.-Menchetti",
                        "structuredName": {
                            "firstName": "Sauro",
                            "lastName": "Menchetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Menchetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144073418"
                        ],
                        "name": "Fabrizio Costa",
                        "slug": "Fabrizio-Costa",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Costa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabrizio Costa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11512858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "242192b67ba08c932b55fd802b20898c8e0e1314",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolution kernels and recursive neural networks (RNN) are both suitable approaches for supervised learning when the input portion of an instance is a discrete structure like a tree or a graph. We report about an empirical comparison between the two architectures in a large scale preference learning problem related to natural language processing, where instances are candidate incremental parse trees. We found that kernels never outperform RNNs, even when a limited number of examples is employed for learning. We argue that convolution kernels may lead to feature space representations that are too sparse and too general because not focused on the specific learning task. The adaptive encoding mechanism in RNNs in this case allows us to obtain better prediction accuracy at smaller computational cost."
            },
            "slug": "Comparing-Convolution-Kernels-and-Recursive-Neural-Menchetti-Costa",
            "title": {
                "fragments": [],
                "text": "Comparing Convolution Kernels and Recursive Neural Networks for Learning Preferences on Structured Data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is argued that convolution kernels may lead to feature space representations that are too sparse and too general because not focused on the specific learning task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145941694"
                        ],
                        "name": "E. Littmann",
                        "slug": "E.-Littmann",
                        "structuredName": {
                            "firstName": "Enno",
                            "lastName": "Littmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Littmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30258243"
                        ],
                        "name": "H. Ritter",
                        "slug": "H.-Ritter",
                        "structuredName": {
                            "firstName": "Helge",
                            "lastName": "Ritter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43781167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da166748ed22670aed64ddcad9e562ffe0a562d4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Incrementally constructed cascade architectures are a promising alternative to networks of predefined size. This paper compares the direct cascade architecture (DCA) proposed in Littmann and Ritter (1992) to the cascade-correlation approach of Fahlman and Lebiere (1990) and to related approaches and discusses the properties on the basis of various benchmark results. One important virtue of DCA is that it allows the cascading of entire subnetworks, even if these admit no error-backpropagation. Exploiting this flexibility and using LLM networks as cascaded elements, we show that the performance of the resulting network cascades can be greatly enhanced compared to the performance of a single network. Our results for the Mackey-Glass time series prediction task indicate that such deeply cascaded network architectures achieve good generalization even on small data sets, when shallow, broad architectures of comparable size suffer from overfitting. We conclude that the DCA approach offers a powerful and flexible alternative to existing schemes such as, e.g., the mixtures of experts approach, for the construction of modular systems from a wide range of subnetwork types."
            },
            "slug": "Learning-and-Generalization-in-Cascade-Network-Littmann-Ritter",
            "title": {
                "fragments": [],
                "text": "Learning and Generalization in Cascade Network Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is concluded that the DCA approach offers a powerful and flexible alternative to existing schemes such as, e.g., the mixtures of experts approach, for the construction of modular systems from a wide range of subnetwork types."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10687508"
                        ],
                        "name": "A. Ceroni",
                        "slug": "A.-Ceroni",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Ceroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ceroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15456539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d9fbc8c4464f7a8d9446f8c8b52b5165849335",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-protein-secondary-structure-from-and-data-Ceroni-Frasconi",
            "title": {
                "fragments": [],
                "text": "Learning protein secondary structure from sequential and relational data"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9861,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30443043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "isKey": false,
            "numCitedBy": 2938,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "slug": "The-Cascade-Correlation-Learning-Architecture-Fahlman-Lebiere",
            "title": {
                "fragments": [],
                "text": "The Cascade-Correlation Learning Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056703"
                        ],
                        "name": "S. Menchetti",
                        "slug": "S.-Menchetti",
                        "structuredName": {
                            "firstName": "Sauro",
                            "lastName": "Menchetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Menchetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144073418"
                        ],
                        "name": "Fabrizio Costa",
                        "slug": "Fabrizio-Costa",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Costa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabrizio Costa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1086137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "723e55901724533dc79c2255ce044fef858db9ae",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wide-coverage-natural-language-processing-using-and-Menchetti-Costa",
            "title": {
                "fragments": [],
                "text": "Wide coverage natural language processing using kernel methods and neural networks for structured data"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27551792"
                        ],
                        "name": "J. Steil",
                        "slug": "J.-Steil",
                        "structuredName": {
                            "firstName": "Jochen",
                            "lastName": "Steil",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Steil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1359697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e26cecd18e7e5ae038b6fc444b83c17ec5504c3c",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an overview of current lines of research on learning with recurrent neural networks (RNNs). Topics covered are: understanding and unifi- cation of algorithms, theoretical foundations, new efforts to circumvent gradient vanishing, new architectures, and fusion with other learning methods and dynam- ical systems theory. The structuring guideline is to understand many new ap- proaches as different efforts to regularize and thereby improve recurrent learning. Often this is done on two levels: by restricting the learning objective by con- straints, for instance derived from stability conditions or weight normalization, and by imposing architectural constraints as for instance local recurrence."
            },
            "slug": "Tutorial:-Perspectives-on-Learning-with-RNNs-Hammer-Steil",
            "title": {
                "fragments": [],
                "text": "Tutorial: Perspectives on Learning with RNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An overview of current lines of research on learning with recurrent neural networks (RNNs) is presented, including understanding and understanding of algorithms, theoretical foundations, new efforts to circumvent gradient vanishing, new architectures, and fusion with other learning methods and dynam- ical systems theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7784541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386aa3d2f0be04bc5f982729861a270ee876de06",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past few years there has been a lot of work lying at the intersection of probability theory, logic programming and machine learning [14,18,13,9,6,1,11]. This work is known under the names of statistical relational learning [7,5], probabilistic logic learning [4], or probabilistic inductive logic programming. Whereas most of the existing works have started from a probabilistic learning perspective and extended probabilistic formalisms with relational aspects, I shall take a di.erent perspective, in which I shall start from inductive logic programming and study how inductive logic programming formalisms, settings and techniques can be extended to deal with probabilistic issues. This tradition has already contributed a rich variety of valuable formalisms and techniques, including probabilistic Horn abduction by David Poole, PRISMs by Sato, stochastic logic programs by Muggleton [13] and Cussens [2], Bayesian logic programs [10,8] by Kersting and De Raedt, and Logical Hidden Markov Models [11]. \n \nThe main contribution of this talk is the introduction of three probabilistic inductive logic programming settings which are derived from the learning from entailment, from interpretations and from proofs settings of the field of inductive logic programming [3]. Each of these settings contributes di.erent notions of probabilistic logic representations, examples and probability distributions. The first setting, probabilistic learning from entailment, is incorporated in the wellknown PRISM system [19] and Cussens's Failure Adjusted Maximisation approach to parameter estimation in stochastic logic programs [2]. A novel system that was recently developed and that fits this paradigm is the nFOIL system [12]. It combines key principles of the well-known inductive logic programming system FOIL [15] with the naive Bayes' appraoch. In probabilistic learning from entailment, examples are ground facts that should be probabilistically entailed by the target logic program. The second setting, probabilistic learning from interpretations, is incorporated in Bayesian logic programs [10,8], which integrate Bayesian networks with logic programs. This setting is also adopted by [6]. Examples in this setting are Herbrand interpretations that should be a probabilistic model for the target theory. The third setting, learning from proofs [17], is novel. It is motivated by the learning of stochastic context free grammars from tree banks. In this setting, examples are proof trees that should be probabilistically provable from the unknown stochastic logic programs. The sketched settings (and their instances presented) are by no means the only possible settings for probabilistic inductive logic programming, but still \u2013 I hope \u2013 provide useful insights into the state-of-the-art of this exciting field. \n \nFor a full survey of statistical relational learning or probabilistic inductive logic programming, the author would like to refer to [4], and for more details on the probabilistic inductive logic programming settings to [16], where a longer and earlier version of this contribution can be found."
            },
            "slug": "Statistical-Relational-Learning:-An-Inductive-Logic-Raedt",
            "title": {
                "fragments": [],
                "text": "Statistical Relational Learning: An Inductive Logic Programming Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The main contribution of this talk is the introduction of three probabilistic inductive logic programming settings which are derived from the learning from entailment, from interpretations and from proofs settings of the field of inductive Logic programming."
            },
            "venue": {
                "fragments": [],
                "text": "PKDD"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8169197"
                        ],
                        "name": "S. Brunak",
                        "slug": "S.-Brunak",
                        "structuredName": {
                            "firstName": "S\u00f8ren",
                            "lastName": "Brunak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brunak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15343954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ac3c0f5c9cb6632447c314082151b6b45112941",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nPredicting the secondary structure of a protein (alpha-helix, beta-sheet, coil) is an important step towards elucidating its three-dimensional structure, as well as its function. Presently, the best predictors are based on machine learning approaches, in particular neural network architectures with a fixed, and relatively short, input window of amino acids, centered at the prediction site. Although a fixed small window avoids overfitting problems, it does not permit capturing variable long-rang information.\n\n\nRESULTS\nWe introduce a family of novel architectures which can learn to make predictions based on variable ranges of dependencies. These architectures extend recurrent neural networks, introducing non-causal bidirectional dynamics to capture both upstream and downstream information. The prediction algorithm is completed by the use of mixtures of estimators that leverage evolutionary information, expressed in terms of multiple alignments, both at the input and output levels. While our system currently achieves an overall performance close to 76% correct prediction--at least comparable to the best existing systems--the main emphasis here is on the development of new algorithmic ideas.\n\n\nAVAILABILITY\nThe executable program for predicting protein secondary structure is available from the authors free of charge.\n\n\nCONTACT\npfbaldi@ics.uci.edu, gpollast@ics.uci.edu, brunak@cbs.dtu.dk, paolo@dsi.unifi.it."
            },
            "slug": "Exploiting-the-past-and-the-future-in-protein-Baldi-Brunak",
            "title": {
                "fragments": [],
                "text": "Exploiting the past and the future in protein secondary structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A family of novel architectures which can learn to make predictions based on variable ranges of dependencies are introduced, extending recurrent neural networks and introducing non-causal bidirectional dynamics to capture both upstream and downstream information."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144563617"
                        ],
                        "name": "J. Ramon",
                        "slug": "J.-Ramon",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Ramon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ramon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059125011"
                        ],
                        "name": "T. Gaertner",
                        "slug": "T.-Gaertner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Gaertner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gaertner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12726277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f5c802e44c1076c418f1bff7f266983fe1da577",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, kernel methods have become a popular tool for machine learning and data mining. As most \u2018real-world\u2019 data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. In this paper we study the trade-off between expressivity and efficiency of graph kernels. First, we motivate the need for this discussion by showing that fully general graph kernels can not even be approximated efficiently. We also discuss generalizations of graph kernels defined in literature and show that they are either not positive definite or not very useful. Finally, we propose a new graph kernel based on subtree patterns. We argue that while a little more computationally expensive, this kernel is more expressive than kernels based on walks."
            },
            "slug": "Expressivity-versus-efficiency-of-graph-kernels-Ramon-Gaertner",
            "title": {
                "fragments": [],
                "text": "Expressivity versus efficiency of graph kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The trade-off between expressivity and efficiency of graph kernels is studied and a new graph kernel based on subtree patterns is proposed, arguing that while a little more computationally expensive, this kernel is more expressive than kernels based on walks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 35730151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0f9f3c338dd84b054dabcbd50b725f2b3609ad9",
            "isKey": false,
            "numCitedBy": 4628,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "slug": "Kernel-Methods-for-Pattern-Analysis-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Kernel Methods for Pattern Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "venue": {
                "fragments": [],
                "text": "ICTAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144581812"
                        ],
                        "name": "R. Ilin",
                        "slug": "R.-Ilin",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Ilin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ilin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143730789"
                        ],
                        "name": "R. Kozma",
                        "slug": "R.-Kozma",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kozma",
                            "middleNames": [
                                "Thijs"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kozma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3249282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d399404458eaf5c3e1ea040bc959744b0a1a4ca",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Cellular simultaneous recurrent neural network (SRN) has been shown to be a function approximator more powerful than the multilayer perceptron (MLP). This means that the complexity of MLP would be prohibitively large for some problems while SRN could realize the desired mapping with acceptable computational constraints. The speed of training of complex recurrent networks is crucial to their successful application. This work improves the previous results by training the network with extended Kalman filter (EKF). We implemented a generic cellular SRN (CSRN) and applied it for solving two challenging problems: 2-D maze navigation and a subset of the connectedness problem. The speed of convergence has been improved by several orders of magnitude in comparison with the earlier results in the case of maze navigation, and superior generalization has been demonstrated in the case of connectedness. The implications of this improvements are discussed."
            },
            "slug": "Beyond-Feedforward-Models-Trained-by-A-Practical-a-Ilin-Kozma",
            "title": {
                "fragments": [],
                "text": "Beyond Feedforward Models Trained by Backpropagation: A Practical Training Tool for a More Efficient Universal Approximator"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work implemented a generic cellular SRN (CSRN) and applied it for solving two challenging problems: 2-D maze navigation and a subset of the connectedness problem, and superior generalization has been demonstrated in the case of connectedness."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707908"
                        ],
                        "name": "B. Jain",
                        "slug": "B.-Jain",
                        "structuredName": {
                            "firstName": "Brijnesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12228236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98587d738c1923d3f465ea5d0a0f9ad319220a4",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard pattern recognition provides effective and noise-tolerant tools for machine learning tasks; however, most approaches only deal with real vectors of a finite and fixed dimensionality. In this tutorial paper, we give an overview about extensions of pattern recognition towards non-standard data which are not contained in a finite dimensional space, such as strings, sequences, trees, graphs, or functions. Two major directions can be distinguished in the neural networks literature: models can be based on a similarity measure adapted to non-standard data, including kernel methods for structures as a very prominent approach, but also alternative metric based algorithms and functional networks; alternatively, non-standard data can be processed recursively within supervised and unsupervised recurrent and recursive networks and fully recurrent systems."
            },
            "slug": "Neural-methods-for-non-standard-data-Hammer-Jain",
            "title": {
                "fragments": [],
                "text": "Neural methods for non-standard data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This tutorial paper gives an overview about extensions of pattern recognition towards non-standard data which are not contained in a finite dimensional space, such as strings, sequences, trees, graphs, or functions."
            },
            "venue": {
                "fragments": [],
                "text": "ESANN"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3340574"
                        ],
                        "name": "O. Jesus",
                        "slug": "O.-Jesus",
                        "structuredName": {
                            "firstName": "Orlando",
                            "lastName": "Jesus",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Jesus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144250204"
                        ],
                        "name": "M. Hagan",
                        "slug": "M.-Hagan",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Hagan",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hagan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8830948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2562461b529222b5e8ff9c35d3e21ae9fad4eb66",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a general framework for describing dynamic neural networks-the layered digital dynamic network (LDDN). This framework allows the development of two general algorithms for computing the gradients and Jacobians for these dynamic networks: backpropagation-through-time (BPTT) and real-time recurrent learning (RTRL). The structure of the LDDN framework enables an efficient implementation of both algorithms for arbitrary dynamic networks. This paper demonstrates that the BPTT algorithm is more efficient for gradient calculations, but the RTRL algorithm is more efficient for Jacobian calculations"
            },
            "slug": "Backpropagation-Algorithms-for-a-Broad-Class-of-Jesus-Hagan",
            "title": {
                "fragments": [],
                "text": "Backpropagation Algorithms for a Broad Class of Dynamic Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that the BPTT algorithm is more efficient for gradient calculations, but the RTRL algorithm isMore efficient for Jacobian calculations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34531521"
                        ],
                        "name": "G. Adami",
                        "slug": "G.-Adami",
                        "structuredName": {
                            "firstName": "Giordano",
                            "lastName": "Adami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744234"
                        ],
                        "name": "P. Avesani",
                        "slug": "P.-Avesani",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Avesani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Avesani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727490"
                        ],
                        "name": "Diego Sona",
                        "slug": "Diego-Sona",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Sona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diego Sona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17869572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ebb53888206625230881a0b1e083b3245e3c7a2",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Clustering-documents-into-a-web-directory-for-a-Adami-Avesani",
            "title": {
                "fragments": [],
                "text": "Clustering documents into a web directory for bootstrapping a supervised classification"
            },
            "venue": {
                "fragments": [],
                "text": "Data Knowl. Eng."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17646686,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d119dc0690f9cd554b272fbb412a4e57cffb9d7",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this paper is to introduce the reader to new developments in Neural Networks and Kernel Machines concerning the treatment of structured domains. Specifically, we discuss the research on these relatively new models to introduce a novel and more general approach to QSPR/QSAR analysis. The focus is on the computational side and not on the experimental one."
            },
            "slug": "An-introduction-to-recursive-neural-networks-and-Micheli-Sperduti",
            "title": {
                "fragments": [],
                "text": "An introduction to recursive neural networks and kernel methods for cheminformatics."
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "New developments in Neural Networks and Kernel Machines concerning the treatment of structured domains are introduced to introduce a novel and more general approach to QSPR/QSAR analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Current pharmaceutical design"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734576"
                        ],
                        "name": "A. Bianucci",
                        "slug": "A.-Bianucci",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Bianucci",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bianucci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 93625325,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "fbb957405d4eaaa7acd2ddf0a3962346fc16782b",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach based on neural networks for structures to QSPR (quantitative structure-property relationships) and QSAR (quantitative structure-activity relationships) analysis. We face two quite different chemical applications using the same model, i.e. predicting the boiling point of a class of alkanes and QSAR of a class of benzodiazepines. The model, Cascade Correlation for structures, is a class of recursive neural networks recently proposed for the processing of structured domains. Through the use of this model we can represent and process the structure of chemical compounds in the form of labeled trees. We report our results on preliminary applications to show that the model, adopting this representation of molecular structure, can adaptively capture significant topological aspects and chemical fnnctionalities for each specific class of the molecules, just on the basis of the association between the molecular morphology and the target property."
            },
            "slug": "A-Novel-Approach-to-QSPR/QSAR-Based-on-Neural-for-Bianucci-Micheli",
            "title": {
                "fragments": [],
                "text": "A Novel Approach to QSPR/QSAR Based on Neural Networks for Structures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2861064"
                        ],
                        "name": "Wu Yilei",
                        "slug": "Wu-Yilei",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Yilei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wu Yilei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068479767"
                        ],
                        "name": "Song Qing",
                        "slug": "Song-Qing",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Qing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Qing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053947965"
                        ],
                        "name": "Liu Sheng",
                        "slug": "Liu-Sheng",
                        "structuredName": {
                            "firstName": "Liu",
                            "lastName": "Sheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liu Sheng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15383176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ade8bef7f5429fd1d0065f04fcaa45838b0b578",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "For training algorithms of recurrent neural networks (RNN), convergent speed and training error are always two contradictory performances. In this letter, we propose a normalized adaptive recurrent learning (NARL) to obtain a tradeoff between transient and steady-state response. An augmented term is added to error gradient to exactly model the derivative of cost function with respect to hidden layer weight. The influence of the induced gain of activation function on training stability is also taken into consideration. Moreover, adaptive learning rate is employed to improve the robustness of the gradient training. Finally, computer simulations of a model prediction problem are synthesized to give comparisons between NARL and conventional normalized real-time recurrent learning (N-RTRL)."
            },
            "slug": "A-Normalized-Adaptive-Training-of-Recurrent-Neural-Yilei-Qing",
            "title": {
                "fragments": [],
                "text": "A Normalized Adaptive Training of Recurrent Neural Networks With Augmented Error Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This letter proposes a normalized adaptive recurrent learning (NARL) to obtain a tradeoff between transient and steady-state response and adds an augmented term to error gradient to exactly model the derivative of cost function with respect to hidden layer weight."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34628173"
                        ],
                        "name": "K. Tsuda",
                        "slug": "K.-Tsuda",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Tsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60452768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b69df93991a1f5a712b20e832f5b0281acb3153b",
            "isKey": false,
            "numCitedBy": 928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology.Following three introductory chapters -- an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology -- the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods."
            },
            "slug": "Kernel-Methods-in-Computational-Biology-Sch\u00f6lkopf-Tsuda",
            "title": {
                "fragments": [],
                "text": "Kernel Methods in Computational Biology"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book presents different ideas for the design of kernel functions specifically adapted to various biological data, and covers different approaches to learning from heterogeneous data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115487"
                        ],
                        "name": "D. Cherqaoui",
                        "slug": "D.-Cherqaoui",
                        "structuredName": {
                            "firstName": "Driss",
                            "lastName": "Cherqaoui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cherqaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3067906"
                        ],
                        "name": "D. Villemin",
                        "slug": "D.-Villemin",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Villemin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Villemin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 97470833,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "a0e7ceec5e85604b62ce27ae59a51b4dfcff1590",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Back-propagation neural networks (NNs) are useful for the study of quantitative structure\u2013activity relationships or structure\u2013property correlations. Models of relationships between structure and boiling point (bp) of 150 alkanes were constructed by means of a multilayer neural network (NN) using the back-propagation algorithm. The results of our NN were compared with those of other models from the literature, and found to be better. The boiling points of the 150 alkanes were then predicted by removing 15 compounds (test set) and using the 135 other molecules as a training set. Using the same process, all the compounds in the data bank were then predicted in groups of 15 compounds. The results obtained were satisfying."
            },
            "slug": "Use-of-a-neural-network-to-determine-the-boiling-of-Cherqaoui-Villemin",
            "title": {
                "fragments": [],
                "text": "Use of a neural network to determine the boiling point of alkanes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734576"
                        ],
                        "name": "A. Bianucci",
                        "slug": "A.-Bianucci",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Bianucci",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bianucci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10031212,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "5220ea903fb9b90c035bf97595acceee0cd24c49",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the application of Cascade Correlation for structures to QSPR (quantitative structure-property relationships) and QSAR (quantitative structure-activity relationships) analysis. Cascade Correlation for structures is a neural network model recently proposed for the processing of structured data. This allows the direct treatment of chemical compounds as labeled trees, which constitutes a novel approach to QSPR/QSAR. We report the results obtained for QSPR on Alkanes (predicting the boiling point) and QSAR of a class of Benzodiazepines. Our approach compares favorably versus the traditional QSAR treatment based on equations and it is competitive with \u2018ad hoc\u2019 MLPs for the QSPR problem."
            },
            "slug": "Application-of-Cascade-Correlation-Networks-for-to-Bianucci-Micheli",
            "title": {
                "fragments": [],
                "text": "Application of Cascade Correlation Networks for Structures to Chemistry"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work reports the results obtained for QSPR on Alkanes (predicting the boiling point) and QSAR of a class of Benzodiazepines and it is competitive with \u2018ad hoc\u2019 MLPs for theQSPR problem."
            },
            "venue": {
                "fragments": [],
                "text": "Applied Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693945"
                        ],
                        "name": "T. G\u00e4rtner",
                        "slug": "T.-G\u00e4rtner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "G\u00e4rtner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. G\u00e4rtner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47840704"
                        ],
                        "name": "Peter A. Flach",
                        "slug": "Peter-A.-Flach",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Flach",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter A. Flach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145057418"
                        ],
                        "name": "S. Wrobel",
                        "slug": "S.-Wrobel",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Wrobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wrobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10856944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "647fdaacfcea319677175464b7db39e3b22c7808",
            "isKey": false,
            "numCitedBy": 883,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "As most \u2018real-world\u2019 data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. An interesting and important challenge is thus to investigate kernels on instances that are represented by graphs. So far, only very specific graphs such as trees and strings have been considered."
            },
            "slug": "On-Graph-Kernels:-Hardness-Results-and-Efficient-G\u00e4rtner-Flach",
            "title": {
                "fragments": [],
                "text": "On Graph Kernels: Hardness Results and Efficient Alternatives"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "As most \u2018real-world\u2019 data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data, but only very specific graphs such as trees and strings have been considered."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": false,
            "numCitedBy": 9897,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693945"
                        ],
                        "name": "T. G\u00e4rtner",
                        "slug": "T.-G\u00e4rtner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "G\u00e4rtner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. G\u00e4rtner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4471326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56fd28e8db60a696adc5b0f7acb9ef41d9ce1ec4",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods in general and support vector machines in particular have been successful in various learning tasks on data represented in a single table. Much 'real-world' data, however, is structured - it has no natural representation in a single table. Usually, to apply kernel methods to 'real-world' data, extensive pre-processing is performed to embed the data into areal vector space and thus in a single table. This survey describes several approaches of defining positive definite kernels on structured instances directly."
            },
            "slug": "A-survey-of-kernels-for-structured-data-G\u00e4rtner",
            "title": {
                "fragments": [],
                "text": "A survey of kernels for structured data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This survey describes several approaches of defining positive definite kernels on structured instances directly on the basis of areal vector space and thus in a single table."
            },
            "venue": {
                "fragments": [],
                "text": "SKDD"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103655437"
                        ],
                        "name": "W. T. Vettering",
                        "slug": "W.-T.-Vettering",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Vettering",
                            "middleNames": [
                                "T"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. T. Vettering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35046585"
                        ],
                        "name": "B. Flannery",
                        "slug": "B.-Flannery",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Flannery",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Flannery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "have also applied the direct computation of the weights by pseudoinverse using singular value decomposition (SVD) instead of normal equation [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119493087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7f2eb5ae3cac743a109bc707a04f27520310ad6",
            "isKey": false,
            "numCitedBy": 1497,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The two Numerical Recipes books are marvellous. The principal book, The Art of Scientific Computing, contains program listings for almost every conceivable requirement, and it also contains a well written discussion of the algorithms and the numerical methods involved. The Example Book provides a complete driving program, with helpful notes, for nearly all the routines in the principal book. The first edition of Numerical Recipes: The Art of Scientific Computing was published in 1986 in two versions, one with programs in Fortran, the other with programs in Pascal. There were subsequent versions with programs in BASIC and in C. The second, enlarged edition was published in 1992, again in two versions, one with programs in Fortran (NR(F)), the other with programs in C (NR(C)). In 1996 the authors produced Numerical Recipes in Fortran 90: The Art of Parallel Scientific Computing as a supplement, called Volume 2, with the original (Fortran) version referred to as Volume 1. Numerical Recipes in C++ (NR(C++)) is another version of the 1992 edition. The numerical recipes are also available on a CD ROM: if you want to use any of the recipes, I would strongly advise you to buy the CD ROM. The CD ROM contains the programs in all the languages. When the first edition was published I bought it, and have also bought copies of the other editions as they have appeared. Anyone involved in scientific computing ought to have a copy of at least one version of Numerical Recipes, and there also ought to be copies in every library. If you already have NR(F), should you buy the NR(C++) and, if not, which version should you buy? In the preface to Volume 2 of NR(F), the authors say 'C and C++ programmers have not been far from our minds as we have written this volume, and we think that you will find that time spent in absorbing its principal lessons will be amply repaid in the future as C and C++ eventually develop standard parallel extensions'. In the preface and introduction to NR(C++), the authors point out some of the problems in the use of C++ in scientific computing. I have not found any mention of parallel computing in NR(C++). Fortran has quite a lot going for it. As someone who has used it in most of its versions from Fortran II, I have seen it develop and leave behind other languages promoted by various enthusiasts: who now uses Algol or Pascal? I think it unlikely that C++ will disappear: it was devised as a systems language, and can also be used for other purposes such as scientific computing. It is possible that Fortran will disappear, but Fortran has the strengths that it can develop, that there are extensive Fortran subroutine libraries, and that it has been developed for parallel computing. To argue with programmers as to which is the best language to use is sterile. If you wish to use C++, then buy NR(C++), but you should also look at volume 2 of NR(F). If you are a Fortran programmer, then make sure you have NR(F), volumes 1 and 2. But whichever language you use, make sure you have one version or the other, and the CD ROM. The Example Book provides listings of complete programs to run nearly all the routines in NR, frequently based on cases where an anlytical solution is available. It is helpful when developing a new program incorporating an unfamiliar routine to see that routine actually working, and this is what the programs in the Example Book achieve. I started teaching computational physics before Numerical Recipes was published. If I were starting again, I would make heavy use of both The Art of Scientific Computing and of the Example Book. Every computational physics teaching laboratory should have both volumes: the programs in the Example Book are included on the CD ROM, but the extra commentary in the book itself is of considerable value. P Borcherds"
            },
            "slug": "Numerical-Recipes-in-C++:-The-Art-of-Scientific-CD-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical Recipes in C++: The Art of Scientific Computing (2nd edn)1 Numerical Recipes Example Book (C++) (2nd edn)2 Numerical Recipes Multi-Language Code CD ROM with LINUX or UNIX Single-Screen License Revised Version3"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Anyone involved in scientific computing ought to have a copy of at least one version of Numerical Recipes, and there also ought to be copies in every library."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583597"
                        ],
                        "name": "J. Kolen",
                        "slug": "J.-Kolen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kolen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kolen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492622"
                        ],
                        "name": "S. C. Kremer",
                        "slug": "S.-C.-Kremer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kremer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Kremer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60905440,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4f6bf6f955396254b04c2cb93fd29a84d1ee5900",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction Historical Remarks Adaptive Processing of Structured Information Applications Conclusion ]]>"
            },
            "slug": "From-Sequences-to-Data-Structures:-Theory-and-Kolen-Kremer",
            "title": {
                "fragments": [],
                "text": "From Sequences to Data Structures: Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction Historical Remarks Adaptive Processing of Structured Information Applications Conclusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60838698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ced4db5d559d4614efb5bbbcef353f7aac7a4f25",
            "isKey": false,
            "numCitedBy": 577,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Introduction-to-Statistical-Relational-Learning-and-Getoor-Taskar",
            "title": {
                "fragments": [],
                "text": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be relied on as a substitute for professional advice on how to deal with ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702610"
                        ],
                        "name": "Andrea Passerini",
                        "slug": "Andrea-Passerini",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Passerini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Passerini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9311765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df15e1a43d6560158ec9d5086a806fbf6ce3664",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a family of kernels over untyped and typed Prolog ground terms and show that they can be applied for learning in structured domains, presenting experimental results in a QSPR task."
            },
            "slug": "Kernels-on-Prolog-Ground-Terms-Passerini-Frasconi",
            "title": {
                "fragments": [],
                "text": "Kernels on Prolog Ground Terms"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A family of kernels over untyped and typed Prolog ground terms are described and it is shown that they can be applied for learning in structured domains, presenting experimental results in a QSPR task."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714441"
                        ],
                        "name": "F. Schleif",
                        "slug": "F.-Schleif",
                        "structuredName": {
                            "firstName": "Frank-Michael",
                            "lastName": "Schleif",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Schleif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46717733"
                        ],
                        "name": "U. Clauss",
                        "slug": "U.-Clauss",
                        "structuredName": {
                            "firstName": "U.",
                            "lastName": "Clauss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Clauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9320960"
                        ],
                        "name": "T. Villmann",
                        "slug": "T.-Villmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Villmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Villmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09300d908a38b92946f049c4e67f9e244baf1f8f",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper deals with the application of the novel generalized relevance learning vector quantization based classification algorithm in comparison to the unified maximum separability analysis as a special variant of support vector machine algorithms. The algorithms are compared and their performance on real life data, taken from clinical studies, is demonstrated. It is shown that the vector quantization classifier gives competitive results in comparison to the considered support vector machine algorithm and shows the recently theoretical proven equivalence in classification capability of both paradigms."
            },
            "slug": "Supervised-relevance-neural-gas-and-unified-maximum-Schleif-Clauss",
            "title": {
                "fragments": [],
                "text": "Supervised relevance neural gas and unified maximum separability analysis for classification of mass spectrometric data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the vector quantization classifier gives competitive results in comparison to the considered support vector machine algorithm and shows the recently theoretical proven equivalence in classification capability of both paradigms."
            },
            "venue": {
                "fragments": [],
                "text": "2004 International Conference on Machine Learning and Applications, 2004. Proceedings."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40200766"
                        ],
                        "name": "A. Sestito",
                        "slug": "A.-Sestito",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Sestito",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sestito"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61433378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5327a5f2bcbb4458d2460b59ea2ac98a72dd4e5",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dealing-with-Graphs-by-Neural-Networks-Micheli-Sestito",
            "title": {
                "fragments": [],
                "text": "Dealing with Graphs by Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61428447,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76ac2f265b589ff21153f1cf39367343d8a9d84f",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-Processing-of-Structured-Domains-in-PhD-Micheli",
            "title": {
                "fragments": [],
                "text": "Recursive Processing of Structured Domains in Machine Learning -PhD Thesis, TD-13/03"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123962567"
                        ],
                        "name": "M. C. Seiler",
                        "slug": "M.-C.-Seiler",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Seiler",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Seiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50678756"
                        ],
                        "name": "F. A. Seiler",
                        "slug": "F.-A.-Seiler",
                        "structuredName": {
                            "firstName": "Fritz",
                            "lastName": "Seiler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. A. Seiler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62717952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e980fbf251ecb28ba85eb092fc66ce284bb63be",
            "isKey": false,
            "numCitedBy": 13115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Recipes-in-C:-The-Art-of-Scientific-Seiler-Seiler",
            "title": {
                "fragments": [],
                "text": "Numerical Recipes in C: The Art of Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39261009"
                        ],
                        "name": "A. Cayley",
                        "slug": "A.-Cayley",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Cayley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cayley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The set of alkanes (saturated hydrocarbons) molecules provides a concrete example of real objects belonging to an SD. It is interesting to note that, as a historical fact, the mathematical notion of tree structure has been introduced by the mathematician A. Cayley in 1857 [ 38 ] when he was trying to model and to enumerate alkanes compounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60839177,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d67dd78cb0f7875abf0c2ea587ad600cb5210af3",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Collected-Mathematical-Papers:-On-the-Theory-of-Cayley",
            "title": {
                "fragments": [],
                "text": "The Collected Mathematical Papers: On the Theory of the Analytical Forms called Trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57082032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "518a1ed17652617adc2b076c61b7e5eb5bdd0d17",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recurrent-networks:-supervised-learning-Doya",
            "title": {
                "fragments": [],
                "text": "Recurrent networks: supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41231471"
                        ],
                        "name": "A. Micheli",
                        "slug": "A.-Micheli",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Micheli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Micheli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2869385"
                        ],
                        "name": "F. Portera",
                        "slug": "F.-Portera",
                        "structuredName": {
                            "firstName": "Filippo",
                            "lastName": "Portera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Portera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10535856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19accd01a97f884bdf4c7e033aed4df5294598a3",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-preliminary-empirical-comparison-of-recursive-and-Micheli-Portera",
            "title": {
                "fragments": [],
                "text": "A preliminary empirical comparison of recursive neural networks and tree kernel methods on regression tasks for tree structured domains"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": ", backpropagation through time (BPTT) [36], [37] and backpropagation through structure (BPTS) [12] training algorithms), in NN4G, there is no propagation of error signal through unfolded layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "First, with respect to recurrent/recursive approaches, which require a backpropagation of error through the unfolding network (e.g., backpropagation through time (BPTT) [36], [37] and backpropagation through structure (BPTS) [12] training algorithms), in NN4G, there is no propagation of error signal through unfolded layers."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks, A Comprehensive Foundation, 2nd ed"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, NJ: Prentice-Hall,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the theory of analytic forms called trees"
            },
            "venue": {
                "fragments": [],
                "text": "Philos. Mag"
            },
            "year": 1857
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The recurrent cascade-correlation learning architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Carnegie Mellon Univ"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Micheli is a member of the IEEE Computational Intelligence Society and of the Cheminformatics and QSAR Society"
            },
            "venue": {
                "fragments": [],
                "text": "Micheli is a member of the IEEE Computational Intelligence Society and of the Cheminformatics and QSAR Society"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new neural network model for graph processing"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. DII 01/05"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quantitative structure-property relationships for the normal boiling temperatures of acyclic carbonyl compounds"
            },
            "venue": {
                "fragments": [],
                "text": "Internet Electron. J. Molecular Design"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 51,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Neural-Network-for-Graphs:-A-Contextual-Approach-Micheli/ec2b2569b3a0d70a5b45d48b041dec9060d85eb7?sort=total-citations"
}