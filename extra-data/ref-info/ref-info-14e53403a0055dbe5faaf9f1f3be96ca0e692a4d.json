{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We used the simplest of the weak learners tested by Freund and Schapire [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We used the same experimental set-up as Freund and Schapire [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 425,
                                "start": 130
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm wh ich has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Br eiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1 996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997 ; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Ben gio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2685539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc1374bcd952032dabe891114f29092b868e01b8",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new technique for solv- ing multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC). Boosting is a general method of improving the ac- curacy of a given base or \"weak\" learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning al- gorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guar- antees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multi- class problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm. We also compare the new algorithm experimentally to other voting methods."
            },
            "slug": "Using-output-codes-to-boost-multiclass-learning-Schapire",
            "title": {
                "fragments": [],
                "text": "Using output codes to boost multiclass learning problems"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper describes a new technique for multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC), and shows that the new hybrid method has advantages of both."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 150
                            }
                        ],
                        "text": "Finally, there seem to be interesting connections between boosting and other models and their learning algorithms such as generalized additive models (Friedman et al., 1998) and maximum entropy methods (Csisz\u00e1r & Tusn\u00e1dy, 1984) which form a new and exciting research arena."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4829,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "giving a value of Z which is necessarily inferior to Eq. (5), but which  Freund and Schapire (1997)  are able to upper bound by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Freund and Schapire (1997)  gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first, given by  Freund and Schapire (1997) , uses standard VC-theory to bound the generalization error of the final hypothesis in terms of its training error and an additional term which is a function of the VC-dimension of the final hypothesis class and the number of training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ba566223e426677d12a9a18418c023a4deec77e",
            "isKey": false,
            "numCitedBy": 13123,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995;  Drucker & Cortes, 1996;  Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1266014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dfeb731fc0c79e04523cd655413c223f6fa102",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning."
            },
            "slug": "Boosting-Decision-Trees-Drucker-Cortes",
            "title": {
                "fragments": [],
                "text": "Boosting Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A constructive, incremental learning system for regression problems that models data by means of locally linear experts that does not compete for data during learning and derives asymptotic results for this method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068005320"
                        ],
                        "name": "Raj D. Iyer",
                        "slug": "Raj-D.-Iyer",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Iyer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raj D. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We have also used the new boosting framework for devising efficient ranking algorithms ( Freund et al., 1998 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16692650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "isKey": false,
            "numCitedBy": 2182,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations."
            },
            "slug": "An-Efficient-Boosting-Algorithm-for-Combining-Freund-Iyer",
            "title": {
                "fragments": [],
                "text": "An Efficient Boosting Algorithm for Combining Preferences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning, and gives theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709927"
                        ],
                        "name": "R. Maclin",
                        "slug": "R.-Maclin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Maclin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maclin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752379"
                        ],
                        "name": "D. Opitz",
                        "slug": "D.-Opitz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Opitz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Opitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9378257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3653266b5427295bdd54d6a22bf4caaa8c0b6961",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An ensemble consists of a set of independently trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund & Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classification algorithms. Our results clearly show two important facts. The first is that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is a powerful technique that can usually produce better ensembles than Bagging; however, it is more susceptible to noise and can quickly overfit a data set."
            },
            "slug": "An-Empirical-Evaluation-of-Bagging-and-Boosting-Maclin-Opitz",
            "title": {
                "fragments": [],
                "text": "An Empirical Evaluation of Bagging and Boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results clearly show that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "The approach described here is closely related to one used by Freund et al. (1998) for using boosting for more general ranking problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 159
                            }
                        ],
                        "text": "By allowing the weak hypothesis to effectively say \u201cI don\u2019t know,\u201d we introduce a model analogous to the \u201cspecialist\u201d model of Blum (1997), studied further by Freund et al. (1997). For fixedt , let W0, W\u22121, W+1 be defined by Wb = \u2211"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 156255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd2676dfc34f3a7963a93604576ebf0adda4959",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We study online learning algorithms that predict by com- bining the predictions of several subordinate prediction a lgorithms, sometimes called \"experts.\" These simple algorithms belon g to the multiplicative weights family of algorithms. The performance of these algorithms degrades only logarithmically with the number of experts, making them particularly useful in applications where the number of experts is very large. However, in applications such as text categorization, it is often natural for some of the ex perts to abstain from making predictions on some of the instances. We show how to transform algorithms that assume that all experts are always awake to algorithms that do not require this assumption. We also show how to derive corresponding loss bounds. Our method is very general, and can be applied to a large family of online learning algorithms. We also give applications to various prediction models including decision graphs and \"switching\" experts."
            },
            "slug": "Using-and-combining-predictors-that-specialize-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Using and combining predictors that specialize"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown how to transform algorithms that assume that all experts are always awake to algorithms that do not require this assumption, and how to derive corresponding loss bounds."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 420,
                                "start": 129
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14669208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "814cf172298d11db0ac9b839440ed8f3db93e438",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging (Breiman [1996a]) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym-arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly"
            },
            "slug": "Arcing-Classifiers-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two arcing algorithms are explored, they are compared to each other and to bagging, and the definitions of bias and variance for a classifier as components of the test set error are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In other words, rather than the weak learner simply choosing a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which minimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measure turns out to be identical to one earlier proposed by  Kearns and Mansour (1996) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, exactly this splitting criterion was proposed by  Kearns and Mansour (1996) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15982360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "063bf76e0259f5ef9a3eca729c1733e8fc59770b",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the performance of top?down algorithms for decision tree learning, such as those employed by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms areboostingalgorithms. By this we mean that if the functions that label the internal nodes of the decision tree can weakly approximate the unknown target function, then the top?down algorithms we study will amplify this weaks advantage to build a tree achieving any desired level of accuracy. The bounds we obtain for this amplification show an interesting dependence on thesplitting criterionused by the top?down algorithm. More precisely, if the functions used to label the internal nodes have error 1/2??as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees of size (1/?)O(1/?2?2)and (1/?)O(log(1/?)/?2)(respectively) suffice to drive the error below?. Thus (for example), a small constant advantage over random guessing is amplified to any larger constant advantage with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger bound of (1/?)O(1/?2)which is polynomial in 1/?) is obtained, which is provably optimal for decision tree algorithms. The differing bounds have a natured explanation in terms of concavity properties of the splitting criterion. The primary contribution of this work is in proving that some popular and empirically successful heuristics that are base on first principles meet the criteria of an independently motivated theoretical model."
            },
            "slug": "On-the-boosting-ability-of-top-down-decision-tree-Kearns-Mansour",
            "title": {
                "fragments": [],
                "text": "On the boosting ability of top-down decision tree learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work analyzes the performance of top?down algorithms for decision tree learning and proves that some popular and empirically successful heuristics that are base on first principles meet the criteria of an independently motivated theoretical model."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 123349680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc394d074c8504671eb37926d14a3df4a07520a0",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly."
            },
            "slug": "Arcing-classifier-(with-discussion-and-a-rejoinder-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing classifier (with discussion and a rejoinder by the author)"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two arcing algorithms are explored, compared to each other and to bagging, and the definitions of bias and variance for a classifier as components of the test set error are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 425,
                                "start": 130
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm wh ich has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Br eiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1 996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997 ; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Ben gio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 937841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79ea6a5a68e05065f82acd11a478aa7eac5f6c06",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered."
            },
            "slug": "Bagging,-Boosting,-and-C4.5-Quinlan",
            "title": {
                "fragments": [],
                "text": "Bagging, Boosting, and C4.5"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Results of applying Breiman's bagging and Freund and Schapire's boosting to a system that learns decision trees and testing on a representative collection of datasets show boosting shows the greater benefit."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It uses techniques developed by Bartlett (1998) and  Schapire et al. (1998) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Schapire et al. (1998)  proposed an alternative analysis to explain AdaBoost\u2019s empirically observed resistance to overfitting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997;  Schapire et al., 1998;  Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998;  Schwenk & Bengio, 1998 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5244761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d7dbd8503a9fe61c8e02465de2fa327e4d89c05",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Boosting\" is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], and decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 1.5% error on the UCI Letters and 8.1 % error on the UCI satellite data set."
            },
            "slug": "Training-Methods-for-Adaptive-Boosting-of-Neural-Schwenk-Bengio",
            "title": {
                "fragments": [],
                "text": "Training Methods for Adaptive Boosting of Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper uses AdaBoost to improve the performances of neural networks and compares training methods based on sampling the training set and weighting the cost function."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18699764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79c61e707f202e745090be01b03155b8b93a20e5",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. We rst show how to extend the standard notion of classiication by allowing each instance to be associated with multiple labels. We then discuss our approach for multiclass multi-label text categorization which is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identiication from unconstrained spoken customer responses."
            },
            "slug": "BoosTexter:-A-System-for-Multiclass-Multi-label-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A System for Multiclass Multi-label Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work shows how to extend the standard notion of classiication by allowing each instance to be associated with multiple labels and describes the application of the system to automatic call-type identiication from unconstrained spoken customer responses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear;  Dietterich & Bakiri, 1995;  Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47109072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d221bbcbd20c7157e4500f942de8ceec490f8936",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Solving Multiclass Learning Problems via Error-Correcting Output Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085505164"
                        ],
                        "name": "Holger Schwenk Yoshua",
                        "slug": "Holger-Schwenk-Yoshua",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Yoshua",
                            "middleNames": [
                                "Schwenk"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk Yoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6745092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17f6e8cc70685d21c513ce90e237d42a5011eed5",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201dBoosting\u201d is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], in particular decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks applied to character recognition tasks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 2% error on the UCI Letters offline characters data set."
            },
            "slug": "Adaptive-Boosting-of-Neural-Networks-for-Character-Yoshua",
            "title": {
                "fragments": [],
                "text": "Adaptive Boosting of Neural Networks for Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper uses AdaBoost to improve the performances of neural networks applied to character recognition tasks and compares training methods based on sampling the training set and weighting the cost function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 127
                            }
                        ],
                        "text": "By allowing the weak hypothesis to effectively say \u201cI don\u2019t know,\u201d we introduce a model analogous to the \u201cspecialist\u201d model of Blum (1997), studied further by Freund et al. (1997). For fixed , let , , be defined by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "By allowing the weak hypothesis to effectively say \u201cI don\u2019t know,\u201d we introduce a model analogous to the \u201cspecialist\u201d model of Blum (1997), studied further by Freund et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195325954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f23d52268e53f9ea81cc6b367eac55f38090257",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-Support-for-Winnow-and-Weighted-Majority-Blum",
            "title": {
                "fragments": [],
                "text": "Empirical Support for Winnow and Weighted-Majority Based Algorithms: Results on a Calendar Scheduling Domain"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 120
                            }
                        ],
                        "text": "To ext end the margins theory, then, let us define d to be thepseudodimension of H (for definitions, see, for instance, Haussler (1992)). Then using the method sketched in Section 2.4 of Schapire et al. together with Haussler and Long\u2019s (1995) Lemma 13, we can prove the fol lowing upper bound on generalization error which holds with probability 1 for all > 0 and for all f of the form above:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 120
                            }
                        ],
                        "text": "To ext end the margins theory, then, let us define d to be thepseudodimension of H (for definitions, see, for instance, Haussler (1992))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": true,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 85
                            }
                        ],
                        "text": "The VC-dimension of the final hypothes is class can be computed using the methods of Baum and Haussler (1989). Interprettin g the derived upper bound as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1757,
                                "start": 22
                            }
                        ],
                        "text": "Following the work of Bartlett (1 998), this method is based on the \u201cmargins\u201d achieved by the final hypothesis on the training ex amples. The margin is a measure of the \u201cconfidence\u201d of the prediction. Schapire et al. show that larger margins imply lower generalization error \u2014 regardless of the number of ro unds. Moreover, they show that AdaBoost tends to increase the margins of the training e xamples. To a large extent, their analysis can be carried over to the current context, w hich is the focus of this section. As a first step in applying their theory, we assum e that each weak hypothesisht has bounded range. Recall that the final hypothesis has the form H(x) = sign(f(x)) wheref(x) =Xt tht(x): Since theht\u2019s are bounded and since we only care about the sign of f , we can rescale theht\u2019s and normalize the t\u2019s allowing us to assume without loss of generality that each ht : X ! [ 1;+1], each t 2 [0; 1] andPt t = 1. Let us also assume that each ht belongs to a hypothesis space H. Schapire et al. define the marginof a labeled example (x; y) to beyf(x). The margin then is in [ 1;+1], and is positive if and only ifH makes a correct prediction on this example. We further regard the magnitude of the margin as a measure of th e confidence of H \u2019s prediction. Schapire et al.\u2019s results can be applied directly in the present context only in the special case that each 2 H has rangef 1;+1g. This case is not of much interest, however, since our focus is on weak hypotheses with real-valued predictions. To ext end the margins theory, then, let us define d to be thepseudodimension of H (for definitions, see, for instance, Haussler (1992)). Then using the method sketched in Section 2.4 of Schapire et al. together with Haussler and Long\u2019s (1995) Lemma 13, we can prove the fol lowing upper bound on generalization error which holds with probability 1 for all > 0 and for all f of the form above: PrS [yf(x) ]+O 1 pm d log2(m=d) 2 + log(1= ) 1=2! : Here,PrS denotes probability with respect to choosing an example (x; y) uniformly at random from the training set."
                    },
                    "intents": []
                }
            ],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": true,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144054997"
                        ],
                        "name": "A. Offutt",
                        "slug": "A.-Offutt",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Offutt",
                            "middleNames": [
                                "Jefferson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Offutt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108321098"
                        ],
                        "name": "Stephen D. Lee",
                        "slug": "Stephen-D.-Lee",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen D. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61901579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98503eebd77fb882722709c14d29f7e2b4a9f28",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Both optimization techniques and expert systems technologies are popular approaches for developing tools to assist in complex problem-solving tasks. Because of the underlying complexity of many such tasks, however, the models of the world implicitly or explicitly embedded in such tools are often incomplete and the problem-solving methods fallible. The result can be \"brittleness\" in situations that were not anticipated by the system designers. To deal with this weakness, it has been suggested that \"cooperative\" rather than \"automated\" problem-solving systems be designed. Such cooperative systems are proposed to explicitly enhance the collaboration of the person (or a group of people) and the computer system. This study evaluates the impact of alternative design concepts on the performance of 30 airline pilots interacting with such a cooperative system designed to support enroute flight planning. The results clearly demonstrate that different system design concepts can strongly influence the cognitive processes and resultant performances of users. Based on think-aloud protocols, cognitive models are proposed to account for how features of the Computer system interacted with specific types of scenarios to influence exploration and decision making by the pilots. The results are then used to develop recommendations for guiding the design of cooperative systems."
            },
            "slug": "An-Empirical-Evaluation-Offutt-Lee",
            "title": {
                "fragments": [],
                "text": "An Empirical Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This study evaluates the impact of alternative design concepts on the performance of 30 airline pilots interacting with a cooperative system designed to support enroute flight planning and develops recommendations for guiding the design of cooperative systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "A similar scheme was previously proposed by Quinlan (1996) for assigning confidences to the predictions made at the leaves of a decision tree."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 41342869,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "415e448f1b5fdb95b496772fa02d78190d3660e9",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Generalization-of-Sauer's-Lemma-Haussler-Long",
            "title": {
                "fragments": [],
                "text": "A Generalization of Sauer's Lemma"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Theory, Ser. A"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9911,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 32
                            }
                        ],
                        "text": "It uses techniques developed by Bartlett (1998) and Schapire et al. (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "Following the work of Bartlett (1998), this method is based on the \u201cmargins\u201d achieved by the final hypothesis on the training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1723,
                                "start": 22
                            }
                        ],
                        "text": "Following the work of Bartlett (1998), this method is based on the \u201cmargins\u201d achieved by the final hypothesis on the training examples. The margin is a measure of the \u201cconfidence\u201d of the prediction. Schapire et al. show that larger margins imply lower generalization error \u2014 regardless of the number of rounds. Moreover, they show that AdaBoost tends to increase the margins of the training examples. To a large extent, their analysis can be carried over to the current context, which is the focus of this section. As a first step in applying their theory, we assume that each weak hypothesis > has bounded range. Recall that the final hypothesis has the form A where &> G Since the > \u2019s are bounded and since we only care about the sign of , we can rescale the > \u2019s and normalize the \u2019s allowing us to assume without loss of generality that each > ? , B MP2435 7683 O , each M J !3!O and , 3 . Let us also assume that each > belongs to a hypothesis space . Schapire et al. define the margin of a labeled example \" & to be \" . The margin then is in MN243' G683!O , and is positive if and only if makes a correct prediction on this example. We further regard the magnitude of the margin as a measure of the confidence of \u2019s prediction. Schapire et al.\u2019s results can be applied directly in the present context only in the special case that each > has range 0 2435 G68359 . This case is not of much interest, however, since our focus is on weak hypotheses with real-valued predictions. To extend the margins theory, then, let us define to be the pseudodimension of (for definitions, see, for instance, Haussler (1992)). Then using the method sketched in Section 2.4 of Schapire et al. together with Haussler and Long\u2019s (1995) Lemma 13, we can prove the following upper bound on generalization error which holds with probability 3 2 for all H J and for all of the form above: M \" 4( :O 6 3 < \"< 6 $ 3 Here, denotes probability with respect to choosing an example & uniformly at random from the training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 149
                            }
                        ],
                        "text": "\u2026average function satisfies the following generalization-error bound for all F H :\nL E /% H N2% L E 1% :N,4 2 ; \"; 4 &2\nProof: Using techniques from Bartlett (1998), Schapire et al. (1998, Theorem 4) give a theorem which states that, for F H and F H , the probability over the random choice of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": true,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 46
                            }
                        ],
                        "text": "This is essentially the approach advocated by Dietterich and Bakiri (1995) in a somewhat different set ti g."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 49
                            }
                        ],
                        "text": "this method which combines these techniques with Dietterich and Bakiri\u2019s ( 1995) outputcoding method. (Another method of combining boosting and output cod ing was proposed by Schapire (1997). Although superficially similar, his method is in fact quite different from what is presented here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 20
                            }
                        ],
                        "text": "The first, given by Freund and Schapire (1997), uses standard VC-theory to bound the generalization error of the final hypothesis in terms of its training erro r and an additional term which is a function of the VC-dimension of the final hypothesis cl a s and the number of training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 20
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm wh ich has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Br eiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1 996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997 ; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Ben gio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving multiclass learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "A slightly generalized version of Freund and Schapire\u2019s AdaBoost algorithm [lo] is shown in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "We begin by showing how Freund and Schapire\u2019s [lo] version of AdaBoost can be derived as a special case of our new version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "M2 [lo] is a special case of the use of ranking loss in which all data are single-labeled, the weak learner attempts to maximize Irt I as in Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "(4), but which Freund and Schapire [lo] are able to upper bound by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Corollary 2 ([lo]) Using the notation of Figure I, assume each ht has range [ - 1, + 1] and that we choose"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "55( 1): 119-l 39"
            },
            "venue": {
                "fragments": [],
                "text": "August"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2847,
                                "start": 20
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998). To boost using confidencerated predictions, we propose a generalization of AdaBoost in which the main parameters \u03b1t are tuned using one of a number of methods that we describe in detail. Intuitively, the \u03b1t \u2019s control the influence of each of the weak hypotheses. To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters \u03b1t are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clarification of the criterion that should be used in setting \u03b1t . As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hypotheses. Based on this analysis, we give a number of methods for choosing \u03b1t . We show that the optimal tuning (with respect to our criterion) of \u03b1t can be found numerically in general, and we give exact methods of setting \u03b1t in special cases. Freund and Schapire also considered the case in which the individual predictions of the weak hypotheses are allowed to carry a confidence. However, we show that their setting of \u03b1t is only an approximation of the optimal tuning which can be found using our techniques. We next discuss methods for designing weak learners with confidence-rated predictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically assigning a level of confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boosting decision trees. Our analysis can be viewed as a partial theoretical justification for his experimentally successful method. Our technique also leads to a modified criterion for selecting such domain-partitioning weak hypotheses. In other words, rather than the weak learner simply choosing a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which minimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also outline methods that can be used to analyze generalization error as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 15
                            }
                        ],
                        "text": "(5), but which Freund and Schapire (1997) are able to upper bound by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 80
                            }
                        ],
                        "text": "We have thus proved the following corollary of Theorem 1 which is equivalent to Freund and Schapire\u2019s (1997) Theorem 6:"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 8
                            }
                        ],
                        "text": "Indeed, Freund and Schapire\u2019s (1997) \u201cpseudoloss\u201d-based algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 535,
                                "start": 68
                            }
                        ],
                        "text": "Note that, in the single-label case, this algorithm is identical to Freund and Schapire\u2019s (1997) AdaBoost.M2 algorithm. We used these algorithms for two-class and multiclass problems alike. Note, however, that discrete AdaBoost.MR and discrete AdaBoost.MH are equivalent algorithms for two-class problems. We compared the three algorithms on a collection of benchmark problems available from the repository at University of California at Irvine (Merz & Murphy, 1998). We used the same experimental set-up as Freund and Schapire (1996). Namely, if a test set was already provided, experiments were run 20 times and the results averaged (since some of the learning algorithms may be randomized)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2206,
                                "start": 20
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998). To boost using confidencerated predictions, we propose a generalization of AdaBoost in which the main parameters \u03b1t are tuned using one of a number of methods that we describe in detail. Intuitively, the \u03b1t \u2019s control the influence of each of the weak hypotheses. To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters \u03b1t are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clarification of the criterion that should be used in setting \u03b1t . As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hypotheses. Based on this analysis, we give a number of methods for choosing \u03b1t . We show that the optimal tuning (with respect to our criterion) of \u03b1t can be found numerically in general, and we give exact methods of setting \u03b1t in special cases. Freund and Schapire also considered the case in which the individual predictions of the weak hypotheses are allowed to carry a confidence. However, we show that their setting of \u03b1t is only an approximation of the optimal tuning which can be found using our techniques. We next discuss methods for designing weak learners with confidence-rated predictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically assigning a level of confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boosting decision trees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3434,
                                "start": 20
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998). To boost using confidencerated predictions, we propose a generalization of AdaBoost in which the main parameters \u03b1t are tuned using one of a number of methods that we describe in detail. Intuitively, the \u03b1t \u2019s control the influence of each of the weak hypotheses. To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters \u03b1t are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clarification of the criterion that should be used in setting \u03b1t . As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hypotheses. Based on this analysis, we give a number of methods for choosing \u03b1t . We show that the optimal tuning (with respect to our criterion) of \u03b1t can be found numerically in general, and we give exact methods of setting \u03b1t in special cases. Freund and Schapire also considered the case in which the individual predictions of the weak hypotheses are allowed to carry a confidence. However, we show that their setting of \u03b1t is only an approximation of the optimal tuning which can be found using our techniques. We next discuss methods for designing weak learners with confidence-rated predictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically assigning a level of confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boosting decision trees. Our analysis can be viewed as a partial theoretical justification for his experimentally successful method. Our technique also leads to a modified criterion for selecting such domain-partitioning weak hypotheses. In other words, rather than the weak learner simply choosing a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which minimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also outline methods that can be used to analyze generalization error as well. Next, we show how to extend the methods described above for binary classification problems to the multiclass case, and, more generally, to the multi-labelcase in which each example may belong to more than one class. Such problems arise naturally, for instance, in text categorization problems where the same document (say, a news article) may easily be relevant to more than one topic (such as politics, sports, etc.). Freund and Schapire (1997) gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 68
                            }
                        ],
                        "text": "Note that, in the single-label case, this algorithm is identical to Freund and Schapire\u2019s (1997) AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 20
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 9
                            }
                        ],
                        "text": "Deriving Freund and Schapire\u2019s choice of \u03b1t We begin by showing how Freund and Schapire\u2019s (1997) version of AdaBoost can be derived as a special case of our new version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments with a new boosting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 9
                            }
                        ],
                        "text": "Deriving Freund and Schapire\u2019s choice of t We begin by showing how Freund and Schapire\u2019s (1997) version of AdaBoost can be derived as a special case of our new version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 825,
                                "start": 9
                            }
                        ],
                        "text": "Deriving Freund and Schapire\u2019s choice of t We begin by showing how Freund and Schapire\u2019s (1997) version of AdaBoost can be derived as a special case of our new version. For weak hypothese sh with range[ 1;+1\u2104, their choice of can be obtained by approximating Z as follows: Z = Xi D(i)e ui Xi D(i) 1 + ui 2 e + 1 ui 2 e : (4) This upper bound is valid since ui 2 [ 1;+1\u2104, and is in fact exact if h has rangef 1;+1g (so thatui 2 f 1;+1g). (A proof of the bound follows immediately from the convexi ty of e x for any constant 2 R.) Next, we can analytically choose to minimize the right hand side of Eq. (4) giving = 1 2 ln 1 + r 1 r wherer =PiD(i)ui. Plugging into Eq. (4), this choice gives the upper bound Z p1 r2: We have thus proved the following corollary of Theorem 1 whic h is equivalent to Freund and Schapire\u2019s (1997) Theorem 6: COROLLARY 1 ((FREUND & SCHAPIRE, 1997)) Using the notation of Figure 1, assume eachht has range[ 1;+1\u2104 and that we choose t = 1 2 ln 1 + rt 1 rt wherert =Xi Dt(i)yiht(xi) = Ei Dt [yiht(xi)\u2104 : Then the training error of H is at most T Y t=1q1 r2 t :"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 574,
                                "start": 40
                            }
                        ],
                        "text": "We used the same experimental set-up as Freund and Schapire (1996). Namely, if a test set was already provided, experiments were run 20 times and the resu lts averaged (since some of the learning algorithms may be randomized). If no test set wa s provided, then 10-fold cross validation was used and rerun 10 times for a total of 100 runs o f each algorithm. We tested on the same set of benchmarks, except that we dropped the \u201cvow el\u201d dataset. Each version of AdaBoost was run for 1000 rounds. We used the simplest of the weak learners tested by Freund and Schapire (1996). This weak learner finds a weak hypothesis which makes its predicti on based on the result of a single test comparing one of the attributes to one of its poss ible values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1541,
                                "start": 100
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a c le n upper bound on the training error of AdaBoost when the parameters t are left unspecified. For the purposes of minimizing training error, this analysis provides an imm ediate clarification of the criterion that should be used in setting t. As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulat ing i s weak hypotheses. Based on this analysis, we give a number of methods for choosi ng t. We show that the optimal tuning (with respect to our criterion) of t can be found numerically in general, and we give exact methods of setting t in special cases. Freund and Schapire also considered the case in which the ind ividual predictions of the weak hypotheses are allowed to carry a confidence. However, w e show that their setting of t is only an approximation of the optimal tuning which can be fo und using our techniques. We next discuss methods for designing weak learners with con fidence-rated predictions using the criterion provided by our analysis. For weak hypot heses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically ass igning a level of confidence to the predictions which are made within each region. This me thod turns out to be closely related to a heuristic method proposed by Quinlan (1996) for bo sting decision trees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 40
                            }
                        ],
                        "text": "We used the same experimental set-up as Freund and Schapire (1996). Namely, if a test set was already provided, experiments were run 20 times and the resu lts averaged (since some of the learning algorithms may be randomized)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2779,
                                "start": 100
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a c le n upper bound on the training error of AdaBoost when the parameters t are left unspecified. For the purposes of minimizing training error, this analysis provides an imm ediate clarification of the criterion that should be used in setting t. As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulat ing i s weak hypotheses. Based on this analysis, we give a number of methods for choosi ng t. We show that the optimal tuning (with respect to our criterion) of t can be found numerically in general, and we give exact methods of setting t in special cases. Freund and Schapire also considered the case in which the ind ividual predictions of the weak hypotheses are allowed to carry a confidence. However, w e show that their setting of t is only an approximation of the optimal tuning which can be fo und using our techniques. We next discuss methods for designing weak learners with con fidence-rated predictions using the criterion provided by our analysis. For weak hypot heses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically ass igning a level of confidence to the predictions which are made within each region. This me thod turns out to be closely related to a heuristic method proposed by Quinlan (1996) for bo sting decision trees. Our analysis can be viewed as a partial theoretical justificatio n for his experimentally successful method. Our technique also leads to a modified criterion for selectin g such domain-partitioning weak hypotheses. In other words, rather than the weak learne r simply choosing a weak hypothesis with low training error as has usually been done i n the past, we show that, theoretically, our methods work best when combined with a we k l arner which minimizes an alternative measure of \u201cbadness.\u201d For growing decision t rees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour (1 996). Although we primarily focus on minimizing training error, w e also outline methods that can be used to analyze generalization error as well. Next, we show how to extend the methods described above for bi na y classification problems to the multiclass case, and, more generally, to the multi-labelcase in which each example may belong to more than one class. Such problems arise n aturally, for instance, in text categorization problems where the same document (say, a news article) may easily be relevant to more than one topic (such as politics, sports, et c.). Freund and Schapire (1997) gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 69
                            }
                        ],
                        "text": "Note that, in the single-label case, th is algorithm is identical to Freund and Schapire\u2019s (1997) AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments with a new b"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 40
                            }
                        ],
                        "text": "We used the same experimental set-up as Freund and Schapire (1996). Namely, if a tes t set was already provided, experiments were run 20 times and the results averaged (s ince some of the learning algorithms may be randomized)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 9
                            }
                        ],
                        "text": "Deriving Freund and Schapire\u2019s choice of t We begin by showing how Freund and Schapire\u2019s (1997) version of AdaBo ost can be derived as a special case of our new version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1537,
                                "start": 101
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenti ng a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters t are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clari fication of the criterion that should be used in setting t. As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hy pot eses. Based on this analysis, we give a number of methods for choosing t. We show that the optimal tuning (with respect to our criterion) of t can be found numerically in general, and we give exact methods of setting t in special cases. Freund and Schapire also considered the case in which the individual predict ions of the weak hypotheses are allowed to carry a confidence. However, we show that their set ting of t is only an approximation of the optimal tuning which can be found usin g our techniques. We next discuss methods for designing weak learners with confidence-rated pr edictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such a decision trees, we present and analyze a simple method for automatically assigning a level o f confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boostin g decision trees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2184,
                                "start": 101
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenti ng a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters t are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clari fication of the criterion that should be used in setting t. As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hy pot eses. Based on this analysis, we give a number of methods for choosing t. We show that the optimal tuning (with respect to our criterion) of t can be found numerically in general, and we give exact methods of setting t in special cases. Freund and Schapire also considered the case in which the individual predict ions of the weak hypotheses are allowed to carry a confidence. However, we show that their set ting of t is only an approximation of the optimal tuning which can be found usin g our techniques. We next discuss methods for designing weak learners with confidence-rated pr edictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such a decision trees, we present and analyze a simple method for automatically assigning a level o f confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boostin g decision trees. Our analysis can be viewed as a partial theoretical justification for his experimen tally successful method. Our technique also leads to a modified criterion for selecting such domain-p artitioning weak hypotheses. In other words, rather than the weak learner simply choosin g a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which m inimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measur e turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also o utline methods that can be used to analyze generalization error as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 825,
                                "start": 9
                            }
                        ],
                        "text": "Deriving Freund and Schapire\u2019s choice of t We begin by showing how Freund and Schapire\u2019s (1997) version of AdaBo ost can be derived as a special case of our new version. For weak hypotheses h with range[ 1;+1], their choice of can be obtained by approximating Z as follows: Z = Xi D(i)e ui Xi D(i) 1 + ui 2 e + 1 ui 2 e : (4) This upper bound is valid since ui 2 [ 1;+1], and is in fact exact if h has rangef 1;+1g (so thatui 2 f 1;+1g). (A proof of the bound follows immediately from the convexity of e x for any constant 2 R.) Next, we can analytically choose to minimize the right hand side of Eq. (4) giving = 1 2 ln 1 + r 1 r wherer =PiD(i)ui. Plugging into Eq. (4), this choice gives the upper bound Z p1 r2: We have thus proved the following corollary of Theorem 1 which is equi valent to Freund and Schapire\u2019s (1997) Theorem 6: COROLLARY 1 ((FREUND & SCHAPIRE, 1997)) Using the notation of Figure 1, assume eachht has range[ 1;+1] and that we choose t = 1 2 ln 1 + rt 1 rt wherert =Xi Dt(i)yiht(xi) = Ei Dt [yiht(xi)] : Then the training error of H is at most T Y t=1q1 r2 t :"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2772,
                                "start": 101
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenti ng a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters t are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clari fication of the criterion that should be used in setting t. As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hy pot eses. Based on this analysis, we give a number of methods for choosing t. We show that the optimal tuning (with respect to our criterion) of t can be found numerically in general, and we give exact methods of setting t in special cases. Freund and Schapire also considered the case in which the individual predict ions of the weak hypotheses are allowed to carry a confidence. However, we show that their set ting of t is only an approximation of the optimal tuning which can be found usin g our techniques. We next discuss methods for designing weak learners with confidence-rated pr edictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such a decision trees, we present and analyze a simple method for automatically assigning a level o f confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boostin g decision trees. Our analysis can be viewed as a partial theoretical justification for his experimen tally successful method. Our technique also leads to a modified criterion for selecting such domain-p artitioning weak hypotheses. In other words, rather than the weak learner simply choosin g a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which m inimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measur e turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also o utline methods that can be used to analyze generalization error as well. Next, we show how to extend the methods described above for binary class ifi tion problems to the multiclass case, and, more generally, to the multi-labelcase in which each example may belong to more than one class. Such problems arise naturally, fo r instance, in text categorization problems where the same document (say, a news article) may asily be relevant to more than one topic (such as politics, sports, etc.). Freund and Schapire (1997) gave two algorithms for boosting multi class problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 68
                            }
                        ],
                        "text": "Note that, in the single-label case, this algorithm is identical to Freund and Schapire\u2019s (1997) AdaBoost."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments with a new boostin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 52
                            }
                        ],
                        "text": "It uses techniques developed by Bartlett (1998) and Schapire et al. (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997;  Schapire et al., 1998;  Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It uses techniques developed by Bartlett (1998) and  Schapire et al. (1998) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1998) proposed an alternative analysis to explain AdaBoost\u2019s empirically observed resistance to overfitting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Schapire et al. (1998)  proposed an alternative analysis to explain AdaBoost\u2019s empirically observed resistance to overfitting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 125
                            }
                        ],
                        "text": "(4), this choice gives the upper bound % 2 1 We have thus proved the following corollary of Theorem 1 which is equivalent to Freund and Schapire\u2019s (1997) Theorem 6: COROLLARY 1 ((FREUND & SCHAPIRE, 1997)) Using the notation of Figure 1, assume each = has range LM132: 5462!N and that we choose 2 4 2 1 where"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1524,
                                "start": 100
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clarification of the criterion that should be used in setting . As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hypotheses. Based on this analysis, we give a number of methods for choosing ! . We show that the optimal tuning (with respect to our criterion) of \" can be found numerically in general, and we give exact methods of setting in special cases. Freund and Schapire also considered the case in which the individual predictions of the weak hypotheses are allowed to carry a confidence. However, we show that their setting of is only an approximation of the optimal tuning which can be found using our techniques. We next discuss methods for designing weak learners with confidence-rated predictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically assigning a level of confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boosting decision trees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 40
                            }
                        ],
                        "text": "We used the same experimental set-up as Freund and Schapire (1996). Namely, if a test set was already provided, experiments were run 20 times and the results averaged (since some of the learning algorithms may be randomized)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 9
                            }
                        ],
                        "text": "Deriving Freund and Schapire\u2019s choice of We begin by showing how Freund and Schapire\u2019s (1997) version of AdaBoost can be derived as a special case of our new version."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 8
                            }
                        ],
                        "text": "Indeed, Freund and Schapire\u2019s (1997) \u201cpseudoloss\u201dbased algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 141
                            }
                        ],
                        "text": "We can calculate as: * 9 \"JK * 9 \"JK 4 4 It can easily be verified that is minimized when For this setting of , we have 4 (5) For this case, Freund and Schapire\u2019s original AdaBoost algorithm would instead have made the more conservative choice 4 4 giving a value of which is necessarily inferior to Eq. (5), but which Freund and Schapire (1997) are able to upper bound by % $ 4 4 ! (6) If H (so that = has range / 132: 5462:7 ), then the choices of and resulting values of are identical."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 40
                            }
                        ],
                        "text": "We used the same experimental set-up as Freund and Schapire (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 68
                            }
                        ],
                        "text": "Note that, in the single-label case, this algorithm is identical to Freund and Schapire\u2019s (1997) AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2165,
                                "start": 100
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clarification of the criterion that should be used in setting . As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hypotheses. Based on this analysis, we give a number of methods for choosing ! . We show that the optimal tuning (with respect to our criterion) of \" can be found numerically in general, and we give exact methods of setting in special cases. Freund and Schapire also considered the case in which the individual predictions of the weak hypotheses are allowed to carry a confidence. However, we show that their setting of is only an approximation of the optimal tuning which can be found using our techniques. We next discuss methods for designing weak learners with confidence-rated predictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically assigning a level of confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boosting decision trees. Our analysis can be viewed as a partial theoretical justification for his experimentally successful method. Our technique also leads to a modified criterion for selecting such domain-partitioning weak hypotheses. In other words, rather than the weak learner simply choosing a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which minimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also outline methods that can be used to analyze generalization error as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 570,
                                "start": 40
                            }
                        ],
                        "text": "We used the same experimental set-up as Freund and Schapire (1996). Namely, if a test set was already provided, experiments were run 20 times and the results averaged (since some of the learning algorithms may be randomized). If no test set was provided, then 10-fold cross validation was used and rerun 10 times for a total of 100 runs of each algorithm. We tested on the same set of benchmarks, except that we dropped the \u201cvowel\u201d dataset. Each version of AdaBoost was run for 1000 rounds. We used the simplest of the weak learners tested by Freund and Schapire (1996). This weak learner finds a weak hypothesis which makes its prediction based on the result of a single test comparing one of the attributes to one of its possible values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2753,
                                "start": 100
                            }
                        ],
                        "text": "To determine the proper tuning of these parameters, we begin by presenting a streamlined version of Freund and Schapire\u2019s analysis which provides a clean upper bound on the training error of AdaBoost when the parameters are left unspecified. For the purposes of minimizing training error, this analysis provides an immediate clarification of the criterion that should be used in setting . As discussed below, this analysis also provides the criterion that should be used by the weak learner in formulating its weak hypotheses. Based on this analysis, we give a number of methods for choosing ! . We show that the optimal tuning (with respect to our criterion) of \" can be found numerically in general, and we give exact methods of setting in special cases. Freund and Schapire also considered the case in which the individual predictions of the weak hypotheses are allowed to carry a confidence. However, we show that their setting of is only an approximation of the optimal tuning which can be found using our techniques. We next discuss methods for designing weak learners with confidence-rated predictions using the criterion provided by our analysis. For weak hypotheses which partition the instance space into a small number of equivalent prediction regions, such as decision trees, we present and analyze a simple method for automatically assigning a level of confidence to the predictions which are made within each region. This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boosting decision trees. Our analysis can be viewed as a partial theoretical justification for his experimentally successful method. Our technique also leads to a modified criterion for selecting such domain-partitioning weak hypotheses. In other words, rather than the weak learner simply choosing a weak hypothesis with low training error as has usually been done in the past, we show that, theoretically, our methods work best when combined with a weak learner which minimizes an alternative measure of \u201cbadness.\u201d For growing decision trees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also outline methods that can be used to analyze generalization error as well. Next, we show how to extend the methods described above for binary classification problems to the multiclass case, and, more generally, to the multi-label case in which each example may belong to more than one class. Such problems arise naturally, for instance, in text categorization problems where the same document (say, a news article) may easily be relevant to more than one topic (such as politics, sports, etc.). Freund and Schapire (1997) gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 149
                            }
                        ],
                        "text": "\u2026empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 52
                            }
                        ],
                        "text": "We used the simplest of the weak learners tested by Freund and Schapire (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 217
                            }
                        ],
                        "text": "\u2026study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 11
                            }
                        ],
                        "text": "Freund and Schapire (1997) gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 26
                            }
                        ],
                        "text": "(5), but which Freund and Schapire (1997) are able to upper bound by % $ 4 4 !"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 31
                            }
                        ],
                        "text": "The first, given by Freund and Schapire (1997), uses standard VC-theory to bound the generalization error of the final hypothesis in terms of its training error and an additional term which is a function of the VC-dimension of the final hypothesis class and the number of training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 420,
                                "start": 129
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 187
                            }
                        ],
                        "text": "(4), this choice gives the upper bound % 2 1\nWe have thus proved the following corollary of Theorem 1 which is equivalent to Freund and Schapire\u2019s (1997) Theorem 6:\nCOROLLARY 1 ((FREUND & SCHAPIRE, 1997)) Using the notation of Figure 1, assume each = has range LM132: 5462!"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 72
                            }
                        ],
                        "text": "(Another method of combining boosting and output coding was proposed by Schapire (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2685539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc1374bcd952032dabe891114f29092b868e01b8",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new technique for solv- ing multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC). Boosting is a general method of improving the ac- curacy of a given base or \"weak\" learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning al- gorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guar- antees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multi- class problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm. We also compare the new algorithm experimentally to other voting methods."
            },
            "slug": "Using-output-codes-to-boost-multiclass-learning-Schapire",
            "title": {
                "fragments": [],
                "text": "Using output codes to boost multiclass learning problems"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper describes a new technique for multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC), and shows that the new hybrid method has advantages of both."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "In fact, exactly this splitting criterion was proposed by Kearns and Mansour (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 94
                            }
                        ],
                        "text": "For growing decision trees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 58
                            }
                        ],
                        "text": "In fact, exactly this splitting criterion was proposed by Kearns and Mansour (1996). Furthermore, if one wants to boost more than one decision tree then each tree can be built using the splitting criterion given by Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15982360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "063bf76e0259f5ef9a3eca729c1733e8fc59770b",
            "isKey": true,
            "numCitedBy": 156,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the performance of top?down algorithms for decision tree learning, such as those employed by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms areboostingalgorithms. By this we mean that if the functions that label the internal nodes of the decision tree can weakly approximate the unknown target function, then the top?down algorithms we study will amplify this weaks advantage to build a tree achieving any desired level of accuracy. The bounds we obtain for this amplification show an interesting dependence on thesplitting criterionused by the top?down algorithm. More precisely, if the functions used to label the internal nodes have error 1/2??as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees of size (1/?)O(1/?2?2)and (1/?)O(log(1/?)/?2)(respectively) suffice to drive the error below?. Thus (for example), a small constant advantage over random guessing is amplified to any larger constant advantage with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger bound of (1/?)O(1/?2)which is polynomial in 1/?) is obtained, which is provably optimal for decision tree algorithms. The differing bounds have a natured explanation in terms of concavity properties of the splitting criterion. The primary contribution of this work is in proving that some popular and empirically successful heuristics that are base on first principles meet the criteria of an independently motivated theoretical model."
            },
            "slug": "On-the-boosting-ability-of-top-down-decision-tree-Kearns-Mansour",
            "title": {
                "fragments": [],
                "text": "On the boosting ability of top-down decision tree learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work analyzes the performance of top?down algorithms for decision tree learning and proves that some popular and empirically successful heuristics that are base on first principles meet the criteria of an independently motivated theoretical model."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '96"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1139,
                                "start": 0
                            }
                        ],
                        "text": "Recall that the final hypothesis has the fo rm H(x) = sign(f(x)) wheref(x) =Xt tht(x): Since theht\u2019s are bounded and since we only care about the sign of f , we can rescale theht\u2019s and normalize the t\u2019s allowing us to assume without loss of generality that each ht : X ! [ 1;+1\u2104, each t 2 [0; 1\u2104 andPt t = 1. Let us also assume that each ht belongs to a hypothesis space H. Schapire et al. define the margin of a labeled example (x; y) to beyf(x). The margin then is in [ 1;+1\u2104, and is positive if and only ifH makes a correct prediction on this example. We further regard the magnitude of the margin as a me asur of the confidence of H \u2019s prediction. Schapire et al.\u2019s results can be applied directly in the pres ent context only in the special case that each 2 H has rangef 1;+1g. This case is not of much interest, however, since our focus is on weak hypotheses with real-valued predi ctions. To extend the margins theory, then, let us define d to be thepseudodimension of H (for definitions, see, for instance, Haussler (1992)). Then using the method sketched in Section 2.4 of Schapire et al. together with Haussler and Long\u2019s (1995) Lemma 13, we can pro ve the following upper bound on generalization error which holds with probability 1 \u00c6 for all > 0 and for all f of the form above: PrS [yf(x) \u2104+O 1 pm d log2(m=d) 2 + log(1=\u00c6) 1=2! : Here,PrS denotes probability with respect to choosing an example (x; y) uniformly at random from the training set."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training methods for adapt"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1486,
                                "start": 3
                            }
                        ],
                        "text": "IMPROVED BOOSTING ALGORITHMS 25 = vt(i; `0) exp 12 tht(xi; `0) pZt vt(i; `1) exp 1 2 tht(xi; `1) pZt = vt+1(i; `0) vt+1(i; `1) : Finally, note that all space requirements and all per-round computations areO(mk), with the possible exception of the call to the weak learner. H owever, if we want the weak learner to maximizejrj as in Eq. (27), then we also only need to pass mk weights to the weak learner, all of which can be computed in O(mk) time. Omittingt subscripts, we can rewriter as r = 1 2 X i;`0;`1D(i; `0; `1)(h(xi; `1) h(xi; `0)) = 1 2Xi X `0 62Yi;`12Yi v(i; `0)v(i; `1) (h(xi; `1)Yi[`1\u2104 + h(xi; `0)Yi[`0\u2104) = 1 2Xi 24X `0 62Yi v(i; `0) X `12Yi v(i; `1)!Yi[`0\u2104h(xi; `0) + X `12Yi0 v(i; `1) X `0 62Yi v(i; `0)1AYi[`1\u2104h(xi; `1)35 = Xi;` d(i; `)Yi[`\u2104h(xi; `) (30) whered(i; `) = 1 2 v(i; `) X `0:Yi[`0\u21046=Yi[`\u2104 v(i; `0) : All of the weightsd(i; `) can be computed inO(mk) time by first computing the sums which appear in this equation for the two possible cases that Yi[`\u2104 is 1 or +1. Thus, we only need to pass O(mk) weights to the weak learner in this case rather than the full distributionDt of sizeO(mk2). Moreover, note that Eq. (30) has exactly the same form as Eq. (14) which means that, in this setting, the same weak le arner can be used for either Hamming loss or ranking loss. 9.2. Relation to one-error As in Section 7.2, we can use the ranking loss method for minim izing one-error, and therefore also for single-label problems. Indeed, Freund and Sch apire\u2019s (1997) \u201cpseudoloss\u201dbased algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 457,
                                "start": 6
                            }
                        ],
                        "text": "Using Ranking Loss for Multiclass Problems In Section 7, we looked at the problem of finding a hypothesis t hat exactly identifies the labels associated with an instance. In this section, we cons ider a different variation of this problem in which the goal is to find a hypothesis which ranksthe labels with the hope that the correct labels will receive the highest ranks. The appro ach described here is closely related to one used by Freund et al. (1998) for using boosting for more general ranking problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SCHAPIRE AND Y"
            },
            "venue": {
                "fragments": [],
                "text": "SINGER Proof: Using techniques from Bartlett"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire (1997) gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 15
                            }
                        ],
                        "text": "(5), but which Freund and Schapire (1997) are able to upper bound by % $ 4 4 !"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 20
                            }
                        ],
                        "text": "The first, given by Freund and Schapire (1997), uses standard VC-theory to bound the generalization error of the final hypothesis in terms of its training error and an additional term which is a function of the VC-dimension of the final hypothesis class and the number of training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 178
                            }
                        ],
                        "text": "(4), this choice gives the upper bound % 2 1\nWe have thus proved the following corollary of Theorem 1 which is equivalent to Freund and Schapire\u2019s (1997) Theorem 6:\nCOROLLARY 1 ((FREUND & SCHAPIRE, 1997)) Using the notation of Figure 1, assume each = has range LM132: 5462!"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Boosting algorithms, multiclass classification, output coding, decision trees"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 8
                            }
                        ],
                        "text": "For fixed , let , , be defined by\n* 9 J for / 132: H 5462:7 , where, as before, *"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13123,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335246"
                        ],
                        "name": "C. Merz",
                        "slug": "C.-Merz",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Merz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Merz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 209099422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6cf9167aeb2782651156de5e22cad82ee69a225",
            "isKey": false,
            "numCitedBy": 1982,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-Machine-Learning-Databases-Merz",
            "title": {
                "fragments": [],
                "text": "UCI Repository of Machine Learning Databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "\u2026our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123349680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc394d074c8504671eb37926d14a3df4a07520a0",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly."
            },
            "slug": "Arcing-classifier-(with-discussion-and-a-rejoinder-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing classifier (with discussion and a rejoinder by the author)"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two arcing algorithms are explored, compared to each other and to bagging, and the definitions of bias and variance for a classifier as components of the test set error are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069538877"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Becker",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405497839"
                        ],
                        "name": "Z. G. Eds",
                        "slug": "Z.-G.-Eds",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Eds",
                            "middleNames": [
                                "Ghahramani"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. G. Eds"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60630251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89558a43b3b0a24cd0fdb6d5c2283112493af3a0",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "In-Advances-in-Neural-Information-Processing-15-Dietterich-Becker",
            "title": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems 15"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1991"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 127
                            }
                        ],
                        "text": "By allowing the weak hypothesis to effectively say \u201cI don\u2019t know,\u201d we introduce a model analogous to the \u201cspecialist\u201d model of Blum (1997), studied further by Freund et al. (1997). For fixed , let , , be defined by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "By allowing the weak hypothesis to effectively say \u201cI don\u2019t know,\u201d we introduce a model analogous to the \u201cspecialist\u201d model of Blum (1997), studied further by Freund et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "By allowing the weak hypothesis to effectively say \u201cI don\u2019t know,\u201d we introduce a model analogous to the \u201cspecialist\u201d model of Blum (1997), studied further by Freund et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195325954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f23d52268e53f9ea81cc6b367eac55f38090257",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-Support-for-Winnow-and-Weighted-Majority-Blum",
            "title": {
                "fragments": [],
                "text": "Empirical Support for Winnow and Weighted-Majority Based Algorithms: Results on a Calendar Scheduling Domain"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3281031"
                        ],
                        "name": "D. Margineantu",
                        "slug": "D.-Margineantu",
                        "structuredName": {
                            "firstName": "Dragos",
                            "lastName": "Margineantu",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Margineantu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 170
                            }
                        ],
                        "text": "\u2026study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1998) proposed an alternative analysis to explain AdaBoost\u2019s empirically observed resistance to overfitting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6863121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae4c1cbdabef2d6d5eebaa9c5eb154fd7dc82fdb",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pruning-Adaptive-Boosting-Margineantu-Dietterich",
            "title": {
                "fragments": [],
                "text": "Pruning Adaptive Boosting"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3281031"
                        ],
                        "name": "D. Margineantu",
                        "slug": "D.-Margineantu",
                        "structuredName": {
                            "firstName": "Dragos",
                            "lastName": "Margineantu",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Margineantu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6863121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae4c1cbdabef2d6d5eebaa9c5eb154fd7dc82fdb",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pruning-Adaptive-Boosting-Margineantu-Dietterich",
            "title": {
                "fragments": [],
                "text": "Pruning Adaptive Boosting"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 41342869,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "415e448f1b5fdb95b496772fa02d78190d3660e9",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Generalization-of-Sauer's-Lemma-Haussler-Long",
            "title": {
                "fragments": [],
                "text": "A Generalization of Sauer's Lemma"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Theory, Ser. A"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To extend the margins theory, then, let us define d to be the pseudodimension of H (for definitions, see, for instance,  Haussler (1992) )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "To extend the margins theory, then, let us define to be the pseudodimension of (for definitions, see, for instance, Haussler (1992))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14921581,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fedfc9fbcfe46d50b81078560bce724678f90176",
            "isKey": false,
            "numCitedBy": 979,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decision-Theoretic-Generalizations-of-the-PAC-Model-Haussler",
            "title": {
                "fragments": [],
                "text": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "The VC-dimension of the final hypothesis class can be computed using the methods of Baum and Haussler (1989). Interpretting the derived upper bound as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 84
                            }
                        ],
                        "text": "The VC-dimension of the final hypothesis class can be computed using the methods of Baum and Haussler (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068005320"
                        ],
                        "name": "Raj D. Iyer",
                        "slug": "Raj-D.-Iyer",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Iyer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raj D. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 20
                            }
                        ],
                        "text": "The first, given by Freund and Schapire (1997), uses standard VC-theory to bound the generalization error of the final hypothesis in terms of its training error and an additional term which is a function of the VC-dimension of the final hypothesis class and the number of training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 62
                            }
                        ],
                        "text": "The approach described here is closely related to one used by Freund et al. (1998) for using boosting for more general ranking problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "We have also used the new boosting framework for devising efficient ranking algorithms (Freund et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16692650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "isKey": false,
            "numCitedBy": 2182,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations."
            },
            "slug": "An-Efficient-Boosting-Algorithm-for-Combining-Freund-Iyer",
            "title": {
                "fragments": [],
                "text": "An Efficient Boosting Algorithm for Combining Preferences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning, and gives theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "\u2026has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995;  Drucker & Cortes, 1996;  Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1266014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dfeb731fc0c79e04523cd655413c223f6fa102",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning."
            },
            "slug": "Boosting-Decision-Trees-Drucker-Cortes",
            "title": {
                "fragments": [],
                "text": "Boosting Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A constructive, incremental learning system for regression problems that models data by means of locally linear experts that does not compete for data during learning and derives asymptotic results for this method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 155
                            }
                        ],
                        "text": "MR.\nFinally, there seem to be interesting connections between boosting and other models and their learning algorithms such as generalized additive models (Friedman et al., 1998) and maximum entropy methods (Csisza\u0301r & Tusna\u0301dy, 1984) which form a new and exciting research arena."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 2
                            }
                        ],
                        "text": "Output the final hypothesis:\nE &= E\nso that \" !"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4829,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709927"
                        ],
                        "name": "R. Maclin",
                        "slug": "R.-Maclin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Maclin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maclin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752379"
                        ],
                        "name": "D. Opitz",
                        "slug": "D.-Opitz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Opitz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Opitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996;  Maclin & Opitz, 1997;  Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9378257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3653266b5427295bdd54d6a22bf4caaa8c0b6961",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An ensemble consists of a set of independently trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund & Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classification algorithms. Our results clearly show two important facts. The first is that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is a powerful technique that can usually produce better ensembles than Bagging; however, it is more susceptible to noise and can quickly overfit a data set."
            },
            "slug": "An-Empirical-Evaluation-of-Bagging-and-Boosting-Maclin-Opitz",
            "title": {
                "fragments": [],
                "text": "An Empirical Evaluation of Bagging and Boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results clearly show that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 274
                            }
                        ],
                        "text": "\u2026study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Boosting algorithms, multiclass classification, output coding, decision trees"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5244761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d7dbd8503a9fe61c8e02465de2fa327e4d89c05",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Boosting\" is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], and decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 1.5% error on the UCI Letters and 8.1 % error on the UCI satellite data set."
            },
            "slug": "Training-Methods-for-Adaptive-Boosting-of-Neural-Schwenk-Bengio",
            "title": {
                "fragments": [],
                "text": "Training Methods for Adaptive Boosting of Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper uses AdaBoost to improve the performances of neural networks and compares training methods based on sampling the training set and weighting the cost function."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear;  Dietterich & Bakiri, 1995;  Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire et al., 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47109072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d221bbcbd20c7157e4500f942de8ceec490f8936",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Solving Multiclass Learning Problems via Error-Correcting Output Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training methods for adaptive boos"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LetH be a set of real-valued functions on domain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 97
                            }
                        ],
                        "text": "\u201d For growing decision trees, this measur e turns out to be identical to one earlier proposed by Kearns and Mansour (1996). Although we primarily focus on minimizing training error, we also o utline methods that can be used to analyze generalization error as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A generalization of Sauer\u2019s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternaning minimization procedures. Statistics and Decisions"
            },
            "venue": {
                "fragments": [],
                "text": "Supplement Issue,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive logistic regression : A statistical view of boosting Technical Report"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "A similar scheme was previously proposed by Quinlan (1996) for assig ning confidences to the predictions made at the leaves of a decision tree."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "UCI repository of machine learnin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorem 4) give a theorem which states that, for H J and H J , the probability over the random choice of training set that there exists any function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 78
                            }
                        ],
                        "text": "This method turns out to be closely related to a heuristic method proposed by Quinlan (1996) for boostin g decision trees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pruning adaptive boos"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "A similar scheme was previously proposed by Quinlan (1996) for assig ning confidences to the predictions made at the leaves of a decision tree."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "UCI repository of machine learnin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IMPROVED BOOSTING ALGORITHMS"
            },
            "venue": {
                "fragments": [],
                "text": "IMPROVED BOOSTING ALGORITHMS"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 46
                            }
                        ],
                        "text": "This is essentially the approach advocated by Dietterich and Bakiri (1995) in a some what different setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving multiclass l"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dietterich . An experimental comparison of thlce methods for constructing ensembles of decision trees : Bag - L ? ing . boosting , and randomization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive logis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 59
                            }
                        ],
                        "text": "In fact, exactly this split ting criterion was proposed by Kearns and Mansour (1996). Furthermore, if one wants to boos t m re than one decision tree then each tree can be built using the splitting criterio n given by Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the boosting ability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 20
                            }
                        ],
                        "text": "We base our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu & Dietterich, 1997; Quinlan, 1996; Schapire, 1997; Schapire, Freund, Bartlett, & Lee, 1998; Schwenk & Bengio, 1998)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization (Second edition)"
            },
            "venue": {
                "fragments": [],
                "text": "John Wiley."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorem 4) give a theorem which states that, for F H and F H , the probability over the random choice of training set that there exists any function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorem 4) give a theorem which states that, for F H and F H , the probability over the random choice of training set that there exists any function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00bf\u00bf \u00ca\u00ba \u00ba \u00cb\u00cb\u00c0\u00c0\u00c8\u00c1\u00ca\u00ca AEAE \u00ba \u00cb\u00c1AEAEAE\u00ca"
            },
            "venue": {
                "fragments": [],
                "text": "\u00bf\u00bf \u00ca\u00ba \u00ba \u00cb\u00cb\u00c0\u00c0\u00c8\u00c1\u00ca\u00ca AEAE \u00ba \u00cb\u00c1AEAEAE\u00ca"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training methods for adaptive boos"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pruning adaptive boost"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arcing classifiers. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Arcing classifiers. The Annals of Statistics"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternaning minimization procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Decisions"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 207
                            }
                        ],
                        "text": "MR.\nFinally, there seem to be interesting connections between boosting and other models and their learning algorithms such as generalized additive models (Friedman et al., 1998) and maximum entropy methods (Csisza\u0301r & Tusna\u0301dy, 1984) which form a new and exciting research arena."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternaning minimization procedures. Statistics and Decisions"
            },
            "venue": {
                "fragments": [],
                "text": "Supplement Issue,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and altern"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic g"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "\u2026our work on Freund and Schapire\u2019s (1997) AdaBoost algorithm which has received extensive empirical and theoretical study (Bauer & Kohavi, to appear; Breiman, 1998; Dietterich, to appear; Dietterich & Bakiri, 1995; Drucker & Cortes, 1996; Freund & Schapire, 1996; Maclin & Opitz, 1997; Margineantu\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arcing classifiers. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Arcing classifiers. The Annals of Statistics"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of thlce methods for constructing ensembles of decision trees: Bag- L?ing. boosting, and randomization. I Jnpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "An experimental comparison of thlce methods for constructing ensembles of decision trees: Bag- L?ing. boosting, and randomization. I Jnpublished manuscript"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and altern"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theorem 4) give a theorem which states that, for H J and H J , the probability over the random choice of training set that there exists any function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 38,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 83,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/+/14e53403a0055dbe5faaf9f1f3be96ca0e692a4d?sort=total-citations"
}