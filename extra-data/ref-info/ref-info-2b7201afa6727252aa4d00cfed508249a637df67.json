{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144538257"
                        ],
                        "name": "Y. Weiss",
                        "slug": "Y.-Weiss",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116433755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea3bdbcc7b35f679d5de970134a000db693245ae",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models, such as Bayesian networks and Markov Random Fields represent joint distributions over a set of variables by means of a graph. When the graph is singly connected, local \"belief propagation\" rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities. Recently, a number of researchers have empirically demonstrated good performance \"loopy belief propagation\" -- using these same rules on graphs with loops. Perhaps the most dramatic instance is the near Shannon-limit performance of \"Turbo Codes\" whose decoding algorithm is equivalent to loopy belief propagation. Except for the case of graphs with a single loop, there has been very little theoretical understanding of the performance of loopy propagation. Here we prove that when the nodes in the graph describe jointly Gaussian random variables, if belief propagation converges then it will give the correct posterior means for all graph topologies, not just networks with a single loop. This justifies using belief propagation in a broader class of networks, and helps clarify the empirical performance results."
            },
            "slug": "Loopy-Belief-Propagation-Gives-Exact-Posterior-for-Weiss-Freeman",
            "title": {
                "fragments": [],
                "text": "Loopy Belief Propagation Gives Exact Posterior Means for Gaussian"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that when the nodes in the graph describe jointly Gaussian random variables, if belief propagation converges then it will give the correct posterior means for all graph topologies, not just networks with a single loop."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15402308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "300f73c89bfeb6b88b9b18f63793568c3d06bee6",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models, such as Bayesian networks and Markov networks, represent joint distributions over a set of variables by means of a graph. When the graph is singly connected, local propagation rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities. Recently a number of researchers have empirically demonstrated good performance of these same local propagation schemes on graphs with loops, but a theoretical understanding of this performance has yet to be achieved. For graphical models with a single loop, we derive an analytical relationship between the probabilities computed using local propagation and the correct marginals. Using this relationship we show a category of graphical models with loops for which local propagation gives rise to provably optimal maximum a posteriori assignments (although the computed marginals will be incorrect). We also show how nodes can use local information in the messages they receive in order to correct their computed marginals. We discuss how these results can be extended to graphical models with multiple loops and show simulation results suggesting that some properties of propagation on single-loop graphs may hold for a larger class of graphs. Specifically we discuss the implication of our results for understanding a class of recently proposed error-correcting codes known as turbo codes."
            },
            "slug": "Correctness-of-Local-Probability-Propagation-in-Weiss",
            "title": {
                "fragments": [],
                "text": "Correctness of Local Probability Propagation in Graphical Models with Loops"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An analytical relationship is derived between the probabilities computed using local propagation and the correct marginals and a category of graphical models with loops for which local propagation gives rise to provably optimal maximum a posteriori assignments (although the computed marginals will be incorrect)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6945869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f583eca0700a808c6a802ae8e79835f111b22ce",
            "isKey": false,
            "numCitedBy": 626,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models, such as Bayesian networks and Markov random fields (MRFs), represent statistical dependencies of variables by a graph. The max-product \"belief propagation\" algorithm is a local-message-passing algorithm on this graph that is known to converge to a unique fixed point when the graph is a tree. Furthermore, when the graph is a tree, the assignment based on the fixed point yields the most probable values of the unobserved variables given the observed ones. Good empirical performance has been obtained by running the max-product algorithm (or the equivalent min-sum algorithm) on graphs with loops, for applications including the decoding of \"turbo\" codes. Except for two simple graphs (cycle codes and single-loop graphs) there has been little theoretical understanding of the max-product algorithm on graphs with loops. Here we prove a result on the fixed points of max-product on a graph with arbitrary topology and with arbitrary probability distributions (discrete- or continuous-valued nodes). We show that the assignment based on a fixed point is a \"neighborhood maximum\" of the posterior probability: the posterior probability of the max-product assignment is guaranteed to be greater than all other assignments in a particular large region around that assignment. The region includes all assignments that differ from the max-product assignment in any subset of nodes that form no more than a single loop in the graph. In some graphs, this neighborhood is exponentially large. We illustrate the analysis with examples."
            },
            "slug": "On-the-optimality-of-solutions-of-the-max-product-Weiss-Freeman",
            "title": {
                "fragments": [],
                "text": "On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the assignment based on a fixed point of max-product is a \"neighborhood maximum\" of the posterior probabilities: the posterior probability of the max- product assignment is guaranteed to be greater than all other assignments in a particular large region around that assignment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703039"
                        ],
                        "name": "Paat Rusmevichientong",
                        "slug": "Paat-Rusmevichientong",
                        "structuredName": {
                            "firstName": "Paat",
                            "lastName": "Rusmevichientong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paat Rusmevichientong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731282"
                        ],
                        "name": "Benjamin Van Roy",
                        "slug": "Benjamin-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6812340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc16b922cc85937403abd52a51840c06f551349c",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated by its success in decoding turbo codes, we provide an analysis of the belief propagation algorithm on the turbo decoding graph with Gaussian densities. In this context, we are able to show that, under certain conditions, the algorithm converges and that-somewhat surprisingly-though the density generated by belief propagation may differ significantly from the desired posterior density, the means of these two densities coincide. Since computation of posterior distributions is tractable when densities are Gaussian, use of belief propagation in such a setting may appear unwarranted. Indeed, our primary motivation for studying belief propagation in this context stems from a desire to enhance our understanding of the algorithm's dynamics in a non-Gaussian setting, and to gain insights into its excellent performance in turbo codes. Nevertheless, even when the densities are Gaussian, belief propagation may sometimes provide a more efficient alternative to traditional inference methods."
            },
            "slug": "An-analysis-of-belief-propagation-on-the-turbo-with-Rusmevichientong-Roy",
            "title": {
                "fragments": [],
                "text": "An analysis of belief propagation on the turbo decoding graph with Gaussian densities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, under certain conditions, the algorithm converges and that-somewhat surprisingly-though the density generated by belief propagation may differ significantly from the desired posterior density, the means of these two densities coincide."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16462148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19908640236767427ebf0524dc3a4bb09d65145e",
            "isKey": false,
            "numCitedBy": 1774,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, researchers have demonstrated that \"loopy belief propagation\" -- the use of Pearl's polytree algorithm in a Bayesian network with loops -- can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" -- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. \n \nIn this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? We compare the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results."
            },
            "slug": "Loopy-Belief-Propagation-for-Approximate-Inference:-Murphy-Weiss",
            "title": {
                "fragments": [],
                "text": "Loopy Belief Propagation for Approximate Inference: An Empirical Study"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper compares the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR, and finds that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13951920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46bc5c5f6a846a741a0464241fda05bec9fb0ed1",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Local belief propagation rules of the sort proposed by P earl (1988) are guaranteed to converge to the optimal beliefs for singly connected networks. Recently, a n umber of researchers have empirically demonstrated good performance of these same algorithms on networks with loops, but a theoretical understanding of this performance has yet to be achieved. Here we l a y a foundation for an understanding of belief propagation in networks with loops. For networks with a single loop, we derive an analytical relationship between the steady state beliefs in the loopy network and the true posterior probability. Using this relationship we show a category of networks for which the MAP estimate obtained by belief update and by belief revision can be proven to be optimal (although the beliefs will be incorrect). We s h o w h o w nodes can use local information in the messages they receive in order to correct the steady state beliefs. Furthermore we p r o ve that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states. The result is independent of the length of the cycle and the size of the state space. For networks with multiple loops, we i n troduce the concept of a \\balanced network\" and show simulation results comparing belief revision and update in such networks. We show t h a t t h e T urbo code structure is balanced and present simulations on a toy T urbo code problem indicating the decoding obtained by belief revision at convergence is signiicantly more likely to be correct. A a b Figure 1: a. An example of the types of problems typically solved using belief propagation. Observed nodes are denoted by lled circles. A link between any t wo nodes implies a probabilistic compatability constraint. b. A simple network with a loop. Although belief propagation rules can be generalized to this network, a theoretical understanding of the algorithms behavior in such a n e t work has yet to be achieved."
            },
            "slug": "Belief-Propagation-and-Revision-in-Networks-with-Adelson",
            "title": {
                "fragments": [],
                "text": "Belief Propagation and Revision in Networks with Loops"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that for all networks with a single loop, the MAP estimate obtained by belief revision at convergence is guaranteed to give the globally optimal sequence of states and the result is independent of the length of the cycle and the size of the state space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889982"
                        ],
                        "name": "F. Kschischang",
                        "slug": "F.-Kschischang",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Kschischang",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kschischang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6522238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbd45449e1cdadbf1f0c06a9510b5ac247cb70b9",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unified graphical model framework for describing compound codes and deriving iterative decoding algorithms. After reviewing a variety of graphical models (Markov random fields, Tanner graphs, and Bayesian networks), we derive a general distributed marginalization algorithm for functions described by factor graphs. From this general algorithm, Pearl's (1986) belief propagation algorithm is easily derived as a special case. We point out that iterative decoding algorithms for various codes, including \"turbo decoding\" of parallel-concatenated convolutional codes, may be viewed as probability propagation in a graphical model of the code. We focus on Bayesian network descriptions of codes, which give a natural input/state/output/channel description of a code and channel, and we indicate how iterative decoders can be developed for parallel-and serially concatenated coding systems, product codes, and low-density parity-check codes."
            },
            "slug": "Iterative-Decoding-of-Compound-Codes-by-Probability-Kschischang-Frey",
            "title": {
                "fragments": [],
                "text": "Iterative Decoding of Compound Codes by Probability Propagation in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is pointed out that iterative decoding algorithms for various codes, including \"turbo decoding\" of parallel-concatenated convolutional codes, may be viewed as probability propagation in a graphical model of the code."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Sel. Areas Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144538257"
                        ],
                        "name": "Y. Weiss",
                        "slug": "Y.-Weiss",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17552583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98889414e2d1d80e968d50f4b3bfc7a7bfb5f41f",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models, such as Bayesian networks and Markov random fields represent statistical dependencies of variables by a graph. The max-product b\u0308elief propagation\u0308 algorithm is a localmessage passing algorithm on this graph that is known to converge to a unique fixed point when the graph is a tree. Furthermore, when the graph is a tree, the assignment based on the fixed-point is guaranteed to yields the most probable a posteriori (MAP) values of the unobserved variables given the observed ones. Here we prove a result on the fixed points of max-product on a graph with arbitrary toplogy and with arbitrary probability distributions (discrete or continuous valued nodes). We show that the assignment based on the fixed-point is a n\u0308eighborhood maximum\u0308 of the posterior probability: the posterior probability of the max-product assignment is guaranteed to be greater than all other assignments in a particular large region around that assignment. The region includes all assignments that differ from the max-product assignment in any subset of nodes that form no more than a single loop in the graph. In some graphs this neighborhood is exponentially large. We illustrate the analysis with examples. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c \u00a9Mitsubishi Electric Research Laboratories, Inc., 1999 201 Broadway, Cambridge, Massachusetts 02139"
            },
            "slug": "On-the-fixed-points-of-the-max-product-algorithm-Freeman-Weiss",
            "title": {
                "fragments": [],
                "text": "On the fixed points of the max-product algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Here it is proved a result on the fixed points of max-product on a graph with arbitrary toplogy and with arbitrary probability distributions (discrete or continuous valued nodes) is guaranteed to yields the most probable a posteriori (MAP) values of the unobserved variables given the observed ones."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Frey (2000)  analyzed the graphical model corresponding to factor analysis and gave conditions for the existence of a stable fixed point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2738586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "387d48bced9f76cc20413311979f65d097364d74",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Ever since Pearl's probability propagation algorithm in graphs with cycles was shown to produce excellent results for error-correcting decoding a few years ago, we have been curious about whether local probability propagation could be used successfully for machine learning. One of the simplest adaptive models is the factor analyzer, which is a two-layer network that models bottom layer sensory inputs as a linear combination of top layer factors plus independent Gaussian sensor noise. We show that local probability propagation in the factor analyzer network usually takes just a few iterations to perform accurate inference, even in networks with 320 sensors and 80 factors. We derive an expression for the algorithm's fixed point and show that this fixed point matches the exact solution in a variety of networks, even when the fixed point is unstable. We also show that this method can be used successfully to perform inference for approximate EM and we give results on an online face recognition task."
            },
            "slug": "Local-Probability-Propagation-for-Factor-Analysis-Frey",
            "title": {
                "fragments": [],
                "text": "Local Probability Propagation for Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that local probability propagation in the factor analyzer network usually takes just a few iterations to perform accurate inference, even in networks with 320 sensors and 80 factors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157745208"
                        ],
                        "name": "Jung-Fu Cheng",
                        "slug": "Jung-Fu-Cheng",
                        "structuredName": {
                            "firstName": "Jung-Fu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung-Fu Cheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14553992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26d953005dd08a863c157b528bbabdf5671d18b6",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the close connection between the now celebrated iterative turbo decoding algorithm of Berrou et al. (1993) and an algorithm that has been well known in the artificial intelligence community for a decade, but which is relatively unknown to information theorists: Pearl's (1982) belief propagation algorithm. We see that if Pearl's algorithm is applied to the \"belief network\" of a parallel concatenation of two or more codes, the turbo decoding algorithm immediately results. Unfortunately, however, this belief diagram has loops, and Pearl only proved that his algorithm works when there are no loops, so an explanation of the experimental performance of turbo decoding is still lacking. However, we also show that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's (1962) low-density parity-check codes, serially concatenated codes, and product codes. Thus, belief propagation provides a very attractive general methodology for devising low-complexity iterative decoding algorithms for hybrid coded systems."
            },
            "slug": "Turbo-Decoding-as-an-Instance-of-Pearl's-\"Belief-McEliece-Mackay",
            "title": {
                "fragments": [],
                "text": "Turbo Decoding as an Instance of Pearl's \"Belief Propagation\" Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's low-density parity-check codes, serially concatenated codes, and product codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Sel. Areas Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231485"
                        ],
                        "name": "S. Aji",
                        "slug": "S.-Aji",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Aji",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152559960"
                        ],
                        "name": "G. Horn",
                        "slug": "G.-Horn",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Horn",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [23, 24, 6, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1670812,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ccaa18111bbaee1469fb14bef786325a13e7e756",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "It is now understood that the turbo decoding algorithm is an instance of a probability propagation algorithm (PPA) on a graph with many cycles. In this paper we investigate the behavior of an PPA in graphs with a single cycle such as the graph of a tail-biting code. First, we show that for strictly positive local kernels, the iterations of the PPA converge to a unique fixed point, (which was also observed by Anderson and Hladik (1998) and Weiss (1997)). Secondly, we shall generalize a result of McEliece and Rodemich (1995), by showing that if the hidden variables in the cycle are binary-valued, the PPA will always make an optimal decision. (This was also observed independently by Weiss). When the hidden variables can assume 3 or more values, the behavior of the PPA is much harder to characterize."
            },
            "slug": "Iterative-decoding-on-graphs-with-a-single-cycle-Aji-Horn",
            "title": {
                "fragments": [],
                "text": "Iterative decoding on graphs with a single cycle"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper investigates the behavior of an PPA in graphs with a single cycle such as the graph of a tail-biting code and shows that if the hidden variables in the cycle are binary-valued, the PPA will always make an optimal decision."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE International Symposium on Information Theory (Cat. No.98CH36252)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17026089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2792ccda37a3818b8e33b20f0492c2e6a097fa1f",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Belief-networks,-hidden-Markov-models,-and-Markov-A-Smyth",
            "title": {
                "fragments": [],
                "text": "Belief networks, hidden Markov models, and Markov random fields: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17714994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fff0d503915dddc7aab26a22dfd42b2651e58fb2",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A few years ago, Pearl\u2019s probability propagation algorithm in graphs with cycles was shown to produce excellent results for error-correcting \u201cturbodecoding\u201d. Ever since, we have wondered whether iterative probability propagation could be used successfully for machine learning. As a first step in this direction, we study iterative inference and learning in the simple factor analyzer network \u2013 a two-layer densely connected network that models bottom layer sensory inputs as a linear combination of top layer factors plus independent Gaussian sensor noise. The number of bottom-up/top-down iterations needed to exactly infer the factors given a network and an input scales with the number of factors in the top layer. In online learning, this iterative procedure must be reinitialized upon each pattern presentation and so learning becomes prohibitively slow in big networks, such as those used for face recognition and for large-scale models of the cortex. We show that probability propagation in a factor analyzer usually takes just a few iterations to achieve a low inference error, even in networks with 320 sensors and 80 factors. We derive an expression for the algorithm\u2019s fixed point and provide an eigenvalue condition for global convergence. We also show how iterative inference can be used to do online learning and give results on using this method to do online dimensionality reduction for the purpose of recognizing 560-pixel images of faces using a 40-dimensional subspace. This work suggests that iterative probability propagation in densely connected networks may lead to a broad class of useful algorithms for machine learning. UW/CS Adaptive Computation TR-99-1, Apr 10, 1999. Submitted to Neural Computation, May 25, 1999."
            },
            "slug": "Turbo-Factor-Analysis-Frey",
            "title": {
                "fragments": [],
                "text": "Turbo Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is shown that probability propagation in a factor analyzer usually takes just a few iterations to achieve a low inference error, even in networks with 320 sensors and 80 factors, which suggests that iterative probability propagate in densely connected networks may lead to a broad class of useful algorithms for machine learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145617808"
                        ],
                        "name": "D. Barber",
                        "slug": "D.-Barber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783429"
                        ],
                        "name": "W. Wiegerinck",
                        "slug": "W.-Wiegerinck",
                        "structuredName": {
                            "firstName": "Wim",
                            "lastName": "Wiegerinck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wiegerinck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16049210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eee1c6295c7778a966261424c9d48ff78fb072be",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models provide a broad probabilistic framework with applications in speech recognition (Hidden Markov Models), medical diagnosis (Belief networks) and artificial intelligence (Boltzmann Machines). However, the computing time is typically exponential in the number of nodes in the graph. Within the variational framework for approximating these models, we present two classes of distributions, decimatable Boltzmann Machines and Tractable Belief Networks that go beyond the standard factorized approach. We give generalised mean-field equations for both these directed and undirected approximations. Simulation results on a small benchmark problem suggest using these richer approximations compares favorably against others previously reported in the literature."
            },
            "slug": "Tractable-Variational-Structures-for-Approximating-Barber-Wiegerinck",
            "title": {
                "fragments": [],
                "text": "Tractable Variational Structures for Approximating Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents two classes of distributions, decimatable Boltzmann Machines and Tractable Belief Networks that go beyond the standard factorized approach, and gives generalised mean-field equations for both these directed and undirected approximations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5401657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9de4759cd0bed1b52bd5796551bfa5e89fa93b0b",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We study probabilistic inference in large, layered Bayesian networks represented as directed acyclic graphs. We show that the intractability of exact inference in such networks does not preclude their effective use. We give algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents. We show that these algorithms compute rigorous lower and upper bounds on marginal probabilities of interest, prove that these bounds become exact in the limit of large networks, and provide rates of convergence."
            },
            "slug": "Inference-in-Multilayer-Networks-via-Large-Bounds-Kearns-Saul",
            "title": {
                "fragments": [],
                "text": "Inference in Multilayer Networks via Large Deviation Bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents are given, which compute rigorous lower and upper bounds on marginal probabilities of interest, and prove that these bounds become exact in the limit of large networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 115168171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb44d50bce92b4ce2c0ea53bd8ede95f628ee3cb",
            "isKey": false,
            "numCitedBy": 1007,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative decoding techniques have become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding. The min-sum and sum-product algorithms are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the \u201cTanner graph\u201d. With cycle-free Tanner graphs, the resulting decoding algorithms (e.g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e.g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations."
            },
            "slug": "Codes-and-Decoding-on-General-Graphs-Wiberg",
            "title": {
                "fragments": [],
                "text": "Codes and Decoding on General Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14290328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-of-Belief-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For completeness, we review the belief propagation scheme used in Weiss (2000). At every iteration, each node sends a (different) message to each of its neighbors and receives a message from each neighbor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, mean-field approximations give the exact means for gaussian MRFs, while they work poorly in discrete networks with a single loop in which the connectivity is sparse (Weiss, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In Weiss and Freeman (2001) we prove that an assignment based on a fixed point of the max-product BP algorithm is a \u201cneighborhood maximum\u201d of the posterior probability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1518414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "767e6075c7c8119531b275eec36b32a5b50b3436",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A central theme of computational vision research has been the realization that reliable estimation of local scene properties requires propagating measurements across the image. Many authors have therefore suggested solving vision problems using architectures of locally connected units updating their activity in parallel. Unfortunately, the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global minimum. \n \nIn this paper we show that an architecture in which Bayesian Beliefs about image properties are propagated between neighboring units yields convergence times which are several orders of magnitude faster than traditional methods and avoids local minima. In particular our architecture is non-iterative in the sense of Marr [5]: at every time step, the local estimates at a given location are optimal given the information which has already been propagated to that location. We illustrate the algorithm's performance on real images and compare it to several existing methods."
            },
            "slug": "Interpreting-Images-by-Propagating-Bayesian-Beliefs-Weiss",
            "title": {
                "fragments": [],
                "text": "Interpreting Images by Propagating Bayesian Beliefs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper shows that an architecture in which Bayesian Beliefs about image properties are propagated between neighboring units yields convergence times which are several orders of magnitude faster than traditional methods and avoids local minima."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231485"
                        ],
                        "name": "S. Aji",
                        "slug": "S.-Aji",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Aji",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The belief propagation scheme described here includes as special cases Pearl\u2019s (1988) BP algorithm in directed graphs, the generalized distributive law algorithm of Aji and McEliece (1999), and the factor graph propagation algorithm of Frey et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11355291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e8933300a20f3d799dc9f19e352967f41d8efcc",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a general message passing algorithm, which we call the generalized distributive law (GDL). The GDL is a synthesis of the work of many authors in information theory, digital communications, signal processing, statistics, and artificial intelligence. It includes as special cases the Baum-Welch algorithm, the fast Fourier transform (FFT) on any finite Abelian group, the Gallager-Tanner-Wiberg decoding algorithm, Viterbi's algorithm, the BCJR algorithm, Pearl's \"belief propagation\" algorithm, the Shafer-Shenoy probability propagation algorithm, and the turbo decoding algorithm. Although this algorithm is guaranteed to give exact answers only in certain cases (the \"junction tree\" condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to."
            },
            "slug": "The-generalized-distributive-law-Aji-McEliece",
            "title": {
                "fragments": [],
                "text": "The generalized distributive law"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Although this algorithm is guaranteed to give exact answers only in certain cases (the \"junction tree\" condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18711,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57821331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4a34496869a2ef6dfd2ddb880ae5b5dc9cdf60f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Pattern classification, data compression, and channel coding are tasks that usually must deal with complex but structured natural or artificial systems. Patterns that we wish to classify are a consequence of a causal physical process. Images that we wish to compress are also a consequence of a causal physical process. Noisy outputs from a telephone line are corrupted versions of a signal produced by a structured man-made telephone modem. Not only are these tasks characterized by complex structure, but they also contain random elements. Graphical models such as Bayesian networks provide a way to describe the relationships between random variables in a stochastic system. \nIn this thesis, I use Bayesian networks as an overarching framework to describe and solve problems in the areas of pattern classification, data compression, and channel coding. Results on the classification of handwritten digits show that Bayesian network pattern classifiers outperform other standard methods, such as the k-nearest neighbor method. When Bayesian networks are used as source models for data compression, an exponentially large number of codewords are associated with each input pattern. It turns out that the code can still be used efficiently, if a new technique called \"bits-back coding\" is used. Several new error-correcting decoding algorithms are instances of \"probability propagation\" in various Bayesian networks. These new schemes are rapidly closing the gap between the performances of practical channel coding systems and Shannon's 50-year-old channel coding limit. The Bayesian network framework exposes the similarities between these codes and leads the way to a new class of \"trellis-constraint codes\" which also operate close to Shannon's limit."
            },
            "slug": "Bayesian-networks-for-pattern-classification,-data-Hinton-Frey",
            "title": {
                "fragments": [],
                "text": "Bayesian networks for pattern classification, data compression, and channel coding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Bayesian network framework exposes the similarities between these codes and leads the way to a new class of \"trellis-constraint codes\" which also operate close to Shannon's limit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17467575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d663c3792a6a5ea8974bd8073e4e714e2ffccbc",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a learning algorithm for unsupervised neural networks based on ideas from statistical mechanics. The algorithm is derived from a mean field approximation for large,layered sigmoid belief networks. We show how to (approximately) infer the statistics of these networks without resort to sampling. This is done by solving the mean field equations, which relate the statistics of each unit to those of its Markov blanket. Using these statistics as target values, the weights in the network are adapted by a local delta rule. We evaluate the strengths and weaknesses of these networks for problems in statistical pattern recognition."
            },
            "slug": "A-Mean-Field-Learning-Algorithm-for-Unsupervised-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "A Mean Field Learning Algorithm for Unsupervised Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A learning algorithm for unsupervised neural networks based on ideas from statistical mechanics, derived from a mean field approximation for large,layered sigmoid belief networks, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631309"
                        ],
                        "name": "R. Cowell",
                        "slug": "R.-Cowell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cowell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cowell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is straightforward to write the inverse covariance matrix describing the GMRF which respects the statistical dependencies within the graphical model (Cowell, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56663563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe49ad1a012144be6bbd5138a54b1c90c92224f7",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The previous chapter introduced inference in discrete variable Bayesian networks. This used evidence propagation on the junction tree to find marginal distributions of interest. This chapter presents a tutorial introduction to some of the various types of calculations which can also be performed with the junction tree, specifically: \n \n \nSampling. \n \n \nMost likely configurations. \n \n \nFast retraction. \n \n \nGaussian and conditional Gaussian models."
            },
            "slug": "Advanced-Inference-in-Bayesian-Networks-Cowell",
            "title": {
                "fragments": [],
                "text": "Advanced Inference in Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter presents a tutorial introduction to some of the various types of calculations which can also be performed with the junction tree, specifically: sampling, Gaussian and conditional Gaussian models, and most likely configurations."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12709402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206f827fad201506c315d40c1469b41a45141893",
            "isKey": false,
            "numCitedBy": 10568,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed number j \\geq 3 of l's and each row contains a small fixed number k > j of l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixed j . When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixed j . A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. For j > 3 and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound."
            },
            "slug": "Low-density-parity-check-codes-Gallager",
            "title": {
                "fragments": [],
                "text": "Low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described and the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7424318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a79433b5feacd9e8feeafa629dae5a85f362fef",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "slug": "Mean-Field-Theory-for-Sigmoid-Belief-Networks-Saul-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for Sigmoid Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The utility of a mean field theory for sigmoid belief networks based on ideas from statistical mechanics is demonstrated on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18219,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "The assumed probability distribution is described using a graphical model [14] | the qualitative aspects of the distribution are speci ed by a graph structure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6286159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e16a25faf7428e1fc5ed0a10b8196c0499c7fd0d",
            "isKey": false,
            "numCitedBy": 3412,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing."
            },
            "slug": "Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Some of the basic ideas underlying graphical models are reviewed, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems and examples of graphical models in bioinformatics, error-control coding and language processing are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812671"
                        ],
                        "name": "M. Luettgen",
                        "slug": "M.-Luettgen",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Luettgen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luettgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145325413"
                        ],
                        "name": "W. Karl",
                        "slug": "W.-Karl",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Karl",
                            "middleNames": [
                                "Clem"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Karl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14097228,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ff5cb20e4bbab6001344ece31f444fbb84f3f4da",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach to regularization methods for image processing is introduced and developed using as a vehicle the problem of computing dense optical ow elds in an image sequence. Standard formulations of this problem require the computationally intensive solution of an elliptic partial diierential equation which arises from the often used \\smoothness constraint\" type regularization. We utilize the interpretation of the smoothness constraint as a \\fractal prior\" to motivate regularization based on a recently introduced class of multiscale stochastic models. The solution of the new problem formulation is computed with an eecient multiscale algorithm. Experiments on several image sequences demonstrate the substantial computational savings that can be achieved due to the fact that the algorithm is non-iterative and in fact has a per pixel computational complexity which is independent of image size. The new approach also has a number of other important advantages. Speciically, multiresolution ow eld estimates are available, allowing great exibility in dealing with the tradeoo between resolution and accuracy. Multiscale error covariance information is also available, which is of considerable use in assessing the accuracy of the estimates. In particular, these error statistics can be used as the basis for a rational procedure for determining the spatially-varying optimal reconstruction resolution. Furthermore, if there are compelling reasons to insist upon a standard smoothness constraint, our algorithm provides an excellent initialization for the iterative algorithms associated with the smoothness constraint problem formulation. Finally, the usefulness of our approach should extend to a wide variety of ill-posed inverse problems in which variational techniques seeking a \\smooth\" solution are generally used."
            },
            "slug": "Eecient-Multiscale-Regularization-with-Applications-Luettgen-Karl",
            "title": {
                "fragments": [],
                "text": "Eecient Multiscale Regularization with Applications to the Computation of Optical Flow 1"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703039"
                        ],
                        "name": "Paat Rusmevichientong",
                        "slug": "Paat-Rusmevichientong",
                        "structuredName": {
                            "firstName": "Paat",
                            "lastName": "Rusmevichientong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paat Rusmevichientong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731282"
                        ],
                        "name": "Benjamin Van Roy",
                        "slug": "Benjamin-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2749292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efa3492756d36303370d40c1de80fc790b3451d7",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide an analysis of the turbo decoding algorithm (TDA) in a setting involving Gaussian densities. In this context, we are able to show that the algorithm converges and that - somewhat surprisingly - though the density generated by the TDA may differ significantly from the desired posterior density, the means of these two densities coincide."
            },
            "slug": "An-Analysis-of-Turbo-Decoding-with-Gaussian-Rusmevichientong-Roy",
            "title": {
                "fragments": [],
                "text": "An Analysis of Turbo Decoding with Gaussian Densities"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is shown that the turbo decoding algorithm converges and that - somewhat surprisingly - though the density generated by the TDA may differ significantly from the desired posterior density, the means of these two densities coincide."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32711763"
                        ],
                        "name": "M. Daniel",
                        "slug": "M.-Daniel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Daniel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Daniel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15300870,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "474b95f02506c4eeba216ff2bcfe5366223d7b47",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistically self-similar (SSS) processes can be used to describe a variety of physical phenomena, yet modeling these phenomena has proved challenging. Most of the proposed models for SSS and approximately SSS processes have power spectra that behave as 1/f/sup /spl gamma//, such as fractional Brownian motion (fBm), fractionally differenced noise, and wavelet-based syntheses. The most flexible framework is perhaps that based on wavelets, which provides a powerful tool for the synthesis and estimation of 1/f processes, but assumes a particular distribution of the measurements. An alternative framework is the class of multiresolution processes proposed by Chou et al. (1994), which has already been shown to be useful for the identification of the parameters of fBm. These multiresolution processes are defined by an autoregression in scale that makes them naturally suited to the representation of SSS (and approximately SSS) phenomena, both stationary and nonstationary. Also, this multiresolution framework is accompanied by an efficient estimator, likelihood calculator, and conditional simulator that make no assumptions about the distribution of the measurements. We show how to use the multiscale framework to represent SSS (or approximately SSS) processes such as fBm and fractionally differenced Gaussian noise. The multiscale models are realized by using canonical correlations (CC) and by exploiting the self-similarity and possible stationarity or stationary increments of the desired process. A number of examples are provided to demonstrate the utility of the multiscale framework in simulating and estimating SSS processes."
            },
            "slug": "The-Modeling-and-Estimation-of-Statistically-in-a-Daniel-Willsky",
            "title": {
                "fragments": [],
                "text": "The Modeling and Estimation of Statistically Self-Similar Processes in a Multiresolution Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how to use the multiscale framework to represent SSS (or approximately SSS) processes such as fBm and fractionally differenced Gaussian noise."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812671"
                        ],
                        "name": "M. Luettgen",
                        "slug": "M.-Luettgen",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Luettgen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luettgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145325413"
                        ],
                        "name": "W. Karl",
                        "slug": "W.-Karl",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Karl",
                            "middleNames": [
                                "Clem"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Karl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7162863,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ab4f258c7a6543948e068871b5ba38e8146803c6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach to regularization methods for image processing is introduced and developed using as a vehicle the problem of computing dense optical flow fields in an image sequence. The solution of the new problem formulation is computed with an efficient multiscale algorithm. Experiments on several image sequences demonstrate the substantial computational savings that can be achieved due to the fact that the algorithm is noniterative and in fact has a per pixel computational complexity that is independent of image size. The new approach also has a number of other important advantages. Specifically, multiresolution flow field estimates are available, allowing great flexibility in dealing with the tradeoff between resolution and accuracy. Multiscale error covariance information is also available, which is of considerable use in assessing the accuracy of the estimates. In particular, these error statistics can be used as the basis for a rational procedure for determining the spatially-varying optimal reconstruction resolution. Furthermore, if there are compelling reasons to insist upon a standard smoothness constraint, the new algorithm provides an excellent initialization for the iterative algorithms associated with the smoothness constraint problem formulation. Finally, the usefulness of the approach should extend to a wide variety of ill-posed inverse problems in which variational techniques seeking a \"smooth\" solution are generally used."
            },
            "slug": "Efficient-multiscale-regularization-with-to-the-of-Luettgen-Karl",
            "title": {
                "fragments": [],
                "text": "Efficient multiscale regularization with applications to the computation of optical flow"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The new algorithm provides an excellent initialization for the iterative algorithms associated with the smoothness constraint problem formulation and should extend to a wide variety of ill-posed inverse problems in which variational techniques seeking a \"smooth\" solution are generally used."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191477"
                        ],
                        "name": "F. V. Jensen",
                        "slug": "F.-V.-Jensen",
                        "structuredName": {
                            "firstName": "Finn",
                            "lastName": "Jensen",
                            "middleNames": [
                                "Verner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. V. Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The graph may be either directed, as in a Bayesian network (Pearl, 1988;  Jensen, 1996 ), or undirected, as in a Markov random field (Pearl, 1988; Geman & Geman, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61412478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febde16cb99b8107fecff79905ca61a5e8cd170",
            "isKey": false,
            "numCitedBy": 1493,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational modelling of probability has become a major part of automated decision support systems. In this book, the principal ideas of probabilistic reasoning - known as Bayesian networks - are outlined and their practical implications illustrated. The book is intended for MSc students in knowledge-based systems, artificial intelligence and statistics, and for professionals in decision support systems applications and research."
            },
            "slug": "An-introduction-to-Bayesian-networks-Jensen",
            "title": {
                "fragments": [],
                "text": "An introduction to Bayesian networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The principal ideas of probabilistic reasoning - known as Bayesian networks - are outlined and their practical implications illustrated and are intended for MSc students in knowledge-based systems, artificial intelligence and statistics, and for professionals in decision support systems applications and research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1833925"
                        ],
                        "name": "C. Berrou",
                        "slug": "C.-Berrou",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Berrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berrou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1870588"
                        ],
                        "name": "A. Glavieux",
                        "slug": "A.-Glavieux",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Glavieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Glavieux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952051"
                        ],
                        "name": "P. Thitimajshima",
                        "slug": "P.-Thitimajshima",
                        "structuredName": {
                            "firstName": "Punya",
                            "lastName": "Thitimajshima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Thitimajshima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as \\Turbo codes\" [3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17770377,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "3ba9baa534a8ea39a31c69e72ada959aaa6a4dc1",
            "isKey": false,
            "numCitedBy": 8239,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders.<<ETX>>"
            },
            "slug": "Near-Shannon-limit-error-correcting-coding-and-1-Berrou-Glavieux",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit error-correcting coding and decoding: Turbo-codes. 1"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICC '93 - IEEE International Conference on Communications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018201"
                        ],
                        "name": "G. Forney",
                        "slug": "G.-Forney",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Forney",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Forney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889982"
                        ],
                        "name": "F. Kschischang",
                        "slug": "F.-Kschischang",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Kschischang",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kschischang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37898699"
                        ],
                        "name": "B. Marcus",
                        "slug": "B.-Marcus",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Marcus",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257061"
                        ],
                        "name": "S. Tuncel",
                        "slug": "S.-Tuncel",
                        "structuredName": {
                            "firstName": "Selim",
                            "lastName": "Tuncel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tuncel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119639874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2dbaa4dfcffcb2671b946cf583766a02fa709a8",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The sum-product and min-sum algorithms are used to decode codes defined by trellises. In this paper, we discuss the behavior of these and related algorithms on tail-biting (TB) trellises."
            },
            "slug": "Iterative-Decoding-of-Tail-Biting-Trellises-and-Forney-Kschischang",
            "title": {
                "fragments": [],
                "text": "Iterative Decoding of Tail-Biting Trellises and Connections with Symbolic Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The sum-product and min-sum algorithms are used to decode codes defined by trellises and the behavior of these and related algorithms on tail-biting (TB) trellis is discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891751"
                        ],
                        "name": "E. Pasztor",
                        "slug": "E.-Pasztor",
                        "structuredName": {
                            "firstName": "Egon",
                            "lastName": "Pasztor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Pasztor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60327,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ea2e5eb93f194ebf8a5c350cb5e20c63a66ff6bd",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We seek the scene interpretation that best explains image data. For example, we may want to infer the projected velocities (scene) which best explain two consecutive image frames (image). From synthetic data, we model the relationship between image and scene patches, and between a scene patch and neighboring scene patches. Given a new image, we propagate likelihoods in a Markov network (ignoring the effect of loops) to infer the underlying scene. This yields an efficient method to form low-level scene interpretations. We demonstrate the technique for motion analysis and estimating high resolution images from low-resolution ones."
            },
            "slug": "Learning-to-Estimate-Scenes-from-Images-Freeman-Pasztor",
            "title": {
                "fragments": [],
                "text": "Learning to Estimate Scenes from Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "From synthetic data, the relationship between image and scene patches is modeled, and between a scene patch and neighboring scene patches, and this yields an efficient method to form low-level scene interpretations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143818472"
                        ],
                        "name": "E. Castillo",
                        "slug": "E.-Castillo",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Castillo",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Castillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144614855"
                        ],
                        "name": "J. Guti\u00e9rrez",
                        "slug": "J.-Guti\u00e9rrez",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Guti\u00e9rrez",
                            "middleNames": [
                                "Manuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Guti\u00e9rrez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145353422"
                        ],
                        "name": "A. Hadi",
                        "slug": "A.-Hadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Hadi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41024324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "837ae38d8eba5635fb8a2e0a5cdb4764e8ea348a",
            "isKey": false,
            "numCitedBy": 763,
            "numCiting": 226,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial intelligence and expert systems have seen a great deal of research in recent years, much of which has been devoted to methods for incorporating uncertainty into models. This book is devoted to providing a thorough and up-to-date survey of this field for researchers and students."
            },
            "slug": "Expert-Systems-and-Probabilistic-Network-Models-Castillo-Guti\u00e9rrez",
            "title": {
                "fragments": [],
                "text": "Expert Systems and Probabilistic Network Models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This book is devoted to providing a thorough and up-to-date survey of this field for researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": "Monographs in Computer Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986184"
                        ],
                        "name": "G. Strang",
                        "slug": "G.-Strang",
                        "structuredName": {
                            "firstName": "Gilbert",
                            "lastName": "Strang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Strang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11609445"
                        ],
                        "name": "L. Freund",
                        "slug": "L.-Freund",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Freund",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Figure 4 compares the error in the means as a function of iterations for loopy propagation and successive-over-relaxation (SOR) , considered one of the best relaxation methods [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62132950,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac84c7e453c6848f5b873492e56f74d11bde71aa",
            "isKey": false,
            "numCitedBy": 1733,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to applied mathematics , Introduction to applied mathematics , \u0645\u0631\u06a9\u0632 \u0641\u0646\u0627\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u0648 \u0627\u0637\u0644\u0627\u0639 \u0631\u0633\u0627\u0646\u06cc \u06a9\u0634\u0627\u0648\u0631\u0632\u06cc"
            },
            "slug": "Introduction-to-applied-mathematics-Strang-Freund",
            "title": {
                "fragments": [],
                "text": "Introduction to applied mathematics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47327756"
                        ],
                        "name": "G. B. Smith",
                        "slug": "G.-B.-Smith",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. B. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53839214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c221f946d54118dd062080d36c6e9aae1acdc084",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Preface-to-S.-Geman-and-D.-Geman,-\u201cStochastic-Gibbs-Smith",
            "title": {
                "fragments": [],
                "text": "Preface to S. Geman and D. Geman, \u201cStochastic relaxation, Gibbs distributions, and the Bayesian restoration of images\u201d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46558286"
                        ],
                        "name": "S. K. Andersen",
                        "slug": "S.-K.-Andersen",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Kj\u00e6r"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Andersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40176582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18aa8a0502e717c114312a3ba63dbb76fbcab47c",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Judea-Pearl,-Probabilistic-Reasoning-in-Intelligent-Andersen",
            "title": {
                "fragments": [],
                "text": "Judea Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 148
                            }
                        ],
                        "text": "The slow convergence of SOR on problems such as these lead to the development of multi-resolution models in which the MRF is approximated by a tree [15, 5] and an algorithm equivalent to belief propagation is then run on the tree."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient multiscale regularization with application to the computation of optical ow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on image processing,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [23, 24, 6, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative decoding of tailbiting trellisses"
            },
            "venue": {
                "fragments": [],
                "text": "preprint presented at 1998 Information Theory Workshop in San Diego"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The generalized distributive law. submitted to"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "T urbo factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Neural Information Processing Systems 12"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Pattern Classiication, Data Compression and Channel Coding"
            },
            "venue": {
                "fragments": [],
                "text": "Graphical Models for Pattern Classiication, Data Compression and Channel Coding"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Di erent communities tend to prefer di erent graph formalisms (see [19] for a recent review) | directed graphs are more common in AI, medical diagnosis and statistics while undirected graphs are more common in image processing, statistical physics and error correcting codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "hidden Markov models, and Markov random elds: a unifying view. Pattern Recognition, 18(11):1261{1268"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 146
                            }
                        ],
                        "text": "Several groups have recently reported excellent experimental results by running algorithms equivalent to Pearl's algorithm on networks with loops [9, 18, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Pattern Classi cation, Data Compression and Channel Coding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Applied Mathematics. W ellesley-Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to Applied Mathematics. W ellesley-Cambridge"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Applied Mathematics. Wellesley-Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to Applied Mathematics. Wellesley-Cambridge"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Willsky . E \u000e cient multiscale regularization with application to the computation of optical ow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on image processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the convergence of iterative decoding on graphs with a single cycle Near Shannon limit error - correcting coding and decoding : Turbo codes"
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Applied Mathel1Ultics. Wellesley-Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to Applied Mathel1Ultics. Wellesley-Cambridge"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative decoding of tail-biting trellisses. preprint presented at"
            },
            "venue": {
                "fragments": [],
                "text": "Iterative decoding of tail-biting trellisses. preprint presented at"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 59
                            }
                        ],
                        "text": "The graph may be either directed, as in a Bayesian network (Pearl, 1988; Jensen, 1996), or undirected, as in a Markov random field (Pearl, 1988; Geman & Geman, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to Bayesian networks. New York: Springer-Verlag"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Turbo decision algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "EEcient m ultiscale regularization with application to the computation of optical ow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on image processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "All rights reserved."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Loopy propagation gives the correct posterior means for Gaussians"
            },
            "venue": {
                "fragments": [],
                "text": "Loopy propagation gives the correct posterior means for Gaussians"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The generalized distributive l a w. submitted to"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The generalized distributive l a w"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 146
                            }
                        ],
                        "text": "Several groups have recently reported excellent experimental results by running algorithms equivalent to Pearl\u2019s algorithm on networks with loops (Frey, 1998; Murphy, Weiss, & Jordan, 1999; Freeman, Pasztor, & Carmichael, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical models for pattern classification, data compression and channel coding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 137
                            }
                        ],
                        "text": "It is straightforward to construct the inverse covariance matrix V of the joint Gaussian that describes a given Gaussian graphical model [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced inference in Bayesian networks. In M.1"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Figure 4 compares the error in the means as a function of iterations for loopy propagation and successive-over-relaxation (SOR), considered one of the best relaxation methods [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Applied Mathematics. Wellesley-Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative decoding of tailbiting trellisses. preprint presented at"
            },
            "venue": {
                "fragments": [],
                "text": "Iterative decoding of tailbiting trellisses. preprint presented at"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Pattern Classiication, Data Compression and Channel Coding"
            },
            "venue": {
                "fragments": [],
                "text": "Graphical Models for Pattern Classiication, Data Compression and Channel Coding"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skewness and pseudocode-words in iterative decoding"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Willsky . E \u000e cient multiscale regularization with application to the computation of optical ow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on image processing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [23, 24, 6, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative decoding of tail-  biting trellisses"
            },
            "venue": {
                "fragments": [],
                "text": "preprint presented at 1998 Information Theory Workshop in  San Diego"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Networksfor Pattern Classification, Data Compression and Channel Coding"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Networksfor Pattern Classification, Data Compression and Channel Coding"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 146
                            }
                        ],
                        "text": "Several groups have recently reported excellent experimental results by running algorithms equivalent to Pearl's algorithm on networks with loops [9, 18, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Pattern Classi cation, Data Compres-  sion and Channel Coding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the optimality of solutions of the maxproduct belief propagation algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Turbo decision algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 33rd Allerton Conference on Communications, Control and Computing"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models EEcient m ultiscale regularization with application to the computation of optical ow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on image processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced inference in Bayesian networks Learning in Graphical Models"
            },
            "venue": {
                "fragments": [],
                "text": "Advanced inference in Bayesian networks Learning in Graphical Models"
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 72,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Correctness-of-Belief-Propagation-in-Gaussian-of-Weiss-Freeman/2b7201afa6727252aa4d00cfed508249a637df67?sort=total-citations"
}