{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 105
                            }
                        ],
                        "text": "This is achieved by training a deep neural network, building upon work by (Bengio &#38; \nDucharme, 2001) and (Collobert &#38; Weston, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 158
                            }
                        ],
                        "text": "These range from the syntactic, such as part-of-speech \ntagging, chunking and parsing, to the semantic, such as word\u00adsense disambiguation, semantic-role labeling, \nnamed entity extraction and anaphora resolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 26
                            }
                        ],
                        "text": "Related Architectures In (Collobert &#38; Weston, 2007) we described a NN suited \nfor SRL."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 156
                            }
                        ],
                        "text": "Our best model performed as low as 14.30% in \nper\u00adword error rate, which is to be compared to previ\u00adously published results of 16.36% with an NN archi\u00adtecture \n(Collobert &#38; Weston, 2007) and 16.54% for a state-of-the-art method based on parse trees (Pradhan \net al., 2004)1 ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7376917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03539fc7a0dada577db3fc9bc2b3c8e091877108",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost."
            },
            "slug": "Fast-Semantic-Extraction-Using-a-Novel-Neural-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "Fast Semantic Extraction Using a Novel Neural Network Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel neural network architecture for the problem of semantic role labeling that learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous Work on Language Models ( Bengio & Ducharme, 2001 ) and (Schwenk & Gauvain, 2002) already presented very similar language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "Previous Work on Language Models (Bengio &#38; \nDucharme, 2001) and (Schwenk &#38; Gauvain, 2002) al\u00adready presented very similar language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This work also used a lookup-table to generate word features (see also ( Bengio & Ducharme, 2001 ))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Language Models A language model traditionally estimates the probability of the next word \nbeing w in a sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is achieved by training a deep neural network, building upon work by ( Bengio & Ducharme, 2001 ) and (Collobert & Weston, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 75
                            }
                        ],
                        "text": "This is achieved by training a deep neural network, building upon work by (Bengio &#38; \nDucharme, 2001) and (Collobert &#38; Weston, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "This work also used a lookup-table to generate word features (see also (Bengio &#38; Ducharme, \n2001))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": true,
            "numCitedBy": 6007,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In ( Sutton & McCallum, 2005a ) the authors showed that one could learn the tasks independently, hence using dierent training sets, by only leveraging predictions jointly in a test time decoding step, and still obtain improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "In (Sutton &#38; McCallum, 2005a) the authors showed that one could learn \nthe tasks independently, hence using di.erent training sets, by only leveraging predic\u00adtions jointly \nin a test time decoding step, and still ob\u00adtain improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 16
                            }
                        ],
                        "text": "The authors of (Sutton &#38; McCallum, 2005b) also describe a negative result at the same joint task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 161340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf014a9eca15a451a98a98e3d5436a9b9a27c3d7",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many learning tasks have subtasks for which much training data exists. Therefore, we want to transfer learning from the old, general-purpose subtask to a more specific new task, for which there is often less data. While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old. Specifically, we perform joint decoding of separately-trained sequence models, preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task. On two standard text data sets, we show that joint decoding outperforms cascaded decoding."
            },
            "slug": "Composition-of-Conditional-Random-Fields-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Composition of Conditional Random Fields for Transfer Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Joint decoding of separately-trained sequence models is performed, preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous Work on Language Models (Bengio & Ducharme, 2001) and ( Schwenk & Gauvain, 2002 ) already presented very similar language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 64
                            }
                        ],
                        "text": "Previous Work on Language Models (Bengio &#38; \nDucharme, 2001) and (Schwenk &#38; Gauvain, 2002) al\u00adready presented very similar language models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14249141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e41498c05d4c68e4750fb84a380317a112d97b01",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition. Almost all state.. o. f-the-art systems use statistical n-gram language models estimated on text corpora. One principle problem with such language models is the fact that many of the n-grams are never observed even in very large training corpora, and therefore it is common to back-off to a lower-order model. In this paper we propose to address this problem by carrying out the estimation task in a continuous space, enabling a smooth interpolation of the probabilities. A neural network is used to learn the projection of the words onto a continuous space and to estimate the n-gram probabilities. The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate."
            },
            "slug": "Connectionist-language-modeling-for-large-speech-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "Connectionist language modeling for large vocabulary continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2846438"
                        ],
                        "name": "Daisuke Okanohara",
                        "slug": "Daisuke-Okanohara",
                        "structuredName": {
                            "firstName": "Daisuke",
                            "lastName": "Okanohara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daisuke Okanohara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11099677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "060d5bb21f318efd785e86aa50cb97aed090a0fb",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel discriminative language model, which can be applied quite generally. Compared to the well known N-gram language models, discriminative language models can achieve more accurate discrimination because they can employ overlapping features and nonlocal information. However, discriminative language models have been used only for re-ranking in specific applications because negative examples are not available. We propose sampling pseudo-negative examples taken from probabilistic language models. However, this approach requires prohibitive computational cost if we are dealing with quite a few features and training samples. We tackle the problem by estimating the latent information in sentences using a semiMarkov class model, and then extracting features from them. We also use an online margin-based algorithm with efficient kernel computation. Experimental results show that pseudo-negative examples can be treated as real negative examples and our model can classify these sentences correctly."
            },
            "slug": "A-discriminative-language-model-with-samples-Okanohara-Tsujii",
            "title": {
                "fragments": [],
                "text": "A discriminative language model with pseudo-negative samples"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Experimental results show that pseudo-negative examples can be treated as real negative examples and the proposed discriminative language model can classify these sentences correctly."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The authors of ( Ando & Zhang, 2005 ) propose a setup more similar to ours: they learn from unlabeled data as an auxiliary task in a MTL framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 16
                            }
                        ],
                        "text": "The authors of (Ando &#38; Zhang, 2005) propose a setup \nmore similar to ours: they learn from unlabeled data as an auxiliary task in a MTL framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2581781"
                        ],
                        "name": "Nicola Ueffing",
                        "slug": "Nicola-Ueffing",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Ueffing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicola Ueffing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561045"
                        ],
                        "name": "Gholamreza Haffari",
                        "slug": "Gholamreza-Haffari",
                        "structuredName": {
                            "firstName": "Gholamreza",
                            "lastName": "Haffari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gholamreza Haffari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3028658"
                        ],
                        "name": "Anoop Sarkar",
                        "slug": "Anoop-Sarkar",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop Sarkar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 140
                            }
                        ],
                        "text": "There have been several uses of semi-supervised learning in NLP before, for example in NER (Rosenfeld & Feldman, 2007), machine translation (Ueffing et al., 2007), parsing (McClosky et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12615023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "022f600ff683d0b5158dac85832b8aecd00fe86f",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language. In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality. We propose several algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French\u2010English EuroParl data set and on data from the NIST Chinese\u2010English largedata track. We show a significant improvement in translation quality on both tasks."
            },
            "slug": "Transductive-learning-for-statistical-machine-Ueffing-Haffari",
            "title": {
                "fragments": [],
                "text": "Transductive learning for statistical machine translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper explores the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality and proposes several algorithms with this aim."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "This an old idea in machine learning; a good overview, especially focusing on NNs, can be found in (Caruana, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 100
                            }
                        ],
                        "text": "This \nan old idea in machine learning; a good overview, especially focusing on NNs, can be found in (Caruana, \n1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45998148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47aaeb6dc682162dfe5659c2cad64e5d825ad910",
            "isKey": false,
            "numCitedBy": 3252,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems."
            },
            "slug": "Multitask-Learning-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Prior work on MTL is reviewed, new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals is presented, and new results for MTL with k-nearest neighbor and kernel regression are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning and Data Mining"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "In (Sutton &#38; McCallum, 2005a) the authors showed that one could learn \nthe tasks independently, hence using di.erent training sets, by only leveraging predic\u00adtions jointly \nin a test time decoding step, and still ob\u00adtain improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The authors of ( Sutton & McCallum, 2005b ) also describe a negative result at the same joint task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 16
                            }
                        ],
                        "text": "The authors of (Sutton &#38; McCallum, 2005b) also describe a negative result at the same joint task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1544330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48b4524a3b1207157b1b2f87885c434c96fc7a19",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A striking feature of human syntactic processing is that it is context-dependent, that is, it seems to take into account semantic information from the discourse context and world knowledge. In this paper, we attempt to use this insight to bridge the gap between SRL results from gold parses and from automatically-generated parses. To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser. Our current results are negative, because a locally-trained SRL model can return inaccurate probability estimates."
            },
            "slug": "Joint-Parsing-and-Semantic-Role-Labeling-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Joint Parsing and Semantic Role Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper jointly performs parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a ProbabilisticParser, because a locally-trained SRL model can return inaccurate probability estimates."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274348"
                        ],
                        "name": "Khashayar Rohanimanesh",
                        "slug": "Khashayar-Rohanimanesh",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Rohanimanesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khashayar Rohanimanesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6038991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0329663498462521483612649c0dffc85d9d9419",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "slug": "Dynamic-conditional-random-fields:-factorized-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "On a natural-language chunking task, it is shown that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2489901"
                        ],
                        "name": "Paul R. Kingsbury",
                        "slug": "Paul-R.-Kingsbury",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kingsbury",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul R. Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 16
                            }
                        ],
                        "text": "In the PropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are arguments of a predicate in the sentence, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "Experiments We used Sections 02-21 of the PropBank dataset ver\u00adsion 1 (about 1 million words) for \ntraining and Sec\u00adtion 23 for testing as standard in all SRL experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "For \nall our experiments, training was achieved in a few epochs (about a day) over the PropBank dataset as \nshown in Figure 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 17
                            }
                        ],
                        "text": "In the \nPropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are arguments of a predicate \nin the sentence, e.g. the following sentence might be tagged [John]ARG0 [ate]REL [the apple]ARG1 , where \nate is the predicate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 49
                            }
                        ],
                        "text": "Test error versus number of training epochs over PropBank, for the SRL task alone and \nSRL jointly trained with various other NLP tasks, using deep NNs. Results: Language Model Because the \nlanguage model was trained on a huge database we .rst trained it alone."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2486369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99d2dcdcf4cf05facaa101a48c7e31d140b4736d",
            "isKey": true,
            "numCitedBy": 2394,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank."
            },
            "slug": "The-Proposition-Bank:-An-Annotated-Corpus-of-Roles-Palmer-Kingsbury",
            "title": {
                "fragments": [],
                "text": "The Proposition Bank: An Annotated Corpus of Semantic Roles"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An automatic system for semantic role tagging trained on the corpus is described and the effect on its performance of various types of information is discussed, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "When modeling long-distance dependencies is important, Time-Delay Neural Networks (TDNNs) (Waibel et al., 1989) are a better choice."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "When \nmodeling long-distance dependencies is impor\u00adtant, Time-Delay Neural Networks (TDNNs) (Waibel et al., \n1989) are a better choice."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2786,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 43
                            }
                        ],
                        "text": "This layer is followed by a softmax layer (Bridle, 1990) which makes sure the outputs are positive \nand sum to 1, allowing us to interpret the outputs of the NN as probabilities for each class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 62
                            }
                        ],
                        "text": "The whole network is trained with the cross-entropy criterion (Bridle, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "The whole network is trained with the cross-entropy criterion \n(Bridle, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 42
                            }
                        ],
                        "text": "This layer is followed by a softmax layer (Bridle, 1990) which makes sure the outputs are positive and sum to 1, allowing us to interpret the outputs of the NN as probabilities for each class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": true,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735131"
                        ],
                        "name": "Sameer Pradhan",
                        "slug": "Sameer-Pradhan",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Pradhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Pradhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1866226"
                        ],
                        "name": "W. Ward",
                        "slug": "W.-Ward",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Ward",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2483422"
                        ],
                        "name": "K. Hacioglu",
                        "slug": "K.-Hacioglu",
                        "structuredName": {
                            "firstName": "Kadri",
                            "lastName": "Hacioglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hacioglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 55
                            }
                        ],
                        "text": "54% for a state-of-the-art method based on parse trees (Pradhan et al., 2004)(1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 2
                            }
                        ],
                        "text": ", (Pradhan et al., 2004)) train a POS classifier and use the output as features for training a parser, which is then used for building features for SRL itself."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "For example, in the case of SRL, several methods (e.g., (Pradhan et al., 2004)) train a POS classi.er \nand use the output as features for training a parser, which is then used for building features for SRL \nitself."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 245
                            }
                        ],
                        "text": "Our best model performed as low as 14.30% in \nper\u00adword error rate, which is to be compared to previ\u00adously published results of 16.36% with an NN archi\u00adtecture \n(Collobert &#38; Weston, 2007) and 16.54% for a state-of-the-art method based on parse trees (Pradhan \net al., 2004)1 ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15290012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a16e484824b2580e092c985aa659e8680aeda5ee",
            "isKey": true,
            "numCitedBy": 442,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus."
            },
            "slug": "Shallow-Semantic-Parsing-using-Support-Vector-Pradhan-Ward",
            "title": {
                "fragments": [],
                "text": "Shallow Semantic Parsing using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A machine learning algorithm for shallow semantic parsing based on Support Vector Machines which shows performance improvements through a number of new features and their ability to generalize to a new test set drawn from the AQUAINT corpus."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3261451"
                        ],
                        "name": "Benjamin Rozenfeld",
                        "slug": "Benjamin-Rozenfeld",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Rozenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Rozenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145864794"
                        ],
                        "name": "Ronen Feldman",
                        "slug": "Ronen-Feldman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronen Feldman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "We chose linear models for POS and NER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several uses of semi-supervised learning in NLP before, for example in NER ( Rosenfeld & Feldman, 2007 ), machine translation (Ueng et al., 2007), parsing (McClosky et al., 2006) and text classification (Joachims, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "Improving generalization on the POS task might therefore improve both SRL and NER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "The main \ndi.erence is that they use shallow classi.ers; however they report positive results on POS and NER tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 185
                            }
                        ],
                        "text": "We give word error rates for wsz=15, 50 and 100 and various shared tasks. wsz=15 wsz=50 wsz=100 \n SRL 16.54 17.33 18.40 SRL + POS 15.99 16.57 16.53 SRL + Chunking 16.42 16.39 16.48 SRL + NER 16.67 17.29 \n17.21 SRL + Synonyms 15.46 15.17 15.17 SRL + Language model 14.42 14.30 14.46 SRL + POS + Chunking 16.46 \n15.95 16.41 SRL + POS + NER 16.45 16.89 16.29 SRL + POS + Chunking + NER 16.33 16.36 16.27 SRL + POS \n+ Chunking + NER + Synonyms 15.71 14.76 15.48 SRL + POS + Chunking + NER + Language model 14.63 14.44 \n14.50  Figure 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "We showed our deep NN \ncould be applied to various tasks such as SRL, NER, POS, chunking and language modeling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "Note, we did not evaluate NER error rates because we used \nnon-gold standard annotations in our setup."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 92
                            }
                        ],
                        "text": "There have been several \nuses of semi-supervised learning in NLP before, for exam\u00adple in NER (Rosenfeld &#38; Feldman, 2007), \nmachine translation (Ue.ng et al., 2007), parsing (McClosky et al., 2006) and text classi.cation (Joachims, \n1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "POS, NER, and chunking tasks were trained with the window version with \nksz = 5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "NER labeled data was obtained by running the Stanford Named Entity \nRecognizer (a Table 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "In NLP for example, POS predictions are often used as features \nfor SRL and NER."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "While linear approaches work \nfairly well for POS or NER, more complex tasks like SRL require nonlinear models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Similarly, in (Miller et al., 2000) NER, parsing and relation extraction were \njointly trained in a statistical parsing model achieving improved perfor\u00admance on all tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "Named Entity Recognition (NER) \nlabels atomic elements in the sentence into categories such as PER-SON , COMPANY , or LOCATION ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12001360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fcc3bbdfd2e0421dde947c5485e3ff1abe471d7",
            "isKey": true,
            "numCitedBy": 53,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many errors produced by unsupervised and semi-supervised relation extraction (RE) systems occur because of wrong recognition of entities that participate in the relations. This is especially true for systems that do not use separate named-entity recognition components, instead relying on general-purpose shallow parsing. Such systems have greater applicability, because they are able to extract relations that contain attributes of unknown types. However, this generality comes with the cost in accuracy. In this paper we show how to use corpus statistics to validate and correct the arguments of extracted relation instances, improving the overall RE performance. We test the methods on SRES \u2013 a self-supervised Web relation extraction system. We also compare the performance of corpus-based methods to the performance of validation and correction methods based on supervised NER components."
            },
            "slug": "Using-Corpus-Statistics-on-Entities-to-Improve-from-Rozenfeld-Feldman",
            "title": {
                "fragments": [],
                "text": "Using Corpus Statistics on Entities to Improve Semi-supervised Relation Extraction from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper shows how to use corpus statistics to validate and correct the arguments of extracted relation instances, improving the overall RE performance."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "This is an approach typically used in convolutional networks for vision tasks, such as the LeNet architecture (LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 111
                            }
                        ],
                        "text": "This is an approach typ\u00adically used in convolutional networks \nfor vision tasks, such as the LeNet architecture (LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35242,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 123
                            }
                        ],
                        "text": "This is an important result, given that the NLP community \nconsiders syntax as a mandatory feature for semantic extraction (Gildea &#38; Palmer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is an important result, given that the NLP community considers syntax as a mandatory feature for semantic extraction ( Gildea & Palmer, 2001 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7645153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e5652cca6464981a065184dceb2672bd13984b7",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification."
            },
            "slug": "The-Necessity-of-Parsing-for-Predicate-Argument-Gildea-Palmer",
            "title": {
                "fragments": [],
                "text": "The Necessity of Parsing for Predicate Argument Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The effect of parser accuracy on these systems' performance is quantified, and the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification is examined."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3179731"
                        ],
                        "name": "Afzal Ballim",
                        "slug": "Afzal-Ballim",
                        "structuredName": {
                            "firstName": "Afzal",
                            "lastName": "Ballim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Afzal Ballim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720567"
                        ],
                        "name": "V. Pallotta",
                        "slug": "V.-Pallotta",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Pallotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pallotta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 25
                            }
                        ],
                        "text": "Finally, the authors of (Musillo &#38; Merlo, 2006) made an attempt at improving the \nsemantic role labeling task by joint inference with syntactic parsing, but their results are not state-of-the-art."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1506769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28487f55028b6cd974cda73725bd80bf4e20f890",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "The automated analysis of natural language data has become a central issue in the design of intelligent information systems. Processing unconstrained natural language data is still considered as an AI-hard task. However, various analysis techniques have been proposed to address specific aspects of natural language. In particular, recent interest has been focused on providing approximate analysis techniques, assuming that when perfect analysis is not possible, partial results may be still very useful."
            },
            "slug": "Robust-methods-in-analysis-of-natural-language-data-Ballim-Pallotta",
            "title": {
                "fragments": [],
                "text": "Robust methods in analysis of natural language data"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The automated analysis of natural language data has become a central issue in the design of intelligent information systems and recent interest has been focused on providing approximate analysis techniques, assuming that when perfect analysis is not possible, partial results may be still very useful."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238114"
                        ],
                        "name": "G. Musillo",
                        "slug": "G.-Musillo",
                        "structuredName": {
                            "firstName": "Gabriele",
                            "lastName": "Musillo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Musillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939590"
                        ],
                        "name": "Paola Merlo",
                        "slug": "Paola-Merlo",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Merlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Merlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18074518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d20f3d1f6a8949bb7703910e154d0e3f4b14716b",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps."
            },
            "slug": "Robust-Parsing-of-the-Proposition-Bank-Musillo-Merlo",
            "title": {
                "fragments": [],
                "text": "Robust Parsing of the Proposition Bank"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop On ROMAND Robust Methods In Analysis Of Natural Language Data"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 218
                            }
                        ],
                        "text": "There have been several \nuses of semi-supervised learning in NLP before, for exam\u00adple in NER (Rosenfeld &#38; Feldman, 2007), \nmachine translation (Ue.ng et al., 2007), parsing (McClosky et al., 2006) and text classi.cation (Joachims, \n1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There have been several uses of semi-supervised learning in NLP before, for example in NER (Rosenfeld & Feldman, 2007), machine translation (Ueng et al., 2007), parsing (McClosky et al., 2006) and text classification ( Joachims, 1999 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": false,
            "numCitedBy": 3046,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240597"
                        ],
                        "name": "David McClosky",
                        "slug": "David-McClosky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McClosky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David McClosky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 171
                            }
                        ],
                        "text": "There have been several \nuses of semi-supervised learning in NLP before, for exam\u00adple in NER (Rosenfeld &#38; Feldman, 2007), \nmachine translation (Ue.ng et al., 2007), parsing (McClosky et al., 2006) and text classi.cation (Joachims, \n1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 17
                            }
                        ],
                        "text": ", 2007), parsing (McClosky et al., 2006) and text classification (Joachims, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 628455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78a9513e70f596077179101f6cb6eadc51602039",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon."
            },
            "slug": "Effective-Self-Training-for-Parsing-McClosky-Charniak",
            "title": {
                "fragments": [],
                "text": "Effective Self-Training for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work presents a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data and shows that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1571707516"
                        ],
                        "name": "SuttonCharles",
                        "slug": "SuttonCharles",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "SuttonCharles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "SuttonCharles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644003054"
                        ],
                        "name": "McCallumAndrew",
                        "slug": "McCallumAndrew",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "McCallumAndrew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "McCallumAndrew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002089905"
                        ],
                        "name": "RohanimaneshKhashayar",
                        "slug": "RohanimaneshKhashayar",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "RohanimaneshKhashayar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "RohanimaneshKhashayar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 35
                            }
                        ],
                        "text": "Using this scheme, the authors of (Sutton et al., 2007) proposed a \nconditional random .eld approach where they showed improvements from joint training on POS tagging and \nnoun-phrase chunking tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 34
                            }
                        ],
                        "text": "Using this scheme, the authors of (Sutton et al., 2007) proposed a conditional random field approach where they showed improvements from joint training on POS tagging and noun-phrase chunking tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 224326708,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "57f30d3c5fe7f6e3a7a3a68c995240be34382c63",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies ..."
            },
            "slug": "Dynamic-Conditional-Random-Fields:-Factorized-for-SuttonCharles-McCallumAndrew",
            "title": {
                "fragments": [],
                "text": "Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "In sequence modeling, the authors often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies are considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123937952"
                        ],
                        "name": "Scott Miller",
                        "slug": "Scott-Miller",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20866393"
                        ],
                        "name": "Heidi Fox",
                        "slug": "Heidi-Fox",
                        "structuredName": {
                            "firstName": "Heidi",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heidi Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 15
                            }
                        ],
                        "text": "Similarly, in (Miller et al., 2000) NER, parsing and relation extraction were \njointly trained in a statistical parsing model achieving improved perfor\u00admance on all tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 14
                            }
                        ],
                        "text": "Similarly, in (Miller et al., 2000) NER, parsing and relation extraction were jointly trained in a statistical parsing model achieving improved performance on all tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8945340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2124f8995a9cdf4e168baba426472d2d811c0f1",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations."
            },
            "slug": "A-Novel-Use-of-Statistical-Parsing-to-Extract-from-Miller-Fox",
            "title": {
                "fragments": [],
                "text": "A Novel Use of Statistical Parsing to Extract Information from Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A lexicalized, probabilistic context-free parser is adapted to information extraction and this new technique is evaluated on MUC-7 template elements and template relations."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "Previous Work on Language Models (Bengio &#38; \nDucharme, 2001) and (Schwenk &#38; Gauvain, 2002) al\u00adready presented very similar language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Language Models A language model traditionally estimates the probability of the next word \nbeing w in a sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 128
                            }
                        ],
                        "text": "These range from the syntactic, such as part-of-speech \ntagging, chunking and parsing, to the semantic, such as word\u00adsense disambiguation, semantic-role labeling, \nnamed entity extraction and anaphora resolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 75
                            }
                        ],
                        "text": "This is achieved by training a deep neural network, building upon work by (Bengio &#38; \nDucharme, 2001) and (Collobert &#38; Weston, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "This work also used a lookup-table to generate word features (see also (Bengio &#38; Ducharme, \n2001))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neural probabilistic language model. NIPS 13"
            },
            "venue": {
                "fragments": [],
                "text": "A neural probabilistic language model. NIPS 13"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 92
                            }
                        ],
                        "text": "Previous Work in Semi-Supervised \nLearning For an overview of semi-supervised learning, see (Chapelle et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59913655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "463565c30b7a9c12c2ef0558a51cfc7b05055737",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semi-Supervised-Learning-(Adaptive-Computation-and-Chapelle-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning (Adaptive Computation and Machine Learning)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 171
                            }
                        ],
                        "text": "There have been several \nuses of semi-supervised learning in NLP before, for exam\u00adple in NER (Rosenfeld &#38; Feldman, 2007), \nmachine translation (Ue.ng et al., 2007), parsing (McClosky et al., 2006) and text classi.cation (Joachims, \n1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "fective self - training for parsing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 92
                            }
                        ],
                        "text": "Previous Work in Semi-Supervised \nLearning For an overview of semi-supervised learning, see (Chapelle et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 91
                            }
                        ],
                        "text": "Previous Work in Semi-Supervised Learning For an overview of semi-supervised learning, see (Chapelle et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semisupervised learning. Adaptive computation and machine learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 16,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-unified-architecture-for-natural-language-deep-Collobert-Weston/57458bc1cffe5caa45a885af986d70f723f406b4?sort=total-citations"
}