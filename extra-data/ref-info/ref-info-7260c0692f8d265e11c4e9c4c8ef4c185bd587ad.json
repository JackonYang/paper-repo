{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 122
                            }
                        ],
                        "text": "These models have achieved remarkable gains in many domains spanning object recognition, speech recognition, and control (LeCun et al. 2015; Schmidhuber 2015). In object recognition, Krizhevsky et al. (2012) trained a deep convolutional neural network (ConvNet [LeCun et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 29
                            }
                        ],
                        "text": "Similarly, as related to the Characters Challenge, the way people learn to write a novel handwritten character \u2013 in other words, the causal prescription for producing new examples \u2013 influences later perception and categorization (Freyd, 1983, 1987)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 200
                            }
                        ],
                        "text": "This could be a stepping stone towards building more causal generative models in neural networks, such as a neural version of the Bayesian Program Learning model that could be applied to tackling the Characters Challenge (Section 3.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 130
                            }
                        ],
                        "text": "Additional progress may come by combining deep learning and probabilistic program induction to tackle even richer versions of the Characters Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 111
                            }
                        ],
                        "text": "People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 231
                            }
                        ],
                        "text": "While it outperforms existing methods on various causal prediction tasks, it is unclear how to apply the approach to inferring rich hierarchies of latent causal variables, as needed for the Frostbite challenge and (especially) the Characters Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 7
                            }
                        ],
                        "text": "In the Characters Challenge as presented in Lake, Salakhutdinov, and Tenenbaum (2015), all viable models use \u201cpre-training\u201d on many character concepts in a background set of alphabets to tune the representations they use to learn new character concepts in a test set of alphabets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 186
                            }
                        ],
                        "text": "What additional ingredients may be needed to rapidly learn more powerful and more general-purpose representations? A relevant case study is from our own work on the Characters Challenge (sect. 3.1; Lake 2014; Lake et al. 2015a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 184
                            }
                        ],
                        "text": "But to perform well, current neural network approaches require much more pre-training than do people or our Bayesian program learning approach, and they are still far from solving the Characters Challenge.7 We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 31
                            }
                        ],
                        "text": "Similarly, as described in the Characters Challenge, people can quickly infer motor programs to draw a new character in a similarly guided processes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "A relevant case study is from our own work on the Characters Challenge (Section 3.1; Lake, 2014; Lake, Salakhutdinov, & Tenenbaum, 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 137
                            }
                        ],
                        "text": "We anticipate that these sequential generative neural networks could make sharper one-shot inferences \u2013 with the goal of tackling the full Characters Challenge \u2013 by incorporating additional causal, compositional, and hierarchical structure (and by continuing to utilize learning-to-learn, described next), potentially leading to a more computationally efficient and neurally grounded variant of the BPL model of handwritten characters (Figure 5)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142187326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e953da04138c64f5d7d0ba6d24e621fe15cb8ecc",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 181,
            "paperAbstract": {
                "fragments": [],
                "text": "People can learn a new concept almost perfectly from\njust a single example, yet machine learning algorithms typically\nrequire hundreds or thousands of examples to perform similarly.\nPeople can also use their learned concepts in richer ways than\nconventional machine learning systems - for action, imagination,\nand explanation suggesting that concepts are far more than a set of\nfeatures, exemplars, or rules, the most popular forms of\nrepresentation in machine learning and traditional models of\nconcept learning. For those interested in better understanding this\nhuman ability, or in closing the gap between humans and machines,\nthe key computational questions are the same: How do people learn\nnew concepts from just one or a few examples? And how do people\nlearn such abstract, rich, and flexible representations? An even\ngreater puzzle arises by putting these two questions together: How\ndo people learn such rich concepts from just one or a few examples?\nThis thesis investigates concept learning as a form of Bayesian\nprogram induction, where learning involves selecting a structured\nprocedure that best generates the examples from a category. I\nintroduce a computational framework that utilizes the principles of\ncompositionality, causality, and learning-to-learn to learn good\nprograms from just one or a handful of examples of a new concept.\nNew conceptual representations can be learned compositionally from\npieces of related concepts, where the pieces reflect real part\nstructure in the underlying causal process that generates category\nexamples. This approach is evaluated on a number of natural concept\nlearning tasks where humans and machines can be compared\nside-by-side. Chapter 2 introduces a large-scale data set of novel,\nsimple visual concepts for studying concept learning from sparse\ndata. People were asked to produce new examples of over 1600 novel\ncategories, revealing consistent structure in the generative\nprograms that people used. Initial experiments also show that this\nstructure is useful for one-shot classification. Chapter 3\nintroduces the computational framework called Hierarchical Bayesian\nProgram Learning, and Chapters 4 and 5 compare humans and machines\non six tasks that cover a range of natural conceptual abilities. On\na challenging one-shot classification task, the computational model\nachieves human-level performance while also outperforming several\nrecent deep learning models. Visual \"Turing test\" experiments were\nused to compare humans and machines on more creative conceptual\nabilities, including generating new category examples, predicting\nlatent causal structure, generating new concepts from related\nconcepts, and freely generating new concepts. In each case, fewer\nthan twenty-five percent of judges could reliably distinguish the\nhuman behavior from the machine behavior, showing that the model\ncan generalize in ways similar to human performance. A range of\ncomparisons with lesioned models and alternative modeling\nframeworks reveal that three key ingredients - compositionality,\ncausality, and learning-to-learn\u2026"
            },
            "slug": "Towards-more-human-like-concept-learning-in-:-and-Lake",
            "title": {
                "fragments": [],
                "text": "Towards more human-like concept learning in machines : compositionality, causality, and learning-to-learn"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis investigates concept learning as a form of Bayesian program induction, and introduces a computational framework that utilizes the principles of compositionality, causality, and learning-to-learn to learn good programs from just one or a handful of examples of a new concept."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 44
                            }
                        ],
                        "text": "In the Characters Challenge as presented in Lake, Salakhutdinov, and Tenenbaum (2015), all viable models use \u201cpre-training\u201d on many character concepts in a background set of alphabets to tune the representations they use to learn new character concepts in a test set of alphabets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 17
                            }
                        ],
                        "text": "These principles were explained in the context of the Characters and Frostbite Challenges, with special emphasis on reducing the amount of training data required and facilitating transfer to novel yet related tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 0
                            }
                        ],
                        "text": "Lake, Salakhutdinov, and Tenenbaum (2015) modeled these parts using an additional layer of compositionality, where parts are complex movements created from simpler sub-part movements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 73
                            }
                        ],
                        "text": "It is not clear that it could yet pass any of the \u201cvisual Turing tests\u201d in Lake, Salakhutdinov, and Tenenbaum (2015) (Figure 5B), although we hope DRAW-style networks will continue to be extended and enriched, and could be made to pass these tests."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11790493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "815c84ab906e43f3e6322f2ca3fd5e1360c64285",
            "isKey": false,
            "numCitedBy": 2116,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwritten characters drawn by a model Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look \u201cright\u201d as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms\u2014for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world\u2019s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several \u201cvisual Turing tests\u201d probing the model\u2019s creative generalization abilities, which in many cases are indistinguishable from human behavior."
            },
            "slug": "Human-level-concept-learning-through-probabilistic-Lake-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Human-level concept learning through probabilistic program induction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A computational model is described that learns in a similar fashion and does so better than current deep learning algorithms and can generate new letters of the alphabet that look \u201cright\u201d as judged by Turing-like tests of the model's output in comparison to what real humans produce."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 213
                            }
                        ],
                        "text": "These models have been used to explain the dynamics of human learning-to-learn in many areas of cognition,\nincluding word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "have been used to explain the dynamics of human learning-to-learn in many areas of cognition, including word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 130
                            }
                        ],
                        "text": "Hierarchical Bayesian models operating over probabilistic programs (Goodman et al., 2008; Lake, Salakhutdinov, & Tenenbaum, 2015; Tenenbaum et al., 2011) are equipped to deal with theorylike structures and rich causal representations of the world, yet there are formidable algorithmic challenges for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hierarchical Bayesian models operating over probabilistic programs (Goodman et al., 2008; Lake, Salakhutdinov, & Tenenbaum, 2015; Tenenbaum et al., 2011) are equipped to deal with theorylike structures and rich causal representations of the world, yet there are formidable algorithmic challenges for efficient inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 469646,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "317794c81f54371dda5950a5ee7a41ed10298ab2",
            "isKey": false,
            "numCitedBy": 1326,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "In coming to understand the world\u2014in learning concepts, acquiring language, and grasping causal relations\u2014our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?"
            },
            "slug": "How-to-Grow-a-Mind:-Statistics,-Structure,-and-Tenenbaum-Kemp",
            "title": {
                "fragments": [],
                "text": "How to Grow a Mind: Statistics, Structure, and Abstraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5985692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0811597b0851b7ebe21aadce7cb4daac4664b44f",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples-- having seen new examples just once--providing an important class of general-purpose models for one-shot machine learning."
            },
            "slug": "One-Shot-Generalization-in-Deep-Generative-Models-Rezende-Mohamed",
            "title": {
                "fragments": [],
                "text": "One-Shot Generalization in Deep Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "New deep generative models are developed, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning, and are able to generate compelling and diverse samples, providing an important class of general-purpose models for one-shot machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716847"
                        ],
                        "name": "G. Baldassarre",
                        "slug": "G.-Baldassarre",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Baldassarre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baldassarre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805997"
                        ],
                        "name": "M. Mirolli",
                        "slug": "M.-Mirolli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Mirolli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirolli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34295603,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a7c8b076bc68a53019dc445079535bc79b7a098a",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 918,
            "paperAbstract": {
                "fragments": [],
                "text": "It has become clear to researchers in robotics and adaptive behaviour that current approaches are yielding systems with limited autonomy and capacity for self-improvement. To learn autonomously and in a cumulative fashion is one of the hallmarks of intelligence, and we know that higher mammals engage in exploratory activities that are not directed to pursue goals of immediate relevance for survival and reproduction but are instead driven by intrinsic motivations such as curiosity, interest in novel stimuli or surprising events, and interest in learning new behaviours. The adaptive value of such intrinsically motivated activities lies in the fact that they allow the cumulative acquisition of knowledge and skills that can be used later to accomplish tness-enhancing goals. Intrinsic motivations continue during adulthood, and in humans they underlie lifelong learning, artistic creativity, and scientific discovery, while they are also the basis for processes that strongly affect human well-being, such as the sense of competence, self-determination, and self-esteem. This book has two aims: to present the state of the art in research on intrinsically motivated learning, and to identify the related scientific and technological open challenges and most promising research directions. The book introduces the concept of intrinsic motivation in artificial systems, reviews the relevant literature, offers insights from the neural and behavioural sciences, and presents novel tools for research. The book is organized into six parts: the chapters in Part I give general overviews on the concept of intrinsic motivations, their function, and possible mechanisms for implementing them; Parts II, III, and IV focus on three classes of intrinsic motivation mechanisms, those based on predictors, on novelty, and on competence; Part V discusses mechanisms that are complementary to intrinsic motivations; and Part VI introduces tools and experimental frameworks for investigating intrinsic motivations.The contributing authors are among the pioneers carrying out fundamental work on this topic, drawn from related disciplines such as artificial intelligence, robotics, artificial life, evolution, machine learning, developmental psychology, cognitive science, and neuroscience. The book will be of value to graduate students and academic researchers in these domains, and to engineers engaged with the design of autonomous, adaptive robots. The contributing authors are among the pioneers carrying out fundamental work on this topic, drawn from related disciplines such as artificial intelligence, robotics, artificial life, evolution, machine learning, developmental psychology, cognitive science, and neuroscience. The book will be of value to graduate students and academic researchers in these domains, and to engineers engaged with the design of autonomous, adaptive robots."
            },
            "slug": "Intrinsically-Motivated-Learning-in-Natural-and-Baldassarre-Mirolli",
            "title": {
                "fragments": [],
                "text": "Intrinsically Motivated Learning in Natural and Artificial Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book introduces the concept of intrinsic motivation in artificial systems, reviews the relevant literature, offers insights from the neural and behavioural sciences, and presents novel tools for research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3178759,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks."
            },
            "slug": "Towards-AI-Complete-Question-Answering:-A-Set-of-Weston-Bordes",
            "title": {
                "fragments": [],
                "text": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work argues for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering, and classify these tasks into skill sets so that researchers can identify (and then rectify) the failings of their systems."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 173
                            }
                        ],
                        "text": "In addition to recognizing new examples, people can also generate new examples (Figure 1A-ii), parse a character into its most important parts and relations (Figure 1A-iii; Lake, Salakhutdinov, and Tenenbaum (2012)), and generate new characters given a small set of related characters (Figure 1A-iv)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 691331,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7b45409f45f38c12ea0e901a48ed98c850d7f994",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Concept learning as motor program induction: A large-scale empirical study Brenden M. Lake Ruslan Salakhutdinov Joshua B. Tenenbaum Department of Brain and Cognitive Sciences Massachusetts Institute of Technology Department of Statistics University of Toronto Department of Brain and Cognitive Sciences Massachusetts Institute of Technology Abstract more structured representations that can generalize in deeper and more flexible ways. Concepts have been characterized in terms of \u201cintuitive theories,\u201d which are mental explana- tions that underly a concept (e.g., Murphy & Medin, 1985), or \u201cstructural description\u201d models, which are compositional representations based on parts and relations (e.g., Winston, 1975; Hummel & Biederman, 1992). In the latter framework, the concept \u201cSegway\u201d might be represented as two wheels connected by a platform, which supports a motor, etc. Most recently, research in AI and cognitive science has empha- sized rich generative representations. Concepts like \u201chouse\u201d can vary in both the number and configuration of their parts (windows, doors, balconies, etc.), much like the variable syn- tactic structure of language. This has lead researchers to model objects and scenes using generative grammars (Wang et al., 2006; Savova, Jakel, & Tenenbaum, 2009; Zhu, Chen, & Yuille, 2009) or programs (Stuhlmuller, Tenenbaum, & Goodman, 2010). A different tradition has focused more on rapid learning and less on conceptual richness. People can acquire a concept from as little as one positive example, contrasting with early work in psychology and standard machine learning that has focused on learning from many positive and negative exam- ples. Bayesian analyses have shown how one-shot learning can be explained with appropriately constrained hypothesis spaces and priors (Shepard, 1987; Tenenbaum & Griffiths, 2001), but where do these constraints come from? For sim- ple prototype-based representations of concepts, rapid gen- eralization can occur by just sharpening particular dimen- sions or features, as described in theories of attentional learn- ing (Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002) and overhypotheses in hierarchical Bayesian models (Kemp, Perfors, & Tenenbaum, 2007). From this perspective, prior experience with various object concepts may highlight the most relevant dimensions for whole classes of concepts, like the \u201cshape bias\u201d in learning object names (as opposed to a \u201ccolor\u201d or \u201cmaterial bias\u201d). It is also possible to learn new features over the course of learning the concepts (Schyns, Goldstone, & Thibaut, 1998), and recent work has combined dimensional sharpening with sophisticated methods for fea- ture learning (Salakhutdinov, Tenenbaum, & Torralba, 2011). Despite these different avenues of progress, we are still far from a satisfying unified account. The models that explain how people learn to perform one-shot learning are restricted to the simplest prototype- or feature-based representations; they have not been developed for more sophisticated repre- sentations of concepts such as structural descriptions, gram- mars, or programs. There are also reasons to suspect that these richer representations would be difficult if not impos- Human concept learning is particularly impressive in two re- spects: the internal structure of concepts can be representation- ally rich, and yet the very same concepts can also be learned from just a few examples. Several decades of research have dramatically advanced our understanding of these two aspects of concepts. While the richness and speed of concept learn- ing are most often studied in isolation, the power of human concepts may be best explained through their synthesis. This paper presents a large-scale empirical study of one-shot con- cept learning, suggesting that rich generative knowledge in the form of a motor program can be induced from just a single example of a novel concept. Participants were asked to draw novel handwritten characters given a reference form, and we recorded the motor data used for production. Multiple drawers of the same character not only produced visually similar draw- ings, but they also showed a striking correspondence in their strokes, as measured by their number, shape, order, and direc- tion. This suggests that participants can infer a rich motor- based concept from a single example. We also show that the motor programs induced by individual subjects provide a pow- erful basis for one-shot classification, yielding far higher accu- racy than state-of-the-art pattern recognition methods based on just the visual form. Keywords: concept learning; one-shot learning; structured representations; program induction The power of human thought derives from the power of our concepts. With the concept \u201ccar,\u201d we can classify or even imagine new instances, infer missing or occluded parts, parse an object into its main components (wheels, windows, etc.), reason about a familiar thing in an unfamiliar situation (a car underwater), and even create new compositions of concepts (a car-plane). These abilities to generalize flexibly, to go beyond the data given, suggest that human concepts must be represen- tationally rich. Yet it is remarkable how little data is required to learn a new concept. From just one or a handful of exam- ples, a child can learn a new word and use it appropriately (Carey & Bartlett, 1978; Markman, 1989; Bloom, 2000; Xu & Tenenbaum, 2007). Likewise, after seeing a single \u201cSeg- way\u201d or \u201ciPad,\u201d an adult can grasp the meaning of the word, an ability called \u201cone-shot learning.\u201d A central challenge is thus to explain these two remarkable capacities: what kinds of representations can support such flexible generalizations, and what kinds of learning mechanisms can acquire a new con- cept so quickly? The greater puzzle is putting them together: how can such flexible representations be learned from only one or a few examples? Over the last couple of decades, the cognitive science of concepts has divided into different traditions, focused largely on either the richness of concepts or on learning from sparse data. In contrast to the simple representations popular in early cognitive models (e.g., prototypes; Rosch, Simpson, & Miller, 1976) or conventional machine learning (e.g., sup- port vector machines), one tradition has worked to develop"
            },
            "slug": "Concept-learning-as-motor-program-induction:-A-Lake-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Concept learning as motor program induction: A large-scale empirical study"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A large-scale empirical study of one-shot concept learning as motor program induction is presented, suggesting that rich generative knowledge in the form of a motor program can be induced from just a single example of a novel concept."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145479841"
                        ],
                        "name": "E. Horvitz",
                        "slug": "E.-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Horvitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14818619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afbb25c4346f9a9b75ba5cb751d47845a4a5b584",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience."
            },
            "slug": "Computational-rationality:-A-converging-paradigm-in-Gershman-Horvitz",
            "title": {
                "fragments": [],
                "text": "Computational rationality: A converging paradigm for intelligence in brains, minds, and machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work charts advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation to identify decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6672056"
                        ],
                        "name": "Joshua C. Peterson",
                        "slug": "Joshua-C.-Peterson",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Peterson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4445312"
                        ],
                        "name": "Joshua T. Abbott",
                        "slug": "Joshua-T.-Abbott",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Abbott",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua T. Abbott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1748424,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "72443621b561d9f5145da0b5aef4fb8e0da276f3",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural networks have become increasingly successful at solving classic perception problems (e.g., recognizing objects), often reaching or surpassing human-level accuracy. In this abridged report of Peterson et al. [2016], we examine the relationship between the image representations learned by these networks and those of humans. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not appear to capture some key qualitative aspects of human representations. To close this gap, we present a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments and inform human-centric AI."
            },
            "slug": "Adapting-Deep-Network-Features-to-Capture-Peterson-Abbott",
            "title": {
                "fragments": [],
                "text": "Adapting Deep Network Features to Capture Psychological Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images, but these features do not appear to capture some key qualitative aspects of human representations."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726415"
                        ],
                        "name": "Alexander Toshev",
                        "slug": "Alexander-Toshev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Toshev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Toshev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1169492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "isKey": false,
            "numCitedBy": 4510,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art."
            },
            "slug": "Show-and-tell:-A-neural-image-caption-generator-Vinyals-Toshev",
            "title": {
                "fragments": [],
                "text": "Show and tell: A neural image caption generator"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37774552"
                        ],
                        "name": "T. Ullman",
                        "slug": "T.-Ullman",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Ullman",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 176
                            }
                        ],
                        "text": "In at least some domains, people may not have an especially clever solution to this problem, instead grappling with the full combinatorial complexity of theory learning (T. D. Ullman et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1828746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "824705cc6b405754d8e17cf24bf3ee9ccc1ed1c0",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-learning-as-stochastic-search-in-the-of-Ullman-Goodman",
            "title": {
                "fragments": [],
                "text": "Theory learning as stochastic search in the language of thought"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89504302"
                        ],
                        "name": "Greg Wayne",
                        "slug": "Greg-Wayne",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Wayne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Wayne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447264"
                        ],
                        "name": "Malcolm Reynolds",
                        "slug": "Malcolm-Reynolds",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Reynolds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Malcolm Reynolds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3367786"
                        ],
                        "name": "Tim Harley",
                        "slug": "Tim-Harley",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Harley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398898827"
                        ],
                        "name": "Agnieszka Grabska-Barwinska",
                        "slug": "Agnieszka-Grabska-Barwinska",
                        "structuredName": {
                            "firstName": "Agnieszka",
                            "lastName": "Grabska-Barwinska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agnieszka Grabska-Barwinska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016840"
                        ],
                        "name": "Sergio Gomez Colmenarejo",
                        "slug": "Sergio-Gomez-Colmenarejo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Colmenarejo",
                            "middleNames": [
                                "Gomez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergio Gomez Colmenarejo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34505275"
                        ],
                        "name": "Tiago Ramalho",
                        "slug": "Tiago-Ramalho",
                        "structuredName": {
                            "firstName": "Tiago",
                            "lastName": "Ramalho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tiago Ramalho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70495322"
                        ],
                        "name": "J. Agapiou",
                        "slug": "J.-Agapiou",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Agapiou",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Agapiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36045539"
                        ],
                        "name": "Adri\u00e0 Puigdom\u00e8nech Badia",
                        "slug": "Adri\u00e0-Puigdom\u00e8nech-Badia",
                        "structuredName": {
                            "firstName": "Adri\u00e0",
                            "lastName": "Badia",
                            "middleNames": [
                                "Puigdom\u00e8nech"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adri\u00e0 Puigdom\u00e8nech Badia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3185820"
                        ],
                        "name": "Yori Zwols",
                        "slug": "Yori-Zwols",
                        "structuredName": {
                            "firstName": "Yori",
                            "lastName": "Zwols",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yori Zwols"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2273072"
                        ],
                        "name": "Georg Ostrovski",
                        "slug": "Georg-Ostrovski",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Ostrovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Ostrovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055913310"
                        ],
                        "name": "Adam Cain",
                        "slug": "Adam-Cain",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Cain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Cain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776287"
                        ],
                        "name": "Helen King",
                        "slug": "Helen-King",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helen King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372244"
                        ],
                        "name": "C. Summerfield",
                        "slug": "C.-Summerfield",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Summerfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Summerfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "For example, the Neural Turing Machine is a neural network augmented with a random access external memory with read and write operations that maintains end-to-end differ-\nentiability (Graves et al., 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 39
                            }
                        ],
                        "text": "For example, the Neural Turing Machine (NTM; Graves et al., 2014) and its successor the Differentiable Neural Computer (DNC; Graves et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026selective attention (Bahdanau, Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland, McNaughton, &\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 205251479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "784ee73d5363c711118f784428d1ab89f019daa5",
            "isKey": true,
            "numCitedBy": 1209,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read\u2013write memory."
            },
            "slug": "Hybrid-computing-using-a-neural-network-with-memory-Graves-Wayne",
            "title": {
                "fragments": [],
                "text": "Hybrid computing using a neural network with dynamic external memory"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3351205"
                        ],
                        "name": "N. Kriegeskorte",
                        "slug": "N.-Kriegeskorte",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Kriegeskorte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kriegeskorte"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 157
                            }
                        ],
                        "text": "\u2026of these convolutional nets have also been used to predict patterns of neural response in human and macaque IT cortex (Khaligh-Razavi & Kriegeskorte, 2014; Kriegeskorte, 2015; Yamins et al., 2014) and human typicality ratings for images of common objects (Lake, Zaremba, Fergus, & Gureckis, 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 172
                            }
                        ],
                        "text": ", 2014), where the high-level feature representations of these convolutional nets have also been used to predict patterns of neural response in human and macaque IT cortex (Khaligh-Razavi & Kriegeskorte, 2014; Kriegeskorte, 2015; Yamins et al., 2014) and human typicality ratings for images of common objects (Lake, Zaremba, Fergus, & Gureckis, 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6090985,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Psychology"
            ],
            "id": "69c0dbf6100d7c57ca8fb6da561a2d291b570cf9",
            "isKey": false,
            "numCitedBy": 630,
            "numCiting": 163,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in neural network modeling have enabled major strides in computer vision and other artificial intelligence applications. Human-level visual recognition abilities are coming within reach of artificial systems. Artificial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains find surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision."
            },
            "slug": "Deep-Neural-Networks:-A-New-Framework-for-Modeling-Kriegeskorte",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work states that biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision, are entering an exciting new era."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of vision science"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228824"
                        ],
                        "name": "Andrei A. Rusu",
                        "slug": "Andrei-A.-Rusu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Rusu",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei A. Rusu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144056327"
                        ],
                        "name": "J. Veness",
                        "slug": "J.-Veness",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Veness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792298"
                        ],
                        "name": "Marc G. Bellemare",
                        "slug": "Marc-G.-Bellemare",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Bellemare",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc G. Bellemare"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137672"
                        ],
                        "name": "Martin A. Riedmiller",
                        "slug": "Martin-A.-Riedmiller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Riedmiller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Riedmiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145600108"
                        ],
                        "name": "A. Fidjeland",
                        "slug": "A.-Fidjeland",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Fidjeland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fidjeland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2273072"
                        ],
                        "name": "Georg Ostrovski",
                        "slug": "Georg-Ostrovski",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Ostrovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Ostrovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48348688"
                        ],
                        "name": "Stig Petersen",
                        "slug": "Stig-Petersen",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Petersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stig Petersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50388928"
                        ],
                        "name": "Charlie Beattie",
                        "slug": "Charlie-Beattie",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Beattie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charlie Beattie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49813280"
                        ],
                        "name": "A. Sadik",
                        "slug": "A.-Sadik",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Sadik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sadik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460849"
                        ],
                        "name": "Ioannis Antonoglou",
                        "slug": "Ioannis-Antonoglou",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Antonoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Antonoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776287"
                        ],
                        "name": "Helen King",
                        "slug": "Helen-King",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helen King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2106164"
                        ],
                        "name": "D. Kumaran",
                        "slug": "D.-Kumaran",
                        "structuredName": {
                            "firstName": "Dharshan",
                            "lastName": "Kumaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kumaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34313265"
                        ],
                        "name": "S. Legg",
                        "slug": "S.-Legg",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Legg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Legg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "In V. Mnih et al. (2015), the network architecture and hyper-parameters were fixed, but the network was trained anew for each game, meaning the visual system and the policy are highly\nspecialized for the games it was trained on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 191
                            }
                        ],
                        "text": "Here we present two challenge problems for machine learning and AI: learning simple visual concepts (Lake, Salakhutdinov, & Tenenbaum, 2015) and learning to play the Atari game Frostbite (V. Mnih et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 180
                            }
                        ],
                        "text": "\u2026of learning new handwritten characters or learning to play Frostbite, the MNIST benchmark includes 6000 examples of each handwritten digit (LeCun et al., 1998), and the DQN of V. Mnih et al. (2015) played each Atari video game for approximately 924 hours of unique training experience (Figure 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026expert play on YouTube for just two minutes, we found that we were able to reach scores comparable to or better than the human expert reported in V. Mnih et al. (2015) after at most 15-20 minutes of total practice.4\n2The time required to train the DQN (compute time) is not the same as the game\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "The DQN introduced by V. Mnih et al. (2015) used a simple form of model-free reinforcement learning in a deep neural network that allows for fast selection of actions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 196
                            }
                        ],
                        "text": "Moreover, researchers have trained generic networks to perform structured and even strategic tasks, such as the recent work on using a Deep Q-learning Network (DQN) to play simple video games (V. Mnih et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": "The second challenge concerns the Atari game Frostbite (Figure 2), which was one of the control problems tackled by the DQN of V. Mnih et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 190
                            }
                        ],
                        "text": "\u2026robust \u201c30 no-ops starts\u201d metric is used rather than \u201chuman starts,\u201d the network of Wang et al. (2016) performs even better at 172% of human performance\n4More precisely, the human expert in V. Mnih et al. (2015) scored an average of 4335 points across 30 game sessions of up to five minutes of play."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "V. Mnih et al. (2015) combined ideas from deep learning and reinforcement learning to make a \u201cdeep reinforcement learning\u201d algorithm that learns to play large classes of simple video games from just frames of pixels and the game score, achieving human or superhuman level performance on many of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "In V. Mnih et al. (2015), the DQN was compared with a professional gamer who received approximately two hours of practice on each of the 49 Atari games (although he or she likely had prior experience with some of the games)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 276
                            }
                        ],
                        "text": "\u2026Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland, McNaughton, & O\u2019Reilly, 1995; V. Mnih et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 205242740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d",
            "isKey": true,
            "numCitedBy": 16186,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
            },
            "slug": "Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Human-level control through deep reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723876"
                        ],
                        "name": "C. Blundell",
                        "slug": "C.-Blundell",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Blundell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Blundell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542999"
                        ],
                        "name": "T. Lillicrap",
                        "slug": "T.-Lillicrap",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Lillicrap",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lillicrap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": "& Wierstra, D. (2014) Stochastic backpropagation and approximate inference in deep generative models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 227
                            }
                        ],
                        "text": "\u2026and caption generation, these tasks have been mostly studied in the big data setting that is at odds with the impressive human ability for generalizing from small data sets (although see Rezende, Mohamed, Danihelka, Gregor, & Wierstra, 2016, for a deep learning approach to the Character Challenge)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": "& Wierstra, D. (2016) Oneshot generalization in deep generative models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 2
                            }
                        ],
                        "text": "& Wierstra, D. (2015) DRAW: A recurrent neural network for image generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8909022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6",
            "isKey": true,
            "numCitedBy": 3736,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
            },
            "slug": "Matching-Networks-for-One-Shot-Learning-Vinyals-Blundell",
            "title": {
                "fragments": [],
                "text": "Matching Networks for One Shot Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work employs ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories to learn a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34811128"
                        ],
                        "name": "Michiro Negishi",
                        "slug": "Michiro-Negishi",
                        "structuredName": {
                            "firstName": "Michiro",
                            "lastName": "Negishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michiro Negishi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17514190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bca27b823c9724d910b4637fd489343233570f8",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple associationist neural network learns to factor abstract rules (i.e., grammars) from sequences of arbitrary input symbols by inventing abstract representations that accommodate unseen symbol sets as well as unseen but similar grammars. The neural network is shown to have the ability to transfer grammatical knowledge to both new symbol vocabularies and new grammars. Analysis of the state-space shows that the network learns generalized abstract structures of the input and is not simply memorizing the input strings. These representations are context sensitive, hierarchical, and based on the state variable of the finite-state machines that the neural network has learned. Generalization to new symbol sets or grammars arises from the spatial nature of the internal representations used by the network, allowing new symbol sets to be encoded close to symbol sets that have already been learned in the hidden unit space of the network. The results are counter to the arguments that learning algorithms based on weight adaptation after each exemplar presentation (such as the long term potentiation found in the mammalian nervous system) cannot in principle extract symbolic knowledge from positive examples as prescribed by prevailing human linguistic theory and evolutionary psychology."
            },
            "slug": "On-the-Emergence-of-Rules-in-Neural-Networks-Hanson-Negishi",
            "title": {
                "fragments": [],
                "text": "On the Emergence of Rules in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The neural network is shown to have the ability to transfer grammatical knowledge to both new symbol vocabularies and new grammars, counter to the arguments that learning algorithms based on weight adaptation after each exemplar presentation cannot in principle extract symbolic knowledge from positive examples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144753437"
                        ],
                        "name": "S. Chernova",
                        "slug": "S.-Chernova",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chernova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chernova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682788"
                        ],
                        "name": "A. Thomaz",
                        "slug": "A.-Thomaz",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Thomaz",
                            "middleNames": [
                                "Lockerd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Thomaz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 113
                            }
                        ],
                        "text": "2007; Schmidhuber 1991) and goal exploration (Baranes and Oudeyer 2013), social learning and natural interaction (Chernova and Thomaz 2014; Vollmer et al. 2014), maturation (Oudeyer et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26200231,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f3da1a7501e842d346bbda8c81b229857fc21260",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 255,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning from Demonstration (LfD) explores techniques for learning a task policy from examples provided by a human teacher. The field of LfD has grown into an extensive body of literature over the past 30 years, with a wide variety of approaches for encoding human demonstrations and modeling skills and tasks. Additionally, we have recently seen a focus on gathering data from non-expert human teachers (i.e., domain experts but not robotics experts). In this book, we provide an introduction to the field with a focus on the unique technical challenges associated with designing robots that learn from naive human teachers. We begin, in the introduction, with a unification of the various terminology seen in the literature as well as an outline of the design choices one has in designing an LfD system. Chapter 2 gives a brief survey of the psychology literature that provides insights from human social learning that are relevant to designing robotic social learners. Chapter 3 walks through an LfD interaction, surveying the design choices one makes and state of the art approaches in prior work. First, is the choice of input, how the human teacher interacts with the robot to provide demonstrations. Next, is the choice of modeling technique. Currently, there is a dichotomy in the field between approaches that model low-level motor skills and those that model high-level tasks composed of primitive actions. We devote a chapter to each of these. Chapter 7 is devoted to interactive and active learning approaches that allow the robot to refine an existing task model. And finally, Chapter 8 provides best practices for evaluation of LfD systems, with a focus on how to approach experiments with human subjects in this domain."
            },
            "slug": "Robot-Learning-from-Human-Teachers-Chernova-Thomaz",
            "title": {
                "fragments": [],
                "text": "Robot Learning from Human Teachers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book provides an introduction to the field with a focus on the unique technical challenges associated with designing robots that learn from naive human teachers, and provides best practices for evaluation of LfD systems."
            },
            "venue": {
                "fragments": [],
                "text": "Robot Learning from Human Teachers"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026between the models learned for old objects (or old tasks) and the models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Paz, Bottou, Scholko\u0308pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011; Srivastava & Salakhutdinov, 2013; Torralba,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 260
                            }
                        ],
                        "text": "\u2026work done under the banner of PDP (Rumelhart, McClelland, & the PDP research Group, 1986) is closer to model building than pattern recognition, whereas the recent large-scale discriminative deep learning systems more purely exemplify pattern recognition (see Bottou, 2014, for a related discussion)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1067591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb7853c5d609081ea12cd4db3863a87da2d51808",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "A\u00a0plausible definition of \u201creasoning\u201d could be \u201calgebraically manipulating previously acquired knowledge in order to answer a new question\u201d. This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated \u201call-purpose\u201d inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up."
            },
            "slug": "From-machine-learning-to-machine-reasoning-Bottou",
            "title": {
                "fragments": [],
                "text": "From machine learning to machine reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Instead of trying to bridge the gap between machine learning systems and sophisticated \u201call-purpose\u201d inference mechanisms, the set of manipulations applicable to training systems can be algebraically enriched, and reasoning capabilities from the ground up are built."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152572925"
                        ],
                        "name": "Daniel Harari",
                        "slug": "Daniel-Harari",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Harari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Harari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145364369"
                        ],
                        "name": "N. Dorfman",
                        "slug": "N.-Dorfman",
                        "structuredName": {
                            "firstName": "Nimrod",
                            "lastName": "Dorfman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dorfman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 176
                            }
                        ],
                        "text": "In at least some domains, people may not have an especially clever solution to this problem, instead grappling with the full combinatorial complexity of theory learning (T. D. Ullman et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10693063,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cb139c7bb8dc024cdcd1f4b8caac9384a985b17f",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Early in development, infants learn to solve visual problems that are highly challenging for current computational methods. We present a model that deals with two fundamental problems in which the gap between computational difficulty and infant learning is particularly striking: learning to recognize hands and learning to recognize gaze direction. The model is shown a stream of natural videos and learns without any supervision to detect human hands by appearance and by context, as well as direction of gaze, in complex natural scenes. The algorithm is guided by an empirically motivated innate mechanism\u2014the detection of \u201cmover\u201d events in dynamic images, which are the events of a moving image region causing a stationary region to move or change after contact. Mover events provide an internal teaching signal, which is shown to be more effective than alternative cues and sufficient for the efficient acquisition of hand and gaze representations. The implications go beyond the specific tasks, by showing how domain-specific \u201cproto concepts\u201d can guide the system to acquire meaningful concepts, which are significant to the observer but statistically inconspicuous in the sensory input."
            },
            "slug": "From-simple-innate-biases-to-complex-visual-Ullman-Harari",
            "title": {
                "fragments": [],
                "text": "From simple innate biases to complex visual concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A model that learns without any supervision to detect human hands by appearance and by context, as well as direction of gaze, in complex natural scenes, shows how domain-specific \u201cproto concepts\u201d can guide the system to acquire meaningful concepts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In their influential textbook, Russell and Norvig (2003) state that \u201cThe quest for \u2018artificial flight\u2019 succeeded when the Wright brothers and others stopped imitating birds and started using wind tunnels and learning about aerodynamics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 148
                            }
                        ],
                        "text": "\u2026techniques for model building and selection (Grosse, Salakhutdinov, Freeman, & Tenenbaum, 2012), and probabilistic\n1In their influential textbook, Russell and Norvig (2003) state that \u201cThe quest for \u2018artificial flight\u2019 succeeded when the Wright brothers and others stopped imitating birds and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53142908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3524cdf7cf8344e7eb74886f71fcbb5c6732c337",
            "isKey": false,
            "numCitedBy": 26733,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence."
            },
            "slug": "Artificial-Intelligence:-A-Modern-Approach-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence: A Modern Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 142
                            }
                        ],
                        "text": "\u2026such as a rule for producing the past tense of words (Rumelhart & McClelland, 1986), rules for solving simple balance-beam physics problems (McClelland, 1988), or a tree to represent types of living things (plants and animals) and their distribution of properties (Rogers & McClelland,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 248
                            }
                        ],
                        "text": "This approach has shown that neural networks can behave as if they learned explicitly structured knowledge, such as a rule for producing the past tense of words (Rumelhart & McClelland, 1986), rules for solving simple balance-beam physics problems (McClelland, 1988), or a tree to represent types of living things (plants and animals) and their distribution of properties (Rogers & McClelland, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 120
                            }
                        ],
                        "text": "Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland, 1988; Shultz, 2003) or rules relating distance, velocity, and time in motion (Buckingham & Shultz, 2000), but these networks do not attempt to work with complex scenes\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 33
                            }
                        ],
                        "text": "tasks such as balance-beam rules (McClelland, 1988; Shultz, 2003) or rules relating distance, velocity, and time in motion (Buckingham & Shultz, 2000), but these networks do not attempt"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6144184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06f73a9eb8154224ab5bf8ad0a68f6a8beeb0aa0",
            "isKey": true,
            "numCitedBy": 206,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper provides a brief overview of the connectionist or parallel distributed processing framework for modeling cognitive processes, and considers the application of the connectionist framework to problems of cognitive development. Several aspects of cognitive development might result from the process of learning as it occurs in multi-layer networks. This learning process has the characteristic that it reduces the discrepancy between expected and observed events. As it does this, representations develop on hidden units which dramatically change both the way in which the network represents the environment from which it learns and the expectations that the network generates about environmental events. The learning process exhibits relatively abrupt transitions corresponding to stage shifts in cognitive development. These points are illustrated using a network that learns to anticipate which side of a balance beam will go down, based on the number of weights on each side of the fulcrum and their distance from the fulcrum on each side of the beam. The network is trained in an environment in which weight more frequently governs which side will go down. It recapitulates the states of development seen in children, as well as the stage transitions, as it learns to represent weight and distance information. Keywords: Parallel processing; Data processing."
            },
            "slug": "Parallel-Distributed-Processing:-Implications-for-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Implications for Cognition and Development"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The application of the connectionist framework to problems of cognitive development is considered, and a network that learns to anticipate which side of a balance beam will go down is illustrated, based on the number of weights on each side of the fulcrum and their distance from the Fulcrum."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 270
                            }
                        ],
                        "text": "Much larger random-access memories can be implemented using Memory Networks which automatically embed and store each incoming piece of information in memory, especially useful for question answering tasks and other aspects of language modeling (Sukhbaatar et al., 2015; Weston et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 187
                            }
                        ],
                        "text": "\u2026Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland, McNaughton, & O\u2019Reilly, 1995; V. Mnih et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2926851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71ae756c75ac89e2d731c9c79649562b5768ff39",
            "isKey": false,
            "numCitedBy": 1146,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs."
            },
            "slug": "Memory-Networks-Weston-Chopra",
            "title": {
                "fragments": [],
                "text": "Memory Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This work describes a new class of learning models called memory networks, which reason with inference components combined with a long-term memory component; they learn how to use these jointly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166516"
                        ],
                        "name": "Emilio Parisotto",
                        "slug": "Emilio-Parisotto",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Parisotto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Parisotto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 174
                            }
                        ],
                        "text": "This has been the rationale behind multi-task learning or transfer learning, a strategy with a long history that has shown some promising results recently with deep networks (e.g., Donahue et al., 2013; Luong, Le, Sutskever, Vinyals, & Kaiser, 2015; Parisotto et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8241258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1def5d3711ebd1d86787b1ed57c91832c5ddc90b",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods."
            },
            "slug": "Actor-Mimic:-Deep-Multitask-and-Transfer-Learning-Parisotto-Ba",
            "title": {
                "fragments": [],
                "text": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work defines a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains, and uses Atari games as a testing environment to demonstrate these methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563432"
                        ],
                        "name": "Wojciech Zaremba",
                        "slug": "Wojciech-Zaremba",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Zaremba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Zaremba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3013567"
                        ],
                        "name": "T. Gureckis",
                        "slug": "T.-Gureckis",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Gureckis",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gureckis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15414451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b0f04d64c3275d7c88a490f7087a5378643f0ff",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The latest generation of neural networks has made major performance advances in object categorization from raw images. In particular, deep convolutional neural networks currently outperform alternative approaches on standard benchmarks by wide margins and achieve human-like accuracy on some tasks. These engineering successes present an opportunity to explore long-standing questions about the nature of human concepts by putting psychological theories to test at an unprecedented scale. This paper evaluates deep convolutional networks trained for classification on their ability to predict category typicality \u2013 a variable of paramount importance in the psychology of concepts \u2013 from the raw pixels of naturalistic images of objects. We find that these models have substantial predictive power, unlike simpler features computed from the same massive dataset, showing how typicality might emerge as a byproduct of a complex model trained to maximize classification performance."
            },
            "slug": "Deep-Neural-Networks-Predict-Category-Typicality-Lake-Zaremba",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks Predict Category Typicality Ratings for Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is found that deep convolutional networks trained for classification have substantial predictive power, unlike simpler features computed from the same massive dataset, showing how typicality might emerge as a byproduct of a complex model trained to maximize classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2180063"
                        ],
                        "name": "G. Lupyan",
                        "slug": "G.-Lupyan",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Lupyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lupyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24316216"
                        ],
                        "name": "B. Bergen",
                        "slug": "B.-Bergen",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Bergen",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bergen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3085468,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fcac76554c35df221992521b7c6d5f1deba3ff3d",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": "Many animals can be trained to perform novel tasks. People, too, can be trained, but sometime in early childhood people transition from being trainable to something qualitatively more powerful-being programmable. We argue that such programmability constitutes a leap in the way that organisms learn, interact, and transmit knowledge, and that what facilitates or enables this programmability is the learning and use of language. We then examine how language programs the mind and argue that it does so through the manipulation of embodied, sensorimotor representations. The role language plays in controlling mental representations offers important insights for understanding its origin and evolution."
            },
            "slug": "How-Language-Programs-the-Mind-Lupyan-Bergen",
            "title": {
                "fragments": [],
                "text": "How Language Programs the Mind"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines how language programs the mind and argues that it does so through the manipulation of embodied, sensorimotor representations, and suggests the role language plays in controlling mental representations offers important insights for understanding its origin and evolution."
            },
            "venue": {
                "fragments": [],
                "text": "Top. Cogn. Sci."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ows for fast selection of actions. There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks (see Niv, 2009, for a review). In particular, the phasic ring of midbrain dopaminergic neurons is qualitatively (Schultz, Dayan, &amp; Montague, 1997) and quantitatively (Bayer &amp; Glimcher, 2005) consistent with"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks (see Niv, 2009, for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9123356,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "59bf2a4efdd6fce10f9d0e37ffc8ef689e35f315",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 191,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-in-the-brain-Niv",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning in the brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954876"
                        ],
                        "name": "Tejas D. Kulkarni",
                        "slug": "Tejas-D.-Kulkarni",
                        "structuredName": {
                            "firstName": "Tejas",
                            "lastName": "Kulkarni",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tejas D. Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735083"
                        ],
                        "name": "Vikash K. Mansinghka",
                        "slug": "Vikash-K.-Mansinghka",
                        "structuredName": {
                            "firstName": "Vikash",
                            "lastName": "Mansinghka",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikash K. Mansinghka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In each case, the free combination of parts is not enough on its own: While compositionality and learning-to-learn can provide the parts for new ideas, causality provides the glue that gives them coherence and purpose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14334489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbd616a659e8412ba37f1bd54cfe8ed543a35eb6",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent progress on probabilistic modeling and statistical learning, coupled with the availability of large training datasets, has led to remarkable progress in computer vision. Generative probabilistic models, or \u201canalysis-by-synthesis\u201d approaches, can capture rich scene structure but have been less widely applied than their discriminative counterparts, as they often require considerable problem-specific engineering in modeling and inference, and inference is typically seen as requiring slow, hypothesize-and-test Monte Carlo methods. Here we present Picture, a probabilistic programming language for scene understanding that allows researchers to express complex generative vision models, while automatically solving them using fast general-purpose inference machinery. Picture provides a stochastic scene language that can express generative models for arbitrary 2D/3D scenes, as well as a hierarchy of representation layers for comparing scene hypotheses with observed images by matching not simply pixels, but also more abstract features (e.g., contours, deep neural network activations). Inference can flexibly integrate advanced Monte Carlo strategies with fast bottom-up data-driven methods. Thus both representations and inference strategies can build directly on progress in discriminatively trained systems to make generative vision more robust and efficient. We use Picture to write programs for 3D face analysis, 3D human pose estimation, and 3D object reconstruction - each competitive with specially engineered baselines."
            },
            "slug": "Picture:-A-probabilistic-programming-language-for-Kulkarni-Kohli",
            "title": {
                "fragments": [],
                "text": "Picture: A probabilistic programming language for scene perception"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Picture is presented, a probabilistic programming language for scene understanding that allows researchers to express complex generative vision models, while automatically solving them using fast general-purpose inference machinery."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144366430"
                        ],
                        "name": "S. Carey",
                        "slug": "S.-Carey",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Carey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "The underlying cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54493789,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "952b80fd87afb130d406be3547d8a2a74104c6ce",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "beings create scienti1\u20442c theories, mathematics, literature, moral systems, and complex technology. And only humans have the capacity to acquire such culturally constructed knowledge in the normal course of immersion in the adult world. There are many reasons for the differences between the minds of humans and other animals. We have bigger brains, and hence more powerful information processors; sometimes differences in the power of a processor can create what look like qualitative differences in kind. And of course human beings also have language\u2013the main medium for the cultural transmission of acquired knowledge. Comparative studies of humans and other primates suggest that we differ from them as well in our substantive cognitive abilities\u2013 for example, our capacity for causal analysis and our capacity to reason about the mental states of others. Each of these factors doubtless contributes to our prodigious ability to learn. But in my view another factor is even more important: our uniquely human ability to \u2018bootstrap.\u2019 Many psychologists, historians, and philosophers of science have appealed to the metaphor of bootstrapping in order to explain learning of a particularly dif1\u20442cult sort\u2013 those cases in which the endpoint of the process transcends in some qualitative way the starting point. The choice of metaphor may seem puzzling\u2013it is selfevidently impossible to pull oneself up by one\u2019s own bootstrap. After all, the process I describe below is not impossible, but I keep the term because of its historical credentials and because it seeks to explain cases of learning that many have argued are impossible. Sometimes learning requires the creation of new representational resources that are more powerful than those present at the outset. Early in the cultural history of mathematics, for instance, the concept of the number included only positive integers: with subsequent development the concept came to encompass zero, rational numbers (fractions), negative numbers, irrational numbers like pi, and so on."
            },
            "slug": "Bootstrapping-&-the-origin-of-concepts-Carey",
            "title": {
                "fragments": [],
                "text": "Bootstrapping & the origin of concepts"
            },
            "venue": {
                "fragments": [],
                "text": "Daedalus"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965424"
                        ],
                        "name": "J. Yosinski",
                        "slug": "J.-Yosinski",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Yosinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yosinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552141"
                        ],
                        "name": "J. Clune",
                        "slug": "J.-Clune",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Clune",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clune"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747909"
                        ],
                        "name": "Hod Lipson",
                        "slug": "Hod-Lipson",
                        "structuredName": {
                            "firstName": "Hod",
                            "lastName": "Lipson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hod Lipson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 362467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "081651b38ff7533550a3adfc1c00da333a8fe86c",
            "isKey": false,
            "numCitedBy": 5776,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
            },
            "slug": "How-transferable-are-features-in-deep-neural-Yosinski-Clune",
            "title": {
                "fragments": [],
                "text": "How transferable are features in deep neural networks?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper quantifies the generality versus specificity of neurons in each layer of a deep convolutional neural network and reports a few surprising results, including that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143947744"
                        ],
                        "name": "T. Weber",
                        "slug": "T.-Weber",
                        "structuredName": {
                            "firstName": "Th\u00e9ophane",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109481"
                        ],
                        "name": "Yuval Tassa",
                        "slug": "Yuval-Tassa",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Tassa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Tassa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7635903"
                        ],
                        "name": "David Szepesvari",
                        "slug": "David-Szepesvari",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Szepesvari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Szepesvari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8122361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization."
            },
            "slug": "Attend,-Infer,-Repeat:-Fast-Scene-Understanding-Eslami-Heess",
            "title": {
                "fragments": [],
                "text": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work presents a framework for efficient inference in structured image models that explicitly reason about objects by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144366429"
                        ],
                        "name": "S. Carey",
                        "slug": "S.-Carey",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Carey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8886171,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "75ac7e9fe69de3a368d1c618121100de06f9f457",
            "isKey": false,
            "numCitedBy": 1733,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Only human beings have a rich conceptual repertoire with concepts like tort, entropy, Abelian group, mannerism, icon and deconstruction. How have humans constructed these concepts? And once they have been constructed by adults, how do children acquire them? While primarily focusing on the second question, in The Origin of Concepts, Susan Carey shows that the answers to both overlap substantially. Carey begins by characterizing the innate starting point for conceptual development, namely systems of core cognition. Representations of core cognition are the output of dedicated input analyzers, as with perceptual representations, but these core representations differ from perceptual representations in having more abstract contents and richer functional roles. Carey argues that the key to understanding cognitive development lies in recognizing conceptual discontinuities in which new representational systems emerge that have more expressive power than core cognition and are also incommensurate with core cognition and other earlier representational systems. Finally, Carey fleshes out Quinian bootstrapping, a learning mechanism that has been repeatedly sketched in the literature on the history and philosophy of science. She demonstrates that Quinian bootstrapping is a major mechanism in the construction of new representational resources over the course of childrens cognitive development. Carey shows how developmental cognitive science resolves aspects of long-standing philosophical debates about the existence, nature, content, and format of innate knowledge. She also shows that understanding the processes of conceptual development in children illuminates the historical process by which concepts are constructed, and transforms the way we think about philosophical problems about the nature of concepts and the relations between language and thought."
            },
            "slug": "The-Origin-of-Concepts-Carey",
            "title": {
                "fragments": [],
                "text": "The Origin of Concepts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885349"
                        ],
                        "name": "Aja Huang",
                        "slug": "Aja-Huang",
                        "structuredName": {
                            "firstName": "Aja",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aja Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2772217"
                        ],
                        "name": "Chris J. Maddison",
                        "slug": "Chris-J.-Maddison",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Maddison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris J. Maddison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35099444"
                        ],
                        "name": "A. Guez",
                        "slug": "A.-Guez",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Guez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Guez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2175946"
                        ],
                        "name": "L. Sifre",
                        "slug": "L.-Sifre",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Sifre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sifre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47568983"
                        ],
                        "name": "George van den Driessche",
                        "slug": "George-van-den-Driessche",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Driessche",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George van den Driessche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4337102"
                        ],
                        "name": "Julian Schrittwieser",
                        "slug": "Julian-Schrittwieser",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Schrittwieser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Schrittwieser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460849"
                        ],
                        "name": "Ioannis Antonoglou",
                        "slug": "Ioannis-Antonoglou",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Antonoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Antonoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2749418"
                        ],
                        "name": "Vedavyas Panneershelvam",
                        "slug": "Vedavyas-Panneershelvam",
                        "structuredName": {
                            "firstName": "Vedavyas",
                            "lastName": "Panneershelvam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vedavyas Panneershelvam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1975889"
                        ],
                        "name": "Marc Lanctot",
                        "slug": "Marc-Lanctot",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Lanctot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Lanctot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48373216"
                        ],
                        "name": "S. Dieleman",
                        "slug": "S.-Dieleman",
                        "structuredName": {
                            "firstName": "Sander",
                            "lastName": "Dieleman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dieleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2401609"
                        ],
                        "name": "Dominik Grewe",
                        "slug": "Dominik-Grewe",
                        "structuredName": {
                            "firstName": "Dominik",
                            "lastName": "Grewe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominik Grewe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4111313"
                        ],
                        "name": "John Nham",
                        "slug": "John-Nham",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Nham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Nham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542999"
                        ],
                        "name": "T. Lillicrap",
                        "slug": "T.-Lillicrap",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Lillicrap",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lillicrap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40662181"
                        ],
                        "name": "M. Leach",
                        "slug": "M.-Leach",
                        "structuredName": {
                            "firstName": "Madeleine",
                            "lastName": "Leach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Leach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686971"
                        ],
                        "name": "T. Graepel",
                        "slug": "T.-Graepel",
                        "structuredName": {
                            "firstName": "Thore",
                            "lastName": "Graepel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Graepel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 108
                            }
                        ],
                        "text": "Each of these components has made gains against artificial and real Go players (Gelly & Silver, 2008, 2011; Silver et al., 2016; Tian & Zhu, 2015), and the notion of combining pattern recognition and model-based search goes back decades in Go and other games."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 27
                            }
                        ],
                        "text": "Between the publication of Silver et al. (2016) and before facing world champion Lee Sedol, AlphaGo was iteratively retrained several times in this way; the basic system always learned from 30 million games, but it played against successively stronger versions of itself, effectively learning from\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 259
                            }
                        ],
                        "text": "Go is considerably more difficult for AI than chess, and it was only recently that a computer program \u2013 AlphaGo \u2013 first beat a world-class player (Chouard, 2016) by using a combination of deep convolutional neural networks (convnets) and Monte Carlo Tree search (Silver et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 515925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "846aedd869a00c09b40f1f1f35673cb22bc87490",
            "isKey": false,
            "numCitedBy": 11396,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away."
            },
            "slug": "Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang",
            "title": {
                "fragments": [],
                "text": "Mastering the game of Go with deep neural networks and tree search"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using this search algorithm, the program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.5, the first time that a computer program has defeated a human professional player in the full-sized game of Go."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716847"
                        ],
                        "name": "G. Baldassarre",
                        "slug": "G.-Baldassarre",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Baldassarre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baldassarre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3074452"
                        ],
                        "name": "Francesco Mannella",
                        "slug": "Francesco-Mannella",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Mannella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Mannella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34440194"
                        ],
                        "name": "V. Fiore",
                        "slug": "V.-Fiore",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Fiore",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Fiore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3544623"
                        ],
                        "name": "P. Redgrave",
                        "slug": "P.-Redgrave",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Redgrave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Redgrave"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38756714"
                        ],
                        "name": "K. Gurney",
                        "slug": "K.-Gurney",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gurney",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gurney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805997"
                        ],
                        "name": "M. Mirolli",
                        "slug": "M.-Mirolli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Mirolli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirolli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13282523,
            "fieldsOfStudy": [
                "Psychology",
                "Biology",
                "Computer Science"
            ],
            "id": "ce283cc4c62670fdbd7b6e81ecba94d684562949",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Intrinsically-motivated-action-outcome-learning-and-Baldassarre-Mannella",
            "title": {
                "fragments": [],
                "text": "Intrinsically motivated action-outcome learning and goal-based action recall: a system-level bio-constrained computational model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural networks : the official journal of the International Neural Network Society"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48603437"
                        ],
                        "name": "A. Newell",
                        "slug": "A.-Newell",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Newell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Newell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94059053"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Simon",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14328775,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "02256c909265b66403b6cb08102174aa0b6ede1d",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "GPS,-a-program-that-simulates-human-thought-Newell-Simon",
            "title": {
                "fragments": [],
                "text": "GPS, a program that simulates human thought"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399215344"
                        ],
                        "name": "Seyed-Mahdi Khaligh-Razavi",
                        "slug": "Seyed-Mahdi-Khaligh-Razavi",
                        "structuredName": {
                            "firstName": "Seyed-Mahdi",
                            "lastName": "Khaligh-Razavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seyed-Mahdi Khaligh-Razavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3351205"
                        ],
                        "name": "N. Kriegeskorte",
                        "slug": "N.-Kriegeskorte",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Kriegeskorte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kriegeskorte"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14942477,
            "fieldsOfStudy": [
                "Psychology",
                "Biology",
                "Computer Science"
            ],
            "id": "e5e3a4a13e719ce770e036b4eeb82c95527c3296",
            "isKey": false,
            "numCitedBy": 874,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "Inferior temporal (IT) cortex in human and nonhuman primates serves visual object recognition. Computational object-vision models, although continually improving, do not yet reach human performance. It is unclear to what extent the internal representations of computational models can explain the IT representation. Here we investigate a wide range of computational model representations (37 in total), testing their categorization performance and their ability to account for the IT representational geometry. The models include well-known neuroscientific object-recognition models (e.g. HMAX, VisNet) along with several models from computer vision (e.g. SIFT, GIST, self-similarity features, and a deep convolutional neural network). We compared the representational dissimilarity matrices (RDMs) of the model representations with the RDMs obtained from human IT (measured with fMRI) and monkey IT (measured with cell recording) for the same set of stimuli (not used in training the models). Better performing models were more similar to IT in that they showed greater clustering of representational patterns by category. In addition, better performing models also more strongly resembled IT in terms of their within-category representational dissimilarities. Representational geometries were significantly correlated between IT and many of the models. However, the categorical clustering observed in IT was largely unexplained by the unsupervised models. The deep convolutional network, which was trained by supervision with over a million category-labeled images, reached the highest categorization performance and also best explained IT, although it did not fully explain the IT data. Combining the features of this model with appropriate weights and adding linear combinations that maximize the margin between animate and inanimate objects and between faces and other objects yielded a representation that fully explained our IT data. Overall, our results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT."
            },
            "slug": "Deep-Supervised,-but-Not-Unsupervised,-Models-May-Khaligh-Razavi-Kriegeskorte",
            "title": {
                "fragments": [],
                "text": "Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 228
                            }
                        ],
                        "text": "\u2026master language, and they provide the building\n8Michael Jordan made this point forcefully in his 2015 speech accepting the Rumelhart Prize.\nblocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 148
                            }
                        ],
                        "text": "These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64469087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbb22ad941724909cccc7d93e0cef15224847da5",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 281,
            "paperAbstract": {
                "fragments": [],
                "text": "Human learners routinely make inductive inferences, or inferences that go beyond the data they have observed. Inferences like these must be supported by constraints, some of which are innate, although others are almost certainly learned. This thesis presents a hierarchical Bayesian framework that helps to explain the nature, use and acquisition of inductive constraints. Hierarchical Bayesian models include multiple levels of abstraction, and the representations at the upper levels place constraints on the representations at the lower levels. The probabilistic nature of these models allows them to make statistical inferences at multiple levels of abstraction. In particular, they show how knowledge can be acquired at levels quite remote from the data of experience\u2014levels where the representations learned are naturally described as inductive constraints. \nHierarchical Bayesian models can address inductive problems from many domains but this thesis focuses on models that address three aspects of high-level cognition. The first model is sensitive to patterns of feature variability, and acquires constraints similar to the shape bias in word learning. The second model acquires causal schemata\u2014systems of abstract causal knowledge that allow learners to discover causal relationships given very sparse data. The final model discovers the structural form of a domain\u2014for instance, it discovers whether the relationships between a set of entities are best described by a tree, a chain, a ring, or some other kind of representation. \nThe hierarchical Bayesian approach captures several principles that go beyond traditional formulations of learning theory. It supports learning at multiple levels of abstraction, it handles structured representations, and it helps to explain how learning can succeed given sparse and noisy data. Principles like these are needed to explain how humans acquire rich systems of knowledge, and hierarchical Bayesian models point the way towards a modern learning theory that is better able to capture the sophistication of human learning. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "The-acquisition-of-inductive-constraints-Tenenbaum-Kemp",
            "title": {
                "fragments": [],
                "text": "The acquisition of inductive constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis presents a hierarchical Bayesian framework that helps to explain the nature, use and acquisition of inductive constraints, and focuses on models that address three aspects of high-level cognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 167
                            }
                        ],
                        "text": "Moreover, these claims usually pertain to the cellular and synaptic levels, with few connections made to systems-level neuroscience and subcortical brain organization (Edelman 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 99
                            }
                        ],
                        "text": "There may also be an intrinsic drive to reduce uncertainty and construct models of the environment (Edelman 2015; Schmidhuber 2015), closely related to learning-to-learn and multitask learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43995313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "327011da361759ecd0aab04989c9c70c48fe30ca",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 180,
            "paperAbstract": {
                "fragments": [],
                "text": "Reverse-engineering the brain involves adopting and testing a hierarchy of working hypotheses regarding the computational problems that it solves, the representations and algorithms that it employs and the manner in which these are implemented. Because problem-level assumptions set the course for the entire research programme, it is particularly important to be open to the possibility that we have them wrong, but tacit algorithm- and implementation-level hypotheses can also benefit from occasional scrutiny. This paper focuses on the extent to which our computational understanding of how the brain works is shaped by three such rarely discussed assumptions, which span the levels of Marr's hierarchy: (i) that animal behaviour amounts to a series of stimulus/response bouts, (ii) that learning can be adequately modelled as being driven by the optimisation of a fixed objective function and (iii) that massively parallel, uniformly connected layered or recurrent network architectures suffice to support learning and behaviour. In comparison, a more realistic approach acknowledges that animal behaviour in the wild is characterised by dynamically branching serial order and is often agentic rather than reactive. Arguably, such behaviour calls for open-ended learning of world structure and may require a neural architecture that includes precisely wired circuits reflecting the serial and branching structure of behavioural tasks."
            },
            "slug": "The-minority-report:-some-common-assumptions-to-in-Edelman",
            "title": {
                "fragments": [],
                "text": "The minority report: some common assumptions to reconsider in the modelling of the brain and behaviour"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper focuses on the extent to which the computational understanding of how the brain works is shaped by three rarely discussed assumptions, which span the levels of Marr's hierarchy: that animal behaviour amounts to a series of stimulus/response bouts, and that massively parallel, uniformly connected layered or recurrent network architectures suffice to support learning and behaviour."
            },
            "venue": {
                "fragments": [],
                "text": "J. Exp. Theor. Artif. Intell."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50593332"
                        ],
                        "name": "Anne G E Collins",
                        "slug": "Anne-G-E-Collins",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Collins",
                            "middleNames": [
                                "G",
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anne G E Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645751"
                        ],
                        "name": "M. Frank",
                        "slug": "M.-Frank",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Frank",
                            "middleNames": [
                                "Joshua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Frank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14940603,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "abb2ef3d934797dc8058636a593eff3c833b0961",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning and executive functions such as task-switching share common neural substrates, notably prefrontal cortex and basal ganglia. Understanding how they interact requires studying how cognitive control facilitates learning but also how learning provides the (potentially hidden) structure, such as abstract rules or task-sets, needed for cognitive control. We investigate this question from 3 complementary angles. First, we develop a new context-task-set (C-TS) model, inspired by nonparametric Bayesian methods, specifying how the learner might infer hidden structure (hierarchical rules) and decide to reuse or create new structure in novel situations. Second, we develop a neurobiologically explicit network model to assess mechanisms of such structured learning in hierarchical frontal cortex and basal ganglia circuits. We systematically explore the link between these modeling levels across task demands. We find that the network provides an approximate implementation of high-level C-TS computations, with specific neural mechanisms modulating distinct C-TS parameters. Third, this synergism yields predictions about the nature of human optimal and suboptimal choices and response times during learning and task-switching. In particular, the models suggest that participants spontaneously build task-set structure into a learning problem when not cued to do so, which predicts positive and negative transfer in subsequent generalization tests. We provide experimental evidence for these predictions and show that C-TS provides a good quantitative fit to human sequences of choices. These findings implicate a strong tendency to interactively engage cognitive control and learning, resulting in structured abstract representations that afford generalization opportunities and, thus, potentially long-term rather than short-term optimality."
            },
            "slug": "Cognitive-control-over-learning:-creating,-and-Collins-Frank",
            "title": {
                "fragments": [],
                "text": "Cognitive control over learning: creating, clustering, and generalizing task-set structure."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new context-task-set (C-TS) model is developed, inspired by nonparametric Bayesian methods, that suggests that participants spontaneously build task-set structure into a learning problem when not cued to do so, and shows that C-TS provides a good quantitative fit to human sequences of choices."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977806"
                        ],
                        "name": "Adam Lerer",
                        "slug": "Adam-Lerer",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Lerer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39793298"
                        ],
                        "name": "S. Gross",
                        "slug": "S.-Gross",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Lerer et al. (2016) trained a deep convolutional network-based system (PhysNet) to predict the stability of block towers from simulated images similar to those in Figure 4A but with much simpler configurations of two, three or four cubical blocks stacked vertically."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7771457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd7a59a101fcecc6fe0e7aed517e84a7df7d2e",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feedforward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects."
            },
            "slug": "Learning-Physical-Intuition-of-Block-Towers-by-Lerer-Gross",
            "title": {
                "fragments": [],
                "text": "Learning Physical Intuition of Block Towers by Example"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper creates small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright) to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153628144"
                        ],
                        "name": "A. Lovett",
                        "slug": "A.-Lovett",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Lovett",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lovett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 40865510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49830f605c316fec8e9e365e7e64adeea47e89d8",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a computational model of visual problem solving, designed to solve problems from the Raven\u2019s Progressive Matrices intelligence test. The model builds on the claim that analogical reasoning lies at the heart of visual problem solving, and intelligence more broadly. Images are compared via structure mapping, aligning the common relational structure in 2 images to identify commonalities and differences. These commonalities or differences can themselves be reified and used as the input for future comparisons. When images fail to align, the model dynamically rerepresents them to facilitate the comparison. In our analysis, we find that the model matches adult human performance on the Standard Progressive Matrices test, and that problems which are difficult for the model are also difficult for people. Furthermore, we show that model operations involving abstraction and rerepresentation are particularly difficult for people, suggesting that these operations may be critical for performing visual problem solving, and reasoning more generally, at the highest level."
            },
            "slug": "Modeling-Visual-Problem-Solving-as-Analogical-Lovett-Forbus",
            "title": {
                "fragments": [],
                "text": "Modeling Visual Problem Solving as Analogical Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The model builds on the claim that analogical reasoning lies at the heart of visual problem solving, and intelligence more broadly, and shows that model operations involving abstraction and rerepresentation are particularly difficult for people, suggesting that these operations may be critical for performing visual problem solve, and reasoning more generally, at the highest level."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46378362"
                        ],
                        "name": "M. Botvinick",
                        "slug": "M.-Botvinick",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Botvinick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Botvinick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145042542"
                        ],
                        "name": "D. Noelle",
                        "slug": "D.-Noelle",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Noelle",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Noelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836466"
                        ],
                        "name": "Linda B. Smith",
                        "slug": "Linda-B.-Smith",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda B. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9230418,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c9105564eecfb44a37259258507d4eb5c662eab6",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 102,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Letting-structure-emerge:-connectionist-and-systems-McClelland-Botvinick",
            "title": {
                "fragments": [],
                "text": "Letting structure emerge: connectionist and dynamical systems approaches to cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1924398"
                        ],
                        "name": "W. Freiwald",
                        "slug": "W.-Freiwald",
                        "structuredName": {
                            "firstName": "Winrich",
                            "lastName": "Freiwald",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freiwald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 394680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cae5d2e6a18db3967e15276250057a567adc5374",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A glance at an object is often sufficient to recognize it and recover fine details of its shape and appearance, even under highly variable viewpoint and lighting conditions. How can vision be so rich, but at the same time fast? The analysisby-synthesis approach to vision offers an account of the richness of our percepts, but it is typically considered too slow to explain perception in the brain. Here we propose a version of analysis-by-synthesis in the spirit of the Helmholtz machine (Dayan, Hinton, Neal, & Zemel, 1995) that can be implemented efficiently, by combining a generative model based on a realistic 3D computer graphics engine with a recognition model based on a deep convolutional network. The recognition model initializes inference in the generative model, which is then refined by brief runs of MCMC. We test this approach in the domain of face recognition and show that it meets several challenging desiderata: it can reconstruct the approximate shape and texture of a novel face from a single view, at a level indistinguishable to humans; it accounts quantitatively for human behavior in \u201chard\u201d recognition tasks that foil conventional machine systems; and it qualitatively matches neural responses in a network of face-selective brain areas. Comparison to other models provides insights to the success of our model."
            },
            "slug": "Efficient-analysis-by-synthesis-in-vision-:-A-,-,-Freiwald-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Efficient analysis-by-synthesis in vision : A computational framework , behavioral tests , and comparison with neural representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This model can reconstruct the approximate shape and texture of a novel face from a single view, at a level indistinguishable to humans; it accounts quantitatively for human behavior in \u201chard\u201d recognition tasks that foil conventional machine systems; and it qualitatively matches neural responses in a network of face-selective brain areas."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923674"
                        ],
                        "name": "G. Marcus",
                        "slug": "G.-Marcus",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Marcus",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Marcus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "& Marcus, G. (2014) The scope and limits of simulation in cognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick, 1989; Fodor & Pylyshyn, 1988; Marcus, 1998, 2001; Minsky & Papert, 1969; Pinker & Prince, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "& Marcus, G. (2015) Commonsense reasoning and commonsense knowl-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 392,
                                "start": 327
                            }
                        ],
                        "text": "Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives, just as the mind can think an infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor 1975; Fodor & Pylyshyn 1988; Marcus 2001; Piantadosi 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 142639115,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "a6383f155fa9d3e9b15092bfefbf613f982eb263",
            "isKey": true,
            "numCitedBy": 525,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In The Algebraic Mind, Gary Marcus attempts to integrate two theories about how the mind works, one that says that the mind is a computer-like manipulator of symbols, and another that says that the mind is a large network of neurons working together in parallel. Resisting the conventional wisdom that says that if the mind is a large neural network it cannot simultaneously be a manipulator of symbols, Marcus outlines a variety of ways in which neural systems could be organized so as to manipulate symbols, and he shows why such systems are more likely to provide an adequate substrate for language and cognition than neural systems that are inconsistent with the manipulation of symbols. Concluding with a discussion of how a neurally realized system of symbol-manipulation could have evolved and how such a system could unfold developmentally within the womb, Marcus helps to set the future agenda of cognitive neuroscience."
            },
            "slug": "The-Algebraic-Mind:-Integrating-Connectionism-and-Marcus",
            "title": {
                "fragments": [],
                "text": "The Algebraic Mind: Integrating Connectionism and Cognitive Science"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Gary Marcus outlines a variety of ways in which neural systems could be organized so as to manipulate symbols, and he shows why such systems are more likely to provide an adequate substrate for language and cognition than neural systems that are inconsistent with the manipulation of symbols."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2180063"
                        ],
                        "name": "G. Lupyan",
                        "slug": "G.-Lupyan",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Lupyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lupyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37700983"
                        ],
                        "name": "A. Clark",
                        "slug": "A.-Clark",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 271
                            }
                        ],
                        "text": "\u2026also provides much more powerful learning-to-learn abilities (Mikolov et al., 2016), with a compositional representation that allows children to express new concepts and thoughts in relation to existing concepts much more flexibly and stably than they previously could (Lupyan & Clark, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8115628,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "93fab1e3c0e60f79468fbe6936fbfd6ddcf1fafc",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Can what we know change what we see? Does language affect cognition and perception? The last few years have seen increased attention to these seemingly disparate questions, but with little theoretical advance. We argue that substantial clarity can be gained by considering these questions through the lens of predictive processing, a framework in which mental representations\u2014from the perceptual to the cognitive\u2014reflect an interplay between downward-flowing predictions and upward-flowing sensory signals. This framework provides a parsimonious account of how (and when) what we know ought to change what we see and helps us understand how a putatively high-level trait such as language can impact putatively low-level processes such as perception. Within this framework, language begins to take on a surprisingly central role in cognition by providing a uniquely focused and flexible means of constructing predictions against which sensory signals can be evaluated. Predictive processing thus provides a plausible mechanism for many of the reported effects of language on perception, thought, and action, and new insights on how and when speakers of different languages construct the same \u201creality\u201d in alternate ways."
            },
            "slug": "Words-and-the-World-Lupyan-Clark",
            "title": {
                "fragments": [],
                "text": "Words and the World"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33498578"
                        ],
                        "name": "Chia-ying Lee",
                        "slug": "Chia-ying-Lee",
                        "structuredName": {
                            "firstName": "Chia-ying",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chia-ying Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6454660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc362caf22c206d1d22df495c2bd4eef2f537e0c",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "One-shot learning of generative speech concepts Brenden M. Lake* Chia-ying Lee* James R. Glass Joshua B. Tenenbaum Brain and Cognitive Sciences MIT CSAIL MIT CSAIL MIT Brain and Cognitive Sciences MIT Abstract 2007). Related computational work has investigated other factors that contribute to learning word meaning, including learning-to-learn which features are important (Colunga & Smith, 2005; Kemp et al., 2007) and cross-situational word learning (Smith & Yu, 2008; Frank, Goodman, & Tenen- baum, 2009). But by any account, the acquisition of mean- ing is only possible because the child can also learn the spo- ken word as a category, mapping all instances (and exclud- ing non-instances) of a word like \u201celephant\u201d to the same phonological representation, regardless of speaker identify and other sources of acoustic variability. This is the focus of the current paper. Previous work has shown that chil- dren can do one-shot spoken word learning (Carey & Bartlett, 1978). When children (ages 3-4) were asked to bring over a \u201cchromium\u201d colored object, they seemed to flag the sound as a new word; some even later produced their own approxima- tion of the word \u201cchromium.\u201d Furthermore, acquiring new spoken words remains an important problem well into adult- hood whether its learning a second language, a new name, or a new vocabulary word. The goal of our work is twofold: to develop one-shot learn- ing tasks that can compare people and models side-by-side, and to develop a computational model that performs well on these tasks. Since the tasks must contain novel words for both people and algorithms, we tested English speakers on their ability to learn Japanese words. This language pairing also offers an interesting test case for learning-to-learn through the transfer of phonetic structure, since the Japanese analogs to English phonemes fall roughly within a subset of English phonemes (Ohata, 2004). Can the recent progress on models of one-shot learning be leveraged for learning new spoken words from raw speech? How could a generative model of a word be learned from just one example? Recent behavioral and computational work suggests that compositionality, combined with Hierarchical Bayesian modeling, can be a powerful way to build a \u201cgen- erative model for generative models\u201d that supports one-shot learning (Lake, Salakhutdinov, & Tenenbaum, 2012; Lake et al., 2013). This idea was applied to the one-shot learning of handwritten characters, a similarly high-dimensional do- main of natural concepts, using an \u201canalysis-by-synthesis\u201d approach. Given a raw image of a novel character, the model learns to represent it by a latent dynamic causal process, com- posed of pen strokes and their spatial relations (Fig. 1a). The sharing of stochastic motor primitives across concepts (Fig. 1a-i) provides a means of synthesizing new generative mod- els out of pieces of existing ones (Fig. 1a-iii). Compositional generative models are well-suited for the problem of spoken word acquisition, as they relate to classic One-shot learning \u2013 the human ability to learn a new concept from just one or a few examples \u2013 poses a challenge to tradi- tional learning algorithms, although approaches based on Hi- erarchical Bayesian models and compositional representations have been making headway. This paper investigates how chil- dren and adults readily learn the spoken form of new words from one example \u2013 recognizing arbitrary instances of a novel phonological sequence, and excluding non-instances, regard- less of speaker identity and acoustic variability. This is an es- sential step on the way to learning a word\u2019s meaning and learn- ing to use it, and we develop a Hierarchical Bayesian acoustic model that can learn spoken words from one example, utiliz- ing compositions of phoneme-like units that are the product of unsupervised learning. We compare people and computa- tional models on one-shot classification and generation tasks with novel Japanese words, finding that the learned units play an important role in achieving good performance. Keywords: one-shot learning; speech recognition; category learning; exemplar generation Introduction People can learn a new concept from just one or a few ex- amples, making meaningful generalizations that go far be- yond the observed data. Replicating this ability in machines has been challenging, since standard learning algorithms re- quire tens, hundreds, or thousands of examples before reach- ing a high level of classification performance. Nonetheless, recent interest from cognitive science and machine learning has advanced our computational understanding of \u201cone-shot learning,\u201d and several key themes have emerged. Proba- bilistic generative models can predict how people general- ize from just one or a few examples, as shown for data ly- ing in a low-dimensional space (Shepard, 1987; Tenenbaum & Griffiths, 2001). Another theme has developed around learning-to-learn, the idea that one-shot learning itself de- velops from previous learning with related concepts, and Hi- erarchical Bayesian (HB) models can learn-to-learn by high- lighting the dimensions or features that are most important for generalization (Fei-Fei, Fergus, & Perona, 2006; Kemp, Perfors, & Tenenbaum, 2007; Salakhutdinov, Tenenbaum, & Torralba, 2012). In this paper, we study the problem of learning new spoken words, an essential ingredient for language development. By one estimate, children learn an average of ten new words per day from the age of one to the end of high school (Bloom, 2000). For learning to proceed at such an astounding rate, children must be learning new words from very little data. Previous computational work has focused on the problem of learning the meaning of words from a few examples; for in- stance, upon hearing the word \u201celephant\u201d paired with an ex- emplar, the child must decide which objects belong to the set of \u201celephants\u201d and which do not (e.g., Xu & Tenenbaum, * The first two authors contributed equally to this work."
            },
            "slug": "One-shot-learning-of-generative-speech-concepts-Lake-Lee",
            "title": {
                "fragments": [],
                "text": "One-shot learning of generative speech concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper investigates how chil- dren and adults readily learn the spoken form of new words from one example \u2013 recognizing arbitrary instances of a novel phonological sequence, and excluding non-instances, regard- less of speaker identity and acoustic variability."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954876"
                        ],
                        "name": "Tejas D. Kulkarni",
                        "slug": "Tejas-D.-Kulkarni",
                        "structuredName": {
                            "firstName": "Tejas",
                            "lastName": "Kulkarni",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tejas D. Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144958935"
                        ],
                        "name": "Karthik Narasimhan",
                        "slug": "Karthik-Narasimhan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Narasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Narasimhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231182"
                        ],
                        "name": "Ardavan Saeedi",
                        "slug": "Ardavan-Saeedi",
                        "structuredName": {
                            "firstName": "Ardavan",
                            "lastName": "Saeedi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ardavan Saeedi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 94
                            }
                        ],
                        "text": "Deep reinforcement learning is only just starting to address intrinsically motivated learning (Kulkarni et al., 2016; Mohamed & Rezende, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4669377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d37620e6f8fe678a43e12930743281cd8cca6a66",
            "isKey": false,
            "numCitedBy": 781,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'."
            },
            "slug": "Hierarchical-Deep-Reinforcement-Learning:-Temporal-Kulkarni-Narasimhan",
            "title": {
                "fragments": [],
                "text": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "h-DQN is presented, a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning, and allows for flexible goal specifications, such as functions over entities and relations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542999"
                        ],
                        "name": "T. Lillicrap",
                        "slug": "T.-Lillicrap",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Lillicrap",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lillicrap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2881754"
                        ],
                        "name": "D. Cownden",
                        "slug": "D.-Cownden",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cownden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cownden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144254941"
                        ],
                        "name": "D. Tweed",
                        "slug": "D.-Tweed",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Tweed",
                            "middleNames": [
                                "Blair"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tweed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126637"
                        ],
                        "name": "C. Akerman",
                        "slug": "C.-Akerman",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Akerman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Akerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8007850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a99c2453c4239662d093eb0715c846aef4cb84a",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits."
            },
            "slug": "Random-feedback-weights-support-learning-in-deep-Lillicrap-Cownden",
            "title": {
                "fragments": [],
                "text": "Random feedback weights support learning in deep neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A surprisingly simple algorithm is presented, which assigns blame by multiplying error signals by random synaptic weights, and it is shown that a network can learn to extract useful information from signals sent through these random feedback connections, in essence, the network learns to learn."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2049973"
                        ],
                        "name": "Marcos Economides",
                        "slug": "Marcos-Economides",
                        "structuredName": {
                            "firstName": "Marcos",
                            "lastName": "Economides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcos Economides"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399114225"
                        ],
                        "name": "Z. Kurth-Nelson",
                        "slug": "Z.-Kurth-Nelson",
                        "structuredName": {
                            "firstName": "Zeb",
                            "lastName": "Kurth-Nelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Kurth-Nelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064001550"
                        ],
                        "name": "Annika L\u00fcbbert",
                        "slug": "Annika-L\u00fcbbert",
                        "structuredName": {
                            "firstName": "Annika",
                            "lastName": "L\u00fcbbert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Annika L\u00fcbbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399143077"
                        ],
                        "name": "M. Guitart-Masip",
                        "slug": "M.-Guitart-Masip",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Guitart-Masip",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guitart-Masip"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2231343"
                        ],
                        "name": "R. Dolan",
                        "slug": "R.-Dolan",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Dolan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13786295,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "499941eb1a260c9c3332c9b18a4d8d4fcb4d8c86",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Model-based and model-free reinforcement learning (RL) have been suggested as algorithmic realizations of goal-directed and habitual action strategies. Model-based RL is more flexible than model-free but requires sophisticated calculations using a learnt model of the world. This has led model-based RL to be identified with slow, deliberative processing, and model-free RL with fast, automatic processing. In support of this distinction, it has recently been shown that model-based reasoning is impaired by placing subjects under cognitive load\u2014a hallmark of non-automaticity. Here, using the same task, we show that cognitive load does not impair model-based reasoning if subjects receive prior training on the task. This finding is replicated across two studies and a variety of analysis methods. Thus, task familiarity permits use of model-based reasoning in parallel with other cognitive demands. The ability to deploy model-based reasoning in an automatic, parallelizable fashion has widespread theoretical implications, particularly for the learning and execution of complex behaviors. It also suggests a range of important failure modes in psychiatric disorders."
            },
            "slug": "Model-Based-Reasoning-in-Humans-Becomes-Automatic-Economides-Kurth-Nelson",
            "title": {
                "fragments": [],
                "text": "Model-Based Reasoning in Humans Becomes Automatic with Training"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that cognitive load does not impair model-based reasoning if subjects receive prior training on the task, and this finding is replicated across two studies and a variety of analysis methods."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319608"
                        ],
                        "name": "Armand Joulin",
                        "slug": "Armand-Joulin",
                        "structuredName": {
                            "firstName": "Armand",
                            "lastName": "Joulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armand Joulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 133
                            }
                        ],
                        "text": "Certainly one could argue that language should be included on any short list of key ingredients in human intelligence: for instance, Mikolov et al. (2016) featured language prominently in their recent paper sketching challenge problems and a road map for AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "Language also provides much more powerful learning-to-learn abilities (Mikolov et al., 2016), with a compositional representation that allows children to express new concepts and thoughts in relation to existing concepts much more flexibly and stably than they previously could (Lupyan & Clark,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 79
                            }
                        ],
                        "text": "Language also facilitates more powerful learning-to-learn and compositionality (Mikolov et al., 2016), allowing people to learn more quickly and flexibly by representing new concepts and thoughts in relation to existing concepts (Lupyan & Bergen, 2016; Lupyan & Clark, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2854692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3457ddb9b9f614aad52052d680f9b11c08b3a4cf",
            "isKey": true,
            "numCitedBy": 99,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment."
            },
            "slug": "A-Roadmap-Towards-Machine-Intelligence-Mikolov-Joulin",
            "title": {
                "fragments": [],
                "text": "A Roadmap Towards Machine Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2265067"
                        ],
                        "name": "Sainbayar Sukhbaatar",
                        "slug": "Sainbayar-Sukhbaatar",
                        "structuredName": {
                            "firstName": "Sainbayar",
                            "lastName": "Sukhbaatar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sainbayar Sukhbaatar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149531"
                        ],
                        "name": "Arthur D. Szlam",
                        "slug": "Arthur-D.-Szlam",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Szlam",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arthur D. Szlam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1399322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
            "isKey": false,
            "numCitedBy": 1990,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results."
            },
            "slug": "End-To-End-Memory-Networks-Sukhbaatar-Szlam",
            "title": {
                "fragments": [],
                "text": "End-To-End Memory Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A neural network with a recurrent attention model over a possibly large external memory that is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145606164"
                        ],
                        "name": "Joscha Bach",
                        "slug": "Joscha-Bach",
                        "structuredName": {
                            "firstName": "Joscha",
                            "lastName": "Bach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joscha Bach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 7
                            }
                        ],
                        "text": "[rBML] Bach, J. (2009) Principles of synthetic intelligence."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142192604,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "7be534097fff35e2adc6b6f4042a532cea586a49",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 420,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Foreword: \"In this book Joscha Bach introduces Dietrich Dorner's PSI architecture and Joscha's implementation of the MicroPSI architecture. These architectures and their implementation have several lessons for other architectures and models. Most notably, the PSI architecture includes drives and thus directly addresses questions of emotional behavior. An architecture including drives helps clarify how emotions could arise. It also changes the way that the architecture works on a fundamental level, providing an architecture more suited for behaving autonomously in a simulated world. PSI includes three types of drives, physiological (e.g., hunger), social (i.e., affiliation needs), and cognitive (i.e., reduction of uncertainty and expression of competency). These drives routinely influence goal formation and knowledge selection and application. The resulting architecture generates new kinds of behaviors, including context dependent memories, socially motivated behavior, and internally motivated task switching. This architecture illustrates how emotions and physical drives can be included in an embodied cognitive architecture. The PSI architecture, while including perceptual, motor, learning, and cognitive processing components, also includes several novel knowledge representations: temporal structures, spatial memories, and several new information processing mechanisms and behaviors, including progress through types of knowledge sources when problem solving (the Rasmussen ladder), and knowledge-based hierarchical active vision. These mechanisms and representations suggest ways for making other architectures more realistic, more accurate, and easier to use. The architecture is demonstrated in the Island simulated environment. While it may look like a simple game, it was carefully designed to allow multiple tasks to be pursued and provides ways to satisfy the multiple drives. It would be useful in its own right for developing other architectures interested in multi-tasking, long-term learning, social interaction, embodied architectures, and related aspects of behavior that arise in a complex but tractable real-time environment. The resulting models are not presented as validated cognitive models, but as theoretical explorations in the space of architectures for generating behavior. The sweep of the architecture can thus be larger-it presents a new cognitive architecture attempting to provide a unified theory of cognition. It attempts to cover perhaps the largest number of phenomena to date. This is not a typical cognitive modeling work, but one that I believe that we can learn much from.\" --Frank E. Ritter, Series Editor Although computational models of cognition have become very popular, these models are relatively limited in their coverage of cognition-- they usually only emphasize problem solving and reasoning, or treat perception and motivation as isolated modules. The first architecture to cover cognition more broadly is PSI theory, developed by Dietrich Dorner. By integrating motivation and emotion with perception and reasoning, and including grounded neuro-symbolic representations, PSI contributes significantly to an integrated understanding of the mind. It provides a conceptual framework that highlights the relationships between perception and memory, language and mental representation, reasoning and motivation, emotion and cognition, autonomy and social behavior. It is, however, unfortunate that PSI's origin in psychology, its methodology, and its lack of documentation have limited its impact. The proposed book adapts Psi theory to cognitive science and artificial intelligence, by elucidating both its theoretical and technical frameworks, and clarifying its contribution to how we have come to understand cognition."
            },
            "slug": "Principles-of-Synthetic-Intelligence:-Psi:-An-of-Bach",
            "title": {
                "fragments": [],
                "text": "Principles of Synthetic Intelligence: Psi: An Architecture of Motivated Cognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed book adapts Psi theory to cognitive science and artificial intelligence, by elucidating both its theoretical and technical frameworks, and clarifying its contribution to how the authors have come to understand cognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33522441"
                        ],
                        "name": "Aimee E. Stahl",
                        "slug": "Aimee-E.-Stahl",
                        "structuredName": {
                            "firstName": "Aimee",
                            "lastName": "Stahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aimee E. Stahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3096036"
                        ],
                        "name": "L. Feigenson",
                        "slug": "L.-Feigenson",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Feigenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Feigenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 257
                            }
                        ],
                        "text": "In hierarchical Bayesian modeling (Gelman, Carlin, Stern, & Rubin, 2004), a general prior on concepts is shared by multiple specific concepts, and the prior itself is learned over the course of learning the specific concepts (Salakhutdinov, Tenenbaum, & Torralba, 2012, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 420,
                                "start": 417
                            }
                        ],
                        "text": "In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects (or old tasks) and the models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Paz, Bottou, Scholko\u0308pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011; Srivastava & Salakhutdinov, 2013; Torralba, Murphy, & Freeman, 2007; Zeiler & Fergus, 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 223
                            }
                        ],
                        "text": "\u2026variables, test causal hypotheses, make use of the data-generating process in drawing conclusions, and learn selectively from others (Cook, Goodman, & Schulz, 2011; Gweon et al., 2010; L. E. Schulz, Gopnik, & Glymour, 2007; Stahl & Feigenson, 2015; Tsividis, Gershman, Tenenbaum, & Schulz, 2013)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4072059,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "74935e7d9587c108ada1d9b0673c8a884e3ab90d",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning when and what to learn Infants use \u201cunexpectedness\u201d as a cue for learning. Stahl and Feigenson studied how babies reacted when objects behaved in surprising ways (see the Perspective by Schulz). Babies who saw apparently solid and weighty objects moving through a wall or past the edge of a table without falling looked intently at them. When given the opportunity to explore these peculiar objects, they did so by banging them on the floor\u2014as if to test their solidity\u2014or dropping them\u2014as if to test their weightiness. Science, this issue p. 91; see also p. 42 Early in life, violations of core expectations provide a special opportunity for improved learning. [Also see Perspective by Schulz] Given the overwhelming quantity of information available from the environment, how do young learners know what to learn about and what to ignore? We found that 11-month-old infants (N = 110) used violations of prior expectations as special opportunities for learning. The infants were shown events that violated expectations about object behavior or events that were nearly identical but did not violate expectations. The sight of an object that violated expectations enhanced learning and promoted information-seeking behaviors; specifically, infants learned more effectively about objects that committed violations, explored those objects more, and engaged in hypothesis-testing behaviors that reflected the particular kind of violation seen. Thus, early in life, expectancy violations offer a wedge into the problem of what to learn."
            },
            "slug": "Observing-the-unexpected-enhances-infants\u2019-learning-Stahl-Feigenson",
            "title": {
                "fragments": [],
                "text": "Observing the unexpected enhances infants\u2019 learning and exploration"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The sight of an object that violated expectations enhanced learning and promoted information-seeking behaviors; specifically, infants learned more effectively about objects that committed violations, explored those objects more, and engaged in hypothesis-testing behaviors that reflected the particular kind of violation seen."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4508400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8948bea1e2436e51316f131170923cb5b7d870db",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce HD (or \u201cHierarchical-Deep\u201d) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets."
            },
            "slug": "Learning-with-Hierarchical-Deep-Models-Salakhutdinov-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Learning with Hierarchical-Deep Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Efficient learning and inference algorithms for the HDP-DBM model are presented and it is shown that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7034786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b59d91e0699d4e1896a15bae13fd180bdaf77ea5",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms."
            },
            "slug": "Neural-Programmer-Interpreters-Reed-Freitas",
            "title": {
                "fragments": [],
                "text": "Neural Programmer-Interpreters"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The neural programmer-interpreter (NPI) is proposed, a recurrent and compositional neural network that learns to represent and execute programs and has the capability to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117101253"
                        ],
                        "name": "Ke Xu",
                        "slug": "Ke-Xu",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "There has been recent interest in integrating psychological ingredients with deep neural networks, especially selective attention (Bahdanau, Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1055111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
            "isKey": false,
            "numCitedBy": 7252,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."
            },
            "slug": "Show,-Attend-and-Tell:-Neural-Image-Caption-with-Xu-Ba",
            "title": {
                "fragments": [],
                "text": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An attention based model that automatically learns to describe the content of images is introduced that can be trained in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35237816"
                        ],
                        "name": "T. B. Ward",
                        "slug": "T.-B.-Ward",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ward",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. B. Ward"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 36
                            }
                        ],
                        "text": "Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 126
                            }
                        ],
                        "text": "Many commonplace acts of creativity are combinatorial, meaning they are unexpected combinations of familiar concepts or ideas (Boden, 1998; Ward, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54276064,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "49cc605134a378a3f5e36839f7dfbcfdfd6b294a",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract College students imagined animals that might live on a planet somewhere else in the galaxy. In the first experiment, they provided drawings and descriptions of their initial imagined animal, another member of the same species, and a member of a different species. The majority of imagined creatures were structured by properties that are typical of animals on earth: bilateral symmetry, sensory receptors, and appendages. Subjects also allowed shape, appendages and sense receptors to vary often across species but rarely within species. In Experiment 2, subjects\u2032 creations were influenced by correlated attributes; those told that the animal was feathered were more likely to produce creatures with wings and beaks, and those told it lived in water and had scales were more likely to produce creatures with fins and gills relative to subjects who were told the animal was furry or who were given no specific features. Experiments 3 and 4 revealed that many subjects approach the task by retrieving exemplars of known earth animals, but that instructions and task constraints can lead to greater use of broader knowledge frameworks. Experiment 5 revealed that the structuring found in college students\u2032 imagined animals also holds for extraterrestrials developed by science fiction writers. The results are consistent with the idea that similar structures and processes underlie creative and noncreative aspects of cognition, and are discussed in terms of the concept of structured imagination . That is, when subjects create a new member of a known category for an imaginary setting, their imagination is structured by a particular set of properties that are characteristic of that category."
            },
            "slug": "Structured-Imagination:-the-Role-of-Category-in-Ward",
            "title": {
                "fragments": [],
                "text": "Structured Imagination: the Role of Category Structure in Exemplar Generation"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3364063"
                        ],
                        "name": "B. Scellier",
                        "slug": "B.-Scellier",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Scellier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Scellier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 275
                            }
                        ],
                        "text": "\u2026seems to require that information be transmitted backwards along the axon, which does not fit with realistic models of neuronal function (although recent models circumvent this problem in various ways Liao, Leibo, & Poggio, 2015; Lillicrap, Cownden, Tweed, & Akerman, 2014; Scellier & Bengio, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2488516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d21c71ee13f472faaa45bb179af7c102abd8fb",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This work contributes several new elements to the quest for a biologically plausible implementation of backprop in brains. We introduce a very general and abstract framework for machine learning, in which the quantities of interest are defined implicitly through an energy function. In this framework, only one kind of neural computation is involved both for the first phase (when the prediction is made) and the second phase (after the target is revealed), like the contrastive Hebbian learning algorithm in the continuous Hopfield model for example. Contrary to automatic differentiation in computational graphs (i.e. standard backprop), there is no need for special computation in the second phase of our framework. One advantage of our framework over contrastive Hebbian learning is that the second phase corresponds to only nudging the first-phase fixed point towards a configuration that reduces prediction error. In the case of a multi-layer supervised neural network, the output units are slightly nudged towards their target, and the perturbation introduced at the output layer propagates backward in the network. The signal 'back-propagated' during this second phase actually contains information about the error derivatives, which we use to implement a learning rule proved to perform gradient descent with respect to an objective cost function."
            },
            "slug": "Towards-a-Biologically-Plausible-Backprop-Scellier-Bengio",
            "title": {
                "fragments": [],
                "text": "Towards a Biologically Plausible Backprop"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This work contributes several new elements to the quest for a biologically plausible implementation of backprop in brains by introducing a very general and abstract framework for machine learning, in which the quantities of interest are defined implicitly through an energy function."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861371"
                        ],
                        "name": "G. Csibra",
                        "slug": "G.-Csibra",
                        "structuredName": {
                            "firstName": "Gergely",
                            "lastName": "Csibra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Csibra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32025567"
                        ],
                        "name": "S. B\u00edr\u00f3",
                        "slug": "S.-B\u00edr\u00f3",
                        "structuredName": {
                            "firstName": "Szilvia",
                            "lastName": "B\u00edr\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. B\u00edr\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274008"
                        ],
                        "name": "O. Ko\u00f3s",
                        "slug": "O.-Ko\u00f3s",
                        "structuredName": {
                            "firstName": "Orsolya",
                            "lastName": "Ko\u00f3s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Ko\u00f3s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49591043"
                        ],
                        "name": "G. Gergely",
                        "slug": "G.-Gergely",
                        "structuredName": {
                            "firstName": "Gy\u00f6rgy",
                            "lastName": "Gergely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gergely"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 181
                            }
                        ],
                        "text": "Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions toward those goals subject to constraints (Csibra 2008; Csibra et al. 2003; Spelke & Kinzler 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15092356,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f9d70caa3e83edadc4e9d06d24c24f25dadab921",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "One-year-old-infants-use-teleological-of-actions-Csibra-B\u00edr\u00f3",
            "title": {
                "fragments": [],
                "text": "One-year-old infants use teleological representations of actions productively"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39402399"
                        ],
                        "name": "Yuandong Tian",
                        "slug": "Yuandong-Tian",
                        "structuredName": {
                            "firstName": "Yuandong",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuandong Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153092619"
                        ],
                        "name": "Yan Zhu",
                        "slug": "Yan-Zhu",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16305497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04e3c20a738e2f922574b0a66483a37100187563",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Competing with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go's high branching factor makes traditional search techniques ineffective, even on leading-edge hardware, and Go's evaluation function could change drastically with one stone change. Recent works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not strictly necessary for machine Go players. A pure pattern-matching approach, based on a Deep Convolutional Neural Network (DCNN) that predicts the next move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is limited. We extend this idea in our bot named darkforest, which relies on a DCNN designed for long-term predictions. Darkforest substantially improves the win rate for pattern-matching approaches against MCTS-based approaches, even with looser search budgets. Against human players, the newest versions, darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a substantial improvement upon the estimated 4k-5k ranks for DCNN reported in Clark & Storkey (2015) based on games against other machine players. Adding MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000 rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al. (2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament."
            },
            "slug": "Better-Computer-Go-Player-with-Neural-Network-and-Tian-Zhu",
            "title": {
                "fragments": [],
                "text": "Better Computer Go Player with Neural Network and Long-term Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Against human players, the newest versions, darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a substantial improvement upon the estimated 4k-5k ranks for DCNN reported in Clark & Storkey (2015) based on games against other machine players."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3330279"
                        ],
                        "name": "Adrien Baranes",
                        "slug": "Adrien-Baranes",
                        "structuredName": {
                            "firstName": "Adrien",
                            "lastName": "Baranes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrien Baranes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720664"
                        ],
                        "name": "Pierre-Yves Oudeyer",
                        "slug": "Pierre-Yves-Oudeyer",
                        "structuredName": {
                            "firstName": "Pierre-Yves",
                            "lastName": "Oudeyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Yves Oudeyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7902132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9468634b94fbded8b3362bdf230ab2becba0c0ef",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Active-learning-of-inverse-models-with-motivated-in-Baranes-Oudeyer",
            "title": {
                "fragments": [],
                "text": "Active learning of inverse models with intrinsically motivated goal exploration in robots"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "\u2026of features between the models learned for old objects (or old tasks) and the models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Paz, Bottou, Scholko\u0308pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011; Srivastava & Salakhutdinov,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 46
                            }
                        ],
                        "text": "models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Oaz, Bottou, Scholk\u00f6pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011; Srivastava & Salakhutdinov, 2013; Torralba, Murphy, & Freeman, 2007; Zeiler & Fergus, 2014)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9803204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727e1e16ede6eaad241bad11c525da07b154c688",
            "isKey": false,
            "numCitedBy": 972,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task."
            },
            "slug": "A-Model-of-Inductive-Bias-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Model of Inductive Bias Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Under certain restrictions on the set of all hypothesis spaces available to the learner, it is shown that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923674"
                        ],
                        "name": "G. Marcus",
                        "slug": "G.-Marcus",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Marcus",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9484437,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "08dc7b19e679539f0f93db0192a8e8d11538b3dd",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans routinely generalize universal relationships to unfamiliar instances. If we are told \"if glork then frum,\" and \"glork,\" we can infer \"frum\"; any name that serves as the subject of a sentence can appear as the object of a sentence. These universals are pervasive in language and reasoning. One account of how they are generalized holds that humans possess mechanisms that manipulate symbols and variables; an alternative account holds that symbol-manipulation can be eliminated from scientific theories in favor of descriptions couched in terms of networks of interconnected nodes. Can these \"eliminative\" connectionist models offer a genuine alternative? This article shows that eliminative connectionist models cannot account for how we extend universals to arbitrary items. The argument runs as follows. First, if these models, as currently conceived, were to extend universals to arbitrary instances, they would have to generalize outside the space of training examples. Next, it is shown that the class of eliminative connectionist models that is currently popular cannot learn to extend universals outside the training space. This limitation might be avoided through the use of an architecture that implements symbol manipulation."
            },
            "slug": "Rethinking-Eliminative-Connectionism-Marcus",
            "title": {
                "fragments": [],
                "text": "Rethinking Eliminative Connectionism"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the class of eliminative connectionist models that is currently popular cannot learn to extend universals outside the training space, and this limitation might be avoided through the use of an architecture that implements symbol manipulation."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50196944"
                        ],
                        "name": "Judy Hoffman",
                        "slug": "Judy-Hoffman",
                        "structuredName": {
                            "firstName": "Judy",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judy Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152329702"
                        ],
                        "name": "Ning Zhang",
                        "slug": "Ning-Zhang",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368132"
                        ],
                        "name": "Eric Tzeng",
                        "slug": "Eric-Tzeng",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Tzeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Tzeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 233
                            }
                        ],
                        "text": "Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a \u201ccognitive map\u201d of the environment and using it to plan action sequences for more complex tasks (Daw, Niv, & Dayan, 2005; Dolan & Dayan, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6161478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8de958fead0d8a9619b55c7299df3257c624a96",
            "isKey": false,
            "numCitedBy": 4234,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "slug": "DeCAF:-A-Deep-Convolutional-Activation-Feature-for-Donahue-Jia",
            "title": {
                "fragments": [],
                "text": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "DeCAF, an open-source implementation of deep convolutional activation features, along with all associated network parameters, are released to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 449,
                                "start": 6
                            }
                        ],
                        "text": "1995; Hinton et al. 1995) and variational optimization (Gregor et al. 2015; Mnih & Gregor 2014; Rezende et al. 2014), or nearest-neighbor density estimation (Kulkarni et al. 2015a; Stuhlm\u00fcller et al. 2013). One implication of amortization is that solutions to different problems will become correlated because of the sharing of amortized computations. Some evidence for inferential correlations in humans was reported by Gershman and Goodman (2014). This trend is an avenue of potential integration of deep learning models with probabilistic models and probabilistic programming: Training neural networks to help perform probabilistic inference in a generative model or a probabilistic program (Eslami et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 65
                            }
                        ],
                        "text": "Although generative neural networks such as Deep Belief Networks (Hinton et al. 2006) or variational auto-encoders (Gregor et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13408,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 153
                            }
                        ],
                        "text": "These models have achieved remarkable gains in many domains spanning object recognition, speech recognition, and control (LeCun, Bengio, & Hinton, 2015; Schmidhuber, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 131
                            }
                        ],
                        "text": "\u201cDeep\u201d refers to the fact that more powerful models can be built by composing many layers of representation (see LeCun et al., 2015; Schmidhuber, 2015, for recent reviews), still very much in the PDP style while utilizing recent advances in hardware and computing capabilities, as well as massive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11715509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "193edd20cae92c6759c18ce93eeea96afd9528eb",
            "isKey": false,
            "numCitedBy": 11779,
            "numCiting": 1175,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-learning-in-neural-networks:-An-overview-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Deep learning in neural networks: An overview"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065066249"
                        ],
                        "name": "Charles Blundell",
                        "slug": "Charles-Blundell",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Blundell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Blundell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064430351"
                        ],
                        "name": "Benigno Uria",
                        "slug": "Benigno-Uria",
                        "structuredName": {
                            "firstName": "Benigno",
                            "lastName": "Uria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benigno Uria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064022681"
                        ],
                        "name": "Alexander Pritzel",
                        "slug": "Alexander-Pritzel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Pritzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Pritzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110452463"
                        ],
                        "name": "Yazhe Li",
                        "slug": "Yazhe-Li",
                        "structuredName": {
                            "firstName": "Yazhe",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yazhe Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065757507"
                        ],
                        "name": "Avraham Ruderman",
                        "slug": "Avraham-Ruderman",
                        "structuredName": {
                            "firstName": "Avraham",
                            "lastName": "Ruderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Avraham Ruderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066450556"
                        ],
                        "name": "Joel Z. Leibo",
                        "slug": "Joel-Z.-Leibo",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Leibo",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Z. Leibo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065432208"
                        ],
                        "name": "Jack W. Rae",
                        "slug": "Jack-W.-Rae",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Rae",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jack W. Rae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064632007"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064163661"
                        ],
                        "name": "Demis Hassabis",
                        "slug": "Demis-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demis Hassabis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2398966,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Psychology"
            ],
            "id": "ba378579fb44007db9f02699889721dcd2b5b3a0",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains."
            },
            "slug": "Model-Free-Episodic-Control-Blundell-Uria",
            "title": {
                "fragments": [],
                "text": "Model-Free Episodic Control"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work demonstrates that a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks and attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2852732"
                        ],
                        "name": "T. Shultz",
                        "slug": "T.-Shultz",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Shultz",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shultz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 226
                            }
                        ],
                        "text": "What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems? Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland, 1988; Shultz, 2003) or rules relating distance, velocity, and time in motion (Buckingham & Shultz, 2000), but these networks do not attempt to work with complex scenes as input or a wide range of scenarios and judgments as in Figure 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland, 1988; Shultz, 2003) or rules relating distance, velocity, and time in motion (Buckingham & Shultz, 2000), but these networks do not attempt to work with complex scenes\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 141982496,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c62330dafda1b46f5ef21f16536760b0a9a611a2",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite decades of scientific research, the core issues of child development remain too complex to be explained by traditional verbal theories. These issues include structure and transition, representation and processing, innate and experiential determinants of development, stages of development, the purpose and end of development, and the relation between knowledge and learning. In this book Thomas Shultz shows how computational modeling can be used to capture these complex phenomena, and in so doing he lays the foundation for a new subfield of developmental psychology, computational developmental psychology. A principal approach in developmental thinking is the constructivist one. Constructivism is the Piagetian view that the child builds new cognitive structures by using current mental structures to understand new events. In this book Shultz features constructivist models employing networks that grow as well as learn. This allows models to implement synaptogenesis and neurogenesis in a way that allows qualitative changes in processing mechanisms. The book's appendices provide additional background on the mathematical concepts used, and a companion Web site contains easy-to-use computational packages."
            },
            "slug": "Computational-Developmental-Psychology-Shultz",
            "title": {
                "fragments": [],
                "text": "Computational Developmental Psychology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573615"
                        ],
                        "name": "Mustafa Suleyman",
                        "slug": "Mustafa-Suleyman",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Suleyman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mustafa Suleyman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7831483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e837b79de602c69395498c1fbbe39bbb4e6f75ad",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments."
            },
            "slug": "Learning-to-Transduce-with-Unbounded-Memory-Grefenstette-Hermann",
            "title": {
                "fragments": [],
                "text": "Learning to Transduce with Unbounded Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues and shows that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in the transduction experiments."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32306578"
                        ],
                        "name": "Vincent G. Berthiaume",
                        "slug": "Vincent-G.-Berthiaume",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Berthiaume",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent G. Berthiaume"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2852732"
                        ],
                        "name": "T. Shultz",
                        "slug": "T.-Shultz",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Shultz",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8073569"
                        ],
                        "name": "K. Onishi",
                        "slug": "K.-Onishi",
                        "structuredName": {
                            "firstName": "Kristine",
                            "lastName": "Onishi",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Onishi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16211224,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0d7319a9078b96d86cdb93ec96fdb06124057aa2",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-constructivist-connectionist-model-of-transitions-Berthiaume-Shultz",
            "title": {
                "fragments": [],
                "text": "A constructivist connectionist model of transitions on false-belief tasks"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2292273"
                        ],
                        "name": "Daniel Yamins",
                        "slug": "Daniel-Yamins",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Yamins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Yamins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145270377"
                        ],
                        "name": "Ha Hong",
                        "slug": "Ha-Hong",
                        "structuredName": {
                            "firstName": "Ha",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ha Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202046"
                        ],
                        "name": "C. Cadieu",
                        "slug": "C.-Cadieu",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Cadieu",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cadieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913449"
                        ],
                        "name": "E. Solomon",
                        "slug": "E.-Solomon",
                        "structuredName": {
                            "firstName": "Ethan",
                            "lastName": "Solomon",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Solomon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11287004"
                        ],
                        "name": "Darren Seibert",
                        "slug": "Darren-Seibert",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Seibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Seibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 177
                            }
                        ],
                        "text": "\u2026of these convolutional nets have also been used to predict patterns of neural response in human and macaque IT cortex (Khaligh-Razavi & Kriegeskorte, 2014; Kriegeskorte, 2015; Yamins et al., 2014) and human typicality ratings for images of common objects (Lake, Zaremba, Fergus, & Gureckis, 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 104
                            }
                        ],
                        "text": "Perhaps they could, using genetic programming methods (Koza, 1992) or other structure-search algorithms Yamins et al. (2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3792835,
            "fieldsOfStudy": [
                "Biology",
                "Psychology",
                "Computer Science"
            ],
            "id": "2bd2b120ccd5aa88a5927889a973b2204732e435",
            "isKey": false,
            "numCitedBy": 1292,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Significance Humans and monkeys easily recognize objects in scenes. This ability is known to be supported by a network of hierarchically interconnected brain areas. However, understanding neurons in higher levels of this hierarchy has long remained a major challenge in visual systems neuroscience. We use computational techniques to identify a neural network model that matches human performance on challenging object categorization tasks. Although not explicitly constrained to match neural data, this model turns out to be highly predictive of neural responses in both the V4 and inferior temporal cortex, the top two layers of the ventral visual hierarchy. In addition to yielding greatly improved models of visual cortex, these results suggest that a process of biological performance optimization directly shaped neural mechanisms. The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model\u2019s categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model\u2019s intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization\u2014applied in a biologically appropriate model class\u2014can be used to build quantitative predictive models of neural processing."
            },
            "slug": "Performance-optimized-hierarchical-models-predict-Yamins-Hong",
            "title": {
                "fragments": [],
                "text": "Performance-optimized hierarchical models predict neural responses in higher visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses computational techniques to identify a high-performing neural network model that matches human performance on challenging object categorization tasks and shows that performance optimization\u2014applied in a biologically appropriate model class\u2014can be used to build quantitative predictive models of neural processing."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4292727"
                        ],
                        "name": "M. Guasti",
                        "slug": "M.-Guasti",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Guasti",
                            "middleNames": [
                                "Teresa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guasti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 86
                            }
                        ],
                        "text": "Is it the ability to understand others intentionally and build shared intentionality (Bloom, 2000; Frank, Goodman, & Tenenbaum, 2009; Tomasello, 2010)?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 174
                            }
                        ],
                        "text": "Children are far more practiced than adults at learning new concepts \u2013 learning roughly nine or ten new words each day after beginning to speak through the end of high school (Bloom, 2000; Carey, 1978) \u2013 yet the ability for rapid \u201cone-shot\u201d learning does not disappear in adulthood."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 462,
                                "start": 397
                            }
                        ],
                        "text": "speculated about key features of human cognition that gives rise to language and other uniquely human modes of thought: Is it recursion, or some new kind of recursive structure building ability (Berwick & Chomsky, 2016; Hauser, Chomsky, & Fitch, 2002)? Is it the ability to reuse symbols by name (Deacon, 1998)? Is it the ability to understand others intentionally and build shared intentionality (Bloom, 2000; Frank, Goodman, & Tenenbaum, 2009; Tomasello, 2010)? Is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55002532,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "965c88e1afe9742eb06931a393cacf098c3c805d",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "These two books on how children learn their first language differ vastly in accessibility and topic. Guasti (G) assumes some familiarity with \u201cconcepts developed in linguistic research\u201d (xi), but that turns out to mean \u201cthe generative theory of Universal Grammar\u201d (1)\u2014clearly the unmarked case for introductory linguistics courses in much of the world. While G meets the less well-read reader halfway with a summary of background assumptions at the start of each chapter, readers will still find most of the chapters hard going if they are not at home with analyzing sentences in terms of Determiner Phrases, Inflectional Phrases, and Complementizer Phrases, for example. One does well to handle concepts such as syntactic binding comfortably already, too, even though its principles are briefly summarized\u2014and, for those acquainted with \u201cc-command\u201d and \u201cdomain,\u201d clearly summarized at that. [1] (A short glossary helps somewhat, but still assumes a lot of knowledge of current Universal Grammar (UG) terminology.) Bloom (B), by contrast, equally scholarly in his writing, is readily accessible to educated readers even without any background in linguistics or psychology. G advances the generative linguistics student\u2019s knowledge of theories of the timing of children\u2019s acquisition of various principles of UG, and how differences between children\u2019s and adults\u2019 grammars can be expressed in UG terms. For interested readers in general, including parents of young children, B opens up a fascinating range of ideas about \u201chow children learn the meaning of a word\u201d and much more: \u201ccertain nonlinguistic mental capacities, including how children think about the minds of others and how they make sense of the external world\u201d (2)."
            },
            "slug": "How-Children-Learn-the-Meanings-of-Words-Guasti",
            "title": {
                "fragments": [],
                "text": "How Children Learn the Meanings of Words"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 216356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60e801e3dfc9812e294ed9de6d579e0293d61643",
            "isKey": false,
            "numCitedBy": 1139,
            "numCiting": 135,
            "paperAbstract": {
                "fragments": [],
                "text": "How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery."
            },
            "slug": "Probabilistic-machine-learning-and-artificial-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Probabilistic machine learning and artificial intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688552242"
                        ],
                        "name": "Vittorio Gallese \u2020",
                        "slug": "Vittorio-Gallese-\u2020",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Gallese \u2020",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vittorio Gallese \u2020"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143716922"
                        ],
                        "name": "G. Lakoff",
                        "slug": "G.-Lakoff",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Lakoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lakoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Gallistel and Matzel (2013) have persuasively argued that the critical interstimulus interval for LTP is orders of magnitude smaller than the intervals that are behaviorally relevant in most forms of learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "The connection between temperature sensation and social relatedness is argued to reflect neural \u201cbindings\u201d (Gallese and Lakoff 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9149796,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "92e6dc6c68e6c3058e8abc9c84daaf7ceb1abdc2",
            "isKey": false,
            "numCitedBy": 2048,
            "numCiting": 196,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts are the elementary units of reason and linguistic meaning. They are conventional and relatively stable. As such, they must somehow be the result of neural activity in the brain. The questions are: Where? and How? A common philosophical position is that all concepts\u2014even concepts about action and perception\u2014are symbolic and abstract, and therefore must be implemented outside the brain's sensory-motor system. We will argue against this position using (1) neuroscientific evidence; (2) results from neural computation; and (3) results about the nature of concepts from cognitive linguistics. We will propose that the sensory-motor system has the right kind of structure to characterise both sensory-motor and more abstract concepts. Central to this picture are the neural theory of language and the theory of cogs, according to which, brain structures in the sensory-motor regions are exploited to characterise the so-called \u201cabstract\u201d concepts that constitute the meanings of grammatical constructions and general inference patterns."
            },
            "slug": "The-Brain's-concepts:-the-role-of-the-Sensory-motor-VittorioGallese-Lakoff",
            "title": {
                "fragments": [],
                "text": "The Brain's concepts: the role of the Sensory-motor system in conceptual knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proposed that the sensory-motor system has the right kind of structure to characterise both sensory- motor and more abstract concepts, and it is argued against this position using neuroscientific evidence, results from neural computation, and results about the nature of concepts from cognitive linguistics."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive neuropsychology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 210
                            }
                        ],
                        "text": "One implication of amortization is that solutions to different problems will become correlated due to the sharing of amortized computations; some evidence for inferential correlations in humans was reported by Gershman and Goodman (2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 924780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93f5a28d16e04334fcb71cb62d0fd9b1c68883bb",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Amortized Inference in Probabilistic Reasoning Samuel J. Gershman 1 (sjgershm@mit.edu) and Noah D. Goodman 2 (ngoodman@stanford.edu) 1 Department of Brain and Cognitive Sciences, MIT of Psychology, Stanford University 2 Department Abstract similar or related queries. For example, as you view an im- age, your head and eyes are continuously moving, generating an infinitude of slightly different queries. For these queries, it may be inaccurate to reuse a stored inference without modifi- cation. This raises the problem of amortized inference: how to flexibly reuse inferences so as to answer a variety of re- lated queries. Recently, Stuhlm\u00a8uller et al. (2013) addressed this problem by using stored samples to estimate local condi- tional distributions, and then approximating answers to more complex queries by composing the local distributions. The work described in this paper seeks experimental evidence for a similar kind of flexible reuse in human reasoning. We presented subjects with a simple Bayesian network and asked them to answer a series of queries about it. One of these queries (the \u201ctarget\u201d) could be answered by reusing the answer to another query (the \u201csub-query\u201d). We hypothesized that the effects of reuse would be evident compared to an in- ference with the same structure but no re-usable sub-query. Further, we hypothesized that this effect would be present only if the target was presented after the sub-query. Accord- ingly, we manipulated (between subjects) whether the target came before or after the sub-query. This design allowed us to look for two key signatures of reuse: correlations between related inferences (Experiment 1) and faster responses for in- ferences that exploit reuse (Experiment 2). Recent studies of probabilistic reasoning have postulated general-purpose inference algorithms that can be used to an- swer arbitrary queries. These algorithms are memoryless, in the sense that each query is processed independently, without reuse of earlier computation. We argue that the brain oper- ates in the setting of amortized inference, where numerous related queries must be answered (e.g., recognizing a scene from multiple viewpoints); in this setting, memoryless algo- rithms can be computationally wasteful. We propose a simple form of flexible reuse, according to which shared inferences are cached and composed together to answer new queries. We present experimental evidence that humans exploit this form of reuse: the answer to a complex query can be systematically predicted from a person\u2019s response to a simpler query if the simpler query was presented first and entails a sub-inference (i.e., a sub-component of the more complex query). People are also faster at answering a complex query when it is preceded by a sub-inference. Our results suggest that the astonishing ef- ficiency of human probabilistic reasoning may be supported by interactions between inference and memory. Keywords: induction, Bayesian inference, memory \u201cCognition is recognition.\u201d \u2013 Hofstadter (1995) Introduction One view of probabilistic reasoning holds that our brains are equipped with general-purpose inference algorithms that can be used to answer arbitrary queries (Griffiths et al., 2012; Pouget et al., 2013). An under-appreciated property of such algorithms borrowed from computer science is that they are memoryless: each query is (at least in principle) processed independently of others. While this property guarantees that inferences will not interfere with one another, it can also lead to gross computational inefficiency, since inferences are never reused; memorylessness implies that answering the same query twice requires the same amount of computation as answer two unique queries. 1 Whatever inference algorithms the brain uses, they are un- likely to be memoryless. Consider, for example, the image in Figure 1 (Gregory, 1970). Upon viewing it for the first time, most observers find it extremely difficult to identify what the image depicts. 2 However, once the image has been deciphered, all subsequent views are instantly recognized. Clearly, the visual system is not running a computationally expensive inference algorithm upon each viewing; the infer- ence is simply reused. In reality, it is rare to be faced with the exact same query multiple times. Much more pervasive is the appearance of Figure 1: What does this image depict? Amortized inference in Bayesian networks 1 To be fair, inference algorithms for dynamical systems, like Kalman filtering, involve reuse in a certain sense. However, these algorithms are not designed to reuse inferences when applied to sev- eral independent time series (even if the time series are identical). 2 Answer: a dalmatian. In this paper, we will restrict our attention to amortized in- ference for Bayesian networks. Let p(x) denote a probability distribution on variables x = {x 1 , . . . , x M }. A Bayesian net- work G is a directed acyclic graph with nodes corresponding"
            },
            "slug": "Amortized-Inference-in-Probabilistic-Reasoning-Gershman-Goodman",
            "title": {
                "fragments": [],
                "text": "Amortized Inference in Probabilistic Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that the brain oper- ates in the setting of amortized inference, where numerous related queries must be answered (e.g., recognizing a scene from multiple viewpoints); in this setting, memoryless algo- rithms can be computationally wasteful."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803359"
                        ],
                        "name": "N. Chater",
                        "slug": "N.-Chater",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Chater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841005"
                        ],
                        "name": "Amy Perfors",
                        "slug": "Amy-Perfors",
                        "structuredName": {
                            "firstName": "Amy",
                            "lastName": "Perfors",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amy Perfors"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11588994,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "6e1a3ec1552109b66fa05b080c8c9e4120d38cd9",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-models-of-cognition:-exploring-and-Griffiths-Chater",
            "title": {
                "fragments": [],
                "text": "Probabilistic models of cognition: exploring representations and inductive biases"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4551554"
                        ],
                        "name": "Ern\u0151 T\u00e9gl\u00e1s",
                        "slug": "Ern\u0151-T\u00e9gl\u00e1s",
                        "structuredName": {
                            "firstName": "Ern\u0151",
                            "lastName": "T\u00e9gl\u00e1s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ern\u0151 T\u00e9gl\u00e1s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835039"
                        ],
                        "name": "E. Vul",
                        "slug": "E.-Vul",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Vul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2455093"
                        ],
                        "name": "V. Girotto",
                        "slug": "V.-Girotto",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Girotto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Girotto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48854442"
                        ],
                        "name": "Michel Gonzalez",
                        "slug": "Michel-Gonzalez",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Gonzalez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Gonzalez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748496"
                        ],
                        "name": "L. Bonatti",
                        "slug": "L.-Bonatti",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Bonatti",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bonatti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 91
                            }
                        ],
                        "text": ", 2013) as well as simpler kinds of physical predictions that have been studied in infants (T\u00e9gl\u00e1s et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 276
                            }
                        ],
                        "text": "\u2026of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al., 2013) as well as simpler kinds of physical predictions that have been studied in infants (Te\u0301gla\u0301s et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16369850,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4a2c790be620672031557625ceb0adc8e07a8cb5",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Twelve-month-old infants employ Bayesian statistics. Many organisms can predict future events from the statistics of past experience, but humans also excel at making predictions by pure reasoning: integrating multiple sources of information, guided by abstract knowledge, to form rational expectations about novel situations, never directly experienced. Here, we show that this reasoning is surprisingly rich, powerful, and coherent even in preverbal infants. When 12-month-old infants view complex displays of multiple moving objects, they form time-varying expectations about future events that are a systematic and rational function of several stimulus variables. Infants\u2019 looking times are consistent with a Bayesian ideal observer embodying abstract principles of object motion. The model explains infants\u2019 statistical expectations and classic qualitative findings about object cognition in younger babies, not originally viewed as probabilistic inferences."
            },
            "slug": "Pure-Reasoning-in-12-Month-Old-Infants-as-Inference-T\u00e9gl\u00e1s-Vul",
            "title": {
                "fragments": [],
                "text": "Pure Reasoning in 12-Month-Old Infants as Probabilistic Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The model explains infants\u2019 statistical expectations and classic qualitative findings about object cognition in younger babies, not originally viewed as probabilistic inferences."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38663378"
                        ],
                        "name": "J. Fodor",
                        "slug": "J.-Fodor",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fodor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fodor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194015"
                        ],
                        "name": "Z. Pylyshyn",
                        "slug": "Z.-Pylyshyn",
                        "structuredName": {
                            "firstName": "Zenon",
                            "lastName": "Pylyshyn",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Pylyshyn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 114
                            }
                        ],
                        "text": "For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick, 1989; Fodor & Pylyshyn, 1988; Marcus, 1998, 2001; Minsky & Papert, 1969; Pinker & Prince, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 276
                            }
                        ],
                        "text": "\u2026number of representations can be constructed from a finite set of primitives, just as the mind can think an\ninfinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor, 1975; Fodor & Pylyshyn, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29043627,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7",
            "isKey": false,
            "numCitedBy": 3540,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionism-and-cognitive-architecture:-A-Fodor-Pylyshyn",
            "title": {
                "fragments": [],
                "text": "Connectionism and cognitive architecture: A critical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117966548"
                        ],
                        "name": "Ziyun Wang",
                        "slug": "Ziyun-Wang",
                        "structuredName": {
                            "firstName": "Ziyun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziyun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725157"
                        ],
                        "name": "T. Schaul",
                        "slug": "T.-Schaul",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Schaul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schaul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39357484"
                        ],
                        "name": "Matteo Hessel",
                        "slug": "Matteo-Hessel",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Hessel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matteo Hessel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7634925"
                        ],
                        "name": "H. V. Hasselt",
                        "slug": "H.-V.-Hasselt",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Hasselt",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Hasselt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1975889"
                        ],
                        "name": "Marc Lanctot",
                        "slug": "Marc-Lanctot",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Lanctot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Lanctot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 251
                            }
                        ],
                        "text": "\u2026et al., 2016; van Hasselt, Guez, & Silver, 2016; Wang et al., 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al., 2015) and 96% by using smarter replay and more efficient parameter sharing (Wang et al., 2016) (see DQN+ and DQN++ in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 71
                            }
                        ],
                        "text": "More recent variants of the DQN have demonstrated superior performance (Hasselt, Guez, & Silver, 2016; Schaul et al., 2016; Stadie et al., 2016; Wang et al., 2016), reaching 83% of the professional gamer\u2019s score by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 26
                            }
                        ],
                        "text": ", 2016) and higher scores (Wang et al., 2016) have been reported using other metrics, but it is unclear how well the networks are generalizing with these alternative metrics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 93
                            }
                        ],
                        "text": "3If the less robust \u201c30 no-ops starts\u201d metric is used rather than \u201chuman starts,\u201d the network of Wang et al. (2016) performs even better at 172% of human performance\n4More precisely, the human expert in V. Mnih et al. (2015) scored an average of 4335 points across 30 game sessions of up to five\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 77
                            }
                        ],
                        "text": ", 2016) and 96% by using smarter replay and more efficient parameter sharing (Wang et al., 2016) (see DQN+ and DQN++ in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "More recent variants of the DQN have demonstrated superior performance (Schaul et al., 2015; Stadie et al., 2016; van Hasselt, Guez, & Silver, 2016; Wang et al., 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al., 2015) and 96% by using\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5389801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
            "isKey": true,
            "numCitedBy": 2032,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
            },
            "slug": "Dueling-Network-Architectures-for-Deep-Learning-Wang-Schaul",
            "title": {
                "fragments": [],
                "text": "Dueling Network Architectures for Deep Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a new neural network architecture for model-free reinforcement learning that leads to better policy evaluation in the presence of many similar-valued actions and enables the RL agent to outperform the state-of-the-art on the Atari 2600 domain."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054250597"
                        ],
                        "name": "Gregory R. Koch",
                        "slug": "Gregory-R.-Koch",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Koch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory R. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13874643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f216444d4f2959b4520c61d20003fa30a199670a",
            "isKey": false,
            "numCitedBy": 2448,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks."
            },
            "slug": "Siamese-Neural-Networks-for-One-Shot-Image-Koch",
            "title": {
                "fragments": [],
                "text": "Siamese Neural Networks for One-Shot Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs and is able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144366429"
                        ],
                        "name": "S. Carey",
                        "slug": "S.-Carey",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Carey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 187
                            }
                        ],
                        "text": "Children are far more practiced than adults at learning new concepts \u2013 learning roughly nine or ten new words each day after beginning to speak through the end of high school (Bloom, 2000; Carey, 1978) \u2013 yet the ability for rapid \u201cone-shot\u201d learning does not disappear in adulthood."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17710641,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d589c647abb29623d1d6f2ff344a1ed589b1ec62",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "concepts of reference point and polarity, which presuppose concepts of dimension of comparison and zero point. These concepts can be probed nonlinguistically. For example, animals can be taught to choose the smaller, or larger, of two stimuli. It is likely that the standard sizes of objects are represented conceptually for the purpose of object recognition. Presented with a box the size of my desk, I am not likely to entertain the hypothesis that it is a box of Kleenex. Underlying the feature system characterizing the dimensions are many aspects of man's representation of space; concepts like vertical and horizontal, cross-section, and spatial extent itself are reflected in many nonlinguistic sensorimotor routines (see Miller and JohnsonLaird 1976; H. H. Clark 1973). As the child learns a new spatial adjective, what aspects of its conceptual underpinnings are mapped onto it early and what aspects, if any, take years to work out? Two positions within the framework of the missing-feature theory have been held. The first position is that the child's initial mapping is between the word and the features specifying the relevant dimension of comparison. The missing feature is polarity, the direction from zero. On this view, both narrow and wide would have the incomplete lexical entry: [Adj] [comparative] [spatial extent] [-primary] [ vertical], making the two words synonyms (Donaldson and Wales 1970; H. H. Clark 1 970; E. V. Clark 1973; Klatzky et al. 1973). The two words need not have identical incomplete lexical entries simultaneously. For example, [+pole] might be added to the representation of wide before [ pole] is to narrow. In this case narrow means what wide did before"
            },
            "slug": "The-child-as-word-learner-Carey",
            "title": {
                "fragments": [],
                "text": "The child as word learner"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572818"
                        ],
                        "name": "Rachel Magid",
                        "slug": "Rachel-Magid",
                        "structuredName": {
                            "firstName": "Rachel",
                            "lastName": "Magid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachel Magid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4969813"
                        ],
                        "name": "M. Sheskin",
                        "slug": "M.-Sheskin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sheskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sheskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7774026,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "a7016c93f984468aa9edfbb36ff31b5283bf1faf",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Imagination-and-the-generation-of-new-ideas-Magid-Sheskin",
            "title": {
                "fragments": [],
                "text": "Imagination and the generation of new ideas"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 201
                            }
                        ],
                        "text": "Somewhat surprisingly, the incorporation of attention has led to substantial performance gains in a variety of domains, including in machine translation (Bahdanau et al., 2015), object recognition (V. Mnih et al., 2014), and image caption generation (K. Xu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Mnih et al. (2015). The DQN was a significant advance in reinforcement learning, showing that a single algorithm can learn to play a wide variety of complex tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17195923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a756d4d25511d92a45d0f4545fa819de993851d",
            "isKey": false,
            "numCitedBy": 2410,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so."
            },
            "slug": "Recurrent-Models-of-Visual-Attention-Mnih-Heess",
            "title": {
                "fragments": [],
                "text": "Recurrent Models of Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099659431"
                        ],
                        "name": "M. Rutherford",
                        "slug": "M.-Rutherford",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Rutherford",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rutherford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5361244"
                        ],
                        "name": "V. Kuhlmeier",
                        "slug": "V.-Kuhlmeier",
                        "structuredName": {
                            "firstName": "Valerie",
                            "lastName": "Kuhlmeier",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kuhlmeier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "One possibility is that intuitive psychology is simply cues \u201call the way down\u201d (Schlottmann, Cole, Watts, & White, 2013; Scholl & Gao, 2013), though this would require more and more cues as the scenarios become more complex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 142121725,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "489e10081e812b3b5ecfb422b249f5071e238a89",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "As we enter a room full of people, we instantly have a number of social perceptions. We have an automatic perception of others as subjective agents with their own points of view, thoughts, and goals, and we can quickly interpret minimal visual information to infer that something is animate. This book explores the perceptual and cognitive processes that allow humans to perceive and understand this social information quickly and apparently effortlessly. Top researchers in fields ranging from developmental psychology to vision science consider the perception of biological and animate motion, inferences based on this motion, and the early development of these abilities. These innovative contributions reflect a recent renewal of interest in the attribution of agency and the understanding of goal-directed behavior, which has been accompanied by a rapid increase in empirical discoveries enabled by such new experimental techniques as brain imaging. The research presented in Social Perception suggests that an intuitive understanding of others is an integral part of human psychology, develops early, relies on a network of brain regions, and may be compromised in autism. ContributorsDare Baldwin, Lara Bardi, H. Clark Barrett, Erin Cannon, You-jung Choi, Willem E. Frankenhuis, Tao Gao, Emily D. Grossman, Antonia Hamilton, Petra Hauf, Valerie A. Kuhlmeier, Jeff Loucks, Scott A. Love, Yuyan Luo, Elena Mascalzoni, Phil McAleer, Richard Ramsey, Lucia Regolin, M.D. Rutherford, Kara Sage, Brian J. Scholl, Maggie Shiffrar, Francesca Simion, Jessica Sommerville, James P. Thomas, Nikolaus Troje, Amanda Woodward."
            },
            "slug": "Social-perception-:-detection-and-interpretation-of-Rutherford-Kuhlmeier",
            "title": {
                "fragments": [],
                "text": "Social perception : detection and interpretation of animacy, agency, and intention"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11030219"
                        ],
                        "name": "Timothy J. O'Donnell",
                        "slug": "Timothy-J.-O'Donnell",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "O'Donnell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy J. O'Donnell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60737209,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f4050f4b9196602ac0696b7d13b07157472cf1c9",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Language allows us to express and comprehend an unbounded number of thoughts. This fundamental and much-celebrated property is made possible by a division of labor between a large inventory of stored items (e.g., affixes, words, idioms) and a computational system that productively combines these stored units on the fly to create a potentially unlimited array of new expressions. A language learner must discover a language's productive, reusable units and determine which computational processes can give rise to new expressions. But how does the learner differentiate between the reusable, generalizable units (for example, the affix -ness, as in coolness, orderliness, cheapness) and apparent units that do not actually generalize in practice (for example, -th, as in warmth but not coolth)? In this book, Timothy O'Donnell proposes a formal computational model, Fragment Grammars, to answer these questions. This model treats productivity and reuse as the target of inference in a probabilistic framework, asking how an optimal agent can make use of the distribution of forms in the linguistic input to learn the distribution of productive word-formation processes and reusable units in a given language. O'Donnell compares this model to a number of other theoretical and mathematical models, applying them to the English past tense and English derivational morphology, and showing that Fragment Grammars unifies a number of superficially distinct empirical phenomena in these domains and justifies certain seemingly ad hoc assumptions in earlier theories."
            },
            "slug": "Productivity-and-Reuse-in-Language:-A-Theory-of-and-O'Donnell",
            "title": {
                "fragments": [],
                "text": "Productivity and Reuse in Language: A Theory of Linguistic Computation and Storage"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This model treats productivity and reuse as the target of inference in a probabilistic framework, asking how an optimal agent can make use of the distribution of forms in the linguistic input to learn the distributionof productive word-formation processes and reusable units in a given language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49169915"
                        ],
                        "name": "Christopher Bates",
                        "slug": "Christopher-Bates",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Bates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019153"
                        ],
                        "name": "P. Battaglia",
                        "slug": "P.-Battaglia",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Battaglia",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Battaglia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11431595"
                        ],
                        "name": "Ilker Yildirim",
                        "slug": "Ilker-Yildirim",
                        "structuredName": {
                            "firstName": "Ilker",
                            "lastName": "Yildirim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilker Yildirim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4686533,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "1867112a8deb73d72d5e08553e7d065346b7308f",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Liquids can splash, squirt, gush, slosh, soak, drip, drain, trickle, pool, and be poured\u2013complex behaviors that we can easily distinguish, imagine, describe, and, crucially, predict, despite tremendous diversity among different liquids\u2019 material and dynamical characteristics. This proficiency suggests the brain has a sophisticated cognitive mechanism for reasoning about liquids, yet to date there has been little effort to study this mechanism quantitatively or describe it computationally. Here we find evidence that people\u2019s reasoning about how liquids move is consistent with a computational cognitive model based on approximate probabilistic simulation. In a psychophysical experiment, participants predicted how different liquids would flow around solid obstacles, and their judgments agreed with those of a family of models in which volumes of liquid are represented as collections of interacting particles, within a dynamical fluid simulation. Our model explains people\u2019s accuracy, and their predictions\u2019 sensitivity to liquids of different viscosity. We also explored several models that did not involve simulation, and found they could not account for the experimental data as well. Our results are consistent with previous reports that people\u2019s physical understanding of solid objects is based on simulation, but extends this thesis to the more complex and unexplored domain of reasoning about liquids."
            },
            "slug": "Humans-predict-liquid-dynamics-using-probabilistic-Bates-Battaglia",
            "title": {
                "fragments": [],
                "text": "Humans predict liquid dynamics using probabilistic simulation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This thesis finds evidence that people\u2019s reasoning about how liquids move is consistent with a computational cognitive model based on approximate probabilistic simulation and extends this thesis to the more complex and unexplored domain of reasoning about liquids."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50007746"
                        ],
                        "name": "R. Levy",
                        "slug": "R.-Levy",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Levy",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099561"
                        ],
                        "name": "Florencia Reali",
                        "slug": "Florencia-Reali",
                        "structuredName": {
                            "firstName": "Florencia",
                            "lastName": "Reali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florencia Reali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6065813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0786cacf4cc31e09c59119345d3dfd639a814065",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Language comprehension in humans is significantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and fielded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle filter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the first rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information."
            },
            "slug": "Modeling-the-effects-of-memory-on-human-online-with-Levy-Reali",
            "title": {
                "fragments": [],
                "text": "Modeling the effects of memory on human online sentence processing with particle filters"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a new limited-memory model of sentence comprehension which involves an adaptation of the particle filter, a sequential Monte Carlo method, to the problem of incremental parsing and naturally provides the first rational account of an outstanding problem in psycholinguistics."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955964"
                        ],
                        "name": "Xiaoxiao Guo",
                        "slug": "Xiaoxiao-Guo",
                        "structuredName": {
                            "firstName": "Xiaoxiao",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoxiao Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46328485"
                        ],
                        "name": "Richard L. Lewis",
                        "slug": "Richard-L.-Lewis",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lewis",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard L. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107975180"
                        ],
                        "name": "Xiaoshi Wang",
                        "slug": "Xiaoshi-Wang",
                        "structuredName": {
                            "firstName": "Xiaoshi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoshi Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2187487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6cc21b30912bdaecd9f178d700a4c545b1d0838",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN."
            },
            "slug": "Deep-Learning-for-Real-Time-Atari-Game-Play-Using-Guo-Singh",
            "title": {
                "fragments": [],
                "text": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play, and proposed new agents based on this idea are proposed and shown to outperform DQN."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258504"
                        ],
                        "name": "Sergey Bartunov",
                        "slug": "Sergey-Bartunov",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Bartunov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Bartunov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492721"
                        ],
                        "name": "D. Vetrov",
                        "slug": "D.-Vetrov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Vetrov",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Vetrov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10082291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1e1e33947323e6fbcfcf65c318baa6c35b2fa75",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process. The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation. Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models."
            },
            "slug": "Fast-Adaptation-in-Generative-Models-with-Matching-Bartunov-Vetrov",
            "title": {
                "fragments": [],
                "text": "Fast Adaptation in Generative Models with Generative Matching Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work develops a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and can instantly learn new concepts that were not available in the training data but conform to a similar generative process."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11035f85ead2b4b386956f4480b3d38d2e9a5ff5",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset. We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning. To enable model selection, we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules. We use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms. Using a greedy search over our grammar, we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models. The proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise. It learns sensible structures for datasets as diverse as image patches, motion capture, 20 Questions, and U.S. Senate votes, all using exactly the same code."
            },
            "slug": "Exploiting-compositionality-to-explore-a-large-of-Grosse-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Exploiting compositionality to explore a large space of model structures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work organizes a space of matrix decomposition models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules and automatically chooses the decomposition structure from raw data by evaluating only a small fraction of all models."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2206490"
                        ],
                        "name": "Marcin Andrychowicz",
                        "slug": "Marcin-Andrychowicz",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Andrychowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Andrychowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016840"
                        ],
                        "name": "Sergio Gomez Colmenarejo",
                        "slug": "Sergio-Gomez-Colmenarejo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Colmenarejo",
                            "middleNames": [
                                "Gomez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergio Gomez Colmenarejo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3243579"
                        ],
                        "name": "Matthew W. Hoffman",
                        "slug": "Matthew-W.-Hoffman",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hoffman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew W. Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144846367"
                        ],
                        "name": "D. Pfau",
                        "slug": "D.-Pfau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pfau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pfau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725157"
                        ],
                        "name": "T. Schaul",
                        "slug": "T.-Schaul",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Schaul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schaul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 935,
                                "start": 120
                            }
                        ],
                        "text": "Neural networks can also learn-to-learn by optimizing hyper-parameters, including the form of their weight update rule (Andrychowicz et al. 2016), over a set of related tasks. Although transfer learning and multitask learning are already important themes across AI, and in deep learning in particular, they have not yet led to systems that learn new tasks as rapidly and flexibly as humans do. Capturing more human-like learning-to-learn dynamics in deep networks and other machine learning approaches could facilitate much stronger transfer to new tasks and new problems. To gain the full benefit that humans get from learning-tolearn, however, AI systems might first need to adopt the more compositional (or more language-like, see sect. 5) and causal forms of representations that we have argued for above. We can see this potential in both of our challenge problems. In the Characters Challenge as presented in Lake et al. (2015a), all viable models use \u201cpre-training\u201d on many character concepts in a background set of alphabets to tune the representations they use to learn new character concepts in a test set of alphabets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5385,
                                "start": 120
                            }
                        ],
                        "text": "Neural networks can also learn-to-learn by optimizing hyper-parameters, including the form of their weight update rule (Andrychowicz et al. 2016), over a set of related tasks. Although transfer learning and multitask learning are already important themes across AI, and in deep learning in particular, they have not yet led to systems that learn new tasks as rapidly and flexibly as humans do. Capturing more human-like learning-to-learn dynamics in deep networks and other machine learning approaches could facilitate much stronger transfer to new tasks and new problems. To gain the full benefit that humans get from learning-tolearn, however, AI systems might first need to adopt the more compositional (or more language-like, see sect. 5) and causal forms of representations that we have argued for above. We can see this potential in both of our challenge problems. In the Characters Challenge as presented in Lake et al. (2015a), all viable models use \u201cpre-training\u201d on many character concepts in a background set of alphabets to tune the representations they use to learn new character concepts in a test set of alphabets. But to perform well, current neural network approaches require much more pre-training than do people or our Bayesian program learning approach. Humans typically learn only one or a few alphabets, and even with related drawing experience, this likely amounts to the equivalent of a few hundred character-like visual concepts at most. For BPL, pre-training with characters in only five alphabets (for around 150 character types in total) is sufficient to perform human-level one-shot classification and generation of new examples. With this level of pre-training, current neural networks perform much worse on classification and have not even attempted generation; they are still far from solving the Characters Challenge.(8) We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar. BPL transfers readily to new concepts because it learns about object parts, sub-parts, and relations, capturing learning about what each concept is like and what concepts are like in general. It is crucial that learning-to-learn occurs at multiple levels of the hierarchical generative process. Previously learned primitive actions and larger generative pieces can be re-used and re-combined to define new generative models for new characters (Fig. 5A). Further transfer occurs by learning about the typical levels of variability within a typical generative model. This provides knowledge about how far and in what ways to generalize when we have seen only one example of a new character, which on its own could not possibly carry any information about variance. BPL could also benefit from deeper forms of learning-tolearn than it currently does. Some of the important structure it exploits to generalize well is built in to the prior and not learned from the background pre-training, whereas people might learn this knowledge, and ultimately, a human-like machine learning system should as well. Analogous learning-to-learn occurs for humans in learning many new object models, in vision and cognition: Consider the novel two-wheeled vehicle in Figure 1B, where learning-to-learn can operate through the transfer of previously learned parts and relations (sub-concepts such as wheels, motors, handle bars, attached, powered by) that reconfigure compositionally to create a model of the new concept. If deep neural networks could adopt similarly compositional, hierarchical, and causal representations, we expect they could benefit more from learningto-learn. In the Frostbite Challenge, and in video games more generally, there is a similar interdependence between the form of the representation and the effectiveness of learning-to-learn. People seem to transfer knowledge at multiple levels, from low-level perception to high-level strategy, exploiting compositionality at all levels. Most basically, they immediately parse the game environment into objects, types of objects, and causal relations between them. People also understand that video games like these have goals, which often involve approaching or avoiding objects based on their type. Whether the person is a child or a seasoned gamer, it seems obvious that interacting with the birds and fish will change the game state in some way, either good or bad, because video games typically yield costs or rewards for these types of interactions (e.g., dying or points). These types of hypotheses can be quite specific and rely on prior knowledge: When the polar bear first appears and tracks the agent\u2019s location during advanced levels (Fig. 2D), an attentive learner is sure to avoid it. Depending on the level, ice floes can be spaced far apart (Fig. 2A\u2013C) or close together (Fig. 2D), suggesting the agent may be able to cross some gaps, but not others. In this way, general world knowledge and previous video games may help inform exploration and generalization in new scenarios, helping people learn maximally from a single mistake or avoid mistakes altogether. Deep reinforcement learning systems for playing Atari games have had some impressive successes in transfer learning, but they still have not come close to learning to play new games as quickly as humans can. For example, Parisotto et al. (2016) present the \u201cactor-mimic\u201d algorithm that first learns 13 Atari games by watching an expert network play and trying to mimic the expert network action selection and/or internal states (for about 4 million frames of experience each, or 18."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 25
                            }
                        ],
                        "text": "As recent work has shown (Andrychowicz et al. 2016; Denil et al. 2016; Duan et al. 2016; Hochreiter et al. 2001; Santoro et al. 2016; Wang et al. 2017), this learning-to-learn mechanism can allow agents to adapt rapidly to new problems, providing a novel route to install prior knowledge through learning, rather than by hand."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 119
                            }
                        ],
                        "text": "Neural networks can also learn-to-learn by optimizing hyper-parameters, including the form of their weight update rule (Andrychowicz et al. 2016), over a set of related tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2928017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395dd01c0d24777c660cf195c4cfadcdf51fb7e8",
            "isKey": true,
            "numCitedBy": 1329,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art."
            },
            "slug": "Learning-to-learn-by-gradient-descent-by-gradient-Andrychowicz-Denil",
            "title": {
                "fragments": [],
                "text": "Learning to learn by gradient descent by gradient descent"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 120
                            }
                        ],
                        "text": "It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion (Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 234
                            }
                        ],
                        "text": "Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions towards those goals subject to constraints (Csibra, 2008; Csibra, Biro, Koos, & Gergely, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 102
                            }
                        ],
                        "text": "Early in development, humans have a foundational understanding of several core domains (Spelke, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7658142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "High capacity classifiers, such as deep neural networks, often struggle on classes that have very few training examples. We propose a method for improving classification performance for such classes by discovering similar classes and transferring knowledge among them. Our method learns to organize the classes into a tree hierarchy. This tree structure imposes a prior over the classifier's parameters. We show that the performance of deep neural networks can be improved by applying these priors to the weights in the last layer. Our method combines the strength of discriminatively trained deep neural networks, which typically require large amounts of training data, with tree-based priors, making deep neural networks work well on infrequent classes as well. We also propose an algorithm for learning the underlying tree structure. Starting from an initial pre-specified tree, this algorithm modifies the tree to make it more pertinent to the task being solved, for example, removing semantic relationships in favour of visual ones for an image classification task. Our method achieves state-of-the-art classification results on the CIFAR-100 image data set and the MIR Flickr image-text data set."
            },
            "slug": "Discriminative-Transfer-Learning-with-Tree-based-Srivastava-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Discriminative Transfer Learning with Tree-based Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work proposes a method for improving classification performance for high capacity classifiers by discovering similar classes and transferring knowledge among them, which learns to organize the classes into a tree hierarchy, and proposes an algorithm for learning the underlying tree structure."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21161348"
                        ],
                        "name": "Chris L. Baker",
                        "slug": "Chris-L.-Baker",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Baker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276622"
                        ],
                        "name": "R. Saxe",
                        "slug": "R.-Saxe",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Saxe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 165
                            }
                        ],
                        "text": "\u2026or...\u2019\nAn alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning (or \u201cBayesian theory-of-mind\u201d) models of Baker, Saxe, and Tenenbaum (2009) or the \u201cnaive utility calculus\u201d models of Jara-Ettinger, Gweon, Tenenbaum, and Schulz (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1560164,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7dd51cef9bd43d495a12d10b7d0846f9bd60d9fa",
            "isKey": false,
            "numCitedBy": 680,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Action-understanding-as-inverse-planning-Baker-Saxe",
            "title": {
                "fragments": [],
                "text": "Action understanding as inverse planning"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1580222881"
                        ],
                        "name": "J. J. Williams",
                        "slug": "J.-J.-Williams",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Williams",
                            "middleNames": [
                                "Jay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2464187"
                        ],
                        "name": "T. Lombrozo",
                        "slug": "T.-Lombrozo",
                        "structuredName": {
                            "firstName": "Tania",
                            "lastName": "Lombrozo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lombrozo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 34
                            }
                        ],
                        "text": ", 2015), augmented working memory (Graves et al., 2014, 2016; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 270
                            }
                        ],
                        "text": "Much larger random-access memories can be implemented using Memory Networks which automatically embed and store each incoming piece of information in memory, especially useful for question answering tasks and other aspects of language modeling (Sukhbaatar et al., 2015; Weston et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 47
                            }
                        ],
                        "text": "term memory provided by the connection weights (Graves et al., 2014, 2016; Grefenstette et al., 2015; Reed & Freitas, 2016; Sukhbaatar et al., 2015; Weston et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 187
                            }
                        ],
                        "text": "\u2026Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland, McNaughton, & O\u2019Reilly, 1995; V. Mnih et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10018799,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6600f7a27985e9bcaf4565755c92e97f98ee72ae",
            "isKey": true,
            "numCitedBy": 180,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in education and cognitive development suggests that explaining plays a key role in learning and generalization: When learners provide explanations-even to themselves-they learn more effectively and generalize more readily to novel situations. This paper proposes and tests a subsumptive constraints account of this effect. Motivated by philosophical theories of explanation, this account predicts that explaining guides learners to interpret what they are learning in terms of unifying patterns or regularities, which promotes the discovery of broad generalizations. Three experiments provide evidence for the subsumptive constraints account: prompting participants to explain while learning artificial categories promotes the induction of a broad generalization underlying category membership, relative to describing items (Exp. 1), thinking aloud (Exp. 2), or free study (Exp. 3). Although explaining facilitates discovery, Experiment 1 finds that description is more beneficial for learning item details. Experiment 2 additionally suggests that explaining anomalous observations may play a special role in belief revision. The findings provide insight into explanation's role in discovery and generalization."
            },
            "slug": "The-role-of-explanation-in-discovery-and-evidence-Williams-Lombrozo",
            "title": {
                "fragments": [],
                "text": "The role of explanation in discovery and generalization: evidence from category learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A subsumptive constraints account predicts that explaining guides learners to interpret what they are learning in terms of unifying patterns or regularities, which promotes the discovery of broad generalizations."
            },
            "venue": {
                "fragments": [],
                "text": "ICLS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40641364"
                        ],
                        "name": "S. Casper",
                        "slug": "S.-Casper",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Casper",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Casper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "General world knowledge and learning from previous video games helps to inform this type of exploration and generalization, allowing a single success or mistake to immediately produce broad generalizations that help the learner understand the game and how to maximize performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 120
                            }
                        ],
                        "text": "Cognitive science repudiated the over-simplified behaviorist view and came to play a central role in early AI research (Boden, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 48
                            }
                        ],
                        "text": "Newell and Simon (1961) developed their \u201cGeneral Problem Solver\u201d as both an AI algorithm and a model of human problem solving, which they subsequently tested experimentally (Newell & Simon, 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 141250619,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "4325187e54e8807fdaa23001d835901e983f4398",
            "isKey": true,
            "numCitedBy": 180,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Almost all the reviewers of Margaret Boden\u2019s Mind as machine have noted the obvious: at 2 volumes, 1452 pages, 134 pages of references, and seemingly infinite parenthetically cross-references, this book, longer than most editions of War and peace, is impractical, unwieldy, and inaccessible to readers. To be blunt, that seems to be the point. Boden did not intend Mind as machine to be a pleasant read for a weekend\u2019s leisure. She intended it for people whose work includes being active readers, and for them it does represent a useful work of synthesis. \n \nBoden begins by noting that some might mistake \u201cman as machine\u201d for an ancient idea. Yet, according to her, this analogy, as well as its parallel \u201cmind as machine\u201d, is of recent origin. It was only by the close of the nineteenth century that mechanistic theories of mind acquired respectability. These theories, however, were mere analogies; no one seriously contemplated consilience between the behaviours of machines and men. Still less did anyone outside science fiction circles propose that machines could be intelligent in the same way as humans. By the mid-1800s, Charles Babbage had invented an analytical engine, somewhat akin to a programme-controlled digital computer, but he never claimed it to have implications for psychology or biology, though perhaps his student Ada Lovelace hinted at the possibility. Thus, it was during the war years of the 1940s, at the height of collaborations between Anglo-American scientists, that computers began being developed, and with them, some investigators, such as Alan Turing, began to study questions about machine intelligence. These questions would have ramifications for the cognitive sciences, including the hypothesis that a scientific theory might explain, \u201cprocesses in both minds and mindlike artefacts\u201d (p. 168). \n \nIn the 1950s, these claims led to the emergence of the multi-disciplinary field of the cognitive sciences, a discipline well provided for by philanthropic and institutional sources of support, stocked with new venues for publication, and bolstered by artificial intelligence research paradigms. It was, none the less, a field riddled with intellectual divides, which developed over the next half century. Behaviourism, then predominant, was on the wane. Seen as too universalist, it was criticized by Gestaltists, linguists, ethologists, proto-connectionists, anthropologists, and Noam Chomsky alike (the last comes bizarrely in Boden\u2019s narrative with a \u201chealth warning\u201d, p. 591). In this ferment, the \u201cmind as machine\u201d debate took different paths: cyberneticists, for example, assumed that the mind as a machine was identical with the body. Computational psychologists\u2014little more than a smattering of research endeavours\u2014treated the human mind as different from its body, and concerned themselves with questions about how the mind was different. The majority of psychologists, however, focused on what made the mind different. Always lurking in the background was the question of whether human thought was \u201cconstituted by, or identical with\u201d symbolic processes (p. 702). Those questions especially plagued papers and programmes on artificial intelligence\u2014even when their authors were uninterested in the answers. \n \nArtificial intelligence research bolstered this nascent field enormously during the last half of the century. AI research, however, was perhaps more tied to the geopolitical context of the Cold War period and the neo-Liberal period of the 1980s and 1990s than the cognitive sciences. While much AI work focused on developing programming languages and had modest goals (seek general intelligence but not human-like intelligence, appeared almost as an injunction), critics levelled numerous charges at AI-workers, despite the fact that few were seeking to understand the human mind as a machine. Seymour Papert, an early pioneer, for instance, used only simple programmes to understand thinking processes. Yet, as defence spending increased, AI\u2019s proponents and detractors became uncomfortable with the glib assertions being promulgated within policy and media exaggerations, especially the belief that enormous computer systems controlling weapon systems could be \u201cbug\u201d free in their script, and commonsensical in their behaviour. This political and social context was only a part of the story. Connectionists, a new but inchoate group of psychologists, neuroscientists, and philosophers of the mind, also tore into the AI project. They argued that phenomena were represented within emerging networks (usually neurological) and not symbolic systems, which many within old-fashioned AI paradigms had claimed. In hindsight, all of AI\u2019s failed promises and faulty philosophical assumptions have led some to pronounce it a failed research programme. On this point, Boden demurs. She observes that AI enormously advanced both itself and the cognitive sciences. In that sense, and contrary to its critics, AI continued as a fruitful area of research, but like its latest corollaries, computational neuroscience and artificial life, the field remains embryonic even today. \n \nWhether Boden\u2019s volumes really ought to culminate in a penultimate discussion of the philosophies of mind as machine or in a final summary in the last chapter of triumphal sounding claims for the cognitive sciences, I shall leave to others to decide. Having read those chapters alongside M R Bennett and P M S Hacker\u2019s excellent Philosophical foundations of the neurosciences (2003), I find myself having misgivings about the conceptual foundations of much of the cognitive sciences project as outlined by Boden. \n \nIn any case, Boden\u2019s volumes, despite their evident value, will aggravate many. Those least charitable will see them as a rather devoted effort to restore attention to Warren S McCulloch\u2019s contributions to the cognitive sciences. Historians studying periods before 1945 will find fault both with her facts and pithy generalizations. Similarly, those still living cognitive scientists whose careers spanned 1945 and 2000 are bound not to recognize the caricatures of themselves, or people they knew, in her story. Instead they will likely encounter a narrative that for them fails to capture things \u201cas they were\u201d and summarizes scientific arguments without paying them full justice. Such criticisms, which have already begun circulating about this work, strike me as unwarranted, especially because Boden\u2019s practitioner viewpoint brings with it the hindrances such life experience implies. Anyone failing to note Boden\u2019s polemical tone is just not awake. Putting it simply, the work is too large to be free of an agenda. However, for that same reason, criticisms of this work from other practitioners appear no less problematic. In my view, these volumes and the responses of critics to them will be of greater significance as primary source material than they will be in defining the historiography of the cognitive sciences. On balance, these volumes are thought provoking and open a doorway towards improved understanding of the patterns of science in the second half of the twentieth century."
            },
            "slug": "Book-Review:-Mind-as-machine:-a-history-of-science-Casper",
            "title": {
                "fragments": [],
                "text": "Book Review: Mind as machine: a history of cognitive science"
            },
            "venue": {
                "fragments": [],
                "text": "Medical History"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836466"
                        ],
                        "name": "Linda B. Smith",
                        "slug": "Linda-B.-Smith",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda B. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48795139"
                        ],
                        "name": "Susan S. Jones",
                        "slug": "Susan-S.-Jones",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Jones",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan S. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559335"
                        ],
                        "name": "B. Landau",
                        "slug": "B.-Landau",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Landau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Landau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401015125"
                        ],
                        "name": "L. Gershkoff-Stowe",
                        "slug": "L.-Gershkoff-Stowe",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Gershkoff-Stowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gershkoff-Stowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40498952"
                        ],
                        "name": "L. Samuelson",
                        "slug": "L.-Samuelson",
                        "structuredName": {
                            "firstName": "Larissa",
                            "lastName": "Samuelson",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Samuelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2869588,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "39dd0024bddbef0f7e29d25169867c37574a4231",
            "isKey": false,
            "numCitedBy": 526,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "By the age of 3, children easily learn to name new objects, extending new names for unfamiliar objects by similarity in shape. Two experiments tested the proposal that experience in learning object names tunes children's attention to the properties relevant for naming\u2014in the present case, to the property of shape\u2014and thus facilitates the learning of more object names. In Experiment 1, a 9-week longitudinal study, 17-month-old children who repeatedly played with and heard names for members of unfamiliar object categories well organized by shape formed the generalization that only objects with similar shapes have the same name. Trained children also showed a dramatic increase in acquisition of new object names outside of the laboratory during the course of the study. Experiment 2 replicated these findings and showed that they depended on children's learning both a coherent category structure and object names. Thus, children who learn specific names for specific things in categories with a common organizing property\u2014in this case, shape\u2014also learn to attend to just the right property\u2014in this case, shape\u2014for learning more object names."
            },
            "slug": "Object-name-Learning-Provides-On-the-Job-Training-Smith-Jones",
            "title": {
                "fragments": [],
                "text": "Object name Learning Provides On-the-Job Training for Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Two experiments tested the proposal that experience in learning object names tunes children's attention to the properties relevant for naming\u2014in the present case, to the property of shape\u2014and thus facilitates the learning of more object names."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3275284"
                        ],
                        "name": "Bradly C. Stadie",
                        "slug": "Bradly-C.-Stadie",
                        "structuredName": {
                            "firstName": "Bradly",
                            "lastName": "Stadie",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bradly C. Stadie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736651"
                        ],
                        "name": "S. Levine",
                        "slug": "S.-Levine",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More recent variants of the DQN have demonstrated superior performance (Schaul et al., 2015; Stadie et al., 2016; van Hasselt, Guez, & Silver, 2016), and the current best network with smarter replay and other improvements now achieves about 83% of the professional gamer\u2019s score (Schaul et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10296902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2470fcf0f89082de874ac9133ccb3a8667dd89a8",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark."
            },
            "slug": "Incentivizing-Exploration-In-Reinforcement-Learning-Stadie-Levine",
            "title": {
                "fragments": [],
                "text": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper considers the challenging Atari games domain, and proposes a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics that provides the most consistent improvement across a range of games that pose a major challenge for prior methods."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39401021"
                        ],
                        "name": "Sean Tauber",
                        "slug": "Sean-Tauber",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Tauber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sean Tauber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8114729,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2a00bb9ed22da0104a51e3dd3a1d485f5ed6d741",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous research shows that people assign latent goals or intentions to simple animated agents based on the motion behavior of these agents. We propose that human observers can infer that an animated agent has a partial state of belief about its environment and that observers use this information \u2013 in combination with the agent's observable behavior \u2013 to infer its goals. We conducted an experiment that showed that observers used line-of-sight cues \u2013 an agent's orientation relative to various objects in the environment, and the presence or absence of visual obstructions \u2013 to determine the content of an agent's state of belief about the location of objects. Our results are consistent with the hypothesis that human observers use line-of-sight cues to assign belief states to agents and that these belief states can be used to interpret agent behavior. We found that observer models that incorporated inferences about agents\u2019 beliefs outperformed an all-knowing observer model in describing human responses. Additionally, we found that human responses were most consistent with the behavior of a model that incorporates information about both orientation and line-of-sight obstructions."
            },
            "slug": "Using-Inverse-Planning-and-Theory-of-Mind-for-Goal-Tauber-Steyvers",
            "title": {
                "fragments": [],
                "text": "Using Inverse Planning and Theory of Mind for Social Goal Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that observer models that incorporated inferences about agents\u2019 beliefs outperformed an all-knowing observer model in describing human responses and that human responses were most consistent with the behavior of a model that incorporates information about both orientation and line-of-sight obstructions."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21161348"
                        ],
                        "name": "Chris L. Baker",
                        "slug": "Chris-L.-Baker",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Baker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402022761"
                        ],
                        "name": "J. Jara-Ettinger",
                        "slug": "J.-Jara-Ettinger",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Jara-Ettinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jara-Ettinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276622"
                        ],
                        "name": "R. Saxe",
                        "slug": "R.-Saxe",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Saxe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 168
                            }
                        ],
                        "text": "\u201d One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al. (2015) (see also Jern and Kemp [2015] and Tauber and Steyvers [2011] and a related alternative based on predictive coding from Kilner et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 168
                            }
                        ],
                        "text": "\u201d One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 397,
                                "start": 168
                            }
                        ],
                        "text": "\u201d One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al. (2015) (see also Jern and Kemp [2015] and Tauber and Steyvers [2011] and a related alternative based on predictive coding from Kilner et al. [2007])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3338320,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b61cd0fe9d33647cc25f8421f6c8e556b610ed56",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Social cognition depends on our capacity for \u2018mentalizing\u2019, or explaining an agent\u2019s behaviour in terms of their mental states. The development and neural substrates of mentalizing are well-studied, but its computational basis is only beginning to be probed. Here we present a model of core mentalizing computations: inferring jointly an actor\u2019s beliefs, desires and percepts from how they move in the local spatial environment. Our Bayesian theory of mind (BToM) model is based on probabilistically inverting artificial-intelligence approaches to rational planning and state estimation, which extend classical expected-utility agent models to sequential actions in complex, partially observable domains. The model accurately captures the quantitative mental-state judgements of human participants in two experiments, each varying multiple stimulus dimensions across a large number of stimuli. Comparative model fits with both simpler \u2018lesioned\u2019 BToM models and a family of simpler non-mentalistic motion features reveal the value contributed by each component of our model."
            },
            "slug": "Rational-quantitative-attribution-of-beliefs,-and-Baker-Jara-Ettinger",
            "title": {
                "fragments": [],
                "text": "Rational quantitative attribution of beliefs, desires and percepts in human mentalizing"
            },
            "venue": {
                "fragments": [],
                "text": "Nature Human Behaviour"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055325747"
                        ],
                        "name": "Scott E Friedman",
                        "slug": "Scott-E-Friedman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Friedman",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 35
                            }
                        ],
                        "text": "Trends in Cognitive Sciences 16(7):382\u201389. [arBML] Schulz, L. E., Gopnik, A. & Glymour, C. (2007) Preschool children learn about"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 26
                            }
                        ],
                        "text": "Communications of the ACM 38(11):45\u201348. [LRC] Lerer, A., Gross, S. & Fergus, R. (2016) Learning physical intuition of block towers"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1020,
                                "start": 2
                            }
                        ],
                        "text": ", Fukushima 1980; Grossberg 1976; Rosenblatt 1958). The representations and algorithms used by this approach were more directly inspired by neuroscience than by cognitive psychology, although ultimately it would flower into an influential school of thought about the nature of cognition: parallel distributed processing (PDP) (McClelland et al. 1986; Rumelhart et al. 1986b). As its name suggests, PDP emphasizes parallel computation by combining simple units to collectively implement sophisticated computations. The knowledge learned by these neural networks is thus distributed across the collection of units rather than localized as in most symbolic data structures. The resurgence of recent interest in neural networks, more commonly referred to as \u201cdeep learning,\u201d shares the same representational commitments and often even the same learning algorithms as the earlier PDP models. \u201cDeep\u201d refers to the fact that more powerful models can be built by composing many layers of representation (see LeCun et al. [2015] and Schmidhuber [2015] for recent reviews), still very much in the PDP style while utilizing recent advances in hardware and computing capabilities, as well as massive data sets, to learn deeper models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 14
                            }
                        ],
                        "text": "Sciences 2015;38:e31. [JMC, MHT] Koch, G., Zemel, R. S. & Salakhutdinov, R. (2015) Siamese neural networks for one-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 14
                            }
                        ],
                        "text": "Science 325(5938):284\u201388. [KBC] Meltzoff, A. N. & Moore, M. K. (1995) Infants\u2019 understanding of people and things:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 135
                            }
                        ],
                        "text": "This work was supported by the Center for Minds, Brains and Machines (CBMM), under NSF STC award CCF-1231216, and the Moore-Sloan Data Science Environment at NYU."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 31
                            }
                        ],
                        "text": "Opinion in Neurobiology 15(6):638\u201344. [GB] Grefenstette, E., Hermann, K. M., Suleyman, M. & Blunsom, P. (2015). Learning to"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 1
                            }
                        ],
                        "text": "338(6111):1202\u201305. [aBML] Eliasmith, C. & Trujillo, O. (2014) The use and abuse of large-scale brain models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 18
                            }
                        ],
                        "text": "Cognitive Science 38(4):599\u2013637. [aBML] Vygotsky, L. S. (1978) Interaction between learning and development."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46075,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "2b199173078b0d57c520a313c78ad50217635d56",
            "isKey": true,
            "numCitedBy": 23,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding conceptual change is an important problem in modeling human cognition and in making integrated AI systems that can learn autonomously. This paper describes a model of explanation-based conceptual change, integrating sketch understanding, analogical processing, qualitative models, truth-maintenance, and heuristic-based reasoning within the Companions cognitive architecture. Sketch understanding is used to automatically encode stimuli in the form of comic strips. Qualitative models and conceptual quantities are constructed for new phenomena via analogical reasoning and heuristics. Truth-maintenance is used to integrate conceptual and episodic knowledge into explanations, and heuristics are used to modify existing conceptual knowledge in order to produce better explanations. We simulate the learning and revision of the concept of force, testing the concepts learned via a questionnaire of sketches given to students, showing that our model follows a similar learning trajectory."
            },
            "slug": "An-Integrated-Systems-Approach-to-Explanation-Based-Friedman-Forbus",
            "title": {
                "fragments": [],
                "text": "An Integrated Systems Approach to Explanation-Based Conceptual Change"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A model of explanation-based conceptual change is described, integrating sketch understanding, analogical processing, qualitative models, truth-maintenance, and heuristic-based reasoning within the Companions cognitive architecture."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144297714"
                        ],
                        "name": "R. Granger",
                        "slug": "R.-Granger",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Granger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Granger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 84
                            }
                        ],
                        "text": "Now, the leading approaches to speech recognition are fully neural network systems (Graves et al. 2013; Hannun et al. 2014). Ideas from deep learning have also been applied to learning complex control problems. Mnih et al. (2015) combined ideas from deep learning and reinforcement learning to make a \u201cdeep reinforcement learning\u201d algorithm that learns to play large classes of simple video games from just frames of pixels and the game BEHAVIORAL AND BRAIN SCIENCES (2017), Page 1 of 72 doi:10."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5903099,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "42f2a7250f5fc999e0d844e743fa16650544cb61",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Vast information from the neurosciences may enable bottom-up understanding of human intelligence; that is, derivation of function from mechanism. This article describes such a research program: simulation and analysis of the circuits of the brain has led to derivation of a detailed set of elemental and composed operations emerging from individual and combined circuits. The specific hypothesis is forwarded that these operations constitute the \"instruction set\" of the brain, that is, the basic mental operations from which all complex behavioral and cognitive abilities are constructed, establishing a unified formalism for description of human faculties ranging from perception and learning to reasoning and language, and representing a novel and potentially fruitful research path for the construction of human-level intelligence."
            },
            "slug": "Engines-of-the-Brain:-The-Computational-Instruction-Granger",
            "title": {
                "fragments": [],
                "text": "Engines of the Brain: The Computational Instruction Set of Human Cognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Simulation and analysis of the circuits of the brain has led to derivation of a detailed set of elemental and composed operations emerging from individual and combined circuits, representing a novel and potentially fruitful research path for the construction of human-level intelligence."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2764049"
                        ],
                        "name": "Hyowon Gweon",
                        "slug": "Hyowon-Gweon",
                        "structuredName": {
                            "firstName": "Hyowon",
                            "lastName": "Gweon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyowon Gweon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 164
                            }
                        ],
                        "text": "\u2026variables, test causal hypotheses, make use of the data-generating process in drawing conclusions, and learn selectively from others (Cook, Goodman, & Schulz, 2011; Gweon et al., 2010; L. E. Schulz, Gopnik, & Glymour, 2007; Stahl & Feigenson, 2015; Tsividis, Gershman, Tenenbaum, & Schulz, 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 9
                            }
                        ],
                        "text": "Most prominently, it has been proposed that humans can approximate Bayesian inference using Monte Carlo methods, which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge (Bonawitz, Denison, Griffiths, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12787315,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b12ad8dbcbb674c4e2ed3bc80a763f0cd421d918",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to make inductive inferences from sparse data is a critical aspect of human learning. However, the properties observed in a sample of evidence depend not only on the true extension of those properties but also on the process by which evidence is sampled. Because neither the property extension nor the sampling process is directly observable, the learner's ability to make accurate generalizations depends on what is known or can be inferred about both variables. In particular, different inferences are licensed if samples are drawn randomly from the whole population (weak sampling) than if they are drawn only from the property's extension (strong sampling). Given a few positive examples of a concept, only strong sampling supports flexible inferences about how far to generalize as a function of the size and composition of the sample. Here we present a Bayesian model of the joint dependence between observed evidence, the sampling process, and the property extension and test the model behaviorally with human infants (mean age: 15 months). Across five experiments, we show that in the absence of behavioral cues to the sampling process, infants make inferences consistent with the use of strong sampling; given explicit cues to weak or strong sampling, they constrain their inferences accordingly. Finally, consistent with quantitative predictions of the model, we provide suggestive evidence that infants\u2019 inferences are graded with respect to the strength of the evidence they observe."
            },
            "slug": "Infants-consider-both-the-sample-and-the-sampling-Gweon-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Infants consider both the sample and the sampling process in inductive generalization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Bayesian model of the joint dependence between observed evidence, the sampling process, and the property extension is presented and suggestive evidence that infants\u2019 inferences are graded with respect to the strength of the evidence they observe is provided."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536223"
                        ],
                        "name": "Michael C. Frank",
                        "slug": "Michael-C.-Frank",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Frank",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael C. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3262559,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8c9a6dd09afe5fe525c039d4e87b164eaba6abea",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Word learning is a \u201cchicken and egg\u201d problem. If a child could understand speakers' utterances, it would be easy to learn the meanings of individual words, and once a child knows what many words mean, it is easy to infer speakers' intended meanings. To the beginning learner, however, both individual word meanings and speakers' intentions are unknown. We describe a computational model of word learning that solves these two inference problems in parallel, rather than relying exclusively on either the inferred meanings of utterances or cross-situational word-meaning associations. We tested our model using annotated corpus data and found that it inferred pairings between words and object concepts with higher precision than comparison models. Moreover, as the result of making probabilistic inferences about speakers' intentions, our model explains a variety of behavioral phenomena described in the word-learning literature. These phenomena include mutual exclusivity, one-trial learning, cross-situational learning, the role of words in object individuation, and the use of inferred intentions to disambiguate reference."
            },
            "slug": "Using-Speakers'-Referential-Intentions-to-Model-Frank-Goodman",
            "title": {
                "fragments": [],
                "text": "Using Speakers' Referential Intentions to Model Early Cross-Situational Word Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A computational model of word learning is described that solves two inference problems in parallel, rather than relying exclusively on either the inferred meanings of utterances or cross-situational word-meaning associations, and explains a variety of behavioral phenomena described in the word-learning literature."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological science"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11102536"
                        ],
                        "name": "F. Anselmi",
                        "slug": "F.-Anselmi",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Anselmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Anselmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700356"
                        ],
                        "name": "Joel Z. Leibo",
                        "slug": "Joel-Z.-Leibo",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Leibo",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Z. Leibo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690976"
                        ],
                        "name": "L. Rosasco",
                        "slug": "L.-Rosasco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Rosasco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rosasco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243801"
                        ],
                        "name": "Jim Mutch",
                        "slug": "Jim-Mutch",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Mutch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Mutch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844530"
                        ],
                        "name": "A. Tacchetti",
                        "slug": "A.-Tacchetti",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Tacchetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tacchetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 46
                            }
                        ],
                        "text": "models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Oaz, Bottou, Scholk\u00f6pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011; Srivastava & Salakhutdinov, 2013; Torralba, Murphy, & Freeman, 2007; Zeiler & Fergus, 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026through the sharing of features between the models learned for old objects (or old tasks) and the models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Paz, Bottou, Scholko\u0308pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9115505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d66298840e9c4268a4c1487c758dda8e54c61c85",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unsupervised-learning-of-invariant-representations-Anselmi-Leibo",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of invariant representations"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690396"
                        ],
                        "name": "C. Eliasmith",
                        "slug": "C.-Eliasmith",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Eliasmith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Eliasmith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772541"
                        ],
                        "name": "T. Stewart",
                        "slug": "T.-Stewart",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Stewart",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Stewart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3379874"
                        ],
                        "name": "Xuan Choo",
                        "slug": "Xuan-Choo",
                        "structuredName": {
                            "firstName": "Xuan",
                            "lastName": "Choo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuan Choo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090180"
                        ],
                        "name": "Trevor Bekolay",
                        "slug": "Trevor-Bekolay",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Bekolay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Bekolay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2660198"
                        ],
                        "name": "T. DeWolf",
                        "slug": "T.-DeWolf",
                        "structuredName": {
                            "firstName": "Travis",
                            "lastName": "DeWolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. DeWolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34312504"
                        ],
                        "name": "Yichuan Tang",
                        "slug": "Yichuan-Tang",
                        "structuredName": {
                            "firstName": "Yichuan",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichuan Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145046653"
                        ],
                        "name": "Daniel Rasmussen",
                        "slug": "Daniel-Rasmussen",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Rasmussen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Rasmussen, D. (2012) A large-scale model of the functioning brain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1673514,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "38f97960be98c9b523befc21f8f1724715d4184c",
            "isKey": false,
            "numCitedBy": 748,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling the Brain Neurons are pretty complicated cells. They display an endless variety of shapes that sprout highly variable numbers of axons and dendrites; they sport time- and voltage-dependent ion channels along with an impressive array of neurotransmitter receptors; and they connect intimately with near neighbors as well as former neighbors who have since moved away. Simulating a sizeable chunk of brain tissue has recently become achievable, thanks to advances in computer hardware and software. Eliasmith et al. (p. 1202; see the Perspective by Machens) present their million-neuron model of the brain and show that it can recognize numerals, remember lists of digits, and write down those lists\u2014tasks that seem effortless for a human but that encompass the triad of perception, cognition, and behavior. Two-and-a-half million model neurons recognize images, learn via reinforcement, and display fluid intelligence. A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called \u201cSpaun\u201d) that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks."
            },
            "slug": "A-Large-Scale-Model-of-the-Functioning-Brain-Eliasmith-Stewart",
            "title": {
                "fragments": [],
                "text": "A Large-Scale Model of the Functioning Brain"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A 2.5-million-neuron model of the brain (called \u201cSpaun\u201d) is presented that bridges the gap between neural activity and biological function by exhibiting many different behaviors and is presented only with visual image sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 230
                            }
                        ],
                        "text": "Similarly to how probabilistic computations can be amortized for efficiency (see previous section), plans can be amortized into cached values by allowing the model-based system to simulate training data for the model-free system (Sutton, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7962049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f8a0858fb82ce0e50b55446577a70e40137aaf",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integrated-Architectures-for-Learning,-Planning,-on-Sutton",
            "title": {
                "fragments": [],
                "text": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792298"
                        ],
                        "name": "Marc G. Bellemare",
                        "slug": "Marc-G.-Bellemare",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Bellemare",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc G. Bellemare"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294249"
                        ],
                        "name": "Yavar Naddaf",
                        "slug": "Yavar-Naddaf",
                        "structuredName": {
                            "firstName": "Yavar",
                            "lastName": "Naddaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yavar Naddaf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144056327"
                        ],
                        "name": "J. Veness",
                        "slug": "J.-Veness",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Veness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687780"
                        ],
                        "name": "Michael Bowling",
                        "slug": "Michael-Bowling",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bowling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Bowling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1552061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f82e4ff4f003581330338aaae71f60316e58dd26",
            "isKey": false,
            "numCitedBy": 2062,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available."
            },
            "slug": "The-Arcade-Learning-Environment:-An-Evaluation-for-Bellemare-Naddaf",
            "title": {
                "fragments": [],
                "text": "The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The promise of ALE is illustrated by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning, and an evaluation methodology made possible by ALE is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323727"
                        ],
                        "name": "M. Buscema",
                        "slug": "M.-Buscema",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Buscema",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buscema"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60561405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011f34d7060e6bb93776f1100ace01c9ecbfffd6",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new type of Artificial Neural Networks: the Self-Reflexive Networks. We utter the theoretical presuppositions; their dynamics is analogous to the one ascribed to autopoietic systems: self-referentiality, unsupervised learning and unintentionally cooperative and contractual activities of their own units. We also hypothesize a new concept of perception. We present the basic equations of Self-Reflexive Networks, new concepts as the one of dynamic target, of Re-entry with dedicated and fixed connections, of Meta-Units. Therefore, we experiment a specific type of Self-Reflexive Networks, the Monodedicated, within the interpretation of a toy-DB and we have hinted at other already made experimentations, experimentations in process and planned experimentations. From the applicative work that we present a few specifics and novelties of this type of Neural Networks emerge:(a)the capability of answering to complex, strange, wrong or not precise questions, through the same algorithms through which the learning phase took place.(b)the capability of spontaneously transforming their own learning inaccuracy in analogic capability and original self-organization capability.(c)the capability of spontaneously integrate the models that it experienced in different moments in an achronical hyper-model.(d)the capability of behaving as it had explored a decisions graph of large dimensions, both deeply and in extension. With the consequence of behaving as an Addressing Memory forself-dynamic Contents.(e)the capability of always learning, rapidly and anyway, besides the complexity of the learning patterns.(f)the capability of answering simultaneously from different points of view, behaving, in this case, as a network that builds more similarity models for each vector-stimulus that it receives.(g)the capability of adjusting in a biunivocal way, each question to the consulting DB and each DB to the question that are submitted. The consequence of this fact is the continuous creation of new answering models.(h)the capability of building during the learning phase, a weights matrix that provides a subconceptual representation of the bi-directional relations between each couple of input variables.(i)the capability, through the Metaunits, to integrate in a unitary typology, nodes with different saturation speed and, therefore, with different memory: in fact, while the SR units are short memory nodes, since each new stumulus zeros the previous stimulus, the Metaunits memorize the SR different stimulus during time, functioning as an average length memory. This fact should confirm that the avarage length memory is of a different level from the immediate memory and that it is based only uponrelation among perceptive stimulus which are distributed in parallel and in sequence. In this context the weights matrix constitute the SR long term memory. And in this sense it will be opportune to think at a methodic through which the Metaunits can influence during time, the same weights matrix. In any case, in the SR there areservice nodes orfilter nodes andlearning nodes as if they were weights (the Metaunits)."
            },
            "slug": "Self-reflexive-networks:-Theory-\u00b7-topology-\u00b7-Buscema",
            "title": {
                "fragments": [],
                "text": "Self-reflexive networks: Theory \u00b7 topology \u00b7 applications"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A new type of Artificial Neural Networks: the Self-Reflexive Networks, similar to autopoietic systems, with the capability of answering to complex, strange, wrong or not precise questions, through the same algorithms through which the learning phase took place."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019153"
                        ],
                        "name": "P. Battaglia",
                        "slug": "P.-Battaglia",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Battaglia",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Battaglia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158860"
                        ],
                        "name": "Jessica B. Hamrick",
                        "slug": "Jessica-B.-Hamrick",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Hamrick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jessica B. Hamrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026resolving the conflict between fast inference and structured representations, including Helmholtz-machine-style approximate inference in generative models (Dayan, Hinton, Neal, & Zemel, 1995; Hinton et al., 1995) and cooperation between model-free and model-based reinforcement learning systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 114
                            }
                        ],
                        "text": "A complete account of learning and inference must explain how the brain does so much with limited computational resources (Gershman, Horvitz, & Tenenbaum, 2015; Vul, Goodman, Griffiths, & Tenenbaum, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 168
                            }
                        ],
                        "text": "\u2026of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al., 2013) as well as simpler kinds of physical predictions that have been studied in infants (Te\u0301gla\u0301s et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 164
                            }
                        ],
                        "text": "Human and PhysNet confidence were also correlated across towers, although not as strongly as for the approximate probabilistic simulation models and experiments of Battaglia et al. (2013)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1596551,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "29ade9f04f11dd8d434f051563f03928ed62c21b",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In a glance, we can perceive whether a stack of dishes will topple, a branch will support a child\u2019s weight, a grocery bag is poorly packed and liable to tear or crush its contents, or a tool is firmly attached to a table or free to be lifted. Such rapid physical inferences are central to how people interact with the world and with each other, yet their computational underpinnings are poorly understood. We propose a model based on an \u201cintuitive physics engine,\u201d a cognitive mechanism similar to computer engines that simulate rich physics in video games and graphics, but that uses approximate, probabilistic simulations to make robust and fast inferences in complex natural scenes where crucial information is unobserved. This single model fits data from five distinct psychophysical tasks, captures several illusions and biases, and explains core aspects of human mental models and common-sense reasoning that are instrumental to how humans understand their everyday world."
            },
            "slug": "Simulation-as-an-engine-of-physical-scene-Battaglia-Hamrick",
            "title": {
                "fragments": [],
                "text": "Simulation as an engine of physical scene understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a model based on an \u201cintuitive physics engine,\u201d a cognitive mechanism similar to computer engines that simulate rich physics in video games and graphics, but that uses approximate, probabilistic simulations to make robust and fast inferences in complex natural scenes where crucial information is unobserved."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707242"
                        ],
                        "name": "Minh-Thang Luong",
                        "slug": "Minh-Thang-Luong",
                        "structuredName": {
                            "firstName": "Minh-Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh-Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6954272,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d76c07211479e233f7c6a6f32d5346c983c5598f",
            "isKey": false,
            "numCitedBy": 683,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought."
            },
            "slug": "Multi-task-Sequence-to-Sequence-Learning-Luong-Le",
            "title": {
                "fragments": [],
                "text": "Multi-task Sequence to Sequence Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks, and reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2693903"
                        ],
                        "name": "S. Pinker",
                        "slug": "S.-Pinker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Pinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pinker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48435467"
                        ],
                        "name": "Alan S. Prince",
                        "slug": "Alan-S.-Prince",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Prince",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan S. Prince"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 181
                            }
                        ],
                        "text": "For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick, 1989; Fodor & Pylyshyn, 1988; Marcus, 1998, 2001; Minsky & Papert, 1969; Pinker & Prince, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12217058,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "052008dc9dca0f5d7ad0f9a856fee3e8ee9103e9",
            "isKey": false,
            "numCitedBy": 1547,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-language-and-connectionism:-Analysis-of-a-model-Pinker-Prince",
            "title": {
                "fragments": [],
                "text": "On language and connectionism: Analysis of a parallel distributed processing model of language acquisition"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50337526"
                        ],
                        "name": "R. Keller",
                        "slug": "R.-Keller",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Keller",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403824951"
                        ],
                        "name": "S. Kedar-Cabelli",
                        "slug": "S.-Kedar-Cabelli",
                        "structuredName": {
                            "firstName": "Smadar",
                            "lastName": "Kedar-Cabelli",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kedar-Cabelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e0a41e48821d6111329d75dc63354aa8ecd241a",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 102,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of formulating general concepts from specific training examples has long been a major focus of machine learning research. While most previous research has focused on empirical methods for generalizing from a large number of training examples using no domain-specific knowledge, in the past few years new methods have been developed for applying domain-specific knowledge to formulate valid generalizations from single training examples. The characteristic common to these methods is that their ability to generalize from a single example follows from their ability to explain why the training example is a member of the concept being learned. This paper proposes a general, domain-independent mechanism, called EBG, that unifies previous approaches to explanation-based generalization. The EBG method is illustrated in the context of several example problems, and used to contrast several existing systems for explanation-based generalization. The perspective on explanation-based generalization afforded by this general method is also used to identify open research problems in this area."
            },
            "slug": "Explanation-Based-Generalization:-A-Unifying-View-Mitchell-Keller",
            "title": {
                "fragments": [],
                "text": "Explanation-Based Generalization: A Unifying View"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a general, domain-independent mechanism, called EBG, that unifies previous approaches to explanation-based generalization, and is illustrated in the context of several example problems, and used to contrast several existing systems for explanation- based generalization."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206741496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "isKey": false,
            "numCitedBy": 6899,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
            },
            "slug": "Speech-recognition-with-deep-recurrent-neural-Graves-Mohamed",
            "title": {
                "fragments": [],
                "text": "Speech recognition with deep recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2161592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "398c296d0cc7f9d180f84969f8937e6d3a413796",
            "isKey": false,
            "numCitedBy": 3369,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks."
            },
            "slug": "Multi-column-deep-neural-networks-for-image-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Multi-column deep neural networks for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "On the very competitive MNIST handwriting benchmark, this method is the first to achieve near-human performance and improves the state-of-the-art on a plethora of common image classification benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1935092"
                        ],
                        "name": "M. Hauser",
                        "slug": "M.-Hauser",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Hauser",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145529074"
                        ],
                        "name": "W. Fitch",
                        "slug": "W.-Fitch",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Fitch",
                            "middleNames": [
                                "Tecumseh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Fitch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7110414,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "beaca3493aed271bdfc42490fd22dd11cb40ce0e",
            "isKey": false,
            "numCitedBy": 2327,
            "numCiting": 119,
            "paperAbstract": {
                "fragments": [],
                "text": "We argue that an understanding of the faculty of language requires substantial interdisciplinary cooperation. We suggest how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience. We submit that a distinction should be made between the faculty of language in the broad sense (FLB) and in the narrow sense (FLN). FLB includes a sensory-motor system, a conceptual-intentional system, and the computational mechanisms for recursion, providing the capacity to generate an infinite range of expressions from a finite set of elements. We hypothesize that FLN only includes recursion and is the only uniquely human component of the faculty of language. We further argue that FLN may have evolved for reasons other than language, hence comparative studies might look for evidence of such computations outside of the domain of communication (for example, number, navigation, and social relations)."
            },
            "slug": "The-faculty-of-language:-what-is-it,-who-has-it,-it-Hauser-Chomsky",
            "title": {
                "fragments": [],
                "text": "The faculty of language: what is it, who has it, and how did it evolve?"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "It is argued that an understanding of the faculty of language requires substantial interdisciplinary cooperation and how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Parallel to these developments, a radically different approach was being explored, based on neuron-like \u201csub-symbolic\u201d computations (e.g., Fukushima, 1980; Grossberg, 1976; Rosenblatt, 1958)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11572042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495bbf69d338e33c3217c272da9eb4fec0a66212",
            "isKey": false,
            "numCitedBy": 1041,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyses a model for the parallel development and adult coding of neural feature detectors. The model was introduced in Grossberg (1976). We show how experience can retune feature detectors to respond to a prescribed convex set of spatial patterns. In particular, the detectors automatically respond to average features chosen from the set even if the average features have never been experienced. Using this procedure, any set of arbitrary spatial patterns can be recoded, or transformed, into any other spatial patterns (universal recoding), if there are sufficiently many cells in the network's cortex. The network is built from short term memory (STM) and long term memory (LTM) mechanisms, including mechanisms of adaptation, filtering, contrast enhancement, tuning, and nonspecific arousal. These mechanisms capture some experimental properties of plasticity in the kitten visual cortex. The model also suggests a classification of adult feature detector properties in terms of a small number of functional principles. In particular, experiments on retinal dynamics, including amarcrine cell function, are suggested."
            },
            "slug": "Adaptive-pattern-classification-and-universal-I.-of-Grossberg",
            "title": {
                "fragments": [],
                "text": "Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown how experience can retune feature detectors to respond to a prescribed convex set of spatial patterns, and a classification of adult feature detector properties in terms of a small number of functional principles is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11593390,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "4d7149f94a42158c92e29cdf6a512d6082841841",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 161,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-are-the-computations-of-the-cerebellum,-the-Doya",
            "title": {
                "fragments": [],
                "text": "What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8517067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88",
            "isKey": false,
            "numCitedBy": 2576,
            "numCiting": 102,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics."
            },
            "slug": "Deep-Visual-Semantic-Alignments-for-Generating-Karpathy-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Deep Visual-Semantic Alignments for Generating Image Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A model that generates natural language descriptions of images and their regions based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681850"
                        ],
                        "name": "T. Flash",
                        "slug": "T.-Flash",
                        "structuredName": {
                            "firstName": "Tamar",
                            "lastName": "Flash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Flash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403795"
                        ],
                        "name": "B. Hochner",
                        "slug": "B.-Hochner",
                        "structuredName": {
                            "firstName": "Binyamin",
                            "lastName": "Hochner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hochner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 110
                            }
                        ],
                        "text": "Work on low-level muscle synergies also showed how low-level sensorimotor constraints could simplify learning (Flash and Hochner 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5815716,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "bf1f8415b61d0afe31b2e901e888866c5b7e4f96",
            "isKey": false,
            "numCitedBy": 424,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Motor-primitives-in-vertebrates-and-invertebrates-Flash-Hochner",
            "title": {
                "fragments": [],
                "text": "Motor primitives in vertebrates and invertebrates"
            },
            "venue": {
                "fragments": [],
                "text": "Current Opinion in Neurobiology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95326,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Ultimately, the full project of building machines that learn and think like humans must have language at its core."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 90
                            }
                        ],
                        "text": "Networks trained for object recognition encode part-like features in their deeper layers (Zeiler & Fergus, 2014), whereby the presentation of new types of objects can activate novel combinations of feature detectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 276
                            }
                        ],
                        "text": "\u2026old tasks) and the models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Paz, Bottou, Scholko\u0308pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011; Srivastava & Salakhutdinov, 2013; Torralba, Murphy, & Freeman, 2007; Zeiler & Fergus, 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 50
                            }
                        ],
                        "text": "Moreover, in the absence of strong constraints from neuroscience, we can turn the biological argument around: Perhaps a hypothetical biological mechanism should be viewed with skepticism if it is cognitively implausible."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 219
                            }
                        ],
                        "text": "For networks trained on object classification, deeper layers often become sensitive to successively higher-level features, from edges to textures to shape-parts to full objects (Yosinski, Clune, Bengio, & Lipson, 2014; Zeiler & Fergus, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3960646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
            "isKey": true,
            "numCitedBy": 11814,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
            },
            "slug": "Visualizing-and-Understanding-Convolutional-Zeiler-Fergus",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43579858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e00b5a6937f192cc6725cca4020caff62a8aa4df",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 183,
            "paperAbstract": {
                "fragments": [],
                "text": "Inducing causal relationships from observations is a classic problem in scientific inference, statistics, and machine learning. It is also a central part of human learning, and a task that people perform remarkably well given its notorious difficulties. People can learn causal structure in various settings, from diverse forms of data: observations of the co-occurrence frequencies between causes and effects, interactions between physical objects, or patterns of spatial or temporal coincidence. These different modes of learning are typically thought of as distinct psychological processes and are rarely studied together, but at heart they present the same inductive challenge-identifying the unobservable mechanisms that generate observable relations between variables, objects, or events, given only sparse and limited data. We present a computational-level analysis of this inductive problem and a framework for its solution, which allows us to model all these forms of causal learning in a common language. In this framework, causal induction is the product of domain-general statistical inference guided by domain-specific prior knowledge, in the form of an abstract causal theory. We identify 3 key aspects of abstract prior knowledge-the ontology of entities, properties, and relations that organizes a domain; the plausibility of specific causal relationships; and the functional form of those relationships-and show how they provide the constraints that people need to induce useful causal models from sparse data."
            },
            "slug": "Theory-based-causal-induction.-Griffiths-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Theory-based causal induction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work identifies 3 key aspects of abstract prior knowledge-the ontology of entities, properties, and relations that organizes a domain; the plausibility of specific causal relationships; and the functional form of those relationships-and shows how they provide the constraints that people need to induce useful causal models from sparse data."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 153
                            }
                        ],
                        "text": "Somewhat surprisingly, the incorporation of attention has led to substantial performance gains in a variety of domains, including in machine translation (Bahdanau et al., 2015), object recognition (V."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11212020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "isKey": false,
            "numCitedBy": 19342,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
            },
            "slug": "Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation by Jointly Learning to Align and Translate"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1941772"
                        ],
                        "name": "T. Bever",
                        "slug": "T.-Bever",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bever",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709050"
                        ],
                        "name": "D. Poeppel",
                        "slug": "D.-Poeppel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Poeppel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Poeppel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 146
                            }
                        ],
                        "text": "\u201cAnalysis-by-synthesis\u201d theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel, 2010; Eden, 1962; Halle & Stevens, 1962; Neisser, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5680780,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d0a14c4ed8f55e2710506a47942530d21440707e",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution reviews (some of) the history of analysis by synthesis, an approach to perception and comprehension articulated in the 1950s. Whereas much research has focused on bottom-up, feed-forward, inductive mechanisms, analysis by synthesis as a heuristic model emphasizes a balance of bottom-up and knowledge-driven, top-down, predictive steps in speech perception and language comprehension. This idea aligns well with contemporary Bayesian approaches to perception (in language and other domains), which are illustrated with examples from different aspects of perception and comprehension. Results from psycholinguistics, the cognitive neuroscience of language, and visual object recognition suggest that analysis by synthesis can provide a productive way of structuring biolinguistic research. Current evidence suggests that such a model is theoretically well motivated, biologically sensible, and becomes computationally tractable borrowing from Bayesian formalizations."
            },
            "slug": "Analysis-by-Synthesis:-A-(Re-)Emerging-Program-of-Bever-Poeppel",
            "title": {
                "fragments": [],
                "text": "Analysis by Synthesis: A (Re-)Emerging Program of Research for Language and Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Current evidence suggests that analysis by synthesis can provide a productive way of structuring biolinguistic research, and that such a model is theoretically well motivated, biologically sensible, and becomes computationally tractable borrowing from Bayesian formalizations."
            },
            "venue": {
                "fragments": [],
                "text": "Biolinguistics"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077302895"
                        ],
                        "name": "A. Cardon",
                        "slug": "A.-Cardon",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Cardon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cardon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6977929,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "a5d2c317ecfe53691a1e88ccd2553d8cc1ea8f73",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Nowadays for robots, the notion of behavior is reduced to a simple factual concept at the level of the movements. On another hand, consciousness is a very cultural concept, founding the main property of human beings, according to themselves. We propose to develop a computable transposition of the consciousness concepts into artificial brains, able to express emotions and consciousness facts. The production of such artificial brains allows the intentional and really adaptive behavior for the autonomous robots. Such a system managing the robot\u2019s behavior will be made of two parts: the first one computes and generates, in a constructivist manner, a representation for the robot moving in its environment, and using symbols and concepts. The other part achieves the representation of the previous one using morphologies in a dynamic geometrical way. The robot\u2019s body will be seen for itself as the morphologic apprehension of its material substrata. The model goes strictly by the notion of massive multi-agent\u2019s organizations with a morphologic control."
            },
            "slug": "Artificial-consciousness,-artificial-emotions,-and-Cardon",
            "title": {
                "fragments": [],
                "text": "Artificial consciousness, artificial emotions, and autonomous robots"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes to develop a computable transposition of the consciousness concepts into artificial brains, able to express emotions and consciousness facts, and goes strictly by the notion of massive multi-agent's organizations with a morphologic control."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3797168"
                        ],
                        "name": "D. Premack",
                        "slug": "D.-Premack",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Premack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Premack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50230782"
                        ],
                        "name": "A. Premack",
                        "slug": "A.-Premack",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Premack",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Premack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 193
                            }
                        ],
                        "text": "\u2026is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion (Johnson, Slaughter, & Carey, 1998; Premack & Premack, 1997; Schlottmann, Ray, Mitchell, & Demetriou, 2006; Tremoulet & Feldman, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9602418,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "71062f8a8509b9f948823989a4a4ca3bdc0efa4a",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Motion is a fundamental source of information for basic human interpretations; it is basic to the fundamental concept of causality and, the present model argues, equally basic to the fundamental concept of intentionality. The model is based on two main assumptions: When an infant perceives an object (1) moving spontaneously and (2) displaying goaldirected action, it will interpret the object as intentional and assign to it the unique properties of the psychological domain. The key property tested was: Do infants attribute value to interactions between intentional objects using criteria specified by the model? We showed infants (average age 52 weeks) computer-generated animations of spontaneously moving balls, using looking time in a standard habituation/dishabituation paradigm. In two positive interactions, one ball either caressed another, or helped it achieve its goal; whereas in two negative interactions, one ball either hit another, or prevented it from achieving its goal. In keeping with predictions of the model, when transferred to a negative condition, infants who had been habituated on a positive condition showed greater dishabituation than those habituated on a negative condition. The results could not be easily explained by the similarity relations among the animations depicting the interactions. The results suggest that well before the age when the child can ascribe mental states or has a theory of mind, it recognizes the goals of self-propelled objects and attributes value to the interactions between them."
            },
            "slug": "Infants-Attribute-Value-to-the-Goal-Directed-of-Premack-Premack",
            "title": {
                "fragments": [],
                "text": "Infants Attribute Value to the Goal-Directed Actions of Self-propelled Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results suggest that well before the age when the child can ascribe mental states or has a theory of mind, it recognizes the goals of self-propelled objects and attributes value to the interactions between them."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71799216"
                        ],
                        "name": "J. Kilner",
                        "slug": "J.-Kilner",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kilner",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kilner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737497"
                        ],
                        "name": "Karl J. Friston",
                        "slug": "Karl-J.-Friston",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Friston",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl J. Friston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144155759"
                        ],
                        "name": "C. Frith",
                        "slug": "C.-Frith",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Frith",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Frith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10497123,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c741859fba34d1b44bedbb167e9777eaf7a05af4",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Is it possible to understand the intentions of other people by simply observing their actions? Many believe that this ability is made possible by the brain\u2019s mirror neuron system through its direct link between action and observation. However, precisely how intentions can be inferred through action observation has provoked much debate. Here we suggest that the function of the mirror system can be understood within a predictive coding framework that appeals to the statistical approach known as empirical Bayes. Within this scheme the most likely cause of an observed action can be inferred by minimizing the prediction error at all levels of the cortical hierarchy that are engaged during action observation. This account identifies a precise role for the mirror system in our ability to infer intentions from actions and provides the outline of the underlying computational mechanisms."
            },
            "slug": "Predictive-coding:-an-account-of-the-mirror-neuron-Kilner-Friston",
            "title": {
                "fragments": [],
                "text": "Predictive coding: an account of the mirror neuron system"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The function of the mirror system can be understood within a predictive coding framework that appeals to the statistical approach known as empirical Bayes and the outline of the underlying computational mechanisms are provided."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290695"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Watkins",
                            "middleNames": [
                                "J.",
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8223593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7708178d34d069b58a88ea4eb13aa1f57d87bedb",
            "isKey": false,
            "numCitedBy": 2931,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\n$$\\mathcal{Q}$$\n-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem for \n$$\\mathcal{Q}$$\n-learning based on that outlined in Watkins (1989). We show that \n$$\\mathcal{Q}$$\n-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many \n$$\\mathcal{Q}$$\n values can be changed each iteration, rather than just one."
            },
            "slug": "Technical-Note:-Q-Learning-Watkins-Dayan",
            "title": {
                "fragments": [],
                "text": "Technical Note: Q-Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A convergence theorem is presented and proves that Q -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action- values are represented discretely."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1930231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
            "isKey": false,
            "numCitedBy": 1628,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye."
            },
            "slug": "DRAW:-A-Recurrent-Neural-Network-For-Image-Gregor-Danihelka",
            "title": {
                "fragments": [],
                "text": "DRAW: A Recurrent Neural Network For Image Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The Deep Recurrent Attentive Writer neural network architecture for image generation substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835039"
                        ],
                        "name": "E. Vul",
                        "slug": "E.-Vul",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Vul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 277
                            }
                        ],
                        "text": "\u2026which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge (Bonawitz, Denison, Griffiths, & Gopnik, 2014; Gershman, Vul, & Tenenbaum, 2012; T. D. Ullman, Goodman, & Tenenbaum, 2012; Vul et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 76
                            }
                        ],
                        "text": "A holistic model of the scene would require the composition of individual object models, glued together by relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13979657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37bc28e563aa8b1ac8774e8edf81fb4d0e9c9860",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of findings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufficient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples--but as samples are costly--how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We find that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition."
            },
            "slug": "One-and-Done-Optimal-Decisions-From-Very-Few-Vul-Goodman",
            "title": {
                "fragments": [],
                "text": "One and Done? Optimal Decisions From Very Few Samples"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353715"
                        ],
                        "name": "Adam N. Sanborn",
                        "slug": "Adam-N.-Sanborn",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Sanborn",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam N. Sanborn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735083"
                        ],
                        "name": "Vikash K. Mansinghka",
                        "slug": "Vikash-K.-Mansinghka",
                        "structuredName": {
                            "firstName": "Vikash",
                            "lastName": "Mansinghka",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikash K. Mansinghka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15491678,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "c1a64deb8451b8195d93602aa90322cc912c7151",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "People have strong intuitions about the influence objects exert upon one another when they collide. Because people's judgments appear to deviate from Newtonian mechanics, psychologists have suggested that people depend on a variety of task-specific heuristics. This leaves open the question of how these heuristics could be chosen, and how to integrate them into a unified model that can explain human judgments across a wide range of physical reasoning tasks. We propose an alternative framework, in which people's judgments are based on optimal statistical inference over a Newtonian physical model that incorporates sensory noise and intrinsic uncertainty about the physical properties of the objects being viewed. This noisy Newton framework can be applied to a multitude of judgments, with people's answers determined by the uncertainty they have for physical variables and the constraints of Newtonian mechanics. We investigate a range of effects in mass judgments that have been taken as strong evidence for heuristic use and show that they are well explained by the interplay between Newtonian constraints and sensory uncertainty. We also consider an extended model that handles causality judgments, and obtain good quantitative agreement with human judgments across tasks that involve different judgment types with a single consistent set of parameters."
            },
            "slug": "Reconciling-intuitive-physics-and-Newtonian-for-Sanborn-Mansinghka",
            "title": {
                "fragments": [],
                "text": "Reconciling intuitive physics and Newtonian mechanics for colliding objects."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A range of effects in mass judgments that have been taken as strong evidence for heuristic use are investigated and show that they are well explained by the interplay between Newtonian constraints and sensory uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699245"
                        ],
                        "name": "T. Winograd",
                        "slug": "T.-Winograd",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Winograd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 47
                            }
                        ],
                        "text": "For example, Schank (1972), writing in the journal Cognitive Psychology, declared that\nWe hope to be able to build a program that can learn, as a child does, how to do what we have described in this paper instead of being spoon-fed the tremendous information necessary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 200
                            }
                        ],
                        "text": "AI pioneers in other areas of research explicitly referenced human cognition, and even published papers in cognitive psychology journals (e.g.,\nBobrow & Winograd, 1977; Hayes-Roth & Hayes-Roth, 1979; Winograd, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56798209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb20f121c979b535bbeade5ac06676d627d4ad7d",
            "isKey": false,
            "numCitedBy": 2455,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system."
            },
            "slug": "Understanding-natural-language-Winograd",
            "title": {
                "fragments": [],
                "text": "Understanding natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A computer system for understanding English that contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system based on the belief that in modeling language understanding, it must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861371"
                        ],
                        "name": "G. Csibra",
                        "slug": "G.-Csibra",
                        "structuredName": {
                            "firstName": "Gergely",
                            "lastName": "Csibra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Csibra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 183
                            }
                        ],
                        "text": "Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions towards those goals subject to constraints (Csibra, 2008; Csibra, Biro, Koos, & Gergely, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 182
                            }
                        ],
                        "text": "Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions towards those goals subject to constraints (Csibra, 2008; Csibra, B\u0301\u0131r\u00f3, Ko\u00f3s, & Gergely, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18318923,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5a63295a9d6a2e60d696115d35a52614b24a6052",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Goal-attribution-to-inanimate-agents-by-infants-Csibra",
            "title": {
                "fragments": [],
                "text": "Goal attribution to inanimate agents by 6.5-month-old infants"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460849"
                        ],
                        "name": "Ioannis Antonoglou",
                        "slug": "Ioannis-Antonoglou",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Antonoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Antonoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137672"
                        ],
                        "name": "Martin A. Riedmiller",
                        "slug": "Martin-A.-Riedmiller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Riedmiller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Riedmiller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15238391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2319a491378867c7049b3da055c5df60e1671158",
            "isKey": false,
            "numCitedBy": 6820,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
            },
            "slug": "Playing-Atari-with-Deep-Reinforcement-Learning-Mnih-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Playing Atari with Deep Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning, which outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 73
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson &\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 51
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143831277,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ed6780677cc5d7128f0619327aa995a305cf839a",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inductive-judgments-about-natural-categories.-Rips",
            "title": {
                "fragments": [],
                "text": "Inductive judgments about natural categories."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725887"
                        ],
                        "name": "J. Hummel",
                        "slug": "J.-Hummel",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hummel",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hummel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299972"
                        ],
                        "name": "I. Biederman",
                        "slug": "I.-Biederman",
                        "structuredName": {
                            "firstName": "Irving",
                            "lastName": "Biederman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Biederman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 193
                            }
                        ],
                        "text": "Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman, 1987; Hummel & Biederman, 1992; Marr & Nishihara, 1978; van den Hengel et al., 2015; Winston, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "Unfortunately, what we \u201cknow\u201d about the brain is not all that clear-cut."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17575060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b32e9d6daa7ec6a97c393d6b8f239f04a8a4944",
            "isKey": false,
            "numCitedBy": 982,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a single view of an object, humans can readily recognize that object from other views that preserve the parts in the original view. Empirical evidence suggests that this capacity reflects the activation of a viewpoint-invariant structural description specifying the object's parts and the relations among them. This article presents a neural network that generates such a description. Structural description is made possible through a solution to the dynamic binding problem: Temporary conjunctions of attributes (parts and relations) are represented by synchronized oscillatory activity among independent units representing those attributes. Specifically, the model uses synchrony (a) to parse images into their constituent parts, (b) to bind together the attributes of a part, and (c) to bind the relations to the parts to which they apply. Because it conjoins independent units temporarily, dynamic binding allows tremendous economy of representation and permits the representation to reflect the attribute structure of the shapes represented."
            },
            "slug": "Dynamic-binding-in-a-neural-network-for-shape-Hummel-Biederman",
            "title": {
                "fragments": [],
                "text": "Dynamic binding in a neural network for shape recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A neural network is presented that generates a viewpoint-invariant structural description specifying the object's parts and the relations among them and uses synchrony to parse images into their constituent parts."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35455259"
                        ],
                        "name": "Fei Xu",
                        "slug": "Fei-Xu",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 273
                            }
                        ],
                        "text": "\u2026master language, and they provide the building\n8Michael Jordan made this point forcefully in his 2015 speech accepting the Rumelhart Prize.\nblocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "\u2026the context of learning the meanings of words in their native language (Carey & Bartlett, 1978; Landau, Smith, & Jones, 1988; E. M. Markman, 1989; Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002; F. Xu & Tenenbaum, 2007, although see Horst and Samuelson 2008 regarding memory limitations)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11094828,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1a316023e4297ceae99250bc586a6b8a4753d549",
            "isKey": false,
            "numCitedBy": 864,
            "numCiting": 161,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning."
            },
            "slug": "Word-learning-as-Bayesian-inference.-Xu-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "Word learning as Bayesian inference."
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The authors present a Bayesian framework for understanding how adults and children learn the meanings of words, and explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2759821"
                        ],
                        "name": "E. Bonawitz",
                        "slug": "E.-Bonawitz",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Bonawitz",
                            "middleNames": [
                                "Baraff"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bonawitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145828326"
                        ],
                        "name": "S. Denison",
                        "slug": "S.-Denison",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Denison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Denison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222423"
                        ],
                        "name": "A. Gopnik",
                        "slug": "A.-Gopnik",
                        "structuredName": {
                            "firstName": "Alison",
                            "lastName": "Gopnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gopnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 115
                            }
                        ],
                        "text": "Monte Carlo sampling has been invoked to explain behavioral phenomena ranging from children\u2019s response variability (Bonawitz et al., 2014) to garden-path effects in sentence processing (Levy, Reali, & Griffiths, 2009) and perceptual multistability (Gershman et al., 2012; MorenoBote, Knill, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "Finally, causality helps infuse the recognition of existing tools (or the learning of new ones) with an understanding of their use, helping to connect different object models in the proper way (e.g., hammering a nail into a wall, or using a saw horse to support a beam being cut by a saw)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15075937,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ac7b32615240d7862f2d9bea2a3962108efa02d6",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-models,-learning-algorithms,-and-in-Bonawitz-Denison",
            "title": {
                "fragments": [],
                "text": "Probabilistic models, learning algorithms, and response variability: sampling in cognitive development"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33259752"
                        ],
                        "name": "B. McNaughton",
                        "slug": "B.-McNaughton",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "McNaughton",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. McNaughton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390067049"
                        ],
                        "name": "R. O\u2019Reilly",
                        "slug": "R.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Reilly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2832081,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2ebf18e7892e660a833152ddc6cf8f1d21a7b881",
            "isKey": false,
            "numCitedBy": 4390,
            "numCiting": 309,
            "paperAbstract": {
                "fragments": [],
                "text": "Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems."
            },
            "slug": "Why-there-are-complementary-learning-systems-in-the-McClelland-McNaughton",
            "title": {
                "fragments": [],
                "text": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory."
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocorticals changes."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694846"
                        ],
                        "name": "Q. Liao",
                        "slug": "Q.-Liao",
                        "structuredName": {
                            "firstName": "Qianli",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700356"
                        ],
                        "name": "Joel Z. Leibo",
                        "slug": "Joel-Z.-Leibo",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Leibo",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Z. Leibo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8851134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4f55e1a1082f3725d7fa74f12a0d3b5c0ea0af7",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n Gradient backpropagation (BP) requires symmetric feedforward and feedback connections \u2014 the same weights must be used for forward and backward passes. This \"weight transport problem'' (Grossberg 1987) is thought to be one of the main reasons to doubt BP's biologically plausibility. Using 15 different classification datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter \u2014 the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a \"Batch Manhattan'' (BM) update rule.\n \n"
            },
            "slug": "How-Important-Is-Weight-Symmetry-in-Backpropagation-Liao-Leibo",
            "title": {
                "fragments": [],
                "text": "How Important Is Weight Symmetry in Backpropagation?"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This study systematically investigates to what extentient backpropagation really depends on weight symmetry, and indicates that some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) and/or a \"Batch Manhattan\" (BM) update rule."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222423"
                        ],
                        "name": "A. Gopnik",
                        "slug": "A.-Gopnik",
                        "structuredName": {
                            "firstName": "Alison",
                            "lastName": "Gopnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gopnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058012"
                        ],
                        "name": "C. Glymour",
                        "slug": "C.-Glymour",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Glymour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Glymour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124398"
                        ],
                        "name": "D. Sobel",
                        "slug": "D.-Sobel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sobel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sobel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3693228"
                        ],
                        "name": "T. Kushnir",
                        "slug": "T.-Kushnir",
                        "structuredName": {
                            "firstName": "Tamar",
                            "lastName": "Kushnir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kushnir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143752796"
                        ],
                        "name": "D. Danks",
                        "slug": "D.-Danks",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Danks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Danks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 139
                            }
                        ],
                        "text": "The underlying cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 143
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 809336,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "792695c436fd0148a71e7f2830ea5bac7938b014",
            "isKey": false,
            "numCitedBy": 965,
            "numCiting": 161,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors outline a cognitive and computational account of causal learning in children. They propose that children use specialized cognitive systems that allow them to recover an accurate \"causal map\" of the world: an abstract, coherent, learned representation of the causal relations among events. This kind of knowledge can be perspicuously understood in terms of the formalism of directed graphical causal models, or Bayes nets. Children's causal learning and inference may involve computations similar to those for learning causal Bayes nets and for predicting with them. Experimental results suggest that 2- to 4-year-old children construct new causal maps and that their learning is consistent with the Bayes net formalism."
            },
            "slug": "A-theory-of-causal-learning-in-children:-causal-and-Gopnik-Glymour",
            "title": {
                "fragments": [],
                "text": "A theory of causal learning in children: causal maps and Bayes nets."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results suggest that 2- to 4-year-old children construct new causal maps and that their learning is consistent with the Bayes net formalism."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144676410"
                        ],
                        "name": "M. Halle",
                        "slug": "M.-Halle",
                        "structuredName": {
                            "firstName": "Morris",
                            "lastName": "Halle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Halle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708017"
                        ],
                        "name": "J. Bresnan",
                        "slug": "J.-Bresnan",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bresnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bresnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 151335519,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6212e3e2bee0f342d1311d382a6c59e5efdf2295",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Any adequate psychology of man must provide some way to understand the human capacity for language,\" the editors of this volume write. \"It was a belief shared by quite a few among us that developments in linguistics and psychology were leading to similar conclusions by separate routes and that this was an appropriate time to explore the implications of these apparently parallel developments for future, perhaps joint, work. This volume represents a few initial steps in the direction of that goal.\" The nine chapters of this book were written by linguists and psychologists, after extended collaboration and exchange of ideas. In the first chapter, \"A Realistic Transformational Grammar,\" Joan Bresnan (MIT) explores some of the consequences of her proposal that the role of many transformations in generative grammar should be subsumed by the lexical component. The character of lexical entries is the central topic of \"Semantic Relations Among Words,\" by George A. Miller (Rockefeller University); he reports views and suggestions that he has developed since the publication of his and Johnson-Laird's monumental Language and Perception (1976). The chapter by Eric Wanner (Rockefeller University) and Michael Maratsos (University of Minnesota), \"An ATN Approach to Comprehension,\" presents a nontransformational model of language processing that uses concepts developed in automatic parsing systems. The relations between their psychological model and Bresnan's lexical-transformational model is outlined in Chapter 1. \"Anaphora as an Approach to Pragmatics\" by Keith Stenning (University of Liverpool) explores the central problem of pragmatics: a sentence can express different meanings in different contexts. He proposes that a successful account of antecedent-anaphor relations must recognize the relation between a linguistic entity and its context, linguistic or nonlinguistic. Ray Jackendoff (Brandeis), in \"Grammar as Evidence for Conceptual Structure,\" attempts to use the information about semantic structure that is provided by the interpretation of various syntactic configurations in order to gain insights into basic attributes of human cognition. The remaining chapters deal with ways knowledge of a language is acquired and lost. In \"Language and the Brain,\" Edgar B. Zurif (Boston University Medical School) and Sheila E. Blumstein (Brown University and Boston University Medical School) survey some recent work on aphasia in the light of different theoretical models of language. Michael Maratsos, in \"New Models in Linguistics and Language Acquisition,\" inquires into the implications that a language model with restricted transformational component has for understanding of the way children acquire syntax. In \"The Child as Word Learner\" Susan Carey (MIT) examines the rapidity with which children learn words; she proposes that the process involves two stages: an almost instantaneous assignment of a new world to a field of related words, followed by a slow working out of its place in that field. Finally, Morris Halle (MIT), in \"Knowledge Unlearned and Untaught: What speakers know about the sounds of their language,\" cites facts that normal speakers of English demonstrably know but could never have been explicitly taught, nor in some cases even learned. Halle suggests this is a manifestation of innate knowledge that is genetically programmed into organisms."
            },
            "slug": "Linguistic-theory-and-psychological-reality-Halle-Bresnan",
            "title": {
                "fragments": [],
                "text": "Linguistic theory and psychological reality"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158597261"
                        ],
                        "name": "John R. Anderson",
                        "slug": "John-R.-Anderson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John R. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3240261"
                        ],
                        "name": "Daniel Bothell",
                        "slug": "Daniel-Bothell",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bothell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Bothell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731836"
                        ],
                        "name": "M. Byrne",
                        "slug": "M.-Byrne",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Byrne",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Byrne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067498"
                        ],
                        "name": "Scott Douglass",
                        "slug": "Scott-Douglass",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Douglass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Douglass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39669695"
                        ],
                        "name": "Yulin Qin",
                        "slug": "Yulin-Qin",
                        "structuredName": {
                            "firstName": "Yulin",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yulin Qin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 81
                            }
                        ],
                        "text": "Today, Anderson\u2019s ACT-R perhaps provides the most comprehensive simulation model (Anderson et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 186640,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8af78f6f4fe2bdcd3c9d3bafc424b04a22d4e5ea",
            "isKey": false,
            "numCitedBy": 2828,
            "numCiting": 211,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive control of thought-rational (ACT-R; J. R. Anderson & C. Lebiere, 1998) has evolved into a theory that consists of multiple modules but also explains how these modules are integrated to produce coherent cognition. The perceptual-motor modules, the goal module, and the declarative memory module are presented as examples of specialized systems in ACT-R. These modules are associated with distinct cortical regions. These modules place chunks in buffers where they can be detected by a production system that responds to patterns of information in the buffers. At any point in time, a single production rule is selected to respond to the current pattern. Subsymbolic processes serve to guide the selection of rules to fire as well as the internal operations of some modules. Much of learning involves tuning of these subsymbolic processes. A number of simple and complex empirical examples are described to illustrate how these modules function singly and in concert."
            },
            "slug": "An-integrated-theory-of-the-mind.-Anderson-Bothell",
            "title": {
                "fragments": [],
                "text": "An integrated theory of the mind."
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The perceptual-motor modules, the goal module, and the declarative memory module are presented as examples of specialized systems in ACT-R, which consists of multiple modules that are integrated to produce coherent cognition."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2252285"
                        ],
                        "name": "E. Spelke",
                        "slug": "E.-Spelke",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Spelke",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Spelke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7709363"
                        ],
                        "name": "Katherine D. Kinzler",
                        "slug": "Katherine-D.-Kinzler",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Kinzler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine D. Kinzler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 120
                            }
                        ],
                        "text": "It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion (Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 234
                            }
                        ],
                        "text": "Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions towards those goals subject to constraints (Csibra, 2008; Csibra, Biro, Koos, & Gergely, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 102
                            }
                        ],
                        "text": "Early in development, humans have a foundational understanding of several core domains (Spelke, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10185110,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "16d3b1859b935d5ec36116f69b1bfce9df6bf8c4",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits."
            },
            "slug": "Core-knowledge.-Spelke-Kinzler",
            "title": {
                "fragments": [],
                "text": "Core knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits."
            },
            "venue": {
                "fragments": [],
                "text": "Developmental science"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402022761"
                        ],
                        "name": "J. Jara-Ettinger",
                        "slug": "J.-Jara-Ettinger",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Jara-Ettinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jara-Ettinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2764049"
                        ],
                        "name": "Hyowon Gweon",
                        "slug": "Hyowon-Gweon",
                        "structuredName": {
                            "firstName": "Hyowon",
                            "lastName": "Gweon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyowon Gweon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1213949,
            "fieldsOfStudy": [
                "Economics",
                "Biology"
            ],
            "id": "413763bb5fe24c3eb661aac50a65120a938d9b0a",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Children\u2019s-understanding-of-the-costs-and-rewards-Jara-Ettinger-Gweon",
            "title": {
                "fragments": [],
                "text": "Children\u2019s understanding of the costs and rewards underlying rational action"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646034"
                        ],
                        "name": "C. Gallistel",
                        "slug": "C.-Gallistel",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gallistel",
                            "middleNames": [
                                "Randy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gallistel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3624461"
                        ],
                        "name": "L. Matzel",
                        "slug": "L.-Matzel",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Matzel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matzel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Gallistel and Matzel (2013) have persuasively argued that the critical interstimulus interval for LTP is orders of magnitude smaller than the intervals that are behaviorally relevant in most forms of learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17353879,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "fa35595f7a651ad3668e0231606ca6cad7a5b6a8",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "From the traditional perspective of associative learning theory, the hypothesis linking modifications of synaptic transmission to learning and memory is plausible. It is less so from an information-processing perspective, in which learning is mediated by computations that make implicit commitments to physical and mathematical principles governing the domains where domain-specific cognitive mechanisms operate. We compare the properties of associative learning and memory to the properties of long-term potentiation, concluding that the properties of the latter do not explain the fundamental properties of the former. We briefly review the neuroscience of reinforcement learning, emphasizing the representational implications of the neuroscientific findings. We then review more extensively findings that confirm the existence of complex computations in three information-processing domains: probabilistic inference, the representation of uncertainty, and the representation of space. We argue for a change in the conceptual framework within which neuroscientists approach the study of learning mechanisms in the brain."
            },
            "slug": "The-neuroscience-of-learning:-beyond-the-Hebbian-Gallistel-Matzel",
            "title": {
                "fragments": [],
                "text": "The neuroscience of learning: beyond the Hebbian synapse."
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It is argued for a change in the conceptual framework within which neuroscientists approach the study of learning mechanisms in the brain, in which learning is mediated by computations that make implicit commitments to physical and mathematical principles governing the domains where domain-specific cognitive mechanisms operate."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of psychology"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053868213"
                        ],
                        "name": "Thomas Arnold",
                        "slug": "Thomas-Arnold",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Arnold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Arnold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793014"
                        ],
                        "name": "Matthias Scheutz",
                        "slug": "Matthias-Scheutz",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Scheutz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Scheutz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16905155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d176782a6ef007a568e1d7860d3a8f223fc874e",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper argues against the moral Turing test (MTT) as a framework for evaluating the moral performance of autonomous systems. Though the term has been carefully introduced, considered, and cautioned about in previous discussions (Allen et al. in J Exp Theor Artif Intell 12(3):251\u2013261, 2000; Allen and Wallach 2009), it has lingered on as a touchstone for developing computational approaches to moral reasoning (Gerdes and \u00d8hrstr\u00f8m in J Inf Commun Ethics Soc 13(2):98\u2013109, 2015). While these efforts have not led to the detailed development of an MTT, they nonetheless retain the idea to discuss what kinds of action and reasoning should be demanded of autonomous systems. We explore the flawed basis of an MTT in imitation, even one based on scenarios of morally accountable actions. MTT-based evaluations are vulnerable to deception, inadequate reasoning, and inferior moral performance vis a vis a system\u2019s capabilities. We propose verification\u2014which demands the design of transparent, accountable processes of reasoning that reliably prefigure the performance of autonomous systems\u2014serves as a superior framework for both designer and system alike. As autonomous social robots in particular take on an increasing range of critical roles within society, we conclude that verification offers an essential, albeit challenging, moral measure of their design and performance."
            },
            "slug": "Against-the-moral-Turing-test:-accountable-design-Arnold-Scheutz",
            "title": {
                "fragments": [],
                "text": "Against the moral Turing test: accountable design and the moral reasoning of autonomous systems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Verification is proposed\u2014which demands the design of transparent, accountable processes of reasoning that reliably prefigure the performance of autonomous systems\u2014serves as a superior framework for both designer and system alike and concludes that verification offers an essential, albeit challenging, moral measure of their design and performance."
            },
            "venue": {
                "fragments": [],
                "text": "Ethics and Information Technology"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144470585"
                        ],
                        "name": "Kurt Gray",
                        "slug": "Kurt-Gray",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Gray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kurt Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810430"
                        ],
                        "name": "D. Wegner",
                        "slug": "D.-Wegner",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Wegner",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wegner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7828975,
            "fieldsOfStudy": [
                "Psychology",
                "Art"
            ],
            "id": "dafbb17e885aa4fe86da75f4cfb7b1d3009154dd",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feeling-robots-and-human-zombies:-Mind-perception-Gray-Wegner",
            "title": {
                "fragments": [],
                "text": "Feeling robots and human zombies: Mind perception and the uncanny valley"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37774552"
                        ],
                        "name": "T. Ullman",
                        "slug": "T.-Ullman",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Ullman",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21161348"
                        ],
                        "name": "Chris L. Baker",
                        "slug": "Chris-L.-Baker",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Baker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2967041"
                        ],
                        "name": "Owen Macindoe",
                        "slug": "Owen-Macindoe",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Macindoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Macindoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47107786"
                        ],
                        "name": "Owain Evans",
                        "slug": "Owain-Evans",
                        "structuredName": {
                            "firstName": "Owain",
                            "lastName": "Evans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owain Evans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6824910,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "460904bc3ccf228c5365f29e79b2ffb40b4f96f7",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Everyday social interactions are heavily influenced by our snap judgments about others' goals. Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is 'helping' or 'hindering' another's attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs). The model infers the goal most likely to be driving an agent's behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present. We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative."
            },
            "slug": "Help-or-Hinder:-Bayesian-Models-of-Social-Goal-Ullman-Baker",
            "title": {
                "fragments": [],
                "text": "Help or Hinder: Bayesian Models of Social Goal Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A model for how people can infer social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs), is proposed and behavioral evidence is presented in support of this model over a simpler, perceptual cue-based alternative."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716847"
                        ],
                        "name": "G. Baldassarre",
                        "slug": "G.-Baldassarre",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Baldassarre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baldassarre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832617"
                        ],
                        "name": "D. Caligiore",
                        "slug": "D.-Caligiore",
                        "structuredName": {
                            "firstName": "Daniele",
                            "lastName": "Caligiore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Caligiore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3074452"
                        ],
                        "name": "Francesco Mannella",
                        "slug": "Francesco-Mannella",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Mannella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Mannella"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4435920,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "253ca431c4081a7c0440bc2aea6cc50d9d8a2b78",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "To suitably adapt to the challenges posed by reproduction and survival, animals need to learn to select when to perform different behaviours, to have internal criteria for guiding these learning processes, and to perform behaviours efficiently once selected. To implement these processes, their brains must be organised in a suitable hierarchical fashion. Here we briefly review two types of neural/behavioural/computational literatures, focussed, respectively, on cortex and on sub-cortical areas, and highlight their important limitations. Then we review two computational modelling works of the authors that exemplify the problems, brain areas, experiments, main concepts, and limitations of the two research threads. Finally we propose a theoretical integration of the two views, showing how this allows us to solve most of the problems found by the two accounts if taken in isolation. The overall picture that emerges is that the cortical and the basal ganglia systems form two highly-organised hierarchical systems working in close synergy and jointly solving all the challenges of choice, selection, and implementation needed to acquire and express adaptive behaviour."
            },
            "slug": "The-Hierarchical-Organisation-of-Cortical-and-A-and-Baldassarre-Caligiore",
            "title": {
                "fragments": [],
                "text": "The Hierarchical Organisation of Cortical and Basal-Ganglia Systems: A Computationally-Informed Review and Integrated Hypothesis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The overall picture that emerges is that the cortical and the basal ganglia systems form two highly-organised hierarchical systems working in close synergy and jointly solving all the challenges of choice, selection, and implementation needed to acquire and express adaptive behaviour."
            },
            "venue": {
                "fragments": [],
                "text": "Computational and Robotic Models of the Hierarchical Organization of Behavior"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559335"
                        ],
                        "name": "B. Landau",
                        "slug": "B.-Landau",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Landau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Landau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836466"
                        ],
                        "name": "Linda B. Smith",
                        "slug": "Linda-B.-Smith",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Smith",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linda B. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48795139"
                        ],
                        "name": "Susan S. Jones",
                        "slug": "Susan-S.-Jones",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Jones",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan S. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205117480,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "14d1dea46e4ea90176b0a60bb57ecd8b8cf95268",
            "isKey": false,
            "numCitedBy": 1042,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-importance-of-shape-in-early-lexical-learning-Landau-Smith",
            "title": {
                "fragments": [],
                "text": "The importance of shape in early lexical learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12860852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions."
            },
            "slug": "Variational-Information-Maximisation-for-Motivated-Mohamed-Rezende",
            "title": {
                "fragments": [],
                "text": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper develops a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions on the problem of intrinsically-motivated learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145313556"
                        ],
                        "name": "D. McDermott",
                        "slug": "D.-McDermott",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McDermott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 153
                            }
                        ],
                        "text": "infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor, 1975; Fodor & Pylyshyn, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 263
                            }
                        ],
                        "text": "\u2026number of representations can be constructed from a finite set of primitives, just as the mind can think an\ninfinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor, 1975; Fodor & Pylyshyn, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26801195,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "15cad06c606ce6c4f88e186d4e3bbb78aca648e3",
            "isKey": false,
            "numCitedBy": 1244,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction The Language of Thought Hypothesis (LOTH) is a concept in cognitive science which describes mental activity in the brain as a form of language. The hypothesis was developed by Jerry Fodor in his book [1]. It states that the mind works with a language that is similar to regular languages, where an array of \u201cwords\u201d together with syntactic and semantic rules make up the meaning of sentences and that these constructs are processed much like in a computer[2]. The origins for this hypothesis are older than Fodor\u2019s book however. Leibnitz already postulated the existence of an interpretable language of thought, which he called lingua mentis [3]."
            },
            "slug": "LANGUAGE-OF-THOUGHT-McDermott",
            "title": {
                "fragments": [],
                "text": "LANGUAGE OF THOUGHT"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33696979"
                        ],
                        "name": "G. Murphy",
                        "slug": "G.-Murphy",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Murphy",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Murphy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16498299,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "1a73c63556f7a3a33b32ffd718a70d058634fdc4",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Recent theories of concepts have raised the issue of how people combine simple concepts (like engine and repair ) to form complex concepts (like engine repair ). This article approaches this issue by asking how people comprehend modified noun phrases of this sort. One explanation of how complex concepts are understood (the feature weighting model) provides a simple mechanism in which the primary feature of the modifying concept is made more salient in the modified concept. nother explanation focuses on how world knowledge directs the combination process. The two explanations are compared in their ability to account for the interpretation of various kinds of noun phrases. Two experiments are reported which evaluate the feature weighting model's predictions for adjective-noun phrases. These contrasts suggest that the combination process does require reference to world knowledge. The consequences of accepting such an account are discussed."
            },
            "slug": "Comprehending-Complex-Concepts-Murphy",
            "title": {
                "fragments": [],
                "text": "Comprehending Complex Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article approaches this issue by asking how people comprehend modified noun phrases of this sort, and suggests that the combination process does require reference to world knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110711666"
                        ],
                        "name": "T. Lee",
                        "slug": "T.-Lee",
                        "structuredName": {
                            "firstName": "Tai",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 5
                            }
                        ],
                        "text": "& De Leeuw, N. (1994) From things to processes: A theory"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "& Lee, S. A. (2012). Core systems of geometry in animal minds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 78
                            }
                        ],
                        "text": "However, lateral connections are largely ignored in current generative models (Lee 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16256430,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "b4b35938aec46ddad3df338df6d6f1f94beecb1c",
            "isKey": true,
            "numCitedBy": 21,
            "numCiting": 140,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian paradigm has provided a useful conceptual theory for understanding perceptual computation in the brain. While the detailed neural mechanisms of Bayesian inference are not fully understood, recent computational and neurophysiological works have illuminated the underlying computational principles and representational architecture. The fundamental insights are that the visual system is organized as a modular hierarchy to encode an internal model of the world, and that perception is realized by statistical inference based on such internal model. In this paper, we will discuss and analyze the varieties of representational schemes of these internal models and how they might be used to perform learning and inference. We will argue for a unified theoretical framework for relating the internal models to the observed neural phenomena and mechanisms in the visual cortex."
            },
            "slug": "The-Visual-System's-Internal-Model-of-the-World-Lee",
            "title": {
                "fragments": [],
                "text": "The Visual System's Internal Model of the World"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued for a unified theoretical framework for relating the internal models of the visual system to the observed neural phenomena and mechanisms in the visual cortex and how they might be used to perform learning and inference."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835039"
                        ],
                        "name": "E. Vul",
                        "slug": "E.-Vul",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Vul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353715"
                        ],
                        "name": "Adam N. Sanborn",
                        "slug": "Adam-N.-Sanborn",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Sanborn",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam N. Sanborn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15683431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87ea13516b32ec50f0884b6b99cc462b46dd4ab",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic models of cognition characterize the abstract computational problems underlying inductive inferences and identify their ideal solutions. This approach differs from traditional methods of investigating human cognition, which focus on identifying the cognitive or neural processes that underlie behavior and therefore concern alternative levels of analysis. To evaluate the theoretical implications of probabilistic models and increase their predictive power, we must understand the relationships between theories at these different levels of analysis. One strategy for bridging levels of analysis is to explore cognitive processes that have a direct link to probabilistic inference. Recent research employing this strategy has focused on the possibility that the Monte Carlo principle\u2014which concerns sampling from probability distributions in order to perform computations\u2014provides a way to link probabilistic models of cognition to more concrete cognitive and neural processes."
            },
            "slug": "Bridging-Levels-of-Analysis-for-Probabilistic-of-Griffiths-Vul",
            "title": {
                "fragments": [],
                "text": "Bridging Levels of Analysis for Probabilistic Models of Cognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work examines the possibility that the Monte Carlo principle provides a way to link probabilistic models of cognition to more concrete cognitive and neural processes and examines the relationships between theories at these different levels of analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2724259"
                        ],
                        "name": "L. Aitchison",
                        "slug": "L.-Aitchison",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Aitchison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Aitchison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599065"
                        ],
                        "name": "M. Lengyel",
                        "slug": "M.-Lengyel",
                        "structuredName": {
                            "firstName": "M\u00e1t\u00e9",
                            "lastName": "Lengyel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lengyel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 82
                            }
                        ],
                        "text": "Today, Anderson\u2019s ACT-R perhaps provides the most comprehensive simulation model (Anderson et al. 2004). A second road was taken by pioneers like Colby (1975) with PARRY, a computer program simulating a paranoid, Abelson and Carroll (1965) with their True Believer program, andWeizenbaum (1966) with his ELIZA non-directive psychotherapy program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3686147,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ac273902efc075869307f4ce03c022db9f4347c5",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic inference offers a principled framework for understanding both behaviour and cortical computation. However, two basic and ubiquitous properties of cortical responses seem difficult to reconcile with probabilistic inference: neural activity displays prominent oscillations in response to constant input, and large transient changes in response to stimulus onset. Indeed, cortical models of probabilistic inference have typically either concentrated on tuning curve or receptive field properties and remained agnostic as to the underlying circuit dynamics, or had simplistic dynamics that gave neither oscillations nor transients. Here we show that these dynamical behaviours may in fact be understood as hallmarks of the specific representation and algorithm that the cortex employs to perform probabilistic inference. We demonstrate that a particular family of probabilistic inference algorithms, Hamiltonian Monte Carlo (HMC), naturally maps onto the dynamics of excitatory-inhibitory neural networks. Specifically, we constructed a model of an excitatory-inhibitory circuit in primary visual cortex that performed HMC inference, and thus inherently gave rise to oscillations and transients. These oscillations were not mere epiphenomena but served an important functional role: speeding up inference by rapidly spanning a large volume of state space. Inference thus became an order of magnitude more efficient than in a non-oscillatory variant of the model. In addition, the network matched two specific properties of observed neural dynamics that would otherwise be difficult to account for using probabilistic inference. First, the frequency of oscillations as well as the magnitude of transients increased with the contrast of the image stimulus. Second, excitation and inhibition were balanced, and inhibition lagged excitation. These results suggest a new functional role for the separation of cortical populations into excitatory and inhibitory neurons, and for the neural oscillations that emerge in such excitatory-inhibitory networks: enhancing the efficiency of cortical computations."
            },
            "slug": "The-Hamiltonian-Brain:-Efficient-Probabilistic-with-Aitchison-Lengyel",
            "title": {
                "fragments": [],
                "text": "The Hamiltonian Brain: Efficient Probabilistic Inference with Excitatory-Inhibitory Neural Circuit Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new functional role is suggested for the separation of cortical populations into excitatory and inhibitory neurons, and for the neural oscillations that emerge in suchexcitatory-inhibitory networks: enhancing the efficiency of cortical computations."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1981188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "018300f5f0e679cee5241d9c69c8d88e00e8bf31",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset."
            },
            "slug": "Neural-Variational-Inference-and-Learning-in-Belief-Mnih-Gregor",
            "title": {
                "fragments": [],
                "text": "Neural Variational Inference and Learning in Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior and shows that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 149
                            }
                        ],
                        "text": "\u2026has shown that neural networks can behave as if they learned explicitly structured knowledge, such as a rule for producing the past tense of words (Rumelhart & McClelland, 1986), rules for solving simple balance-beam physics problems (McClelland, 1988), or a tree to represent types of living\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 130
                            }
                        ],
                        "text": "Parallel to these developments, a radically different approach was being explored based on neuronlike \u201csub-symbolic\u201d computations (e.g., Fukushima 1980; Grossberg 1976; Rosenblatt 1958)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Parallel to these developments, a radically different approach was being explored, based on neuron-like \u201csub-symbolic\u201d computations (e.g., Fukushima, 1980; Grossberg, 1976; Rosenblatt, 1958)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5623112,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "504c53c667fb50a322b5bb0192920918c0cf8435",
            "isKey": false,
            "numCitedBy": 737,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Part I of this paper describes a model for the parallel development and adult coding of neural feature detectors. It shows how any set of arbitrary spatial patterns can be recoded, or transformed, into any other spatial patterns (universal recoding), if there are sufficiently many cells in the network's cortex. This code is, however, unstable through time if arbitrarily many patterns can perturb a fixed number of cortical cells. This paper shows how to stabilize the code in the general case using feedback between cellular sites. A biochemically defined critical period is not necessary to stabilize the code, nor is it sufficient to ensure useful coding properties.We ask how short term memory can be reset in response to temporal sequences of spatial patterns. This leads to a context-dependent code in which no feature detector need uniquely characterize an input pattern; yet unique classification by the pattern of activity across feature detectors is possible. This property uses learned expectation mechanisms whereby unexpected patterns are temporarily suppressed and/or activate nonspecific arousal. The simplest case describes reciprocal interactions via trainable synaptic pathways (long term memory traces) between two recurrent on-center off-surround networks undergoing mass action (shunting) interactions. This unit can establish an adaptive resonance, or reverberation, between two regions if their coded patterns match, and can suppress the reverberation if their patterns do not match. This concept yields a model of olfactory coding within the olfactory bulb and prepyriform cortex. The resonance idea also includes the establishment of reverberation between conditioned reinforcers and generators of contingent negative variation if presently avialable sensory cues are compatible with the network's drive requirements at that time; and a search and lock mechanism whereby the disparity between two patterns can be minimized and the minimal disparity images locked into position. Stabilizing the code uses attentional mechanisms, in particular nonspecific arousal as a tuning and search device. We suggest that arousal is gated by a chemical transmitter system\u2014for example, norepinephrine\u2014whose relative states of accumulation at antagonistic pairs of on-cells and off-cells through time can shift the spatial pattern of STM activity across a field of feature detectors. For example, a sudden arousal increment in response to an un-expected pattern can reverse, or rebound, these relative activities, thereby suppressing incorrectly classified populations. The rebound mechanism has formal properties analogous to negative afterimages and spatial frequency adaptation."
            },
            "slug": "Adaptive-pattern-classification-and-universal-II.-Grossberg",
            "title": {
                "fragments": [],
                "text": "Adaptive pattern classification and universal recoding: II. Feedback, expectation, olfaction, illusions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is suggested that arousal is gated by a chemical transmitter system\u2014for example, norepinephrine\u2014whose relative states of accumulation at antagonistic pairs of on-cells and off-cells through time can shift the spatial pattern of STM activity across a field of feature detectors."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290695"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Watkins",
                            "middleNames": [
                                "J.",
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 208910339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
            "isKey": false,
            "numCitedBy": 7188,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one."
            },
            "slug": "Q-learning-Watkins-Dayan",
            "title": {
                "fragments": [],
                "text": "Q-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989), showing that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action- values are represented discretely."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 123
                            }
                        ],
                        "text": "Benchmark tasks such as the ImageNet data set for object recognition provides hundreds or thousands of examples per class (Krizhevsky et al., 2012; Russakovsky et al., 2015) \u2013 1000 hairbrushes, 1000 pineapples, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 23
                            }
                        ],
                        "text": "In object recognition, Krizhevsky, Sutskever, and Hinton (2012) trained a deep convolutional neural network (convnets; LeCun et al., 1989) that nearly halved the error rate of the previous state-of-the-art on the most challenging benchmark to date."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 124
                            }
                        ],
                        "text": "Training large-scale relatively generic networks is also the best current approach for object recognition (He et al., 2015; Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2014), where the high-level feature representations of these convolutional nets have also been used to\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 80947,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721210"
                        ],
                        "name": "G. Indiveri",
                        "slug": "G.-Indiveri",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Indiveri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Indiveri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704961"
                        ],
                        "name": "Shih-Chii Liu",
                        "slug": "Shih-Chii-Liu",
                        "structuredName": {
                            "firstName": "Shih-Chii",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Chii Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13984215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3497ea4c3f7c51bd6a242eafc0f88c7d5d5a9ac",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "A striking difference between brain-inspired neuromorphic processors and current von Neumann processor architectures is the way in which memory and processing is organized. As information and communication technologies continue to address the need for increased computational power through the increase of cores within a digital processor, neuromorphic engineers and scientists can complement this need by building processor architectures where memory is distributed with the processing. In this paper, we present a survey of brain-inspired processor architectures that support models of cortical networks and deep neural networks. These architectures range from serial clocked implementations of multineuron systems to massively parallel asynchronous ones and from purely digital systems to mixed analog/digital systems which implement more biological-like models of neurons and synapses together with a suite of adaptation and learning mechanisms analogous to the ones found in biological nervous systems. We describe the advantages of the different approaches being pursued and present the challenges that need to be addressed for building artificial neural processing systems that can display the richness of behaviors seen in biological systems."
            },
            "slug": "Memory-and-Information-Processing-in-Neuromorphic-Indiveri-Liu",
            "title": {
                "fragments": [],
                "text": "Memory and Information Processing in Neuromorphic Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A survey of brain-inspired processor architectures that support models of cortical networks and deep neural networks is presented and the advantages and challenges that need to be addressed for building artificial neural processing systems that can display the richness of behaviors seen in biological systems are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401804750"
                        ],
                        "name": "David Lopez-Paz",
                        "slug": "David-Lopez-Paz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lopez-Paz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Lopez-Paz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276351"
                        ],
                        "name": "Krikamol Muandet",
                        "slug": "Krikamol-Muandet",
                        "structuredName": {
                            "firstName": "Krikamol",
                            "lastName": "Muandet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krikamol Muandet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3121264"
                        ],
                        "name": "I. Tolstikhin",
                        "slug": "I.-Tolstikhin",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Tolstikhin",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Tolstikhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14337806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ed00111447ddb204c8db9fc01ba7122085470c7",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection $\\{(S_i,l_i)\\}_{i=1}^n$, where each $S_i$ is a sample drawn from the probability distribution of $X_i \\times Y_i$, and $l_i$ is a binary label indicating whether \"$X_i \\to Y_i$\" or \"$X_i \\leftarrow Y_i$\". Given these data, we build a causal inference rule in two steps. First, we featurize each $S_i$ using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables."
            },
            "slug": "Towards-a-Learning-Theory-of-Cause-Effect-Inference-Lopez-Paz-Muandet",
            "title": {
                "fragments": [],
                "text": "Towards a Learning Theory of Cause-Effect Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work poses causal inference as the problem of learning to classify probability distributions, and extends the ideas to infer causal relationships between more than two variables."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40152437"
                        ],
                        "name": "A. Schlottmann",
                        "slug": "A.-Schlottmann",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Schlottmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schlottmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48697562"
                        ],
                        "name": "Elizabeth D Ray",
                        "slug": "Elizabeth-D-Ray",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Ray",
                            "middleNames": [
                                "D"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth D Ray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064711128"
                        ],
                        "name": "A. Mitchell",
                        "slug": "A.-Mitchell",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48993307"
                        ],
                        "name": "N. Demetriou",
                        "slug": "N.-Demetriou",
                        "structuredName": {
                            "firstName": "Nathalie",
                            "lastName": "Demetriou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Demetriou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9493575,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3e9b3caf8f5e2167eb9f59d3fbd05b0bd7696582",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perceived-physical-and-social-causality-in-animated-Schlottmann-Ray",
            "title": {
                "fragments": [],
                "text": "Perceived physical and social causality in animated motions: spontaneous reports and ratings."
            },
            "venue": {
                "fragments": [],
                "text": "Acta psychologica"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979826"
                        ],
                        "name": "M. Keramati",
                        "slug": "M.-Keramati",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Keramati",
                            "middleNames": [
                                "Mahdi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Keramati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145587089"
                        ],
                        "name": "A. Dezfouli",
                        "slug": "A.-Dezfouli",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Dezfouli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dezfouli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1883990"
                        ],
                        "name": "Payam Piray",
                        "slug": "Payam-Piray",
                        "structuredName": {
                            "firstName": "Payam",
                            "lastName": "Piray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Payam Piray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10350247,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "c268b5d0ba92192597b82c7166eaf40d0a2f40c5",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "Instrumental responses are hypothesized to be of two kinds: habitual and goal-directed, mediated by the sensorimotor and the associative cortico-basal ganglia circuits, respectively. The existence of the two heterogeneous associative learning mechanisms can be hypothesized to arise from the comparative advantages that they have at different stages of learning. In this paper, we assume that the goal-directed system is behaviourally flexible, but slow in choice selection. The habitual system, in contrast, is fast in responding, but inflexible in adapting its behavioural strategy to new conditions. Based on these assumptions and using the computational theory of reinforcement learning, we propose a normative model for arbitration between the two processes that makes an approximately optimal balance between search-time and accuracy in decision making. Behaviourally, the model can explain experimental evidence on behavioural sensitivity to outcome at the early stages of learning, but insensitivity at the later stages. It also explains that when two choices with equal incentive values are available concurrently, the behaviour remains outcome-sensitive, even after extensive training. Moreover, the model can explain choice reaction time variations during the course of learning, as well as the experimental observation that as the number of choices increases, the reaction time also increases. Neurobiologically, by assuming that phasic and tonic activities of midbrain dopamine neurons carry the reward prediction error and the average reward signals used by the model, respectively, the model predicts that whereas phasic dopamine indirectly affects behaviour through reinforcing stimulus-response associations, tonic dopamine can directly affect behaviour through manipulating the competition between the habitual and the goal-directed systems and thus, affect reaction time."
            },
            "slug": "Speed/Accuracy-Trade-Off-between-the-Habitual-and-Keramati-Dezfouli",
            "title": {
                "fragments": [],
                "text": "Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A normative model for arbitration between the two processes that makes an approximately optimal balance between search-time and accuracy in decision making is proposed and can explain experimental evidence on behavioural sensitivity to outcome at the early stages of learning, but insensitivity at the later stages."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29482,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46272176"
                        ],
                        "name": "M. Rispoli",
                        "slug": "M.-Rispoli",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Rispoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rispoli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2075425,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f8ab1aa15d7ba0cea96b1e8c9b7f91c4f808788b",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Review essay on: ELMAN, J., BATES, E., JOHNSON, M., KARMILOFF-SMITH, A., PARISI, D. & PLUNKETT, K. Rethinking innateness: a connectionist perspective on development. Cambridge, MA: MIT Press (1996). Pp. 447. I believe that the field of developmental psycholinguistics suffers from two major weaknesses. The first is its impressionistic and inexact formulations. The second is its divisive polarizations. One can see the reasons for the first weakness. Developmental psycholinguistics is only about 30 years old (ignoring diary studies which preceded the linguistic and cognitive surge of the sixties). But speculation and hypothesizing on the basis of relatively little data and passing acquaintance with phenomena has reached the level of customary \u2018business as usual\u2019. We are skilful at hypothesis construction, yet we are regretfully delinquent at formulating clear tests of our hypotheses. We are fond of conjecture about causal relationships, but our empirical tests progress no further than weak forms of correlation. With regard to our second major weakness, our knack for polarizing opinion regarding chimerical questions such as the innateness of language can also be understood. After all, are we not following the classical dialectic model of thesis, antithesis and eventual synthesis? I think this is an idealized view of ourselves. In fact, we are driven by hunch and bias far more often than we would like to admit. Following hunches may be a real sign of creativity and vitality in our thinking. However, polarization driven by biases is ultimately detrimental. At some point we must disentangle ourselves from customary dialogue and transcend our deeply rutted patterns of thought. When I began Rethinking innateness, I had hoped that the book might help us overcome these two weaknesses. I believe that it contributes positively to the goal of increasing the precision of our hypotheses and their empirical substantiation. At the same time, I am afraid that it will have a negative impact by aggravating the degree of polarization in our field."
            },
            "slug": "Rethinking-innateness-Rispoli",
            "title": {
                "fragments": [],
                "text": "Rethinking innateness."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Rethinking innateness contributes positively to the goal of increasing the precision of the authors' hypotheses and their empirical substantiation, but I am afraid that it will have a negative impact by aggravating the degree of polarization in their field."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of child language"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144366429"
                        ],
                        "name": "S. Carey",
                        "slug": "S.-Carey",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Carey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50098039"
                        ],
                        "name": "E. Bartlett",
                        "slug": "E.-Bartlett",
                        "structuredName": {
                            "firstName": "Elsa",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Jaffe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026can make meaningful generalizations from very sparse data, especially in the context of learning the meanings of words in their native language (Carey & Bartlett, 1978; Landau, Smith, & Jones, 1988; E. M. Markman, 1989; Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002; F. Xu &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 50145091,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "473f114220bf0004ffbf9d9cac70b36abdb1748f",
            "isKey": false,
            "numCitedBy": 954,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "By the time a child has learned a new word, he or she has gained many distinct kinds of information. To take a hypothetical example, consider the word \"wolf\" being learned by a child who already has a modest animal vocabulary. She must make a new lexical entry: she must note that \"wolf\" is an English word. She must learn its syntactic subcategorization, namely that it is a common noun. She must relate it to other English words, to its supernyms (such as \"animal\") and hyponyms (such as \"Siberian wolf\") and other words in the same lexical domain. She must also learn what \"wolf\" refers to_ And she must restructure the conceptual domain of animals, at least with respect to how they are named. Suppose, for example, that wolves were previously called \"dog.\" Then learning a new word may be the occasion for learning a new concept, for differentiating dogs from wolves. At the very least, it is the occasion for learning that wolves have a different name from dogs. Clearly, then, learning even a single new word involves learning a great deal of information. In the past, word learning has been studied in several different ways-there have been vocabulary counts, diary studies, cross-sectional and longitudinal studies of the acquisition of organized lexical domains such as size, color or quantity. But in all of these paradigms, there is no control over the input to the child. It is not known how much exposure he or she has had to any given word, nor in what contexts. Thus, while much can be learned about word learning from these procedures, they are not ideal for achieving an understanding of the process itself. Surprisingly, the obvious technique of teaching children an unknown word has been little used (see Carey, 1978 for a review). None of these was designed to mimic the circumstances in which children naturally encounter new words and none was designed to probe for partial acquisition along the way. The present study is an attempt to fill this gap. The experimental procedures that we ultimately designed were intended to provide a strong test of the child's word learning skills. For us, that meant several things. First, we wanted to present the new word in a situation that would approximate a child's everyday word-learning experience at its most casual and undirected level. This meant using a situation in which there was not \u2026"
            },
            "slug": "Acquiring-a-Single-New-Word-Carey-Bartlett",
            "title": {
                "fragments": [],
                "text": "Acquiring a Single New Word"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The present study wanted to present the new word in a situation that would approximate a child's everyday word-learning experience at its most casual and undirected level, and intended to provide a strong test of the child's word learning skills."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 111
                            }
                        ],
                        "text": "While techniques for handling variable sized inputs in convnets may help for playing on different board sizes (Sermanet et al., 2014), the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people do."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Could AlphaGo? While techniques for handling variable sized inputs in convnets may help for playing on different board sizes (Sermanet et al., 2014), the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people do."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1109b663453e78a59e4f66446d71720ac58cec25",
            "isKey": false,
            "numCitedBy": 4353,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2440249"
                        ],
                        "name": "Pedro Tsividis",
                        "slug": "Pedro-Tsividis",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Tsividis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro Tsividis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18693099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6929b8700adc7a7a499266282c77d775f9f0e5f5",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Information Selection in Noisy Environments with Large Action Spaces Pedro Tsividis (tsividis@mit.edu), Samuel J. Gershman (sjgershm@mit.edu), Joshua B. Tenenbaum (jbt@mit.edu), Laura Schulz (lshulz@mit.edu) Massachusetts Institute of Technology, Department of Brain and Cognitive Sciences, 77 Massachusetts Ave., Cambridge, MA 02139 USA Abstract A critical aspect of human cognition is the ability to effec- tively query the environment for information. The \u2018real\u2019 world is large and noisy, and therefore designing effec- tive queries involves prioritizing both scope \u2013 the range of hypotheses addressed by the query \u2013 and reliability \u2013 the likelihood of obtaining a correct answer. Here we de- signed a simple information-search game in which partic- ipants had to select an informative query from a large set of queries, trading off scope and reliability. We find that adults are effective information-searchers even in large, noisy environments, and that their information search is best explained by a model that balances scope and re- liability by selecting queries proportionately to their ex- pected information gain. Keywords: exploration; information search; active learn- ing; information gain. Introduction As scientists, we sometimes encounter (or conduct) experimental work that is stunning in its breadth but dis- appointing in its rigor, or work that is categorically deci- sive but disappointingly narrow in scope. As child or adult intuitive scientists searching for information, we often deal with these epistemic virtues \u2013 scope and rigor \u2013 as well; we can make general queries that drastically narrow the hypothesis space of answers or make nar- rower ones, and we can seek information from reliable sources that are more likely to give us correct answers, or from sources that are less so. Our success, whether as professional or intuitive scientists, hinges on our abil- ity to balance these two dimensions in order to produce queries whose answers will be informative. Early work on information search seemed to show that people fail to make rational decisions when it comes to information acquisition; in the Wason (1968) selection task, with a fairly small (12) action set, only 4% of sub- jects made the normative information-acquisition selec- tion. However, as Oaksford & Chater (1994) pointed out, the selection made most often by participants in the original Wason task was normative when environmental statistics were taken into account. Specifically, partici- pants\u2019 decisions were best explained as maximizing ex- pected information gain in the service of helping them to decide between competing hypotheses. More recent work on information search has shown that children have strong intuitions about questions\u2019 use- fulness and search adaptively (Nelson et al., 2013) and that adults can value information over explicit reward when the two are put in opposition (Markant & Gureckis, Our interest is in whether these trends persist when people are confronted with the large, noisy information spaces characteristic of the real world. As we explore the world and its affordances, we must select queries that have scope, in that they rule out large numbers of hy- potheses at once. And, inasmuch as we can help it, we should minimize noise by querying reliable sources. In the real world, these are often in opposition. So, we ask: how do people trade off scope and reliability when ex- ploring large, noisy information spaces? And when the potential questions are many, do they ask the right ones? Information-search task To begin to answer these questions, we designed a novel information-search task. Our goal was for it to be as simple as possible, while having as many features approaching natural exploration as possible. Thus we paired a very simple game \u2013 identifying a hidden number on a number line \u2013 with a relatively complex search pro- cedure. In playing the game, participants make queries that vary in the abstract features of scope and reliabil- ity. Additionally, at each point in the game, partici- pants are faced with a very large number of potential specific queries to choose from. Our task is similar to the Markant & Gureckis (2012) task in that it involves exploring a geometric space to test particular hypothe- ses, but we wanted to make explicit the abstract features of questions, rather than have these be implicit as a func- tion of the hypotheses at hand. Participants play by asking \u2018questions\u2019 about the hid- den number\u2019s location, using \u2018scanners\u2019 that turn blue if the number is under the scanned region and red if the number is not. In each trial, a participant is given four scanners (Fig. 1). In some conditions, the scanners vary in size. Larger scanners can cover larger regions of the number line, ruling out (or in) a larger set of hypotheses than a smaller scanner. Thus, in the context of this study, the \u2018scope\u2019 is directly related to the length of the scan- ner. However, the scanners are not deterministic; they also vary in their reliability, which is the probability of providing an accurate signal about the presence of the hidden number (false positives and false negatives are equally likely). To efficiently find the hidden number, participants have to select scanners that provide a good trade-off be-"
            },
            "slug": "Information-Selection-in-Noisy-Environments-with-Tsividis-Gershman",
            "title": {
                "fragments": [],
                "text": "Information Selection in Noisy Environments with Large Action Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that adults are effective information-searchers even in large, noisy environments, and that their information search is best explained by a model that balances scope and re- liability by selecting queries proportionately to their ex- pected information gain."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657032"
                        ],
                        "name": "M. Asada",
                        "slug": "M.-Asada",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Asada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809835"
                        ],
                        "name": "K. Hosoda",
                        "slug": "K.-Hosoda",
                        "structuredName": {
                            "firstName": "Koh",
                            "lastName": "Hosoda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hosoda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744602"
                        ],
                        "name": "Y. Kuniyoshi",
                        "slug": "Y.-Kuniyoshi",
                        "structuredName": {
                            "firstName": "Yasuo",
                            "lastName": "Kuniyoshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kuniyoshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687808"
                        ],
                        "name": "H. Ishiguro",
                        "slug": "H.-Ishiguro",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Ishiguro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishiguro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37620637"
                        ],
                        "name": "T. Inui",
                        "slug": "T.-Inui",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Inui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Inui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836701"
                        ],
                        "name": "Y. Yoshikawa",
                        "slug": "Y.-Yoshikawa",
                        "structuredName": {
                            "firstName": "Yuichiro",
                            "lastName": "Yoshikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yoshikawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49278671"
                        ],
                        "name": "M. Ogino",
                        "slug": "M.-Ogino",
                        "structuredName": {
                            "firstName": "Masaki",
                            "lastName": "Ogino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ogino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067926361"
                        ],
                        "name": "C. Yoshida",
                        "slug": "C.-Yoshida",
                        "structuredName": {
                            "firstName": "Chisato",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yoshida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 75
                            }
                        ],
                        "text": "In the two last decades, the field of Developmental and Cognitive Robotics (Asada et al. 2009; Cangelosi and Schlesinger 2015), in strong interaction with developmental psychology and neuroscience, has achieved significant advances in computational modeling of mechanisms of autonomous development and learning in human infants, and applied them to solve difficult artificial intelligence (AI) problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10168773,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f2cee33617aff0effaf7066f7d82bf6096c44486",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 241,
            "paperAbstract": {
                "fragments": [],
                "text": "Cognitive developmental robotics (CDR) aims to provide new understanding of how human's higher cognitive functions develop by means of a synthetic approach that developmentally constructs cognitive functions. The core idea of CDR is ldquophysical embodimentrdquo that enables information structuring through interactions with the environment, including other agents. The idea is shaped based on the hypothesized development model of human cognitive functions from body representation to social behavior. Along with the model, studies of CDR and related works are introduced, and discussion on the model and future issues are argued."
            },
            "slug": "Cognitive-Developmental-Robotics:-A-Survey-Asada-Hosoda",
            "title": {
                "fragments": [],
                "text": "Cognitive Developmental Robotics: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Cognitive developmental robotics aims to provide new understanding of how human's higher cognitive functions develop by means of a synthetic approach that developmentally constructs cognitive functions through interactions with the environment, including other agents."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Autonomous Mental Development"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716847"
                        ],
                        "name": "G. Baldassarre",
                        "slug": "G.-Baldassarre",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Baldassarre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baldassarre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 2
                            }
                        ],
                        "text": "& Baldassarre, G. (2015) Selection of cortical dynamics for motor"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 5
                            }
                        ],
                        "text": "[GB] Baldassarre, G. & Mirolli, M., eds. (2013) Intrinsically motivated learning in natural"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 351
                            }
                        ],
                        "text": "2013b; 2014, for collections of works) we believe that humanlevel intelligence can be achieved only through open-ended learning, that is, the cumulative learning of progressively more complex skills and knowledge, driven by intrinsic motivations, which are motivations related to the acquisition of knowledge and skills rather than material resources (Baldassarre 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20261086,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fc50bef8feb5d8a9c2376bb7c3b9e41f415da45e",
            "isKey": true,
            "numCitedBy": 88,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of \u201cintrinsic motivation\u201d, initially proposed and developed within psychology, is gaining an increasing attention within cognitive sciences for its potential to produce open-ended learning machines and robots. However, a clear definition of the phenomenon is not yet available. This theoretical paper aims to clarify what intrinsic motivations are from a biological perspective. To this purpose, it first shows how intrinsic motivations can be defined contrasting them to extrinsic motivations from an evolutionary perspective: whereas extrinsic motivations guide learning of behaviours that directly increase fitness, intrinsic motivations drive the acquisition of knowledge and skills that contribute to produce behaviours that increase fitness only in a later stage. Given this difference, extrinsic motivations generate learning signals on the basis of events involving body homeostatic regulations, whereas intrinsic motivations generate learning signals based on events taking place within the brain itself. These ideas are supported by presenting some examples of biological mechanisms underlying the two types of motivations. The paper closes by linking the theory to the current major computational views on intrinsic motivations and by listing the main open issues of the field."
            },
            "slug": "What-are-intrinsic-motivations-A-biological-Baldassarre",
            "title": {
                "fragments": [],
                "text": "What are intrinsic motivations? A biological perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This theoretical paper shows how intrinsic motivations can be defined contrasting them to extrinsic motivations from an evolutionary perspective, and links the theory to the current major computational views on intrinsic motivations and by listing the main open issues of the field."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Development and Learning (ICDL)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48495381"
                        ],
                        "name": "C. Cook",
                        "slug": "C.-Cook",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cook",
                            "middleNames": [
                                "Kehrwald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19022317,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "87cce3752a730403975cb9f6c0e9521d58494d39",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Where-science-starts:-Spontaneous-experiments-in-Cook-Goodman",
            "title": {
                "fragments": [],
                "text": "Where science starts: Spontaneous experiments in preschoolers\u2019 exploratory play"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40152437"
                        ],
                        "name": "A. Schlottmann",
                        "slug": "A.-Schlottmann",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Schlottmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schlottmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39859676"
                        ],
                        "name": "Katy Cole",
                        "slug": "Katy-Cole",
                        "structuredName": {
                            "firstName": "Katy",
                            "lastName": "Cole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katy Cole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152299215"
                        ],
                        "name": "R. Watts",
                        "slug": "R.-Watts",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Watts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Watts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48312636"
                        ],
                        "name": "Marina White",
                        "slug": "Marina-White",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "White",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marina White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15588605,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c19b15332d69de0a368a8fa10ce7f383baff8015",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans, even babies, perceive causality when one shape moves briefly and linearly after another. Motion timing is crucial in this and causal impressions disappear with short delays between motions. However, the role of temporal information is more complex: it is both a cue to causality and a factor that constrains processing. It affects ability to distinguish causality from non-causality, and social from mechanical causality. Here we study both issues with 3- to 7-year-olds and adults who saw two computer-animated squares and chose if a picture of mechanical, social or non-causality fit each event best. Prior work fit with the standard view that early in development, the distinction between the social and physical domains depends mainly on whether or not the agents make contact, and that this reflects concern with domain-specific motion onset, in particular, whether the motion is self-initiated or not. The present experiments challenge both parts of this position. In Experiments 1 and 2, we showed that not just spatial, but also animacy and temporal information affect how children distinguish between physical and social causality. In Experiments 3 and 4 we showed that children do not seem to use spatio-temporal information in perceptual causality to make inferences about self- or other-initiated motion onset. Overall, spatial contact may be developmentally primary in domain-specific perceptual causality in that it is processed easily and is dominant over competing cues, but it is not the only cue used early on and it is not used to infer motion onset. Instead, domain-specific causal impressions may be automatic reactions to specific perceptual configurations, with a complex role for temporal information."
            },
            "slug": "Domain-specific-perceptual-causality-in-children-on-Schlottmann-Cole",
            "title": {
                "fragments": [],
                "text": "Domain-specific perceptual causality in children depends on the spatio-temporal configuration, not motion onset"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work showed that not just spatial, but also animacy and temporal information affect how children distinguish between physical and social causality, and showed that children do not seem to use spatio-temporal information in perceptual causality to make inferences about self- or other-initiated motion onset."
            },
            "venue": {
                "fragments": [],
                "text": "Front. Psychol."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704065"
                        ],
                        "name": "D. Gentner",
                        "slug": "D.-Gentner",
                        "structuredName": {
                            "firstName": "Dedre",
                            "lastName": "Gentner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gentner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": "& Gentner, D. (2017) Extending SME to handle large-scale cognitive modeling."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15834560,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "78cf27f4b8ba8a4e0fa891b646e8fea809a2a7b8",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "Human cognition is striking in its brilliance and its adaptability. How do we get that way? How do we move from the nearly helpless state of infants to the cognitive proficiency that characterizes adults? In this paper I argue, first, that analogical ability is the key factor in our prodigious capacity, and, second, that possession of a symbol system is crucial to the full expression of analogical ability."
            },
            "slug": "Bootstrapping-the-Mind:-Analogical-Processes-and-Gentner",
            "title": {
                "fragments": [],
                "text": "Bootstrapping the Mind: Analogical Processes and Symbol Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This paper argues, first, that analogical ability is the key factor in the authors' prodigious capacity, and, second, that possession of a symbol system is crucial to the full expression of analogICAL ability."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388372395"
                        ],
                        "name": "Finale Doshi-Velez",
                        "slug": "Finale-Doshi-Velez",
                        "structuredName": {
                            "firstName": "Finale",
                            "lastName": "Doshi-Velez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Finale Doshi-Velez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3351164"
                        ],
                        "name": "Been Kim",
                        "slug": "Been-Kim",
                        "structuredName": {
                            "firstName": "Been",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Been Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17244588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f2d2d14f0c07e4b3d9d636d904afa7087673b62",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "From autonomous cars and adaptive email-filters to predictive policing systems, machine learning (ML) systems are increasingly ubiquitous; they outperform humans on specific tasks [Mnih et al., 2013, Silver et al., 2016, Hamill, 2017] and often guide processes of human understanding and decisions [Carton et al., 2016, Doshi-Velez et al., 2014]. The deployment of ML systems in complex applications has led to a surge of interest in systems optimized not only for expected task performance but also other important criteria such as safety [Otte, 2013, Amodei et al., 2016, Varshney and Alemzadeh, 2016], nondiscrimination [Bostrom and Yudkowsky, 2014, Ruggieri et al., 2010, Hardt et al., 2016], avoiding technical debt [Sculley et al., 2015], or providing the right to explanation [Goodman and Flaxman, 2016]. For ML systems to be used safely, satisfying these auxiliary criteria is critical. However, unlike measures of performance such as accuracy, these criteria often cannot be completely quantified. For example, we might not be able to enumerate all unit tests required for the safe operation of a semi-autonomous car or all confounds that might cause a credit scoring system to be discriminatory. In such cases, a popular fallback is the criterion of interpretability : if the system can explain its reasoning, we then can verify whether that reasoning is sound with respect to these auxiliary criteria. Unfortunately, there is little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking. Current interpretability evaluation typically falls into two categories. The first evaluates interpretability in the context of an application: if the system is useful in either a practical application or a simplified version of it, then it must be somehow interpretable (e.g. Ribeiro et al. [2016], Lei et al. [2016], Kim et al. [2015a], Doshi-Velez et al. [2015], Kim et al. [2015b]). The second evaluates interpretability via a quantifiable proxy: a researcher might first claim that some model class\u2014e.g. sparse linear models, rule lists, gradient boosted trees\u2014are interpretable and then present algorithms to optimize within that class (e.g. Bucilu et al. [2006], Wang et al. [2017], Wang and Rudin [2015], Lou et al. [2012]). To large extent, both evaluation approaches rely on some notion of \u201cyou\u2019ll know it when you see it.\u201d Should we be concerned about a lack of rigor? Yes and no: the notions of interpretability above appear reasonable because they are reasonable: they meet the first test of having facevalidity on the correct test set of subjects: human beings. However, this basic notion leaves many kinds of questions unanswerable: Are all models in all defined-to-be-interpretable model classes equally interpretable? Quantifiable proxies such as sparsity may seem to allow for comparison, but how does one think about comparing a model sparse in features to a model sparse in prototypes? Moreover, do all applications have the same interpretability needs? If we are to move this field forward\u2014to compare methods and understand when methods may generalize\u2014we need to formalize these notions and make them evidence-based. The objective of this review is to chart a path toward the definition and rigorous evaluation of interpretability. The need is urgent: recent European Union regulation will require algorithms"
            },
            "slug": "A-Roadmap-for-a-Rigorous-Science-of-Doshi-Velez-Kim",
            "title": {
                "fragments": [],
                "text": "A Roadmap for a Rigorous Science of Interpretability"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "There is little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking, and the objective of this review is to chart a path toward the definition and rigorous evaluation of interpretability."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 0
                            }
                        ],
                        "text": "(Elman, 2005; Elman et al., 1996), but a simple inductive bias, for example the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of agency"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 109
                            }
                        ],
                        "text": "Connectionists have argued that innate constraints in the form of hard-wired cortical circuits are unlikely (Elman, 2005; Elman et al., 1996), but a simple inductive bias, for example the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15997449,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d67b95030c9a9a114d06905c56114533db39dbb6",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-models-of-cognitive-development:-next-Elman",
            "title": {
                "fragments": [],
                "text": "Connectionist models of cognitive development: where next?"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725157"
                        ],
                        "name": "T. Schaul",
                        "slug": "T.-Schaul",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Schaul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schaul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34660073"
                        ],
                        "name": "John Quan",
                        "slug": "John-Quan",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Quan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Quan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460849"
                        ],
                        "name": "Ioannis Antonoglou",
                        "slug": "Ioannis-Antonoglou",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Antonoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Antonoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 160
                            }
                        ],
                        "text": "\u2026et al., 2016; van Hasselt, Guez, & Silver, 2016; Wang et al., 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al., 2015) and 96% by using smarter replay and more efficient parameter sharing (Wang et al., 2016) (see DQN+ and DQN++ in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "More recent variants of the DQN have demonstrated superior performance (Schaul et al., 2015; Stadie et al., 2016; van Hasselt, Guez, & Silver, 2016), and the current best network with smarter replay and other improvements now achieves about 83% of the professional gamer\u2019s score (Schaul et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2016; van Hasselt, Guez, & Silver, 2016), and the current best network with smarter replay and other improvements now achieves about 83% of the professional gamer\u2019s score (Schaul et al., 2015) (see DQN+ in Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 95
                            }
                        ],
                        "text": "3).3 But they requires a lot of experience to reach this level: the learning curve provided in Schaul et al. (2015) shows performance is around 46% after 231 hours, 19% after 116 hours, and below 3.5% after just 2 hours (which is close to random play, approximately 1.5%)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "More recent variants of the DQN have demonstrated superior performance (Schaul et al., 2015; Stadie et al., 2016; van Hasselt, Guez, & Silver, 2016; Wang et al., 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al., 2015) and 96% by using\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13022595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
            "isKey": true,
            "numCitedBy": 2117,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games."
            },
            "slug": "Prioritized-Experience-Replay-Schaul-Quan",
            "title": {
                "fragments": [],
                "text": "Prioritized Experience Replay"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently, in Deep Q-Networks, a reinforcement learning algorithm that achieved human-level performance across many Atari games."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401845875"
                        ],
                        "name": "Rub\u00e9n Moreno-Bote",
                        "slug": "Rub\u00e9n-Moreno-Bote",
                        "structuredName": {
                            "firstName": "Rub\u00e9n",
                            "lastName": "Moreno-Bote",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rub\u00e9n Moreno-Bote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061773"
                        ],
                        "name": "D. Knill",
                        "slug": "D.-Knill",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Knill",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Knill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469356"
                        ],
                        "name": "A. Pouget",
                        "slug": "A.-Pouget",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Pouget",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pouget"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9302714,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cf34008aad30c9e7a6eee784a67dc04fcfec8467",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well-established that some aspects of perception and action can be understood as probabilistic inferences over underlying probability distributions. In some situations, it would be advantageous for the nervous system to sample interpretations from a probability distribution rather than commit to a particular interpretation. In this study, we asked whether visual percepts correspond to samples from the probability distribution over image interpretations, a form of sampling that we refer to as Bayesian sampling. To test this idea, we manipulated pairs of sensory cues in a bistable display consisting of two superimposed moving drifting gratings, and we asked subjects to report their perceived changes in depth ordering. We report that the fractions of dominance of each percept follow the multiplicative rule predicted by Bayesian sampling. Furthermore, we show that attractor neural networks can sample probability distributions if input currents add linearly and encode probability distributions with probabilistic population codes."
            },
            "slug": "Bayesian-sampling-in-visual-perception-Moreno-Bote-Knill",
            "title": {
                "fragments": [],
                "text": "Bayesian sampling in visual perception"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This study asks whether visual percepts correspond to samples from the probability distribution over image interpretations, a form of sampling that is referred to as Bayesian sampling, and shows that attractor neural networks can sample probability distributions if input currents add linearly and encode probability distributions with probabilistic population codes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145350701"
                        ],
                        "name": "Chao Weng",
                        "slug": "Chao-Weng",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Weng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746678"
                        ],
                        "name": "Shinji Watanabe",
                        "slug": "Shinji-Watanabe",
                        "structuredName": {
                            "firstName": "Shinji",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinji Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14635987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99c4007b1f6cb905788479db7fc886168f05e57c",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we propose recurrent deep neural networks (DNNs) for robust automatic speech recognition (ASR). Full recurrent connections are added to certain hidden layer of a conventional feedforward DNN and allow the model to capture the temporal dependency in deep representations. A new backpropagation through time (BPTT) algorithm is introduced to make the minibatch stochastic gradient descent (SGD) on the proposed recurrent DNNs more efficient and effective. We evaluate the proposed recurrent DNN architecture under the hybrid setup on both the 2nd CHiME challenge (track 2) and Aurora-4 tasks. Experimental results on the CHiME challenge data show that the proposed system can obtain consistent 7% relative WER improvements over the DNN systems, achieving state-of-the-art performance without front-end preprocessing, speaker adaptive training or multiple decoding passes. For the experiments on Aurora-4, the proposed system achieves 4% relative WER improvement over a strong DNN baseline system."
            },
            "slug": "Recurrent-deep-neural-networks-for-robust-speech-Weng-Yu",
            "title": {
                "fragments": [],
                "text": "Recurrent deep neural networks for robust speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Full recurrent connections are added to certain hidden layer of a conventional feedforward DNN and allow the model to capture the temporal dependency in deep representations to achieve state-of-the-art performance without front-end preprocessing, speaker adaptive training or multiple decoding passes."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2800360"
                        ],
                        "name": "Alan Jern",
                        "slug": "Alan-Jern",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Jern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Jern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11716857,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "d3452b0a8dc6f362389ca409c956f8472bd64678",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-decision-network-account-of-reasoning-about-other-Jern-Kemp",
            "title": {
                "fragments": [],
                "text": "A decision network account of reasoning about other people\u2019s choices"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational optimization (Gregor et al., 2015; A. Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In each case, the free combination of parts is not enough on its own: While compositionality and learning-to-learn can provide the parts for new ideas, causality provides the glue that gives them coherence and purpose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1890561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "isKey": false,
            "numCitedBy": 1173,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "slug": "The-Helmholtz-Machine-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "The Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations is described, viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725299"
                        ],
                        "name": "Daniel Tarlow",
                        "slug": "Daniel-Tarlow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Tarlow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Tarlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1569641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d298a5b6a2aaf4dcec033b24dcc850b2f9af7ff1",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing."
            },
            "slug": "Just-In-Time-Learning-for-Fast-and-Flexible-Eslami-Tarlow",
            "title": {
                "fragments": [],
                "text": "Just-In-Time Learning for Fast and Flexible Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time and shows how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954876"
                        ],
                        "name": "Tejas D. Kulkarni",
                        "slug": "Tejas-D.-Kulkarni",
                        "structuredName": {
                            "firstName": "Tejas",
                            "lastName": "Kulkarni",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tejas D. Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3376546"
                        ],
                        "name": "William F. Whitney",
                        "slug": "William-F.-Whitney",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Whitney",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William F. Whitney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14020873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "687e80eb70c7bbad6001006d9269b202650a3354",
            "isKey": false,
            "numCitedBy": 825,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs."
            },
            "slug": "Deep-Convolutional-Inverse-Graphics-Network-Kulkarni-Whitney",
            "title": {
                "fragments": [],
                "text": "Deep Convolutional Inverse Graphics Network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2800360"
                        ],
                        "name": "Alan Jern",
                        "slug": "Alan-Jern",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Jern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Jern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1904809,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2b8f7e169fa33a9ab0145c5b4bfacc386972b505",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-probabilistic-account-of-exemplar-and-category-Jern-Kemp",
            "title": {
                "fragments": [],
                "text": "A probabilistic account of exemplar and category generation"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331233"
                        ],
                        "name": "A. Markman",
                        "slug": "A.-Markman",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Markman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Markman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144326696"
                        ],
                        "name": "A. R. Otto",
                        "slug": "A.-R.-Otto",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Otto",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. R. Otto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 343110,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "bcd7dab63011f919e2059cff0cbef162f31537ec",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent computational theories of decision making in humans and animals have portrayed 2 systems locked in a battle for control of behavior. One system--variously termed model-free or habitual--favors actions that have previously led to reward, whereas a second--called the model-based or goal-directed system--favors actions that causally lead to reward according to the agent's internal model of the environment. Some evidence suggests that control can be shifted between these systems using neural or behavioral manipulations, but other evidence suggests that the systems are more intertwined than a competitive account would imply. In 4 behavioral experiments, using a retrospective revaluation design and a cognitive load manipulation, we show that human decisions are more consistent with a cooperative architecture in which the model-free system controls behavior, whereas the model-based system trains the model-free system by replaying and simulating experience."
            },
            "slug": "Retrospective-revaluation-in-sequential-decision-a-Gershman-Markman",
            "title": {
                "fragments": [],
                "text": "Retrospective revaluation in sequential decision making: a tale of two systems."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "In 4 behavioral experiments, it is shown that human decisions are more consistent with a cooperative architecture in which the model-free system controls behavior, whereas the model\u2019s model-based system trains the models by replaying and simulating experience."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210220"
                        ],
                        "name": "Patrick Shafto",
                        "slug": "Patrick-Shafto",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Shafto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Shafto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2087556,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "bd359b45589bf7983ca35965c4873937d13753ef",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-rational-account-of-pedagogical-reasoning:-by,-Shafto-Goodman",
            "title": {
                "fragments": [],
                "text": "A rational account of pedagogical reasoning: Teaching by, and learning from, examples"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835039"
                        ],
                        "name": "E. Vul",
                        "slug": "E.-Vul",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Vul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 117
                            }
                        ],
                        "text": ", 2014) to garden-path effects in sentence processing (Levy, Reali, & Griffiths, 2009) and perceptual multistability (Gershman et al., 2012; MorenoBote, Knill, & Pouget, 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 237
                            }
                        ],
                        "text": "\u2026sampling has been invoked to explain behavioral phenomena ranging from children\u2019s response variability (Bonawitz et al., 2014) to garden-path effects in sentence processing (Levy, Reali, & Griffiths, 2009) and perceptual multistability (Gershman et al., 2012; MorenoBote, Knill, & Pouget, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16522232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2cce0cf7210b67f8783415bfa3e68e4355eb0da",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 118,
            "paperAbstract": {
                "fragments": [],
                "text": "Ambiguous images present a challenge to the visual system: How can uncertainty about the causes of visual inputs be represented when there are multiple equally plausible causes? A Bayesian ideal observer should represent uncertainty in the form of a posterior probability distribution over causes. However, in many real-world situations, computing this distribution is intractable and requires some form of approximation. We argue that the visual system approximates the posterior over underlying causes with a set of samples and that this approximation strategy produces perceptual multistability\u2014stochastic alternation between percepts in consciousness. Under our analysis, multistability arises from a dynamic sample-generating process that explores the posterior through stochastic diffusion, implementing a rational form of approximate Bayesian inference known as Markov chain Monte Carlo (MCMC). We examine in detail the most extensively studied form of multistability, binocular rivalry, showing how a variety of experimental phenomena\u2014gamma-like stochastic switching, patchy percepts, fusion, and traveling waves\u2014can be understood in terms of MCMC sampling over simple graphical models of the underlying perceptual tasks. We conjecture that the stochastic nature of spiking neurons may lend itself to implementing sample-based posterior approximations in the brain."
            },
            "slug": "Multistability-and-Perceptual-Inference-Gershman-Vul",
            "title": {
                "fragments": [],
                "text": "Multistability and Perceptual Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the visual system approximates the posterior over underlying causes with a set of samples and that this approximation strategy produces perceptual multistability\u2014stochastic alternation between percepts in consciousness."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219829"
                        ],
                        "name": "Susan J. Hespos",
                        "slug": "Susan-J.-Hespos",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Hespos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan J. Hespos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 117
                            }
                        ],
                        "text": "At around 6 months, infants have already developed different expectations for rigid bodies, soft bodies and liquids (Rips & Hespos, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 684510,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "03fdbfc5b1269433f4bcbdd8f404158505eabbeb",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "Our concepts of the physical world distinguish objects, such as chairs, from substances, such as quantities of wood, that constitute them. A particular chair might consist of a single chunk of wood, yet we think about the chair and the wood in different ways. For example, part of the wood is still wood, but part of the chair is not a chair. In this article we examine the basis of the object/substance distinction. We draw together for the first time relevant experiments widely dispersed in the cognitive literature, and view these findings in the light of theories in linguistics and metaphysics. We outline a framework for the difference between objects and substances, based on earlier ideas about form and matter, describing the psychological evidence surrounding it. The framework suggests that concepts of objects include a relation of unity and organization governing their parts, whereas concepts of substances do not. We propose, as a novel twist on this framework, that unity and organization for objects is a function of causal forces that shape the objects. In agreement with this idea, results on the identification of an item as an object depend on clues about the presence of the shaping relation, clues provided by solidity, repetition of shape, and other factors. We also look at results from human infants about the source of the object/substance distinction and conclude that the data support an early origin for both object and substance knowledge. (PsycINFO Database Record"
            },
            "slug": "Divisions-of-the-physical-world:-Concepts-of-and-Rips-Hespos",
            "title": {
                "fragments": [],
                "text": "Divisions of the physical world: Concepts of objects and substances."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The basis of the object/substance distinction is examined and it is proposed, as a novel twist on this framework, that unity and organization for objects is a function of causal forces that shape the objects."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological bulletin"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735083"
                        ],
                        "name": "Vikash K. Mansinghka",
                        "slug": "Vikash-K.-Mansinghka",
                        "structuredName": {
                            "firstName": "Vikash",
                            "lastName": "Mansinghka",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikash K. Mansinghka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196579"
                        ],
                        "name": "Daniel Selsam",
                        "slug": "Daniel-Selsam",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Selsam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Selsam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2851128"
                        ],
                        "name": "Yura N. Perov",
                        "slug": "Yura-N.-Perov",
                        "structuredName": {
                            "firstName": "Yura",
                            "lastName": "Perov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yura N. Perov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15636079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0e6bf7a7f508e4e1fcc84a27722f306c9449008",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe Venture, an interactive virtual machine for probabilistic programming that aims to be sufficiently expressive, extensible, and efficient for general-purpose use. Like Church, probabilistic models and inference problems in Venture are specified via a Turing-complete, higher-order probabilistic language descended from Lisp. Unlike Church, Venture also provides a compositional language for custom inference strategies built out of scalable exact and approximate techniques. We also describe four key aspects of Venture's implementation that build on ideas from probabilistic graphical models. First, we describe the stochastic procedure interface (SPI) that specifies and encapsulates primitive random variables. The SPI supports custom control flow, higher-order probabilistic procedures, partially exchangeable sequences and ``likelihood-free'' stochastic simulators. It also supports external models that do inference over latent variables hidden from Venture. Second, we describe probabilistic execution traces (PETs), which represent execution histories of Venture programs. PETs capture conditional dependencies, existential dependencies and exchangeable coupling. Third, we describe partitions of execution histories called scaffolds that factor global inference problems into coherent sub-problems. Finally, we describe a family of stochastic regeneration algorithms for efficiently modifying PET fragments contained within scaffolds. Stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically. We show how to use stochastic regeneration and the SPI to implement general-purpose inference strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals based on particle Markov chain Monte Carlo and mean-field variational inference techniques."
            },
            "slug": "Venture:-a-higher-order-probabilistic-programming-Mansinghka-Selsam",
            "title": {
                "fragments": [],
                "text": "Venture: a higher-order probabilistic programming platform with programmable inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically is shown, and how to use stochastic regeneration and the SPI to implement general-purpose inference strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals based on particle Markov chain Monte Carlo and mean-field variational inference techniques are shown."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49750831"
                        ],
                        "name": "Daniel J. Graham",
                        "slug": "Daniel-J.-Graham",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Graham",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel J. Graham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923670"
                        ],
                        "name": "D. Rockmore",
                        "slug": "D.-Rockmore",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Rockmore",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rockmore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 180
                            }
                        ],
                        "text": "Such rapid feedback is consistent with the notion that corticothalamic signals could function like the \u201cack\u201d (acknowledgment) system used on the Internet to ensure packet delivery (Graham 2014; Graham and Rockmore 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 252
                            }
                        ],
                        "text": "We should consider that one of the missing components in deep learning models of cognition \u2013 and of most large-scale models of brain and cognitive function \u2013 is an understanding of how signals are selectively routed to different destinations in brains (Graham 2014; Graham and Rockmore 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11447664,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4083d3d64daa636845fe44359cb72e46a32b4f3e",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "The computer metaphor has served brain science well as a tool for comprehending neural systems. Nevertheless, we propose here that this metaphor be replaced or supplemented by a new metaphor, the \u201cInternet metaphor,\u201d to reflect dramatic new network theoretic understandings of brain structure and function. We offer a \u201cweak\u201d form and a \u201cstrong\u201d form of this metaphor: The former suggests that structures and processes unique to Internet-like architectures (e.g., domains and protocols) can profitably guide our thinking about brains, whereas the latter suggests that one particular feature of the Internet\u2014packet switching\u2014may be instantiated in the structure of certain brain networks, particularly mammalian neocortex."
            },
            "slug": "The-Packet-Switching-Brain-Graham-Rockmore",
            "title": {
                "fragments": [],
                "text": "The Packet Switching Brain"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a new metaphor, the \u201cInternet metaphor,\u201d to reflect dramatic new network theoretic understandings of brain structure and function, and offers a \u201cweak\u201d form and a \"strong\" form of this metaphor."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681946"
                        ],
                        "name": "Carlos Diuk",
                        "slug": "Carlos-Diuk",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Diuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Diuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023158"
                        ],
                        "name": "A. Cohen",
                        "slug": "A.-Cohen",
                        "structuredName": {
                            "firstName": "Andre",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207168200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25fd7e9ed8d1a669c7a8d28a8b620479899e6b53",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame."
            },
            "slug": "An-object-oriented-representation-for-efficient-Diuk-Cohen",
            "title": {
                "fragments": [],
                "text": "An object-oriented representation for efficient reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Object-Oriented MDPs (OO-MDPs) are introduced, a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities and a polynomial bound on its sample complexity is proved."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 127
                            }
                        ],
                        "text": "Rumelhart, Hinton, & Williams, 1986), the wake-sleep algorithm (Hinton, Dayan, Frey, & Neal, 1995), and contrastive divergence (Hinton, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 283
                            }
                        ],
                        "text": "\u2026including the perceptron algorithm (Rosenblatt, 1958), Hebbian learning (Hebb, 1949), the BCM rule (Bienenstock, Cooper, & Munro, 1982), backpropagation (Rumelhart, Hinton, & Williams, 1986), the wake-sleep algorithm (Hinton, Dayan, Frey, & Neal, 1995), and contrastive divergence (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4570,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt, 1958), Hebbian learning (Hebb, 1949), the BCM rule (Bienenstock, Cooper, & Munro, 1982), backpropagation (Rumelhart, Hinton, & Williams, 1986), the wake-sleep algorithm (Hinton, Dayan, Frey, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 171
                            }
                        ],
                        "text": "Parallel to these developments, a radically different approach was being explored, based on neuron-like \u201csub-symbolic\u201d computations (e.g., Fukushima, 1980; Grossberg, 1976; Rosenblatt, 1958)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12781225,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
            "isKey": false,
            "numCitedBy": 9073,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus"
            },
            "slug": "The-perceptron:-a-probabilistic-model-for-storage-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "The perceptron: a probabilistic model for information storage and organization in the brain."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2252285"
                        ],
                        "name": "E. Spelke",
                        "slug": "E.-Spelke",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Spelke",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Spelke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "Young infants believe objects should move along smooth paths, not wink in and out of existence, not inter-penetrate and not act at a distance (Spelke, 1990; Spelke, Gutheil, & Van de Walle, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "These expectations guide object segmentation in early infancy, emerging before appearance-based cues such as color, texture, and perceptual goodness (Spelke, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11028587,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2b0752d91bde8b37e00e21219c441d9f6dfe74db",
            "isKey": false,
            "numCitedBy": 1014,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on human infants has begun to shed light on early-developing processes for segmenting perceptual arrays Into objects. Infants appear to perceive objects by analyzing three-dimensional surface arrangements and motions. Their perception does not accord with a general tendency to maximize flgural goodness or to attend to nonaccidental geometric relations in visual arrays. Object perception does accord with principles governing the motions of material bodies: Infants divide perceptual arrays into units that move as connected wholes, that move separately from one another, that tend to maintain their size and shape over motion, and that tend to act upon each other only on contact. These findings suggest that a general representation of object unity and boundaries is interposed between representations of surfaces and representations of objects of familiar kinds. The processes that construct this representation may be related to processes of physical reasoning."
            },
            "slug": "Principles-of-Object-Perception-Spelke",
            "title": {
                "fragments": [],
                "text": "Principles of Object Perception"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Findings suggest that a general representation of object unity and boundaries is interposed between representations of surfaces and representations of objects of familiar kinds, related to processes of physical reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16447573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "isKey": false,
            "numCitedBy": 26053,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
            },
            "slug": "Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Words and Phrases and their Compositionality"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697953"
                        ],
                        "name": "Tobias Gerstenberg",
                        "slug": "Tobias-Gerstenberg",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Gerstenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias Gerstenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1953585"
                        ],
                        "name": "D. Lagnado",
                        "slug": "D.-Lagnado",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lagnado",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lagnado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4692143,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "9b41c7c42cfeee0b8aff5daf1983160619130fa9",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people make causal judgments? Here, we propose a counterfactual simulation model (CSM) of causal judgment that unifies different views on causation. The CSM predicts that people\u2019s causal judgments are influenced by whether a candidate cause made a difference to whether the outcome occurred as well as to how it occurred. We show how whethercausation and how-causation can be implemented in terms of different counterfactual contrasts defined over the same intuitive generative model of the domain. We test the model in an intuitive physics domain where people make judgments about colliding billiard balls. Experiment 1 shows that participants\u2019 counterfactual judgments about what would have happened if one of the balls had been removed, are well-explained by an approximately Newtonian model of physics. In Experiment 2, participants judged to what extent two balls were causally responsible for a third ball going through a gate or missing the gate. As predicted by the CSM, participants\u2019 judgments increased with their belief that a ball was a whether-cause, a how-cause, as well as sufficient for bringing about the outcome."
            },
            "slug": "How,-whether,-why:-Causal-judgments-as-contrasts-Gerstenberg-Goodman",
            "title": {
                "fragments": [],
                "text": "How, whether, why: Causal judgments as counterfactual contrasts"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A counterfactual simulation model of causal judgment that unifies different views on causation predicts that people\u2019s causal judgments are influenced by whether a candidate cause made a difference to whether the outcome occurred as well as to how it occurred."
            },
            "venue": {
                "fragments": [],
                "text": "CogSci"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 163
                            }
                        ],
                        "text": "When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference (Geman et al., 1992; Griffiths, Chater, Kemp, Perfors, & Tenenbaum, 2010; Tenenbaum, Kemp, Griffiths, & Goodman, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Ultimately, the full project of building machines that learn and think like humans must have language at its core."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1069654,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1e17d73e5578a091272825de5700d4972b4465d2",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a hierarchical Bayesian model that learns categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances. The model discovers how to group categories into meaningful super-categories that express different priors for new classes. Given a single example of a novel category, we can efficiently infer which super-category the novel category belongs to, and thereby estimate not only the new category's mean but also an appropriate similarity metric based on parameters inherited from the super-category. On MNIST and MSR Cambridge image datasets the model learns useful representations of novel categories based on just a single training example, and performs significantly better than simpler hierarchical Bayesian approaches. It can also discover new categories in a completely unsupervised fashion, given just one or a few examples."
            },
            "slug": "One-Shot-Learning-with-a-Hierarchical-Nonparametric-Salakhutdinov-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "One-Shot Learning with a Hierarchical Nonparametric Bayesian Model"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A hierarchical Bayesian model that learns categories from single training examples that transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances is developed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML Unsupervised and Transfer Learning"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981334"
                        ],
                        "name": "Lars Buesing",
                        "slug": "Lars-Buesing",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Buesing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lars Buesing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112317"
                        ],
                        "name": "Johannes Bill",
                        "slug": "Johannes-Bill",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Bill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johannes Bill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37082831"
                        ],
                        "name": "Bernhard Nessler",
                        "slug": "Bernhard-Nessler",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Nessler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Nessler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7504633,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "e4e100e44bf7618c7d96188605fd9870012bdb50",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons."
            },
            "slug": "Neural-Dynamics-as-Sampling:-A-Model-for-Stochastic-Buesing-Bill",
            "title": {
                "fragments": [],
                "text": "Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A neural network model is proposed and it is shown by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299972"
                        ],
                        "name": "I. Biederman",
                        "slug": "I.-Biederman",
                        "structuredName": {
                            "firstName": "Irving",
                            "lastName": "Biederman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Biederman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 176
                            }
                        ],
                        "text": "Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman, 1987; Hummel & Biederman, 1992; Marr & Nishihara, 1978; van den Hengel et al., 2015; Winston, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 15
                            }
                        ],
                        "text": "Unfortunately, what we \u201cknow\u201d about the brain is not all that clear-cut."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8054340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b37258659bcdbc380b1e6c4e22cce9ea06397a1",
            "isKey": false,
            "numCitedBy": 5632,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N \u00a3 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensiona l image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position an$ image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. Any single object can project an infinity of image configurations to the retina. The orientation of the object to the viewer can vary continuously, each giving rise to a different two-dimensional projection. The object can be occluded by other objects or texture fields, as when viewed behind foliage. The object need not be presented as a full-colored textured image but instead can be a simplified line drawing. Moreover, the object can even be missing some of its parts or be a novel exemplar of its particular category. But it is only with rare exceptions that an image fails to be rapidly and readily classified, either as an instance of a familiar object category or as an instance that cannot be so classified (itself a form of classification)."
            },
            "slug": "Recognition-by-components:-a-theory-of-human-image-Biederman",
            "title": {
                "fragments": [],
                "text": "Recognition-by-components: a theory of human image understanding."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Recognition-by-components (RBC) provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16895865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "484ad17c926292fbe0d5211540832a8c8a8e958b",
            "isKey": false,
            "numCitedBy": 3901,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation."
            },
            "slug": "Stochastic-Backpropagation-and-Approximate-in-Deep-Rezende-Mohamed",
            "title": {
                "fragments": [],
                "text": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work marries ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning that introduces a recognition model to represent approximate posterior distributions and that acts as a stochastic encoder of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20330,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219829"
                        ],
                        "name": "Susan J. Hespos",
                        "slug": "Susan-J.-Hespos",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Hespos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan J. Hespos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6226925"
                        ],
                        "name": "R. Baillargeon",
                        "slug": "R.-Baillargeon",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Baillargeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baillargeon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 80
                            }
                        ],
                        "text": "As we argued in our discussion of Frostbite, one can design numerous variants of this simple video game that are identical except for the reward function \u2013 that is, governed by an identical environment model of state-action-dependent transitions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 220
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment and collisions (Baillargeon, 2004; Baillargeon, Li, Ng, & Yuan, 2009; Hespos & Baillargeon, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41798270,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ab085827d97e04c50078c817390476c4e60d7b29",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Young-infants\u2019-actions-reveal-their-developing-of-Hespos-Baillargeon",
            "title": {
                "fragments": [],
                "text": "Young infants\u2019 actions reveal their developing knowledge of support variables: Converging evidence for violation-of-expectation findings"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716847"
                        ],
                        "name": "G. Baldassarre",
                        "slug": "G.-Baldassarre",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Baldassarre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baldassarre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059234249"
                        ],
                        "name": "Tom Stafford",
                        "slug": "Tom-Stafford",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Stafford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Stafford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805997"
                        ],
                        "name": "M. Mirolli",
                        "slug": "M.-Mirolli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Mirolli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirolli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3544623"
                        ],
                        "name": "P. Redgrave",
                        "slug": "P.-Redgrave",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Redgrave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Redgrave"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4064306"
                        ],
                        "name": "R. Ryan",
                        "slug": "R.-Ryan",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Ryan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ryan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 85
                            }
                        ],
                        "text": ", Lisman & Grace 2005; Redgrave & Gurney 2006) and computational theories and models (e.g., Baldassarre & Mirolli 2013; Baldassarre et al. 2014; Santucci et al. 2016) indicate how the implementation of these processes indeed requires very sophisticated architectures able to store multiple skills, to transfer knowledge while avoiding catastrophic interference, to explore the environment based on the acquired skills, to self-generate goals/ tasks, and to focus on goals that ensure a maximum knowledge gain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1248142,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fe6031cd4e0734b7d9f2130ab457fe9c6ad71b54",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "This editorial article introduces the Frontiers Research Topic and Electronic Book (eBook) on Intrinsic Motivations (IMs), which involved the publication of 24 articles with the journals Frontiers in Psychology \u2013 Cognitive Science and Frontiers in Neurorobotics. The main objective of this Frontiers Research Topic is to present state-of-the-art research on IMs and open-ended development from an interdisciplinary perspective involving human and animal psychology, neuroscience, and computational perspectives. We first introduce in this section the main themes and concepts on IMs from different interdisciplinary perspectives. These themes and concepts have been reviewed more extensively in other works (e.g., see Barto et al., 2004; Oudeyer and Kaplan, 2007; Mirolli and Baldassarre, 2013; Barto, 2013), but they are briefly reported here both to meet the needs of the reader new to the field and to introduce the concepts and terms we use in the succeeding sections. In the next four sections, we give an overview of the Topic contributions grouped by four themes. A final section draws the conclusions. \n \nAutonomous development and lifelong open-ended learning are hallmarks of intelligence. Higher mammals, and especially humans, engage in activities that do not appear to directly serve the goals of survival, reproduction, or material advantage. Rather, many activities seem to be carried out \u201cfor their own sake\u201d (Berlyne, 1966), play being a prime example, but including other activities driven by curiosity and interest in novel stimuli or surprising events. Autonomously setting goals and working to acquire new forms of competence are also examples of activities that often do not confer obvious evolutionary benefit. Activities like these are thus said to be driven by intrinsic motivations (Baldassarre and Mirolli, 2013a). IMs facilitate the cumulative and virtually open-ended acquisition of knowledge and skills that can later be used to accomplish fitness-enhancing goals (Singh et al., 2010; Baldassarre, 2011). IMs continue during adulthood, and they underlie several important human phenomena such as artistic creativity, scientific discovery, and subjective well-being (Ryan and Deci, 2000b; Schmidhuber, 2010). \n \nIMs were proposed within the animal literature to explain aspects of behavior that could not be explained by the dominant theory of motivation postulating that animals work to reduce physiological imbalances (Hull, 1943). The term \u201cintrinsic motivation\u201d was first used to describe a \u201cmanipulation drive\u201d hypothesized to explain why rhesus monkeys would engage with mechanical puzzles for long periods of time without receiving extrinsic rewards (Harlow et al., 1950). Other studies showed how animal instrumental actions can be conditioned with the delivery of apparently neutral stimuli: for example, monkeys were trained to perform actions to gain access to a window from which they could observe conspecifics (Butler, 1953), and mice were trained to perform actions that resulted in clicks or in moving the cage platform (Kish, 1955). The psychological literature on IMs initially linked them to the perceptual properties of stimuli, such as their complexity, novel appearance, or surprising features (Berlyne, 1950, 1966). Later, IMs were also related to action, in particular to the competence (\u201ceffectance\u201d) that an agent can acquire to willfully make changes in its environment (White, 1959). This relation of IMs with action and their effects was later linked to the possibility of autonomously setting one's own goals (Ryan and Deci, 2000a). \n \nComputational approaches, in particular machine learning and autonomous robotics, are concerned with IMs and open-ended development as these are thought to have the potential to lead to the construction of truly intelligent artificial systems, in particular systems that are capable of improving their own skills and knowledge autonomously and indefinitely. The relation of these studies with those on IMs in psychology were first highlighted by Barto et al. (2004) and Singh et al. (2005). The investigation of IMs from a computational perspective can lead to theoretical clarifications, in particular with respect to the computational mechanisms and functions that might underlie IMs (Mirolli and Baldassarre, 2013). IM mechanisms have been classified as being either knowledge-based or competence-based (Oudeyer and Kaplan, 2007): the former based on measures related to the acquisition of information, and the latter on measures related to the learning of skills. More recently, knowledge-based IMs have been further divided into novelty-based IMs and prediction-based IMs (Baldassarre and Mirolli, 2013b; Barto et al., 2013). Novelty-based IMs are elicited by the experience of stimuli that are not in the agent's memory (e.g., novel objects, or novel object-object or object-context combinations); prediction-based IMs are related to events that surprise the agent by violating its explicit predictions. \n \nThese distinctions have been formalized in the computational models proposed in the literature. Seminal works in machine learning (Schmidhuber, 1991), later developed to function in robots (Oudeyer et al., 2007), have proposed algorithms rewarding actions that allow the agent to improve the quality of a \u201cpredictor\u201d component with which it anticipates the effects that such actions produce on the environment. Other researchers have proposed robots capable of detecting and focussing on novel stimuli (e.g., Marsland et al., 2005), or systems capable of detecting anomalies in datasets (Nehmzow et al., 2013). Additional research threads have focussed on action and control, in particular on IMs guiding the autonomous acquisition of motor skills (Barto et al., 2004), on the decision about which of several skills to practice at any time (Schembri et al., 2007; Santucci et al., 2013), and on the the autonomous formation of goals guiding skill acquisition (Baranes and Oudeyer, 2013). Other computational mechanisms related to the idea of IMs are being proposed in the growing field of active learning, in particular in relation to supervised learning systems (Settles, 2010). \n \nRecent neuroscientific investigations are revealing brain mechanisms that possibly underlie the IM systems investigated in the behavioral and computational literature. However, unfortunately such investigations are carried out under agendas different from the one on IMs, e.g., in relation to dopamine, memory, motor learning, goal-directed behavior, and conflict monitoring, so comprehensive views are still missing. A large body of research shows how the hippocampus, a brain compound system playing pivotal functions for memory, has the capacity to detect the novelty of various aspects of experience, from the novelty of single items to the novelty of item-item and item-context associations (Ranganath and Rainer, 2003; Kumaran and Maguire, 2007). This detection is then capable of triggering the release of neuromodulators, such as dopamine, that modulate the functioning and learning processes of the hippocampus itself and other brain areas, e.g., of the frontal cortex involved in higher cognition, action planning, and action execution (Lisman and Grace, 2005). Other studies have shown that unexpected stimuli can activate the superior colliculus, a midbrain structure that plays a key role in oculomotor control, which in turn causes phasic bursts of dopamine affecting trial-and-error learning processes happening in basal ganglia, a brain region known to be involved in learning to select actions and other cortex contents (Redgrave and Gurney, 2006). Dopamine signals have also been shown to have an interesting direct relationship with information seeking (Bromberg-Martin and Hikosaka, 2009). Noradrenaline, another neuromodulator targeting a large part of brain, has been shown to be involved in signaling violations of the agent's expectations (Sara, 2009). The failure (Carter et al., 1998) or success (Ribas-Fernandes et al., 2011) in accomplishing goals and sub-goals, possibly themselves set by IMs, has been shown to have neural correlates that might affect succeeding motivation, engagement, and learning. Bio-inspired/bio-constrained computational modeling is linking some of these neuroscientific results to specific computational mechanisms, e.g., in relation to dopamine (e.g., see the pioneering work of Kakade and Dayan, 2002, and Mirolli et al., 2013) and goal-directed behavior (Baldassare et al., 2013). \n \nThe 24 interdisciplinary contributions to the present Research Topic can be clustered into four groups. The first group of six contributions (IMs and brain and behavior) focuses on different types of IM mechanisms implemented in the brain. The second group of five contributions (IMs and attention) focuses on the role of IMs in attention. The third group of eight contributions (IMs and motor skills) focuses on IMs as drives for the acquisition of manipulation and navigation skills, often with an emphasis on their function in enabling cumulative, open-ended development. Finally, the fourth group of five contributions (IMs and social interaction) focuses on the relationship between IMs and social phenomena, a novel area of investigation of IMs that is increasingly attracting the attention of researchers."
            },
            "slug": "Intrinsic-motivations-and-open-ended-development-in-Baldassarre-Stafford",
            "title": {
                "fragments": [],
                "text": "Intrinsic motivations and open-ended development in animals, humans, and robots: an overview"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The main objective of this Frontiers Research Topic is to present state-of-the-art research on IMs and open-ended development from an interdisciplinary perspective involving human and animal psychology, neuroscience, and computational perspectives."
            },
            "venue": {
                "fragments": [],
                "text": "Front. Psychol."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143603708"
                        ],
                        "name": "N. Rougier",
                        "slug": "N.-Rougier",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Rougier",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Rougier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145042542"
                        ],
                        "name": "D. Noelle",
                        "slug": "D.-Noelle",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Noelle",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Noelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2723253"
                        ],
                        "name": "T. Braver",
                        "slug": "T.-Braver",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Braver",
                            "middleNames": [
                                "Samuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Braver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153564781"
                        ],
                        "name": "Jonathan D. Cohen",
                        "slug": "Jonathan-D.-Cohen",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390067049"
                        ],
                        "name": "R. O\u2019Reilly",
                        "slug": "R.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Reilly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14144575,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "072ad00bff08a5e6e253ce30908b6dcde3c07b03",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Human cognitive control is uniquely flexible and has been shown to depend on prefrontal cortex (PFC). But exactly how the biological mechanisms of the PFC support flexible cognitive control remains a profound mystery. Existing theoretical models have posited powerful task-specific PFC representations, but not how these develop. We show how this can occur when a set of PFC-specific neural mechanisms interact with breadth of experience to self organize abstract rule-like PFC representations that support flexible generalization in novel tasks. The same model is shown to apply to benchmark PFC tasks (Stroop and Wisconsin card sorting), accurately simulating the behavior of neurologically intact and frontally damaged people."
            },
            "slug": "Prefrontal-cortex-and-flexible-cognitive-control:-Rougier-Noelle",
            "title": {
                "fragments": [],
                "text": "Prefrontal cortex and flexible cognitive control: rules without symbols."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how this can occur when a set of PFC-specific neural mechanisms interact with breadth of experience to self organize abstract rule-like PFC representations that support flexible generalization in novel tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11383109"
                        ],
                        "name": "J. Lloyd",
                        "slug": "J.-Lloyd",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Lloyd",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lloyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704657"
                        ],
                        "name": "D. Duvenaud",
                        "slug": "D.-Duvenaud",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Duvenaud",
                            "middleNames": [
                                "Kristjanson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Duvenaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9535909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1427921e7011412b77dd7d0b911bc66f1ad1cff8",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural-language text. Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.\n \n"
            },
            "slug": "Automatic-Construction-and-Natural-Language-of-Lloyd-Duvenaud",
            "title": {
                "fragments": [],
                "text": "Automatic Construction and Natural-Language Description of Nonparametric Regression Models"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The beginnings of an automatic statistician is presented, focusing on regression problems, which explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural-language text."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2534361"
                        ],
                        "name": "M. Tomasello",
                        "slug": "M.-Tomasello",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tomasello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tomasello"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13224707,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d922e5d062f65273d8f8ebf08d08211de6ecffcf",
            "isKey": false,
            "numCitedBy": 2747,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "Winner, 2009 Eleanor Maccoby Book Award in Developmental Psychology, presented by the American Psychological Association. and Honorable Mention, Literature, Language & Linguistics category, 2008 PROSE Awards presented by the Professional/Scholarly Publishing Division of the Association of American Publishers. Human communication is grounded in fundamentally cooperative, even shared, intentions. In this original and provocative account of the evolutionary origins of human communication, Michael Tomasello connects the fundamentally cooperative structure of human communication (initially discovered by Paul Grice) to the especially cooperative structure of human (as opposed to other primate) social interaction. Tomasello argues that human cooperative communication rests on a psychological infrastructure of shared intentionality (joint attention, common ground), evolved originally for collaboration and culture more generally. The basic motives of the infrastructure are helping and sharing: humans communicate to request help, inform others of things helpfully, and share attitudes as a way of bonding within the cultural group. These cooperative motives each created different functional pressures for conventionalizing grammatical constructions. Requesting help in the immediate you-and-me and here-and-now, for example, required very little grammar, but informing and sharing required increasingly complex grammatical devices. Drawing on empirical research into gestural and vocal communication by great apes and human infants (much of it conducted by his own research team), Tomasello argues further that humans' cooperative communication emerged first in the natural gestures of pointing and pantomiming. Conventional communication, first gestural and then vocal, evolved only after humans already possessed these natural gestures and their shared intentionality infrastructure along with skills of cultural learning for creating and passing along jointly understood communicative conventions. Challenging the Chomskian view that linguistic knowledge is innate, Tomasello proposes instead that the most fundamental aspects of uniquely human communication are biological adaptations for cooperative social interaction in general and that the purely linguistic dimensions of human communication are cultural conventions and constructions created by and passed along within particular cultural groups. Jean Nicod Lectures A Bradford Book"
            },
            "slug": "Origins-of-human-communication-Tomasello",
            "title": {
                "fragments": [],
                "text": "Origins of human communication"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3364063"
                        ],
                        "name": "B. Scellier",
                        "slug": "B.-Scellier",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Scellier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Scellier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 275
                            }
                        ],
                        "text": "\u2026seems to require that information be transmitted backwards along the axon, which does not fit with realistic models of neuronal function (although recent models circumvent this problem in various ways Liao, Leibo, & Poggio, 2015; Lillicrap, Cownden, Tweed, & Akerman, 2014; Scellier & Bengio, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 139945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f61e15e0076a4439c98232ab679680dea0d1372",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well-defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point or stationary distribution) toward a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged toward their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal \u201cback-propagated\u201d during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not. We also show experimentally that multi-layer recurrently connected networks with 1, 2, and 3 hidden layers can be trained by Equilibrium Propagation on the permutation-invariant MNIST task."
            },
            "slug": "Equilibrium-Propagation:-Bridging-the-Gap-between-Scellier-Bengio",
            "title": {
                "fragments": [],
                "text": "Equilibrium Propagation: Bridging the Gap between Energy-Based Models and Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that multi-layer recurrently connected networks with 1, 2, and 3 hidden layers can be trained by Equilibrium Propagation on the permutation-invariant MNIST task, and it makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains."
            },
            "venue": {
                "fragments": [],
                "text": "Front. Comput. Neurosci."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259916"
                        ],
                        "name": "Donald D. Hoffman",
                        "slug": "Donald-D.-Hoffman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hoffman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald D. Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31913541"
                        ],
                        "name": "W. Richards",
                        "slug": "W.-Richards",
                        "structuredName": {
                            "firstName": "Whitman",
                            "lastName": "Richards",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Richards"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 199
                            }
                        ],
                        "text": "While compositionality and learning-to-learn fit naturally together, there are also forms of compositionality that rely less on previous learning, such as the bottom-up parts-based representation of Hoffman and Richards (1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6397710,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "150662e38fc67ca81ac93faf981cf2151fbf6a9a",
            "isKey": false,
            "numCitedBy": 1354,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parts-of-recognition-Hoffman-Richards",
            "title": {
                "fragments": [],
                "text": "Parts of recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33696979"
                        ],
                        "name": "G. Murphy",
                        "slug": "G.-Murphy",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Murphy",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36568337"
                        ],
                        "name": "B. Ross",
                        "slug": "B.-Ross",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson &\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 34476323,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "eaa3de126f9483e03c5cf79d205198b04f1ea183",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Eleven experiments investigated how categorization influences feature prediction. Subjects were provided with sets of categorized exemplars, which they used to make predictions about properties of new exemplars. Because the categories were provided for subjects, this method allowed a test of categorization and prediction processes, bypassing initial concept formation and memory. The experiments tested a Bayesian rule of prediction according to which (1) predictions of an object's features are based on information from multiple categories, and (2) features are treated as independent of one another. With one exception, the studies found evidence against both of these claims. Subjects did not generally alter their predictions as a function of information outside the most likely \"target\" category. In addition, feature relations had reliable effects on these predictions. We discuss the implications of these results for understanding how categories are used in drawing inferences."
            },
            "slug": "Predictions-From-Uncertain-Categorizations-Murphy-Ross",
            "title": {
                "fragments": [],
                "text": "Predictions From Uncertain Categorizations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The experiments tested a Bayesian rule of prediction according to which predictions of an object's features are based on information from multiple categories, and features are treated as independent of one another, finding evidence against both claims."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058463221"
                        ],
                        "name": "Daniel Potter",
                        "slug": "Daniel-Potter",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Potter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Potter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7368216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16618df0617fc33256b0f5d8ebc84afdb0531093",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Images are ambiguous at each of many levels of a contextual hierarchy. Nevertheless, the high-level interpretation of most scenes is unambiguous, as evidenced by the superior performance of humans. This observation argues for global vision models, such as deformable templates. Unfortunately, such models are computationally intractable for unconstrained problems. We propose a compositional model in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings. Ambiguity is propagated up the hierarchy in the form of multiple interpretations, which are later resolved by a Bayesian, equivalently minimum-description-Iength, cost functional."
            },
            "slug": "Compositionality,-MDL-Priors,-and-Object-Bienenstock-Geman",
            "title": {
                "fragments": [],
                "text": "Compositionality, MDL Priors, and Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A compositional model is proposed in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings for global vision models such as deformable templates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 220093382,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "12b9019f99a315a137400389ee7c6faa4cceef35",
            "isKey": false,
            "numCitedBy": 7610,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control."
            },
            "slug": "A-Neural-Substrate-of-Prediction-and-Reward-Schultz-Dayan",
            "title": {
                "fragments": [],
                "text": "A Neural Substrate of Prediction and Reward"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Findings in this work indicate that dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events can be understood through quantitative theories of adaptive optimizing control."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41224113"
                        ],
                        "name": "E. Miller",
                        "slug": "E.-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837031"
                        ],
                        "name": "Nicholas E. Matsakis",
                        "slug": "Nicholas-E.-Matsakis",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Matsakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas E. Matsakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2699786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7aac1045e6943b4a7978e260a3035662d5b3bf8d",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a process called congealing in which elements of a dataset (images) are brought into correspondence with each other jointly, producing a data-defined model. It is based upon minimizing the summed component-wise (pixel-wise) entropies over a continuous set of transforms on the data. One of the biproducts of this minimization is a set of transform, one associated with each original training sample. We then demonstrate a procedure for effectively bringing test data into correspondence with the data-defined model produced in the congealing process. Subsequently; we develop a probability density over the set of transforms that arose from the congealing process. We suggest that this density over transforms may be shared by many classes, and demonstrate how using this density as \"prior knowledge\" can be used to develop a classifier based on only a single training example for each class."
            },
            "slug": "Learning-from-one-example-through-shared-densities-Miller-Matsakis",
            "title": {
                "fragments": [],
                "text": "Learning from one example through shared densities on transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A probability density over the set of transforms that arose from the congealing process is developed, and it is suggested that this density over transforms may be shared by many classes, and used to develop a classifier based on only a single training example for each class."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153324377"
                        ],
                        "name": "Susan C. Johnson",
                        "slug": "Susan-C.-Johnson",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Johnson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan C. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7251004"
                        ],
                        "name": "V. Slaughter",
                        "slug": "V.-Slaughter",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Slaughter",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Slaughter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144366429"
                        ],
                        "name": "S. Carey",
                        "slug": "S.-Carey",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Carey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18802949,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c1a7d6570c2b6bda18260fa51cace569863c0fc5",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Eighty-three 12-month-old infants faced a noisy, active, object for one minute, after which the object turned 45 degrees to the left or the right. Five conditions explored what object features elicited gaze-following behavior in the infants. In one condition, the object was an adult stranger. The other four conditions used a soft, brown, dog-sized, amorphously-shaped, asymmetrical novel object that varied along two dimensions theorized as central to the identification of intentional beings: facial features and contingently interactive behavior. Infants shifted their own attentional direction to match the orientation of the actor or object in every condition except the one in which the object lacked both a face and contingently interactive behavior. Infants\u2019\u2018gaze\u2019-following behavior in general, therefore, appears to have been driven selectively by a particular configuration of behavioral and morphological characteristics, specifically those theorized as underlying attributions of intentionality rather than attributions of person per se."
            },
            "slug": "Whose-gaze-will-infants-follow-The-elicitation-of-Johnson-Slaughter",
            "title": {
                "fragments": [],
                "text": "Whose gaze will infants follow? The elicitation of gaze-following in 12-month-olds"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "\u2026context of learning new handwritten characters or learning to play Frostbite, the MNIST benchmark includes 6000 examples of each handwritten digit (LeCun et al., 1998), and the DQN of V. Mnih et al. (2015) played each Atari video game for approximately 924 hours of unique training experience\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 247
                            }
                        ],
                        "text": "With a large amount of training data available, many algorithms achieve respectable performance, including K-nearest neighbors (5% test error), support vector machines (about 1% test error), and convolutional neural networks (below 1% test error; LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also use the problems as running examples to illustrate the importance of core cognitive ingredients in the sections that follow."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35262,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398974449"
                        ],
                        "name": "B. Hayes-Roth",
                        "slug": "B.-Hayes-Roth",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hayes-Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hayes-Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398681259"
                        ],
                        "name": "F. Hayes-Roth",
                        "slug": "F.-Hayes-Roth",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Hayes-Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hayes-Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 169
                            }
                        ],
                        "text": "AI pioneers in other areas of research explicitly referenced human cognition, and even published papers in cognitive psychology journals (e.g.,\nBobrow & Winograd, 1977; Hayes-Roth & Hayes-Roth, 1979; Winograd, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26945382,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "87b08864f2b59d4c9f25cc893669b78c79fda235",
            "isKey": false,
            "numCitedBy": 1349,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a cognitive model of the planning process. The model generalizes the theoretical architecture of the Hearsay-II system. Thus, it assumes that planning comprises the activities of a variety of cognitive \u201cspecialists.\u201d Each specialist can suggest certain kinds of decisions for incorporation into the plan in progress. These include decisions about: (a) how to approach the planning problem; (b) what knowledge bears on the problem; (c) what kinds of actions to try to plan; (d) what specific actions to plan; and (e) how to allocate cognitive resources during planning. Within each of these categories, different specialists suggest decisions at different levels of abstraction. The activities of the various specialists are not coordinated in any systematic way. Instead, the specialists operate opportunistically, suggesting decisions whenever promising opportunities arise. The paper presents a detailed account of the model and illustrates its assumptions with a \u201cthinking aloud\u201d protocol. It also describes the performance of a computer simulation of the model. The paper contrasts the proposed model with successive refinement models and attempts to resolve apparent differences between the two points of view."
            },
            "slug": "A-Cognitive-Model-of-Planning-Hayes-Roth-Hayes-Roth",
            "title": {
                "fragments": [],
                "text": "A Cognitive Model of Planning"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A cognitive model of the planning process that generalizes the theoretical architecture of the Hearsay-II system and illustrates its assumptions with a \u201cthinking aloud\u201d protocol is presented and the performance of a computer simulation of the model is described."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6055988"
                        ],
                        "name": "J. Kiley Hamlin",
                        "slug": "J.-Kiley-Hamlin",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Kiley Hamlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kiley Hamlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37774552"
                        ],
                        "name": "T. Ullman",
                        "slug": "T.-Ullman",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Ullman",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074160"
                        ],
                        "name": "C. Baker",
                        "slug": "C.-Baker",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Baker",
                            "middleNames": [
                                "Ian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Baker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12082052,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "52e954d6b389e65ed2ab23fb8a4a54ede5b997ce",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluating individuals based on their pro- and anti-social behaviors is fundamental to successful human interaction. Recent research suggests that even preverbal infants engage in social evaluation; however, it remains an open question whether infants' judgments are driven uniquely by an analysis of the mental states that motivate others' helpful and unhelpful actions, or whether non-mentalistic inferences are at play. Here we present evidence from 10-month-olds, motivated and supported by a Bayesian computational model, for mentalistic social evaluation in the first year of life.A video abstract of this article can be viewed at http://youtu.be/rD_Ry5oqCYE."
            },
            "slug": "The-mentalistic-basis-of-core-social-cognition:-in-Hamlin-Ullman",
            "title": {
                "fragments": [],
                "text": "The mentalistic basis of core social cognition: experiments in preverbal infants and a computational model."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Evidence from 10-month-olds is presented, motivated and supported by a Bayesian computational model, for mentalistic social evaluation in the first year of life."
            },
            "venue": {
                "fragments": [],
                "text": "Developmental science"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152871994"
                        ],
                        "name": "J. Hamlin",
                        "slug": "J.-Hamlin",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hamlin",
                            "middleNames": [
                                "Kiley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hamlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5745658"
                        ],
                        "name": "K. Wynn",
                        "slug": "K.-Wynn",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Wynn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3594555"
                        ],
                        "name": "P. Bloom",
                        "slug": "P.-Bloom",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bloom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bloom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1255426,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e8d904621f75209af0c1a8fc12e14dede0dffece",
            "isKey": false,
            "numCitedBy": 864,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The capacity to evaluate other people is essential for navigating the social world. Humans must be able to assess the actions and intentions of the people around them, and make accurate decisions about who is friend and who is foe, who is an appropriate social partner and who is not. Indeed, all social animals benefit from the capacity to identify individual conspecifics that may help them, and to distinguish these individuals from others that may harm them. Human adults evaluate people rapidly and automatically on the basis of both behaviour and physical features, but the ontogenetic origins and development of this capacity are not well understood. Here we show that 6- and 10-month-old infants take into account an individual\u2019s actions towards others in evaluating that individual as appealing or aversive: infants prefer an individual who helps another to one who hinders another, prefer a helping individual to a neutral individual, and prefer a neutral individual to a hindering individual. These findings constitute evidence that preverbal infants assess individuals on the basis of their behaviour towards others. This capacity may serve as the foundation for moral thought and action, and its early developmental emergence supports the view that social evaluation is a biological adaptation."
            },
            "slug": "Social-evaluation-by-preverbal-infants-Hamlin-Wynn",
            "title": {
                "fragments": [],
                "text": "Social evaluation by preverbal infants"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that 6- and 10-month-old infants take into account an individual\u2019s actions towards others in evaluating that individual as appealing or aversive: infants prefer an individual who helps another to one who hinders another, prefer a helping individual to a neutral individual, and prefer a neutralindividual to a hindering individual."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262347"
                        ],
                        "name": "A. Turing",
                        "slug": "A.-Turing",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Turing",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Turing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "Alan Turing suspected that it is easier to build and educate a child-machine than try to fully capture adult human cognition (Turing, 1950)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14636783,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "2d5673caa9e6af3a7b82a43f19ee920992db07ad",
            "isKey": false,
            "numCitedBy": 4502,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose to consider the question, \u201cCan machines think?\u201d\u2663 This should begin with definitions of the meaning of the terms \u201cmachine\u201d and \u201cthink\u201d. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words \u201cmachine\u201d and \u201cthink\u201d are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, \u201cCan machines think?\u201d is to be sought in a statistical survey such as a Gallup poll."
            },
            "slug": "Computing-Machinery-and-Intelligence-Turing",
            "title": {
                "fragments": [],
                "text": "Computing Machinery and Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The question, \u201cCan machines think?\u201d is considered, and the question is replaced by another, which is closely related to it and is expressed in relatively unambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2967041"
                        ],
                        "name": "Owen Macindoe",
                        "slug": "Owen-Macindoe",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Macindoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Macindoe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 216
                            }
                        ],
                        "text": "This agent can be useful in different ways under different circumstances, such as getting items, clearing paths, fighting, defending, healing, and providing information \u2013 all under the general notion of being helpful (Macindoe, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7639788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69332e874da533547eebf0de374d0495d2f86822",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Effective AI sidekicks must solve the interlinked problems of understanding what their human collaborator's intentions are and planning actions to support them. This thesis explores a range of approximate but tractable approaches to planning for AI sidekicks based on decision-theoretic methods that reason about how the sidekick's actions will effect their beliefs about unobservable states of the world, including their collaborator's intentions. In doing so we extend an existing body of work on decision-theoretic models of assistance to support information gathering and communication actions. We also apply Monte Carlo tree search methods for partially observable domains to the problem and introduce an ensemble-based parallelization strategy. These planning techniques are demonstrated across a range of video game domains. (Copies available exclusively from MIT Libraries, libraries.mit.edu/docs - docs@mit.edu)"
            },
            "slug": "Sidekick-agents-for-sequential-planning-problems-Macindoe",
            "title": {
                "fragments": [],
                "text": "Sidekick agents for sequential planning problems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A range of approximate but tractable approaches to planning for AI sidekicks based on decision-theoretic methods that reason about how the sidekick's actions will effect their beliefs about unobservable states of the world, including their collaborator's intentions are explored."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2741819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd626564bd47e9fc67a5b276301282ba2fe3d833",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the runtime cost of the classifier, is observed to scale approximately logarithmically with the number of classes. The features selected by joint training are generic edge-like features, whereas the features chosen by training each class separately tend to be more object-specific. The generic features generalize better and considerably reduce the computational cost of multiclass object detection"
            },
            "slug": "Sharing-Visual-Features-for-Multiclass-and-Object-Torralba-Murphy",
            "title": {
                "fragments": [],
                "text": "Sharing Visual Features for Multiclass and Multiview Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views) and considerably reduce the computational cost of multiclass object detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7211457"
                        ],
                        "name": "Donald Favareau",
                        "slug": "Donald-Favareau",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Favareau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Favareau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 44
                            }
                        ],
                        "text": "Is it the ability to reuse symbols by name (Deacon, 1998)?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 51
                            }
                        ],
                        "text": "2002)? Is it the ability to re-use symbols by name (Deacon 1998)? Is it the ability to understand others intentionally and build shared intentionality (Bloom 2000; Frank et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55239623,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "75ad326821587b1baf98725582a700621782c353",
            "isKey": false,
            "numCitedBy": 875,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The Symbolic Species: The Co-evolution of Language and the Brain by Terrence W. Deacon. New York: W.W. Norton, 1997, 527 pp. Reviewed by Donald Favareau University of California, Los Angeles In 866, the recently formed Societe Linguistique de Paris passed an official resolution banning the presentation of any further papers regarding the origins of human language. The nature of the inquiry itself, it was felt, it lacked even the dis- possibility of scientific certainty, and all work pertaining to was likewise missed on the grounds of being empirically irresolvable, incorrigibly speculative, and unproductively divisive. Terrence W. Deacon, almost a century and a half vocative polemic that will doubtlessly incur even later, has authored a pro- more violent censure on the part of his detractors. Empirically vigorous, incisively speculative and with the poten- tial to be productively divisive, Deacon's The Symbolic Species: The Co-Evolu- if tion of Language and the Brain challenges many, not most, of the assumptions underlying modern linguistic theory. Of particular interest to linguists will be Deacon's refutation of Chomsky's (1972) Universal Grammar paradigm, as well as his corollary rejection of the pos- sibility of innate syntactic processing or language-learning modules nestled deep within the human brain. bolic representation at all, Instead, claims Deacon, language itself and the sym- which it evinces and encodes lies not inside individual brains but at the interface between biology and culture. A biological anthropologist with extensive experience in neurology. Deacon supports this argument not, in itself, first with an appeal to evolutionary theory. Universality reliable indicator of is Deacon proposes, a what evolution has built into human brains (p. 339). Accordingly, the universal grammatical Chomskian notion that some kind of knowledge must be innate in human beings in order to ac- Precisely because count for certain otherwise unexplainable universal features regarding language is an argument which Deacon considers specious. some ver- sion of Chomsky's model is so deeply embedded in contemporary devoted to linguistic theory, a considerable portion of this The Symbolic Species is its refutation. It is argument, to the exclusion of so many other fascinating and corollary argu- ments presented throughout the work, struct. that this review will endeavor to recon- Fundamental gist ral selection to Deacon's argument is nineteenth century American psycholo- James Mark Baldwin's that this modification 895; 1902) theory that the very context wherein natu- its takes place can itself be modified by the behavior of inhabitants and may, in turn, generate subsequent new sets of selection Issues in Applied Linguistics ISSN 1050-4273 Vol. 9 No. 2, 1998, Regents of the University of California"
            },
            "slug": "The-Symbolic-Species:-The-Co-evolution-of-Language-Favareau",
            "title": {
                "fragments": [],
                "text": "The Symbolic Species: The Co-evolution of Language and the Brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2733044"
                        ],
                        "name": "Dejan Pecevski",
                        "slug": "Dejan-Pecevski",
                        "structuredName": {
                            "firstName": "Dejan",
                            "lastName": "Pecevski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dejan Pecevski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981334"
                        ],
                        "name": "Lars Buesing",
                        "slug": "Lars-Buesing",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Buesing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lars Buesing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10452288,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "76ec892e84cb3e6d6942c90d973ba64783068468",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "An important open problem of computational neuroscience is the generic organization of computations in networks of neurons in the brain. We show here through rigorous theoretical analysis that inherent stochastic features of spiking neurons, in combination with simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of spiking neurons to carry out probabilistic inference through sampling in general graphical models. In particular, it enables them to carry out probabilistic inference in Bayesian networks with converging arrows (\u201cexplaining away\u201d) and with undirected loops, that occur in many real-world tasks. Ubiquitous stochastic features of networks of spiking neurons, such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization. We demonstrate through computer simulations that this approach can be scaled up to neural emulations of probabilistic inference in fairly large graphical models, yielding some of the most complex computations that have been carried out so far in networks of spiking neurons."
            },
            "slug": "Probabilistic-Inference-in-General-Graphical-Models-Pecevski-Buesing",
            "title": {
                "fragments": [],
                "text": "Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Ubiquitous stochastic features of networks of spiking neurons, such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization and can be scaled up to neural emulations of probabilistic inference in fairly large graphical models."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49750831"
                        ],
                        "name": "Daniel J. Graham",
                        "slug": "Daniel-J.-Graham",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Graham",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel J. Graham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 180
                            }
                        ],
                        "text": "Such rapid feedback is consistent with the notion that corticothalamic signals could function like the \u201cack\u201d (acknowledgment) system used on the Internet to ensure packet delivery (Graham 2014; Graham and Rockmore 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 252
                            }
                        ],
                        "text": "We should consider that one of the missing components in deep learning models of cognition \u2013 and of most large-scale models of brain and cognitive function \u2013 is an understanding of how signals are selectively routed to different destinations in brains (Graham 2014; Graham and Rockmore 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7244496,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ebdf23c23a2de20f38af7a0456e45c665640cbe9",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "As mapping the genome was the great biological challenge a generation ago, so today is mapping brain network dynamics, thanks in part to President Obama's BRAIN initiative (Insel et al., 2013). Factors influencing the emergence of network dynamics, both in the brain and in other networks, can be roughly divided into three classes: those pertaining to node dynamics; those pertaining to topology (connectivity); and those pertaining to routing (how signals are passed across the network). But while single neuron dynamics are reasonably well understood, and while researchers have begun to elucidate key aspects of network topology in brains, very little work has been devoted to possible routing schemes in the brain (Graham and Rockmore, 2011). Indeed, brain networks must possess a systematic routing scheme, but current methods and models often make implicit assumptions about routing\u2014or ignore it altogether."
            },
            "slug": "Routing-in-the-brain-Graham",
            "title": {
                "fragments": [],
                "text": "Routing in the brain"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Mapping brain network dynamics, thanks in part to President Obama's BRAIN initiative, shows that brain networks must possess a systematic routing scheme, but current methods and models often make implicit assumptions about routing and ignore it altogether."
            },
            "venue": {
                "fragments": [],
                "text": "Front. Comput. Neurosci."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2440249"
                        ],
                        "name": "Pedro Tsividis",
                        "slug": "Pedro-Tsividis",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Tsividis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro Tsividis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16958521,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "a3ec6e6ed4dff457f42b19368318560312d6cbb0",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "How do children identify promising hypotheses worth testing? Many studies have shown that preschoolers can use patterns of covariation together with prior knowledge to learn causal relationships. However, covariation data are not always available and myriad hypotheses may be commensurate with substantive knowledge about content domains. We propose that children can identify high-level abstract features common to effects and their candidate causes and use these to guide their search. We investigate children\u2019s sensitivity to two such high-level features \u2014 proportion and dynamics, and show that preschoolers can use these to link effects and candidate causes, even in the absence of other disambiguating information."
            },
            "slug": "Constraints-on-Hypothesis-Selection-in-Causal-Tsividis-Schulz",
            "title": {
                "fragments": [],
                "text": "Constraints on Hypothesis Selection in Causal Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work investigates children\u2019s sensitivity to two high-level features \u2014 proportion and dynamics, and shows that preschoolers can use these to link effects and candidate causes, even in the absence of other disambiguating information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743610"
                        ],
                        "name": "B. Tversky",
                        "slug": "B.-Tversky",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Tversky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Tversky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141671"
                        ],
                        "name": "K. Hemenway",
                        "slug": "K.-Hemenway",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Hemenway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hemenway"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 272
                            }
                        ],
                        "text": "\u2026as two wheels connected by a platform, which provides the base for a post, which holds the handlebars, etc. Parts can themselves be composed of sub-parts, forming a \u201cpartonomy\u201d of part-whole relationships that can be used to construct conceptual representations (Tversky & Hemenway, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15789598,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0ccd1512e0b6c60cd9a149c2ae61078432d16302",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts may be organized into taxonomies varying in inclusiveness or abstraction, such as furniture, table, card table or animal, bird, robin. For taxonomies of common objects and organisms, the basic level, the level of table and bird, has been determined to be most informative (Rosch, Mervis, Gray, Johnson, & Boyes-Braem, 1976). Psychology, linguistics, and anthropology have produced a variety of measures of perception, behavior, and communication that converge on the basic level. Here, we present data showing that the basic level differs qualitatively from other levels in taxonomies of objects and of living things and present an explanation for why so many measures converge at that level. We have found that part terms proliferate in subjects' listings of attributes characterizing category members at the basic level, but are rarely listed at a general level. At a more specific level, fewer parts are listed, though more are judged to be true. Basic level objects are distinguished from one another by parts, but members of subordinate categories share parts and differ from one another on other attributes. Informants agree on the parts of objects, and also on relative \"goodness\" of the various parts. Perceptual salience and functional significance both appear to contribute to perceived part goodness. Names of parts frequently enjoy a duality not evident in names of other attributes; they refer at once to a particular appearance and to a particular function. We propose that part configuration underlies the various empirical operations of perception, behavior, and communication that converge at the basic level. Part configuration underlies the perceptual measures because it determines the shapes of objects to a large degree. Parts underlie the behavioral tasks because most of our behaviors is indirect toward parts of objects. Labeling appears to follow the natural breaks of perception and behavior; consequently, part configuration also underlies communication measures. Because elements of more abstract taxonomies, such as scenes and events, can also be decomposed into parts, this analysis provides a bridge to organization in other domains of knowledge. Knowledge organization by parts (partonomy) is contrasted to organization by kinds (taxonomy). Taxonomies serve to organize numerous classes of entities and to allow inference from larger sets to sets included in them. Partonomies serve to separate entities into their structural components and to organize knowledge of function by components of structure. The informativeness of the basic level may originate from the availability of inference from structure to function at that level."
            },
            "slug": "Objects,-parts,-and-categories.-Tversky-Hemenway",
            "title": {
                "fragments": [],
                "text": "Objects, parts, and categories."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Data is presented showing that the basic level differs qualitatively from other levels in taxonomies of objects and of living things and an explanation for why so many measures converge at that level is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657032"
                        ],
                        "name": "M. Asada",
                        "slug": "M.-Asada",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Asada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 2
                            }
                        ],
                        "text": "& Asada, M. (2011). Emergence of mirror neuron system:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1938,
                                "start": 76
                            }
                        ],
                        "text": "In the two last decades, the field of Developmental and Cognitive Robotics (Asada et al. 2009; Cangelosi and Schlesinger 2015), in strong interaction with developmental psychology and neuroscience, has achieved significant advances in computational modeling of mechanisms of autonomous development and learning in human infants, and applied them to solve difficult artificial intelligence (AI) problems. These mechanisms include the interaction between several systems that guide active exploration in large and open environments: curiosity, intrinsically motivated reinforcement learning (Barto 2013; Oudeyer et al. 2007; Schmidhuber 1991) and goal exploration (Baranes and Oudeyer 2013), social learning and natural interaction (Chernova and Thomaz 2014; Vollmer et al. 2014), maturation (Oudeyer et al. 2013), and embodiment (Pfeifer et al. 2007). These mechanisms crucially complement processes of incremental online model building (Nguyen and Peters 2011), as well as inference and representation learning approaches discussed in the target article. Intrinsic motivation, curiosity and free play. For example, models of how motivational systems allow children to choose which goals to pursue, or which objects or skills to practice in contexts of free play, and how this can affect the formation of developmental structures in lifelong learning have flourished in the last decade (Baldassarre and Mirolli 2013; Gottlieb et al. 2013). Indepth models of intrinsically motivated exploration, and their links with curiosity, information seeking, and the \u201cchild-as-ascientist\u201d hypothesis (see Gottlieb et al. [2013] for a review), have generated new formal frameworks and hypotheses to understand their structure and function. For example, it was shown that intrinsically motivated exploration, driven by maximization of learning progress (i.e., maximal improvement of predictive or control models of the world; see Oudeyer et al. [2007] and Schmidhuber [1991]) can self-organize long-term developmental structures, where skills are acquired in an order and with timing that share fundamental properties with human development (Oudeyer and Smith 2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 443,
                                "start": 231
                            }
                        ],
                        "text": "However, similar reported strategies for machine architectures, algorithms, and performance demonstrate only marginal success when used as protocols to reach nearer cognitive-emotional humanness in trending social robot archetypes (Arbib & Fellous 2004; Asada 2015; Berdahl 2010; Di & Wu 2015; Han et al. 2013; Hiolle et al. 2014; Kaipa et al. 2010; McShea 2013; Read et al. 2010; Thomaz & Cakmak 2013; Wallach et al. 2010; Youyou et al. 2015), emphasizing serious need for improved adaptive quasimodel-free/-based neural nets, trainable distributed cognitionemotion mapping, and artificial personality trait parameterization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 528,
                                "start": 312
                            }
                        ],
                        "text": "Even simplistic artificial cognitive-emotional profiles and personalities thus effect varying control over acquisition and lean of machine domain-general/-specific knowledge, perception and expression of flat or excessive machine affect, and rationality and use of inferential machine attitudes/opinions/beliefs (Arbib & Fellous 2004; Asada 2015; Berdahl 2010; Cardon 2006; Davies 2016; Di & Wu 2015; Han et al. 2013; Hiolle et al. 2014; Kaipa et al. 2010; McShea 2013; Read et al. 2010; Wallach et al. 2010; Youyou et al. 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1616,
                                "start": 76
                            }
                        ],
                        "text": "In the two last decades, the field of Developmental and Cognitive Robotics (Asada et al. 2009; Cangelosi and Schlesinger 2015), in strong interaction with developmental psychology and neuroscience, has achieved significant advances in computational modeling of mechanisms of autonomous development and learning in human infants, and applied them to solve difficult artificial intelligence (AI) problems. These mechanisms include the interaction between several systems that guide active exploration in large and open environments: curiosity, intrinsically motivated reinforcement learning (Barto 2013; Oudeyer et al. 2007; Schmidhuber 1991) and goal exploration (Baranes and Oudeyer 2013), social learning and natural interaction (Chernova and Thomaz 2014; Vollmer et al. 2014), maturation (Oudeyer et al. 2013), and embodiment (Pfeifer et al. 2007). These mechanisms crucially complement processes of incremental online model building (Nguyen and Peters 2011), as well as inference and representation learning approaches discussed in the target article. Intrinsic motivation, curiosity and free play. For example, models of how motivational systems allow children to choose which goals to pursue, or which objects or skills to practice in contexts of free play, and how this can affect the formation of developmental structures in lifelong learning have flourished in the last decade (Baldassarre and Mirolli 2013; Gottlieb et al. 2013). Indepth models of intrinsically motivated exploration, and their links with curiosity, information seeking, and the \u201cchild-as-ascientist\u201d hypothesis (see Gottlieb et al. [2013] for a review), have generated new formal frameworks and hypotheses to understand their structure and function."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3877631,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fe723b3b979c8c4e11a9e525f042b63a74a5d8a1",
            "isKey": true,
            "numCitedBy": 42,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Development-of-artificial-empathy-Asada",
            "title": {
                "fragments": [],
                "text": "Development of artificial empathy"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience Research"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145303461"
                        ],
                        "name": "D. Buckingham",
                        "slug": "D.-Buckingham",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Buckingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Buckingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2852732"
                        ],
                        "name": "T. Shultz",
                        "slug": "T.-Shultz",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Shultz",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 147
                            }
                        ],
                        "text": "\u2026to physical reasoning tasks such as balance-beam rules (McClelland, 1988; Shultz, 2003) or rules relating distance, velocity, and time in motion (Buckingham & Shultz, 2000), but these networks do not attempt to work with complex scenes as input or a wide range of scenarios and judgments as in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144049430,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "23a2585fd423cb8402b93a9db9c5f1dd44d863f9",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist simulations of children's acquisition of distance (d), time (t), and velocity (v) concepts using a generative algorithm-cascade-correlation-are reported. Rules that correlated most highly with network responses during training were consistent with the developmental course of children's concepts. Networks integrated the defining dimensions of the concepts first by identity rules (e.g., v = d), then additive rules (e.g., v = d - t), and finally multiplicative rules (e.g., v = d \u00f7 t. The results are discussed in terms of similarity to children's development, the contribution of connectionism to the study of cognitive development, contrasts with alternative models, and directions for future research."
            },
            "slug": "The-Developmental-Course-of-Distance,-Time,-and-Buckingham-Shultz",
            "title": {
                "fragments": [],
                "text": "The Developmental Course of Distance, Time, and Velocity Concepts:A Generative Connectionist Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144999249"
                        ],
                        "name": "G. Kane",
                        "slug": "G.-Kane",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kane",
                            "middleNames": [
                                "Stanley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kane"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71267845,
            "fieldsOfStudy": [
                "Computer Science",
                "History"
            ],
            "id": "78f6f0ac3d501cb0073a7d94edde5267044a59ae",
            "isKey": false,
            "numCitedBy": 2758,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Computing: Theory and Practice , by Philip Wasserman, 230 pp, $41.95, with illus, ISBN 0-442-20743-3, New York, NY, Van Nostrand Reinhold, 1989. Neural Networks: A Tutorial , by Michael Chester, 182 pp, $38, with illus, ISBN 0-13-368903-4, Englewood Cliffs, NJ, Prentice Hall, 1993. Neural Networks: Algorithms, Applications, and Programming Techniques , by James Freeman and David Skapura, 401 pp, $50.50, with illus, ISBN 0-201-51376-5, Reading, Mass, Addison-Wesley, 1991. Understanding Neural Networks: Computer Explorations , vol 1: Basic Networks , vol 2: Advanced Networks , 309, 367 pp, by Maureen Caudill and Charles Butler, paper, with illus, spiral-bound, with 1 diskette/vol, $39.95/vol, vol 1: ISBN0-262-53102-X (Macintosh), 0-262-53099-6 (IBM), vol 2: ISBN 0-262-53103-8 (Macintosh), 0-262-53100-3 (IBM), Cambridge, Mass, The MIT Press, 1992. Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons , in which they proved that neural networks, as then conceived, can"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Kane",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol 1: Foundations, vol 2: Psychological and Biological Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons, in which they proved that neural networks, as then conceived, can be proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2963454"
                        ],
                        "name": "Leon Rozenblit",
                        "slug": "Leon-Rozenblit",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Rozenblit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leon Rozenblit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391042"
                        ],
                        "name": "F. Keil",
                        "slug": "F.-Keil",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keil",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Keil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 231923,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2958f3946bcf913e09a16d95bc23260ba33447a8",
            "isKey": false,
            "numCitedBy": 604,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "People feel they understand complex phenomena with far greater precision, coherence, and depth than they really do; they are subject to an illusion-an illusion of explanatory depth. The illusion is far stronger for explanatory knowledge than many other kinds of knowledge, such as that for facts, procedures or narratives. The illusion for explanatory knowledge is most robust where the environment supports real-time explanations with visible mechanisms. We demonstrate the illusion of depth with explanatory knowledge in Studies 1-6. Then we show differences in overconfidence about knowledge across different knowledge domains in Studies 7-10. Finally, we explore the mechanisms behind the initial confidence and behind overconfidence in Studies 11 and 12. Implications for the roles of intuitive theories in models of concepts and cognition are discussed."
            },
            "slug": "The-misunderstood-limits-of-folk-science:-an-of-Rozenblit-Keil",
            "title": {
                "fragments": [],
                "text": "The misunderstood limits of folk science: an illusion of explanatory depth"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The mechanisms behind the initial confidence and behind overconfidence, the illusion of depth with explanatory knowledge, and the roles of intuitive theories in models of concepts and cognition are explored."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3378440"
                        ],
                        "name": "Jessica S. Horst",
                        "slug": "Jessica-S.-Horst",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Horst",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jessica S. Horst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40498952"
                        ],
                        "name": "L. Samuelson",
                        "slug": "L.-Samuelson",
                        "structuredName": {
                            "firstName": "Larissa",
                            "lastName": "Samuelson",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Samuelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 244
                            }
                        ],
                        "text": "\u2026the context of learning the meanings of words in their native language (Carey & Bartlett, 1978; Landau, Smith, & Jones, 1988; E. M. Markman, 1989; Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002; F. Xu & Tenenbaum, 2007, although see Horst and Samuelson 2008 regarding memory limitations)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145499887,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a14d5baf7c92b4dcde7925b5202663ba8292c540",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Four experiments explored the processes that bridge between referent selection and word learning. Twenty-four-month-old infants were presented with several novel names during a referent selection task that included both familiar and novel objects and tested for retention after a 5-min delay. The 5-min delay ensured that word learning was based on retrieval from long-term memory. Moreover, the relative familiarity of objects used during the retention test was explicitly controlled. Across experiments, infants were excellent at referent selection, but very poor at retention. Although the highly controlled retention test was clearly challenging, infants were able to demonstrate retention of the first 4 novel names presented in the session when referent selection was augmented with ostensive naming. These results suggest that fast mapping is robust for reference selection but might be more transient than previously reported for lexical retention. The relations between reference selection and retention are discussed in terms of competitive processes on 2 timescales: competition among objects on individual referent selection trials and competition among multiple novel name-object mappings made across an experimental session."
            },
            "slug": "Fast-Mapping-but-Poor-Retention-by-24-Month-Old-Horst-Samuelson",
            "title": {
                "fragments": [],
                "text": "Fast Mapping but Poor Retention by 24-Month-Old Infants."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results suggest that fast mapping is robust for reference selection but might be more transient than previously reported for lexical retention, and competitive processes on 2 timescales: competition among objects on individual referent selection trials and competition among multiple novel name-object mappings made across an experimental session."
            },
            "venue": {
                "fragments": [],
                "text": "Infancy : the official journal of the International Society on Infant Studies"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10947180"
                        ],
                        "name": "K. O. Solomon",
                        "slug": "K.-O.-Solomon",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Solomon",
                            "middleNames": [
                                "Olseth"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. O. Solomon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023259"
                        ],
                        "name": "D. Medin",
                        "slug": "D.-Medin",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Medin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Medin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123421073"
                        ],
                        "name": "E. Lynch",
                        "slug": "E.-Lynch",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Lynch",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lynch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "These abilities are not independent; rather they hang together and interact (Solomon et al., 1999), coming for free with the acquisition of the underlying concept."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44463078,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0bda01e55d36f821440d6bf5b68ec1c94f950f65",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Concepts-do-more-than-categorize-Solomon-Medin",
            "title": {
                "fragments": [],
                "text": "Concepts do more than categorize"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2471378"
                        ],
                        "name": "L. Barsalou",
                        "slug": "L.-Barsalou",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Barsalou",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Barsalou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson &\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 93
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15591839,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f376765c81c99074db4ac8ddceb8b18c6d649a24",
            "isKey": false,
            "numCitedBy": 1599,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "People construct ad hoc categories to achieve goals. For example, constructing the category of \u201cthings to sell at a garage sale\u201d can be instrumental to achieving the goal of selling unwanted possessions. These categories differ from common categories (e.g., \u201cfruit,\u201d \u201cfurniture\u201d) in that ad hoc categories violate the correlational structure of the environment and are not well established in memory. Regarding the latter property, the category concepts, concept-to-instance associations, and instance-to-concept associations structuring ad hoc categories are shown to be much less established in memory than those of common categories. Regardless of these differences, however, ad hoc categories possess graded structures (i.e., typicality gradients) as salient as those structuring common categories. This appears to be the result of a similarity comparison process that imposes graded structure on any category regardless of type."
            },
            "slug": "Ad-hoc-categories-Barsalou",
            "title": {
                "fragments": [],
                "text": "Ad hoc categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Ad hoc categories possess graded structures as salient as those structuring common categories irrespective of type, the result of a similarity comparison process that imposes graded structure on any category regardless of type."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1960857"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708163"
                        ],
                        "name": "R. Miall",
                        "slug": "R.-Miall",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Miall",
                            "middleNames": [
                                "Chris"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144391220"
                        ],
                        "name": "M. Kawato",
                        "slug": "M.-Kawato",
                        "structuredName": {
                            "firstName": "Mitsuo",
                            "lastName": "Kawato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kawato"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 107
                            }
                        ],
                        "text": "1995), the cerebellum to implement fast time-scale computations possibly acquired with supervised learning (Kawato et al. 2011; Wolpert et al. 1998), and the limbic brain structures interfacing the brain to the body and generating motivations, emotions, and the value of things (Mirolli et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10383815,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "21e47a5b98afa4c56844a18c117461dc6150956d",
            "isKey": false,
            "numCitedBy": 2021,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Internal-models-in-the-cerebellum-Wolpert-Miall",
            "title": {
                "fragments": [],
                "text": "Internal models in the cerebellum"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847175"
                        ],
                        "name": "M. Minsky",
                        "slug": "M.-Minsky",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Minsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Minsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61610148,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6d801505d744dff6bb787b284ded9c2ef901ebbc",
            "isKey": false,
            "numCitedBy": 6444,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-framework-for-representing-knowledge-Minsky",
            "title": {
                "fragments": [],
                "text": "A framework for representing knowledge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 137
                            }
                        ],
                        "text": "Parallel to these developments, a radically different approach was being explored, based on neuron-like \u201csub-symbolic\u201d computations (e.g., Fukushima, 1980; Grossberg, 1976; Rosenblatt, 1958)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206775608,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "69e68bfaadf2dccff800158749f5a50fe82d173b",
            "isKey": false,
            "numCitedBy": 3717,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern."
            },
            "slug": "Neocognitron:-A-self-organizing-neural-network-for-Fukushima",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A neural network model for a mechanism of visual pattern recognition that is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity of their shapes without affected by their positions."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46636867"
                        ],
                        "name": "H. Bayer",
                        "slug": "H.-Bayer",
                        "structuredName": {
                            "firstName": "Hannah",
                            "lastName": "Bayer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47328071"
                        ],
                        "name": "P. Glimcher",
                        "slug": "P.-Glimcher",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Glimcher",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Glimcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 138
                            }
                        ],
                        "text": "In particular, the phasic firing of midbrain dopaminergic neurons is qualitatively (Schultz, Dayan, & Montague, 1997) and quantitatively (Bayer & Glimcher, 2005) consistent with the reward prediction error that drives updating of model-free value estimates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500693,
            "fieldsOfStudy": [
                "Psychology",
                "Biology",
                "Computer Science"
            ],
            "id": "a731e1d4b4960b3cb137700c1d236439e146b705",
            "isKey": false,
            "numCitedBy": 1159,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Midbrain-Dopamine-Neurons-Encode-a-Quantitative-Bayer-Glimcher",
            "title": {
                "fragments": [],
                "text": "Midbrain Dopamine Neurons Encode a Quantitative Reward Prediction Error Signal"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725299"
                        ],
                        "name": "Daniel Tarlow",
                        "slug": "Daniel-Tarlow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Tarlow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Tarlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9249495,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84e2df16a6913a3679780071a939d9eb178d41c0",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex non-Gaussian factors, there is still a significant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising."
            },
            "slug": "Learning-to-Pass-Expectation-Propagation-Messages-Heess-Tarlow",
            "title": {
                "fragments": [],
                "text": "Learning to Pass Expectation Propagation Messages"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work studies whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model to map EP message inputs to EP message outputs, and provides empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742472"
                        ],
                        "name": "S. Craw",
                        "slug": "S.-Craw",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Craw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1260952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8386e17bb2b2b568f0808bfc490d459cbaf7c92",
            "isKey": false,
            "numCitedBy": 401,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "with the purpose of recalling cases that are similar to a target problem in order to help solve the problem. People commonly use this approach informally in problem solving and forecasting (see analogy). It can also be used as the basis for designing expert systems by starting with examples rather than with the process. CBR is a term used in the fields of cognitive science and artificial intelligence. The forecasting method of structured analogies could be viewed as one type of CBR. We have been unable to locate any tests of the predictive validity of CBR."
            },
            "slug": "Case-Based-Reasoning-Craw",
            "title": {
                "fragments": [],
                "text": "Case-Based Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "CBR is a term used in the fields of cognitive science and artificial intelligence for recalling cases that are similar to a target problem in order to help solve the problem."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145423516"
                        ],
                        "name": "S. Ma",
                        "slug": "S.-Ma",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "Benchmark tasks such as the ImageNet data set for object recognition provides hundreds or thousands of examples per class (Krizhevsky et al., 2012; Russakovsky et al., 2015) \u2013 1000 hairbrushes, 1000 pineapples, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 202
                            }
                        ],
                        "text": "Similarly, recent results applying convolutional nets to the far more challenging ImageNet object recognition benchmark have shown that human-level performance is within reach on that data set as well (Russakovsky et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 66
                            }
                        ],
                        "text": "networks is also the best current approach for object recognition (He et al., 2015; Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2014), where the high-level feature representations of these convolutional nets have also been used to predict patterns of neural response in human and macaque IT cortex (Khaligh-Razavi & Kriegeskorte, 2014; Kriegeskorte, 2015; Yamins et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 149
                            }
                        ],
                        "text": "Training large-scale relatively generic networks is also the best current approach for object recognition (He et al., 2015; Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2014), where the high-level feature representations of these convolutional nets have also been used to\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 164
                            }
                        ],
                        "text": "In the years since, convnets continue to dominate, recently approaching human-level performance on some object recognition benchmarks (He, Zhang, Ren, & Sun, 2015; Russakovsky et al., 2015; Szegedy et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2930547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "isKey": true,
            "numCitedBy": 25491,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements."
            },
            "slug": "ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "ImageNet Large Scale Visual Recognition Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7230302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e33cbb25a8c7390aec6a398e36381f4f7770c283",
            "isKey": false,
            "numCitedBy": 2219,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current speech recognition systems use hidden Markov models ( HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternati ve way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients a s input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech rec ognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and repres nts the shared views of four research groups who have had recent successes in using deep neural networks for a coustic modeling in speech recognition."
            },
            "slug": "Deep-Neural-Networks-for-Acoustic-Modeling-in-Hinton-Deng",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks for Acoustic Modeling in Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper provides an overview of this progress and repres nts the shared views of four research groups who have had recent successes in using deep neural networks for a coustic modeling in speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700569"
                        ],
                        "name": "T. Moeslund",
                        "slug": "T.-Moeslund",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Moeslund",
                            "middleNames": [
                                "Baltzer"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moeslund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144046599"
                        ],
                        "name": "A. Hilton",
                        "slug": "A.-Hilton",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Hilton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48378203"
                        ],
                        "name": "V. Kr\u00fcger",
                        "slug": "V.-Kr\u00fcger",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Kr\u00fcger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kr\u00fcger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9815253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4e16d64b77d133b7382440a08091d64008dd923",
            "isKey": false,
            "numCitedBy": 2738,
            "numCiting": 520,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-advances-in-vision-based-human-motion-Moeslund-Hilton",
            "title": {
                "fragments": [],
                "text": "A survey of advances in vision-based human motion capture and analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144815"
                        ],
                        "name": "A. Graybiel",
                        "slug": "A.-Graybiel",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Graybiel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graybiel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 244
                            }
                        ],
                        "text": "2016; Doya 1999): the cortex to statically and dynamically store knowledge acquired by associative learning processes (Penhune & Steele 2012; Shadmehr & Krakauer 2008), the basal ganglia to learn to select information by reinforcement learning (Graybiel 2005; Houk et al. 1995), the cerebellum to implement fast time-scale computations possibly acquired with supervised learning (Kawato et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12490490,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "e7edb8c748e011b834b1fa5454991d6e541d5a0e",
            "isKey": false,
            "numCitedBy": 649,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-basal-ganglia:-learning-new-tricks-and-loving-Graybiel",
            "title": {
                "fragments": [],
                "text": "The basal ganglia: learning new tricks and loving it"
            },
            "venue": {
                "fragments": [],
                "text": "Current Opinion in Neurobiology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753394"
                        ],
                        "name": "D. Bobrow",
                        "slug": "D.-Bobrow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bobrow",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bobrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699245"
                        ],
                        "name": "T. Winograd",
                        "slug": "T.-Winograd",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Winograd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "AI pioneers in other areas of research explicitly referenced human cognition, and even published papers in cognitive psychology journals (e.g.,\nBobrow & Winograd, 1977; Hayes-Roth & Hayes-Roth, 1979; Winograd, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7965074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "033ecd544c4e71b14a7d3ee0611a300a20b91232",
            "isKey": false,
            "numCitedBy": 950,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes KRL, a Knowledge Representation Language designed for use in understander systems. It outlines both the general concepts which underlie our research and the details of KRL-0, an experimental implementation of some of these concepts. KRL is an attempt to integrate procedural knowledge with a broad base of declarative forms. These forms provide a variety of ways to express the logical structure of the knowledge, in order to give flexibility in associating procedures (for memory and reasoning) with specific pieces of knowledge, and to control the relative accessibility of different facts and descriptions. The formalism for declarative knowledge is based on structured conceptual objects with associated descriptions. These objects form a network of memory units with several different sorts of linkages, each having well-specified implications for the retrieval process. Procedures can be associated directly with the internal structure of a conceptual object. This procedural attachment allows the steps for a particular operation to be determined by characteristics of the specific entities involved. The control structure of KRL is based on the belief that the next generation of intelligent programs will integrate data-directed and goal-directed processing by using multi-processing. It provides for a priority-ordered multi-process agenda with explicit (user-provided) strategies for scheduling and resource allocation. It provides procedure directories which operate along with process frameworks to allow procedural parameterization of the fundamental system processes for building, comparing, and retrieving memory structures. Future development of KRL will include integrating procedure definition with the descriptive formalism."
            },
            "slug": "On-Overview-of-KRL,-a-Knowledge-Representation-Bobrow-Winograd",
            "title": {
                "fragments": [],
                "text": "On Overview of KRL, a Knowledge Representation Language"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "KRL is an attempt to integrate procedural knowledge with a broad base of declarative forms to give flexibility in associating procedures with specific pieces of knowledge, and to control the relative accessibility of different facts and descriptions."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 2
                            }
                        ],
                        "text": ", Graves, A., Rezende, D. J. & Wierstra, D. (2015) DRAW: A recurrent neural network for image generation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1441,
                                "start": 0
                            }
                        ],
                        "text": "Graves (2014) learned a generative model of cursive handwriting using a recurrent neural network trained on handwriting data. Although it synthesizes impressive examples of handwriting in various styles, it requires a large training corpus and has not been applied to other tasks. The DRAW network performs both recognition and generation of handwritten digits using recurrent neural networks with a window of attention, producing a limited circular area of the image at each time step (Gregor et al. 2015). A more recent variant of DRAW was applied to generating examples of a novel character from just a single training example (Rezende et al. 2016). The model demonstrates an impressive ability to make plausible generalizations that go beyond the training examples, yet it generalizes too broadly in other cases, in ways that are not especially human-like. It is not clear that it could yet pass any of the \u201cvisual Turing tests\u201d in Lake et al. (2015a) (Fig. 5B), although we hope DRAW-style networks will continue to be extended and enriched, and could be made to pass these tests. Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process. An attentional window is only a crude approximation of the true causal process of drawing with a pen, and in Rezende et al. (2016) the attentional window is not pen-like at all, although a more accurate pen model could be incorporated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 5
                            }
                        ],
                        "text": "[DG] Graves, A. (2014) Generating sequences with recurrent neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Graves (2014) learned a generative model of cursive handwriting using a recurrent neural network trained on handwriting data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1697424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b1f4740ae37fd04f6ac007577bdd34621f0861",
            "isKey": true,
            "numCitedBy": 3151,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
            },
            "slug": "Generating-Sequences-With-Recurrent-Neural-Networks-Graves",
            "title": {
                "fragments": [],
                "text": "Generating Sequences With Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784997"
                        ],
                        "name": "N. Daw",
                        "slug": "N.-Daw",
                        "structuredName": {
                            "firstName": "Nathaniel",
                            "lastName": "Daw",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "This shift may arise from a rational arbitration between learning systems to balance the trade-off between flexibility and speed (Daw et al., 2005; Keramati, Dezfouli, & Piray, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16385268,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "767ed9b4c974411b46c445bd3e235515760ecc65",
            "isKey": false,
            "numCitedBy": 2048,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "A broad range of neural and behavioral data suggests that the brain contains multiple systems for behavioral choice, including one associated with prefrontal cortex and another with dorsolateral striatum. However, such a surfeit of control raises an additional choice problem: how to arbitrate between the systems when they disagree. Here, we consider dual-action choice systems from a normative perspective, using the computational theory of reinforcement learning. We identify a key trade-off pitting computational simplicity against the flexible and statistically efficient use of experience. The trade-off is realized in a competition between the dorsolateral striatal and prefrontal systems. We suggest a Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate. This provides a unifying account of a wealth of experimental evidence about the factors favoring dominance by either system."
            },
            "slug": "Uncertainty-based-competition-between-prefrontal-Daw-Niv",
            "title": {
                "fragments": [],
                "text": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work considers dual-action choice systems from a normative perspective, and suggests a Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 226
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026(rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (L. Schulz, 2012), problem-solving (Sternberg & Davidson, 1995), and epoch-making discoveries in scientific research (Langley, Bradshaw, Simon, & Zytkow,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 224
                            }
                        ],
                        "text": "In contrast, while representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "L. Schulz (2012) has suggested that abstract structural properties of problems contain information about the abstract forms of their solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17346427,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a5c602afb195c2203b1fe1138c5c6f6184f61da6",
            "isKey": true,
            "numCitedBy": 141,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-origins-of-inquiry:-inductive-inference-and-in-Schulz",
            "title": {
                "fragments": [],
                "text": "The origins of inquiry: inductive inference and exploration in early childhood"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145438593"
                        ],
                        "name": "Yanping Huang",
                        "slug": "Yanping-Huang",
                        "structuredName": {
                            "firstName": "Yanping",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanping Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "Moreover, we are beginning to understand how such methods could be implemented in neural circuits (Buesing, Bill, Nessler, & Maass, 2011; Huang & Rao, 2014; Pecevski, Buesing, & Maass, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14625799,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "626d879d735da1c6ef043550160362680fac9f44",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a spiking network model capable of performing both approximate inference and learning for any hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown. We demonstrate how such networks can sequentially learn hidden Markov models using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates."
            },
            "slug": "Neurons-as-Monte-Carlo-Samplers:-Bayesian-Inference-Huang-Rao",
            "title": {
                "fragments": [],
                "text": "Neurons as Monte Carlo Samplers: Bayesian Inference and Learning in Spiking Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed spiking network model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses and shows how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6377199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66ad2fbc8b73242a889699868611fcf239e3435d",
            "isKey": false,
            "numCitedBy": 1990,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning."
            },
            "slug": "Semi-supervised-Learning-with-Deep-Generative-Kingma-Mohamed",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Learning with Deep Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50638441"
                        ],
                        "name": "S. Shanker",
                        "slug": "S.-Shanker",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shanker",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 173
                            }
                        ],
                        "text": "\u2026than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (L. Schulz, 2012), problem-solving (Sternberg & Davidson, 1995), and epoch-making discoveries in scientific research (Langley, Bradshaw, Simon, & Zytkow, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 130
                            }
                        ],
                        "text": "Like with handwritten characters, a system may be able to quickly learn new concepts by constructing them from pre-existing primitive actions, informed by knowledge of the underlying causal process and learning-to-learn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26058082,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "f4db145ad5ad8d7057791c4d1bd8ad3b99f9743e",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The Greeks had a ready answer for what happens when the mind suddenly finds the answer to a question for which it had been searching: insight was regarded as a gift of the Muses, its origins were \u2018divine\u2019. It served to highlight the Greeks' belief that there are some things which are not meant to be scientifically explained. The essence of insight is that it comes from some supernatural source: unpredicted and unfettered. In other words, the origins of insight are unconscious, and hence, unexplainable. Wittgenstein felt that, as long as there continues to be a noun expression like \u2018to have a moment of insight\u2019 which functions in the same way as the expression \u2018to have a hunger pang\u2019, thereby inducing us to treat \u2018moment of insight\u2019 as the name of an experience, then \u201cpeople will keep stumbling over the same puzzling difficulties and find themselves staring at something which no explanation seems capable of clearing up.\u201d To the founders of AI, this argument reeked of obscurantism. The moment of insight, they felt, is indeed a mystery, but it is one that begs to be explained in causal terms. Indeed, the problem of insight served as one of the leading problems in the evolution of AI. Hence anyone interested in the foundations of AI is compelled to examine the manner in which the early pioneers of the field sought to explain the \u2018eureka experience\u2019. In this paper I will look at some of the key conceptual developments which paved the way for Newell and Simon's theory of GPS: the fundamental changes in the notion of the unconscious \u2014 the emergence of the \u2018cognitive unconscious\u2019 \u2014 which took place in the nineteenth- and early twentieth-century. In so doing, I hope to clarify what Wittgenstein may have had in mind in his strictures against mechanist attempts to analyse the nature of insight."
            },
            "slug": "The-nature-of-insight-Shanker",
            "title": {
                "fragments": [],
                "text": "The nature of insight"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Some of the key conceptual developments which paved the way for Newell and Simon's theory of GPS are looked at: the fundamental changes in the notion of the unconscious \u2014 the emergence of the \u2018cognitive unconscious\u2019 \u2014 which took place in the nineteenth- and early twentieth-century."
            },
            "venue": {
                "fragments": [],
                "text": "Minds and Machines"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331233"
                        ],
                        "name": "A. Markman",
                        "slug": "A.-Markman",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Markman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Markman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089520450"
                        ],
                        "name": "V. S. Makin",
                        "slug": "V.-S.-Makin",
                        "structuredName": {
                            "firstName": "Valerie",
                            "lastName": "Makin",
                            "middleNames": [
                                "Susan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. S. Makin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson &\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 17948537,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b58ccbd1c71659ed2fb6d83acac7fb82e9b86bbd",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Explanations of category coherence include that categories reflect feature correlations in the world, that the human conceptual system is designed to create systematic categories, and that people have theories about the world that bind together seemingly unrelated features. The authors have suggested that the need to establish reference in communication also influences category coherence. This proposal was tested in 2 studies involving a referential communication task. In these studies, consistency was promoted between individuals by communication, which synchronized the category structures of different people. Further, people were focused on the commonalities of objects and on the differences related to the commonalities by communication--a pattern that is compatible with what has been observed in existing categories. These results suggest that categorization research must incorporate communication tasks into the canon of methodologies used to study category structure."
            },
            "slug": "Referential-communication-and-category-acquisition.-Markman-Makin",
            "title": {
                "fragments": [],
                "text": "Referential communication and category acquisition."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Categorization research must incorporate communication tasks into the canon of methodologies used to study category structure, and consistency was promoted between individuals by communication, which synchronized the category structures of different people."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887861"
                        ],
                        "name": "J. Kruschke",
                        "slug": "J.-Kruschke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kruschke",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kruschke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7101807,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "4e2f43dab69d690dc86422949e410ebf37f522d4",
            "isKey": false,
            "numCitedBy": 6947,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright \u00a9 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website."
            },
            "slug": "Bayesian-data-analysis.-Kruschke",
            "title": {
                "fragments": [],
                "text": "Bayesian data analysis."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A fatal flaw of NHST is reviewed and some benefits of Bayesian data analysis are introduced and illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Wiley interdisciplinary reviews. Cognitive science"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152871994"
                        ],
                        "name": "J. Hamlin",
                        "slug": "J.-Hamlin",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hamlin",
                            "middleNames": [
                                "Kiley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hamlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "\u2026socially directed; at around three months of age, infants begin to discriminate anti-social agents that hurt or hinder others from neutral agents (Hamlin, 2013; Hamlin, Wynn, & Bloom, 2010), and they later distinguish between anti-social, neutral, and pro-social agents (Hamlin, Ullman,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 165
                            }
                        ],
                        "text": "These goals can be socially directed; at around three months of age, infants begin to discriminate anti-social agents that hurt or hinder others from neutral agents (Hamlin, 2013; Hamlin, Wynn, & Bloom, 2010), and they later distinguish between anti-social, neutral, and pro-social agents (Hamlin, Ullman, Tenenbaum, Goodman, & Baker, 2013; Hamlin, Wynn, & Bloom, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 113
                            }
                        ],
                        "text": "Suppose A was already negatively associated (a \u2018bad guy\u2019); acting negatively towards A could\nthen be seen as good (Hamlin, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 69
                            }
                        ],
                        "text": "Infants and adults are likely to interpret B\u2019s behavior as \u2018hindering\u2019 (Hamlin, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 114
                            }
                        ],
                        "text": "Suppose A was already negatively associated (a \u2018bad guy\u2019); acting negatively towards A could then be seen as good (Hamlin, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145724333,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a51f0adde6f7ba67a8ab9cc2a699819fb84e6636",
            "isKey": true,
            "numCitedBy": 278,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Although developmental psychologists traditionally explore morality from a learning and development perspective, some aspects of the human moral sense may be built-in, having evolved to sustain collective action and cooperation as required for successful group living. In this article, I review a recent body of research with infants and toddlers, demonstrating surprisingly sophisticated and flexible moral behavior and evaluation in a preverbal population whose opportunity for moral learning is limited at best. Although this work itself is in its infancy, it supports theoretical claims that human morality is a core aspect of human nature."
            },
            "slug": "Moral-Judgment-and-Action-in-Preverbal-Infants-and-Hamlin",
            "title": {
                "fragments": [],
                "text": "Moral Judgment and Action in Preverbal Infants and Toddlers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219829"
                        ],
                        "name": "Susan J. Hespos",
                        "slug": "Susan-J.-Hespos",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Hespos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan J. Hespos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39632405"
                        ],
                        "name": "Alissa Ferry",
                        "slug": "Alissa-Ferry",
                        "structuredName": {
                            "firstName": "Alissa",
                            "lastName": "Ferry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alissa Ferry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1486258,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a4d356872c4b09a96e7dc7ac638bb191a432a30a",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Many studies have established that 2-month-old infants have knowledge of solid objects' basic physical properties. Evidence about infants' understanding of nonsolid substances, however, is relatively sparse and equivocal. We present two experiments demonstrating that 5-month-old infants have distinct expectations for how solids and liquids behave. Experiment 1 showed that infants use the motion cues from the surface of a contained liquid or solid to predict whether it will pour or tumble from a cup if the cup is upended. Experiment 2 extended these findings to show that motion cues lead to distinct expectations about whether a new object will pass through or remain on top of a substance. Together, these experiments demonstrate that 5-month-old infants are able to use movement cues and solidity to discriminate a liquid from an object of similar appearance, providing the earliest evidence that infants can reason about nonsolid substances."
            },
            "slug": "Five-Month-Old-Infants-Have-Different-Expectations-Hespos-Ferry",
            "title": {
                "fragments": [],
                "text": "Five-Month-Old Infants Have Different Expectations for Solids and Liquids"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "These experiments demonstrate that 5-month-old infants are able to use movement cues and solidity to discriminate a liquid from an object of similar appearance, providing the earliest evidence that infants can reason about nonsolid substances."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological science"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056598470"
                        ],
                        "name": "Robert",
                        "slug": "Robert",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Robert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410494"
                        ],
                        "name": "P. Abelson",
                        "slug": "P.-Abelson",
                        "structuredName": {
                            "firstName": "P",
                            "lastName": "Abelson",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141496657"
                        ],
                        "name": "J. Carroll",
                        "slug": "J.-Carroll",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Carroll",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 108
                            }
                        ],
                        "text": "A second road was taken by pioneers like Colby (1975) with PARRY, a computer program simulating a paranoid, Abelson and Carroll (1965) with their True Believer program, andWeizenbaum (1966) with his ELIZA non-directive psychotherapy program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 672,
                                "start": 108
                            }
                        ],
                        "text": "A second road was taken by pioneers like Colby (1975) with PARRY, a computer program simulating a paranoid, Abelson and Carroll (1965) with their True Believer program, andWeizenbaum (1966) with his ELIZA non-directive psychotherapy program. The idea in this research was to understand people\u2019s often suboptimal performances in learning and thinking. These programs recognized that people are often emotional, a-rational, and function at levels well below their capabilities. Many of these ideas have been formalized in recent psychological research. For example, Stanovich (2009) has shown that rationality and intelligence are largely distinct. Mayer and Salovey (1993) have shown the importance of emotional intelligence to people\u2019s thinking, and Sternberg (1997) has argued both for the importance of practical intelligence and for its relative independence from analytical or more academic aspects of intelligence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 581,
                                "start": 108
                            }
                        ],
                        "text": "A second road was taken by pioneers like Colby (1975) with PARRY, a computer program simulating a paranoid, Abelson and Carroll (1965) with their True Believer program, andWeizenbaum (1966) with his ELIZA non-directive psychotherapy program. The idea in this research was to understand people\u2019s often suboptimal performances in learning and thinking. These programs recognized that people are often emotional, a-rational, and function at levels well below their capabilities. Many of these ideas have been formalized in recent psychological research. For example, Stanovich (2009) has shown that rationality and intelligence are largely distinct."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202592152,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3cc46dbaa049ec668b906c5c113180b4b9755314",
            "isKey": true,
            "numCitedBy": 41,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we sketch an attempt at computer simulation of individual belief systems. Although rooted in a series of earlier formulations (Abelson and Rosenberg [1958], 1 Abelson [1959],Rosenberg and Abelson [1960],3 Abelson [1963]'), this complex project has not heretofore been totally outlined in print. We shall begin with several clarifying comments on the nature of our goals and the problems we have faced, thence proceeding further and further into the details of operation of our simulation. By an individual belief system we refer to an interrelated set of affect-laden cognitions concerning some aspects of the psychological world of a single individual. In simulating such a system, our intention is not only to represent its structure of interrelationships, but also some of the processes by which the system maintains itself against the intrusion of new and potentially upsetting information. Our use of the technique of computer simulation is intended to maximizethe explicitness with which we state our assumptions and the vividness with which the consequences of these assumptions are made apparent. The operation of simulated belief systems can be played out on the computer and the details scrutinized in order to refine our level of approximation to real systems. The psychology of belief systems lies athwart the ancient philosophical battleground of whether man is basically \"rational\" or \"irrational.\" Without claiming to have resolved this honorable controversy, we adopt the useful compromise position current among social psychologists that man is \"subjectively rational,\" i.e., rational within the constraints of his own experience and motivation. This position owes much to Heider's' r' analysis of \"naive psychology,\" and can be found implicitly or explicitly in the work of many \"cognitive consistency\" theorists (Festinger7 Osgood and Tannenbaum\"; Rosenberg and Abelson3 ). To be sure, there is still much room within this compromise position for differences in emphasis as between motivational and cognitive components of the total system. The work of Rokeach and Smith, Bruner, and White'\" among others reminds us that individual belief systems areheavily determined by personality needs. Our * The work herein reported was in part supported by Grant MH-"
            },
            "slug": "Computer-Simulation-of-Individual-Belief-Systems-*-Robert-Abelson",
            "title": {
                "fragments": [],
                "text": "Computer Simulation of Individual Belief Systems *"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33696979"
                        ],
                        "name": "G. Murphy",
                        "slug": "G.-Murphy",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Murphy",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023259"
                        ],
                        "name": "D. Medin",
                        "slug": "D.-Medin",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Medin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Medin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 230
                            }
                        ],
                        "text": "To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick while other equally applicable features wash away (Murphy & Medin, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "Borrowing examples from Murphy and Medin (1985), the feature \u201cflammable\u201d is more closely attached to wood than money due to the underlying causal roles of the concepts, even though the feature is equally applicable to both; these causal roles derive from the functions of objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14525617,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "4336c33f0ab03e8722712792d25e46922eed73ed",
            "isKey": false,
            "numCitedBy": 2555,
            "numCiting": 147,
            "paperAbstract": {
                "fragments": [],
                "text": "The question of what makes a concept coherent (what makes its members form a comprehensible class) has received a variety of answers. In this article we review accounts based on similarity, feature correlations, and various theories of categorization. We find that each theory provides an inadequate account of conceptual coherence (or no account at all) because none provides enough constraints on possible concepts. We propose that concepts are coherent to the extent that they fit people's background knowledge or naive theories about the world. These theories help to relate the concepts in a domain and to structure the attributes that are internal to a concept. Evidence of the influence of theories on various conceptual tasks is presented, and the possible importance of theories in cognitive development is discussed."
            },
            "slug": "The-role-of-theories-in-conceptual-coherence.-Murphy-Medin",
            "title": {
                "fragments": [],
                "text": "The role of theories in conceptual coherence."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proposed that concepts are coherent to the extent that they fit people's background knowledge or naive theories about the world and to structure the attributes that are internal to a concept."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331233"
                        ],
                        "name": "A. Markman",
                        "slug": "A.-Markman",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Markman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Markman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36568337"
                        ],
                        "name": "B. Ross",
                        "slug": "B.-Ross",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "One indicator\nof richness is the variety of functions that these models support (A. B. Markman & Ross, 2003; Solomon, Medin, & Lynch, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16480088,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "16668a1774c333517d03d9c372e0a94e2752f538",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "Categorization models based on laboratory research focus on a narrower range of explanatory constructs than appears necessary for explaining the structure of natural categories. This mismatch is caused by the reliance on classification as the basis of laboratory studies. Category representations are formed in the process of interacting with category members. Thus, laboratory studies must explore a range of category uses. The authors review the effects of a variety of category uses on category learning. First, there is an extensive discussion contrasting classification with a predictive inference task that is formally equivalent to classification but leads to a very different pattern of learning. Then, research on the effects of problem solving, communication, and combining inference and classification is reviewed."
            },
            "slug": "Category-use-and-category-learning.-Markman-Ross",
            "title": {
                "fragments": [],
                "text": "Category use and category learning."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors review the effects of a variety of category uses on category learning and an extensive discussion contrasting classification with a predictive inference task that is formally equivalent to classification but leads to a very different pattern of learning."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological bulletin"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735083"
                        ],
                        "name": "Vikash K. Mansinghka",
                        "slug": "Vikash-K.-Mansinghka",
                        "structuredName": {
                            "firstName": "Vikash",
                            "lastName": "Mansinghka",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikash K. Mansinghka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39331522"
                        ],
                        "name": "Daniel M. Roy",
                        "slug": "Daniel-M.-Roy",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Roy",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel M. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2039588"
                        ],
                        "name": "Keith Bonawitz",
                        "slug": "Keith-Bonawitz",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Bonawitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keith Bonawitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 68
                            }
                        ],
                        "text": "Hierarchical Bayesian models operating over probabilistic programs (Goodman et al., 2008; Lake, Salakhutdinov, & Tenenbaum, 2015; Tenenbaum et al., 2011) are equipped to deal with theorylike structures and rich causal representations of the world, yet there are formidable algorithmic challenges for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "In this paper, we suggested some ingredients for building computational models with more humanlike learning and thought."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1617294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8f57509a228f1c84bf67094ec1fa8a99407368b",
            "isKey": false,
            "numCitedBy": 729,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Formal languages for probabilistic modeling enable re-use, modularity, and descriptive clarity, and can foster generic inference techniques. We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques."
            },
            "slug": "Church:-a-language-for-generative-models-Goodman-Mansinghka",
            "title": {
                "fragments": [],
                "text": "Church: a language for generative models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work introduces Church, a universal language for describing stochastic generative processes, based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401804750"
                        ],
                        "name": "David Lopez-Paz",
                        "slug": "David-Lopez-Paz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lopez-Paz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Lopez-Paz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8125776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f341fa61da527e64a349334836d52626fe9d6c79",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data."
            },
            "slug": "Unifying-distillation-and-privileged-information-Lopez-Paz-Bottou",
            "title": {
                "fragments": [],
                "text": "Unifying distillation and privileged information"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The theoretical and causal insight about the inner workings of generalized distillation is provided, it is extended to unsupervised, semisupervised and multitask learning scenarios, and its efficacy on a variety of numerical simulations on both synthetic and real-world data is illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704065"
                        ],
                        "name": "D. Gentner",
                        "slug": "D.-Gentner",
                        "structuredName": {
                            "firstName": "Dedre",
                            "lastName": "Gentner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gentner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10293784,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9242ffbaa2a4d822b4c2813637ddf6cf473817fe",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the original motivations for qualitative physics research was the creation of a computational account of mental models. For instance, a key intuition often associated with mental models is that they are runnable, i.e., there is a sense of deriving answers via mental simulation rather than logical reasoning. This paper examines three explanations for runnability, and argues that none of them is sufficient. Instead, a hybrid model combining aspects of all three is proposed, focusing on the integration of ideas from qualitative physics with ideas from analogical processing. Some psychological implications of this hybrid model are discussed."
            },
            "slug": "Qualitative-Mental-Models:-Simulations-or-Memories-Forbus-Gentner",
            "title": {
                "fragments": [],
                "text": "Qualitative Mental Models: Simulations or Memories?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690396"
                        ],
                        "name": "C. Eliasmith",
                        "slug": "C.-Eliasmith",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Eliasmith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Eliasmith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144765382"
                        ],
                        "name": "Oliver Trujillo",
                        "slug": "Oliver-Trujillo",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Trujillo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver Trujillo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15749520,
            "fieldsOfStudy": [
                "Biology",
                "Psychology",
                "Economics"
            ],
            "id": "61733a2fc1ce51b29dcf2d20626fc58482967797",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-use-and-abuse-of-large-scale-brain-models-Eliasmith-Trujillo",
            "title": {
                "fragments": [],
                "text": "The use and abuse of large-scale brain models"
            },
            "venue": {
                "fragments": [],
                "text": "Current Opinion in Neurobiology"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3292907"
                        ],
                        "name": "Patrice D. Tremoulet",
                        "slug": "Patrice-D.-Tremoulet",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Tremoulet",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrice D. Tremoulet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143617733"
                        ],
                        "name": "J. Feldman",
                        "slug": "J.-Feldman",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Feldman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 265
                            }
                        ],
                        "text": "\u2026is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion (Johnson, Slaughter, & Carey, 1998; Premack & Premack, 1997; Schlottmann, Ray, Mitchell, & Demetriou, 2006; Tremoulet & Feldman, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14170095,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "80d0a898169bfb661b492049c6ec2c64268ec966",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that a single moving object can create the subjective impression that it is alive, based solely on its pattern of movement. Our displays differ from conventional biological motion displays (which normally involve multiple moving points, usually integrated to suggest a human form) in that they contain only a single rigid object moving across a uniform field. We focus on motion paths in which the speed and direction of the target object change simultaneously. Naive subjects' ratings of animacy were significantly influenced by (i) the magnitude of the speed change, (ii) the angular magnitude of the direction change, (iii) the shape of the object, and (iv) the alignment between the principal axis of the object and its direction of motion. These findings are consistent with the hypothesis that observers classify as animate only those objects whose motion trajectories are otherwise unlikely to occur in the observed setting."
            },
            "slug": "Perception-of-Animacy-from-the-Motion-of-a-Single-Tremoulet-Feldman",
            "title": {
                "fragments": [],
                "text": "Perception of Animacy from the Motion of a Single Object"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "It is demonstrated that a single moving object can create the subjective impression that it is alive, based solely on its pattern of movement, which is consistent with the hypothesis that observers classify as animate only those objects whose motion trajectories are otherwise unlikely to occur in the observed setting."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094648"
                        ],
                        "name": "P. Munro",
                        "slug": "P.-Munro",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Munro",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Munro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1607496,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9f22cf81654dd50b95e65b86b1125cfe6803a67b",
            "isKey": false,
            "numCitedBy": 2695,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further."
            },
            "slug": "Theory-for-the-development-of-neuron-selectivity:-Bienenstock-Cooper",
            "title": {
                "fragments": [],
                "text": "Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework and a synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802148"
                        ],
                        "name": "S. Gelly",
                        "slug": "S.-Gelly",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Gelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 80
                            }
                        ],
                        "text": "Each of these components has made gains against artificial and real Go players (Gelly & Silver, 2008, 2011; Silver et al., 2016; Tian & Zhu, 2015), and the notion of combining pattern recognition and model-based search goes back decades in Go and other games."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14992088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "923bd20b51b35b0aa8198fd43747f7cb223693f4",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The UCT algorithm uses Monte-Carlo simulation to estimate the value of states in a search tree from the current state. However, the first time a state is encountered, UCT has no knowledge, and is unable to generalise from previous experience. We describe two extensions that address these weaknesses. Our first algorithm, heuristic UCT, incorporates prior knowledge inthe form of avalue function. The value function can be learned offline, using a linear combination of a million binary features, with weights trained by temporal-difference learning. Our second algorithm, UCT\u2010RAVE, forms a rapid online generalisation based on the value of moves. We applied our algorithms to the domain of 9 ! 9 Computer Go, using the program MoGo. Using both heuristic UCT and RAVE, MoGo became the first program to achieve human master level in competitive play."
            },
            "slug": "Achieving-Master-Level-Play-in-9-x-9-Computer-Go-Gelly-Silver",
            "title": {
                "fragments": [],
                "text": "Achieving Master Level Play in 9 x 9 Computer Go"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Using both heuristic UCT and RAVE, MoGo became the first program to achieve human master level in competitive play and forms a rapid online generalisation based on the value of moves."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 396,
                                "start": 327
                            }
                        ],
                        "text": "Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives, just as the mind can think an infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor, 1975; Fodor & Pylyshyn, 1988; Marcus, 2001; Piantadosi, 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33954786,
            "fieldsOfStudy": [
                "Philosophy",
                "Computer Science"
            ],
            "id": "95e7b8a5bf17366b8a60a38bd76fc4bfa31a491a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Logic and probability are key themes of cognitive science that have long had an uneasy coexistence. I will describe the Probabilistic Language of Thought approach that brings them together into compositional representations with probabilistic meaning - formalized as stochastic lambda calculus. I will describe how this general framework is realized in the probabilistic programming language Church."
            },
            "slug": "Learning-and-the-language-of-thought-Goodman",
            "title": {
                "fragments": [],
                "text": "Learning and the language of thought"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Probabilistic Language of Thought approach that brings logic and probability together into compositional representations with probabilistic meaning - formalized as stochastic lambda calculus is described."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2777533"
                        ],
                        "name": "R. Brachman",
                        "slug": "R.-Brachman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Brachman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brachman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634377"
                        ],
                        "name": "H. Levesque",
                        "slug": "H.-Levesque",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Levesque",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Levesque"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 176
                            }
                        ],
                        "text": "Representing this compositional structure explicitly is both more economical and better for generalization, as noted in previous work on object-oriented reinforcement learning (Diuk et al. 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123746144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b45bf78354ca2ffae7e417f56909a39b05ad1ff5",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-Oriented-Representation-Brachman-Levesque",
            "title": {
                "fragments": [],
                "text": "Object-Oriented Representation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40081727"
                        ],
                        "name": "Emily L. Denton",
                        "slug": "Emily-L.-Denton",
                        "structuredName": {
                            "firstName": "Emily",
                            "lastName": "Denton",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emily L. Denton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149531"
                        ],
                        "name": "Arthur D. Szlam",
                        "slug": "Arthur-D.-Szlam",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Szlam",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arthur D. Szlam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1282515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47900aca2f0b50da3010ad59b394c870f0e6c02e",
            "isKey": false,
            "numCitedBy": 1855,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset."
            },
            "slug": "Deep-Generative-Image-Models-using-a-Laplacian-of-Denton-Chintala",
            "title": {
                "fragments": [],
                "text": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A generative parametric model capable of producing high quality samples of natural images using a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2231343"
                        ],
                        "name": "R. Dolan",
                        "slug": "R.-Dolan",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Dolan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026outlined in this article will prove useful for working towards this goal: seeing objects and agents rather than features, building causal models and not just recognizing patterns, recombining representations without needing to retrain, and learning-to-learn rather than starting from scratch."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 233
                            }
                        ],
                        "text": "Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a \u201ccognitive map\u201d of the environment and using it to plan action sequences for more complex tasks (Daw, Niv, & Dayan, 2005; Dolan & Dayan, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7199332,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d5e9f40771f7006f4901e437e6c54bfdf4f453e7",
            "isKey": false,
            "numCitedBy": 676,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Goals-and-Habits-in-the-Brain-Dolan-Dayan",
            "title": {
                "fragments": [],
                "text": "Goals and Habits in the Brain"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832617"
                        ],
                        "name": "D. Caligiore",
                        "slug": "D.-Caligiore",
                        "structuredName": {
                            "firstName": "Daniele",
                            "lastName": "Caligiore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Caligiore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746694"
                        ],
                        "name": "G. Pezzulo",
                        "slug": "G.-Pezzulo",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Pezzulo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pezzulo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716847"
                        ],
                        "name": "G. Baldassarre",
                        "slug": "G.-Baldassarre",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Baldassarre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Baldassarre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536211"
                        ],
                        "name": "Andreea C. Bostan",
                        "slug": "Andreea-C.-Bostan",
                        "structuredName": {
                            "firstName": "Andreea",
                            "lastName": "Bostan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreea C. Bostan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3336745"
                        ],
                        "name": "P. Strick",
                        "slug": "P.-Strick",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Strick",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Strick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051897"
                        ],
                        "name": "R. Helmich",
                        "slug": "R.-Helmich",
                        "structuredName": {
                            "firstName": "Rick",
                            "lastName": "Helmich",
                            "middleNames": [
                                "C",
                                "G"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Helmich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3948468"
                        ],
                        "name": "M. Dirkx",
                        "slug": "M.-Dirkx",
                        "structuredName": {
                            "firstName": "Michiel",
                            "lastName": "Dirkx",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dirkx"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719882"
                        ],
                        "name": "J. Houk",
                        "slug": "J.-Houk",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Houk",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Houk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2783252"
                        ],
                        "name": "H. J\u00f6rntell",
                        "slug": "H.-J\u00f6rntell",
                        "structuredName": {
                            "firstName": "Henrik",
                            "lastName": "J\u00f6rntell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J\u00f6rntell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1422630371"
                        ],
                        "name": "\u00c1. Lago-Rodr\u00edguez",
                        "slug": "\u00c1.-Lago-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "\u00c1ngel",
                            "lastName": "Lago-Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1. Lago-Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2555266"
                        ],
                        "name": "J. Galea",
                        "slug": "J.-Galea",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Galea",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Galea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743842"
                        ],
                        "name": "R. Miall",
                        "slug": "R.-Miall",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Miall",
                            "middleNames": [
                                "Chris"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113845911"
                        ],
                        "name": "Traian Popa",
                        "slug": "Traian-Popa",
                        "structuredName": {
                            "firstName": "Traian",
                            "lastName": "Popa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Traian Popa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47396982"
                        ],
                        "name": "A. Kishore",
                        "slug": "A.-Kishore",
                        "structuredName": {
                            "firstName": "Asha",
                            "lastName": "Kishore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kishore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144193091"
                        ],
                        "name": "P. Verschure",
                        "slug": "P.-Verschure",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Verschure",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Verschure"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34581273"
                        ],
                        "name": "R. Zucca",
                        "slug": "R.-Zucca",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Zucca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zucca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3741172"
                        ],
                        "name": "I. Herreros",
                        "slug": "I.-Herreros",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Herreros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Herreros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5701812,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "f7fcdc631e45172a33611c9e7a87a1091ea03955",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite increasing evidence suggesting the cerebellum works in concert with the cortex and basal ganglia, the nature of the reciprocal interactions between these three brain regions remains unclear. This consensus paper gathers diverse recent views on a variety of important roles played by the cerebellum within the cerebello-basal ganglia-thalamo-cortical system across a range of motor and cognitive functions. The paper includes theoretical and empirical contributions, which cover the following topics: recent evidence supporting the dynamical interplay between cerebellum, basal ganglia, and cortical areas in humans and other animals; theoretical neuroscience perspectives and empirical evidence on the reciprocal influences between cerebellum, basal ganglia, and cortex in learning and control processes; and data suggesting possible roles of the cerebellum in basal ganglia movement disorders. Although starting from different backgrounds and dealing with different topics, all the contributors agree that viewing the cerebellum, basal ganglia, and cortex as an integrated system enables us to understand the function of these areas in radically different ways. In addition, there is unanimous consensus between the authors that future experimental and computational work is needed to understand the function of cerebellar-basal ganglia circuitry in both motor and non-motor functions. The paper reports the most advanced perspectives on the role of the cerebellum within the cerebello-basal ganglia-thalamo-cortical system and illustrates other elements of consensus as well as disagreements and open questions in the field."
            },
            "slug": "Consensus-Paper:-Towards-a-Systems-Level-View-of-Caligiore-Pezzulo",
            "title": {
                "fragments": [],
                "text": "Consensus Paper: Towards a Systems-Level View of Cerebellar Function: the Interplay Between Cerebellum, Basal Ganglia, and Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "There is unanimous consensus between the authors that future experimental and computational work is needed to understand the function of cerebellar-basal ganglia circuitry in both motor and non-motor functions."
            },
            "venue": {
                "fragments": [],
                "text": "The Cerebellum"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143923544"
                        ],
                        "name": "F. Besse",
                        "slug": "F.-Besse",
                        "structuredName": {
                            "firstName": "Frederic",
                            "lastName": "Besse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Besse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7441501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bda89e0d181eda7e49ea831225eda86d075e111c",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'."
            },
            "slug": "Towards-Conceptual-Compression-Gregor-Besse",
            "title": {
                "fragments": [],
                "text": "Towards Conceptual Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A simple recurrent variational auto-encoder architecture that significantly improves image modeling and shows that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2214496"
                        ],
                        "name": "Andreas Stuhlm\u00fcller",
                        "slug": "Andreas-Stuhlm\u00fcller",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stuhlm\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Stuhlm\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144364160"
                        ],
                        "name": "Jessica Taylor",
                        "slug": "Jessica-Taylor",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jessica Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002017"
                        ],
                        "name": "Noah D. Goodman",
                        "slug": "Noah-D.-Goodman",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Goodman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah D. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 96
                            }
                        ],
                        "text": "Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density estimation (Kulkarni, Kohli, et al., 2015; Stuhlm\u00fcller et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 270
                            }
                        ],
                        "text": "\u2026using paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational optimization (Gregor et al., 2015; A. Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density estimation (Kulkarni, Kohli, et al., 2015; Stuhlmu\u0308ller et al., 2013)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10112543,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f887b89684157c2c842010ed63f12bea7787745b",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efficiency of this sampler for a variety of parameter regimes and Bayes nets."
            },
            "slug": "Learning-Stochastic-Inverses-Stuhlm\u00fcller-Taylor",
            "title": {
                "fragments": [],
                "text": "Learning Stochastic Inverses"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Inverse MCMC algorithm is described, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler, and the efficiency of this sampler for a variety of parameter regimes and Bayes nets is explored."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081995230"
                        ],
                        "name": "Donna Lockery",
                        "slug": "Donna-Lockery",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Lockery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donna Lockery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 48
                            }
                        ],
                        "text": "\u201d One might think that it is only stupid people (Sternberg 2002; 2004) who think and act foolishly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2996393,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "aab067af7846f259401499a85389a96709223d59",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This book may not be reproduced, in whole or in part, including illustrations, in any form (beyond that copying permitted by Sections 107 and 108 of the U.S. Copyright Law and except by reviewers for the public press), without written permission from the publishers. Library of Congress Cataloging-in-Publication Data Why smart people can be so stupid / edited by Robert J. Sternberg. p. cm. Includes bibliographical references and index. A catalogue record for this book is available from the British Library. List of Contributors 243 Index 245 vii preface Those who have wondered if smart people can be stupid do not have to look very far, nor do they have to look through the lenses of any particular ideology. \u221e A president of the United States, graduate of Yale Law School, and Rhodes Scholar showed behavior so ''stupid'' that few people can understand why he did what he did. Beyond any hormonally motivated behavior on his part, the whole world wondered how a trained lawyer could have allowed himself to become entangled in such a legal nightmare. \u221e A seasoned prosecutor and judge with a reputation for some brilliance damaged his good name among much of the U.S. population with his apparent vendetta against a president. His campaign left many people convinced that the prosecutor was more interested in ''winning'' than in pursuing any reasonable legal case. \u221e A U.S. congressman known for being ideological but balanced and wise left the fray with his reputation in tatters when he and his fellow House ''managers'' pursued a case they could not win. \u221e A former prosecutor and state's attorney general in Delaware was sentenced to death for murdering a girlfriend who jilted him. \u221e A world-renowned geologist, while being investigated for and charged with storing child pornography, involved himself with a boy whom he was later accused of molesting. Whether one believes in a single intelligence (g or IQ) or multiple intel-ligences or anything in between, the behavior of the individuals mentioned above (and, indeed, at times, our own behavior) seems inexplicable in terms of what we know about intelligence. Why do people think and behave in such stupid ways that they end up destroying their livelihood or even their lives? This book is devoted to addressing these questions, which the vast majority of theories in psychology, including theories of intelligence, seem to neglect. The world supports a multi-million-dollar industry in \u2026"
            },
            "slug": "Why-Smart-People-Can-Be-So-Stupid-Lockery",
            "title": {
                "fragments": [],
                "text": "Why Smart People Can Be So Stupid"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3441674"
                        ],
                        "name": "Eva Ansermin",
                        "slug": "Eva-Ansermin",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Ansermin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eva Ansermin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786311"
                        ],
                        "name": "G. Mostafaoui",
                        "slug": "G.-Mostafaoui",
                        "structuredName": {
                            "firstName": "Ghil\u00e8s",
                            "lastName": "Mostafaoui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mostafaoui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103741143"
                        ],
                        "name": "Nils Beauss\u00e9",
                        "slug": "Nils-Beauss\u00e9",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Beauss\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nils Beauss\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3305788"
                        ],
                        "name": "P. Gaussier",
                        "slug": "P.-Gaussier",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Gaussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gaussier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 197
                            }
                        ],
                        "text": "An extension of this work demonstrated that a robot could quickly and \u201conline\u201d learn more complex gestures and synchronize its behavior to the human partner based on the same sensorimotor approach (Ansermin et al. 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1252,
                                "start": 198
                            }
                        ],
                        "text": "An extension of this work demonstrated that a robot could quickly and \u201conline\u201d learn more complex gestures and synchronize its behavior to the human partner based on the same sensorimotor approach (Ansermin et al. 2016). Second, even to learn (or understand) what a simple object is, people need to act on it (O\u2019Regan 2011). For example, if we do not know what a \u201cchair\u201d is, we will understand its representation by sitting on it, touching it. The definition is then easy: A chair is an object on which we can sit, regardless of its precise shape. Now, if we try to define its representation before acting, it becomes very difficult to describe it. This requires determining the general shape, number of legs, with or without arms or wheels, texture, and so on. Hence, when programming a machine, this latter definition brings a high computational cost that drastically slows down the speed of the learning (and pushes away the idea of learning as fast as humans do). In that case, the machines/robots should be able to learn directly by acting and perceiving the consequences of their actions on the object/person. Finally, from a more low-level aspect, even shape recognition is strongly connected to our motor experience. Viviani and Stucchi (1992) demonstrated that when they showed a participant a point light performing a perfect circle, as soon as this point slowed down at the upper and lower parts of this circle, the participant did not perceive the trajectory as a circle any longer, but as an ellipse."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2068685,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f5502d25d216a33265e56ca8147767ab919ec96a",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Synchronisation and coordination are omnipresent and essential in humans interactions. Because of their unavoidable and unintentional aspect, those phenomena could be the consequences of a low level mechanism: a driving force originating from external stimuli called the entrainment effect. In the light of its importance in interaction and wishing to define new HRI, we suggest to model this entrainment to highlight its efficiency for gesture learning during imitative games and for reducing the computational complexity. We will put forward the capacity of adaptation offered by the entrainment effect. Hence, we present in this paper a neural model for gesture learning by imitation using entrainment effect applied to a NAO robot interacting with a human partner."
            },
            "slug": "Learning-to-Synchronously-Imitate-Gestures-Using-Ansermin-Mostafaoui",
            "title": {
                "fragments": [],
                "text": "Learning to Synchronously Imitate Gestures Using Entrainment Effect"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A neural model for gesture learning by imitation using entrainment effect applied to a NAO robot interacting with a human partner to highlight its efficiency for gesturelearning during imitative games and for reducing the computational complexity."
            },
            "venue": {
                "fragments": [],
                "text": "SAB"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47393520"
                        ],
                        "name": "Michael L. Anderson",
                        "slug": "Michael-L.-Anderson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Anderson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael L. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 6
                            }
                        ],
                        "text": "[RJS] Anderson, M. L. (2003) Embodied cognition: A field guide."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 72
                            }
                        ],
                        "text": "); that is, it is shaped to adaptively interact with the physical world (Anderson 2003; Pfeifer & G\u00f3mez 2009) to satisfy the organism\u2019s needs and goals (Mannella et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7191928,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "b8e83115abc04069ae3270536c9b0e6372b940dc",
            "isKey": false,
            "numCitedBy": 980,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Embodied-Cognition:-A-field-guide-Anderson",
            "title": {
                "fragments": [],
                "text": "Embodied Cognition: A field guide"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2438157"
                        ],
                        "name": "H. Wellman",
                        "slug": "H.-Wellman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Wellman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339997"
                        ],
                        "name": "S. Gelman",
                        "slug": "S.-Gelman",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Gelman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34910792,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology",
                "Medicine"
            ],
            "id": "541fd95a45575e8b94b23553e408fd220b1b8d18",
            "isKey": false,
            "numCitedBy": 832,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "DOMAINS AND THEORIES . .. . ..... . . . ..... 338 Framework Theories 341 NAIVE PHySICS.... . . . . . ... . . . . ... . . . . . . . . . . .. 343 Infants 344 Later Developments . . . . .. . . . .. . . . . . . . . .. .... . . . . ..... . . . .... . ........ ... 345 Conclusions 350 NAIVE PSyCHOLOGy ... . . . .. .... 350 Theory of Mind 351 Earlier Developments 353 Auti sm...... 356 Conclusions 357 NAIVE BIOLOGy. . . . . . .. . . ..... ....... . . . 357 Ontology: Living Things 358 Beliefs about Biological Kinds 360 Biologically Specific Causal Processes \". . . . . . . .. 363 Summary. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . 365 CONCLUSIONS.. . . . ... . . . . . . . . ... . . .. .. . . . . . . .. ... ...... 365"
            },
            "slug": "Cognitive-development:-foundational-theories-of-Wellman-Gelman",
            "title": {
                "fragments": [],
                "text": "Cognitive development: foundational theories of core domains."
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "Theories about Biological Kinds, Ontology: Living Things, and Biologically Specific Causal Processes are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of psychology"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2855190"
                        ],
                        "name": "E. Moser",
                        "slug": "E.-Moser",
                        "structuredName": {
                            "firstName": "Edvard",
                            "lastName": "Moser",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Moser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3094769"
                        ],
                        "name": "E. Kropff",
                        "slug": "E.-Kropff",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Kropff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kropff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37661215"
                        ],
                        "name": "M. Moser",
                        "slug": "M.-Moser",
                        "structuredName": {
                            "firstName": "May-Britt",
                            "lastName": "Moser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 248
                            }
                        ],
                        "text": "Without a deeper understanding of how and why we learn and remember what we do, however, the designers of current technologies are working in the dark, even when they design devices to aid navigation, one of our best-understood cognitive functions (e.g., O\u2019Keefe 2014; Moser et al. 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16036900,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8b9496321c1e3c3495e60fbc3627141a1f45fa21",
            "isKey": false,
            "numCitedBy": 1486,
            "numCiting": 178,
            "paperAbstract": {
                "fragments": [],
                "text": "More than three decades of research have demonstrated a role for hippocampal place cells in representation of the spatial environment in the brain. New studies have shown that place cells are part of a broader circuit for dynamic representation of self-location. A key component of this network is the entorhinal grid cells, which, by virtue of their tessellating firing fields, may provide the elements of a path integration-based neural map. Here we review how place cells and grid cells may form the basis for quantitative spatiotemporal representation of places, routes, and associated experiences during behavior and in memory. Because these cell types have some of the most conspicuous behavioral correlates among neurons in nonsensory cortical systems, and because their spatial firing structure reflects computations internally in the system, studies of entorhinal-hippocampal representations may offer considerable insight into general principles of cortical network dynamics."
            },
            "slug": "Place-cells,-grid-cells,-and-the-brain's-spatial-Moser-Kropff",
            "title": {
                "fragments": [],
                "text": "Place cells, grid cells, and the brain's spatial representation system."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "How place cells and grid cells may form the basis for quantitative spatiotemporal representation of places, routes, and associated experiences during behavior and in memory is reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144877155"
                        ],
                        "name": "L. Schulz",
                        "slug": "L.-Schulz",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Schulz",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222423"
                        ],
                        "name": "A. Gopnik",
                        "slug": "A.-Gopnik",
                        "structuredName": {
                            "firstName": "Alison",
                            "lastName": "Gopnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gopnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058012"
                        ],
                        "name": "C. Glymour",
                        "slug": "C.-Glymour",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Glymour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Glymour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13614590,
            "fieldsOfStudy": [
                "Psychology",
                "Philosophy"
            ],
            "id": "75fc1a101f72a308b683be47f8fdfc8d65632f15",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The conditional intervention principle is a formal principle that relates patterns of interventions and outcomes to causal structure. It is a central assumption of experimental design and the causal Bayes net formalism. Two studies suggest that preschoolers can use the conditional intervention principle to distinguish causal chains, common cause and interactive causal structures even in the absence of differential spatiotemporal cues and specific mechanism knowledge. Children were also able to use knowledge of causal structure to predict the patterns of evidence that would result from interventions. A third study suggests that children's spontaneous play can generate evidence that would support such accurate causal learning."
            },
            "slug": "Preschool-children-learn-about-causal-structure-Schulz-Gopnik",
            "title": {
                "fragments": [],
                "text": "Preschool children learn about causal structure from conditional interventions."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Two studies suggest that preschoolers can use the conditional intervention principle to distinguish causal chains, common cause and interactive causal structures even in the absence of differential spatiotemporal cues and specific mechanism knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Developmental science"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143898851"
                        ],
                        "name": "S. Wolfram",
                        "slug": "S.-Wolfram",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wolfram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wolfram"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 84
                            }
                        ],
                        "text": "Classic interpretations of perfect humanness arising from the fallibility of humans (e.g., Clark 2012; Nisbett & Ross 1980; Parker & McKinney 1999; Wolfram 2002) appreciably impact the technical feasibility and socio-cultural significance of building and deploying human-emulating personified machines under both nonsocial and social constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5189103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcff09077f2272d1b4b5fe8ba1ef50b77d5324f0",
            "isKey": false,
            "numCitedBy": 3173,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Early in the 1980s Stephen Wolfram began to work in earnest upon cellular automata. These are a class of computer model which may be visualized as a set of memory locations, each containing one bit. These bits are updated in a succession of time steps. In each step, the new value of each bit depends upon the values of neighboring bits. Wolfram particularly studied the class of automata in which all the bits are arranged upon a line, and each bit is updated using the very same functional dependence upon the bits at that site and its two nearest neighbors. There are 256 different automata of this type. Wolfram made it his business to study systematically all of these different automata, using extensive computer simulations, and to think about and generalize from what he thereby uncovered. A New Kind of Science, written and published by Stephen Wolfram, is the outcome of these, and related, studies."
            },
            "slug": "A-new-kind-of-science-Wolfram",
            "title": {
                "fragments": [],
                "text": "A new kind of science"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A New Kind of Science, written and published by Stephen Wolfram, is the outcome of the studies he conducted systematically upon cellular automata, a class of computer model which may be visualized as a set of memory locations, each containing one bit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2773880"
                        ],
                        "name": "R. Passingham",
                        "slug": "R.-Passingham",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Passingham",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Passingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54346290,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "ba1c520d8f4ac9aeaf33bd9e032a229ac81ca14b",
            "isKey": false,
            "numCitedBy": 3976,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-hippocampus-as-a-cognitive-map-J.-O'Keefe-&-L.-Passingham",
            "title": {
                "fragments": [],
                "text": "The hippocampus as a cognitive map \n \n J. O'Keefe & L. Nadel, Oxford University Press, Oxford (1978). 570 pp., \u00a325.00\n"
            },
            "venue": {
                "fragments": [],
                "text": "Neuroscience"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9920696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47d588f746949b066af6957a524d82ca3c6c961b",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a hierarchical classification model that allows rare objects to borrow statistical strength from related objects that have many training examples. Unlike many of the existing object detection and recognition systems that treat different classes as unrelated entities, our model learns both a hierarchy for sharing visual appearance across 200 object categories and hierarchical parameters. Our experimental results on the challenging object localization and detection task demonstrate that the proposed model substantially improves the accuracy of the standard single object detectors that ignore hierarchical structure altogether."
            },
            "slug": "Learning-to-share-visual-appearance-for-multiclass-Salakhutdinov-Torralba",
            "title": {
                "fragments": [],
                "text": "Learning to share visual appearance for multiclass object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A hierarchical classification model that allows rare objects to borrow statistical strength from related objects that have many training examples and learns both a hierarchy for sharing visual appearance across 200 object categories and hierarchical parameters is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2199658"
                        ],
                        "name": "R. Hastie",
                        "slug": "R.-Hastie",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Hastie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hastie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "For example, the structure of the causal network underlying the features of a category influences how people categorize new examples (Rehder, 2003; Rehder & Hastie, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40353978,
            "fieldsOfStudy": [
                "Psychology",
                "Philosophy"
            ],
            "id": "4d1f50b938feb2fb695fb39b6d1e57ef49816083",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the recent interest in the theoretical knowledge embedded in human representations of categories, little research has systematically manipulated the structure of such knowledge. Across four experiments this study assessed the effects of interattribute causal laws on a number of category-based judgments. The authors found that (a) any attribute occupying a central position in a network of causal relationships comes to dominate category membership, (b) combinations of attribute values are important to category membership to the extent they jointly confirm or violate the causal laws, and (c) the presence of causal knowledge affects the induction of new properties to the category. These effects were a result of the causal laws, rather than the empirical correlations produced by those laws. Implications for the doctrine of psychological essentialism, similarity-based models of categorization, and the representation of causal knowledge are discussed."
            },
            "slug": "Causal-knowledge-and-categories:-the-effects-of-on-Rehder-Hastie",
            "title": {
                "fragments": [],
                "text": "Causal knowledge and categories: the effects of causal beliefs on categorization, induction, and similarity."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors found that any attribute occupying a central position in a network of causal relationships comes to dominate category membership, and combinations of attribute values are important to category membership to the extent they jointly confirm or violate the causal laws."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2296579"
                        ],
                        "name": "K. Rohlfing",
                        "slug": "K.-Rohlfing",
                        "structuredName": {
                            "firstName": "Katharina",
                            "lastName": "Rohlfing",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rohlfing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3248147"
                        ],
                        "name": "Iris Nomikou",
                        "slug": "Iris-Nomikou",
                        "structuredName": {
                            "firstName": "Iris",
                            "lastName": "Nomikou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iris Nomikou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 225
                            }
                        ],
                        "text": "The domain knowledge that is used later in life can be derived from the primitives that are encountered early in childhood, for example, in interactions between infants and parents, and is referred to as intermodal synchrony (Rohlfing and Nomikou 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55806285,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "486e7756a332a022bb43e987a5cc482371e51d5e",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Research findings indicate that synchrony between events in two different modalities is a key concept in early social learning. Our pilot study with 14 mother\u2013child dyads is the first to support the idea that synchrony between action and language as a form of responsive behaviour in mothers relates to later language acquisition in their children. We conducted a fine-grained coding of multimodal behaviour within the dyad during an everyday diapering activity when the children were three and six months old. When the children attained 24 months, their mothers completed language surveys; this data was then related to the dyadic measures in early interaction. We propose a \u2018switching-roles\u2019 model according to which it is important for three-month-olds to be exposed to multimodal input for a great deal of time, whereas for six-month-old infants, the mother should respond to the infant\u2019s attention and provide multimodal input when her child is gazing at her."
            },
            "slug": "Intermodal-synchrony-\u2013-as-a-form-of-maternal-\u2013-is-Rohlfing-Nomikou",
            "title": {
                "fragments": [],
                "text": "Intermodal synchrony \u2013 as a form of maternal responsiveness \u2013 is associated with language development"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732302"
                        ],
                        "name": "J. Koza",
                        "slug": "J.-Koza",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Koza",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koza"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "Perhaps they could, using genetic programming methods (Koza, 1992) or other structure-search algorithms Yamins et al. (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 54
                            }
                        ],
                        "text": "Perhaps they could, using genetic programming methods (Koza, 1992) or other structure-search algorithms Yamins et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31978081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "505c58c2c100e7512b7f7d906a9d4af72f6e8415",
            "isKey": false,
            "numCitedBy": 14068,
            "numCiting": 245,
            "paperAbstract": {
                "fragments": [],
                "text": "Background on genetic algorithms, LISP, and genetic programming hierarchical problem-solving introduction to automatically-defined functions - the two-boxes problem problems that straddle the breakeven point for computational effort Boolean parity functions determining the architecture of the program the lawnmower problem the bumblebee problem the increasing benefits of ADFs as problems are scaled up finding an impulse response function artificial ant on the San Mateo trail obstacle-avoiding robot the minesweeper problem automatic discovery of detectors for letter recognition flushes and four-of-a-kinds in a pinochle deck introduction to biochemistry and molecular biology prediction of transmembrane domains in proteins prediction of omega loops in proteins lookahead version of the transmembrane problem evolutionary selection of the architecture of the program evolution of primitives and sufficiency evolutionary selection of terminals evolution of closure simultaneous evolution of architecture, primitive functions, terminals, sufficiency, and closure the role of representation and the lens effect. Appendices: list of special symbols list of special functions list of type fonts default parameters computer implementation annotated bibliography of genetic programming electronic mailing list and public repository."
            },
            "slug": "Genetic-programming-on-the-programming-of-computers-Koza",
            "title": {
                "fragments": [],
                "text": "Genetic programming - on the programming of computers by means of natural selection"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This book discusses the evolution of architecture, primitive functions, terminals, sufficiency, and closure, and the role of representation and the lens effect in genetic programming."
            },
            "venue": {
                "fragments": [],
                "text": "Complex adaptive systems"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2683931"
                        ],
                        "name": "J. Freyd",
                        "slug": "J.-Freyd",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Freyd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Freyd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 64
                            }
                        ],
                        "text": "But it is also true that language builds on the core abilities for intuitive physics, intuitive psychology, and rapid learning with compositional, causal models that we do focus on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 226
                            }
                        ],
                        "text": "Similarly, as related to the Characters Challenge, the way people learn to write a novel handwritten character \u2013 in other words, the causal prescription for producing new examples \u2013 influences later perception and categorization (Freyd, 1983, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11768529,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c10b39dc76ac35a5dbdcac2968d1dd6773d457dc",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional \u201cfeature analysis\u201d theories of letter perception do a poor job of accounting for our ability to read handwritten letters. In this paper, an alternative theory is considered: Perhaps handwriting recognition makes use of information about how letters areformed, as well as of knowledge of static characteristics of letters, such as \u201cdistinctive features.\u201d In an experiment testing this hypothesis, subjects saw artificial characters drawn in real time and were later asked to identify distorted versions of the same characters presented statically. Subjects were faster on static characters distorted in a manner consistent with the drawing method they had witnessed than on static characters equally distorted but inconsistent with the drawing method. This finding suggests that humans can use dynamic information in the perception of static forms."
            },
            "slug": "Representing-the-dynamics-of-a-static-form-Freyd",
            "title": {
                "fragments": [],
                "text": "Representing the dynamics of a static form"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Testing the hypothesis that handwriting recognition makes use of information about how letters are formed, as well as of knowledge of static characteristics of letters, such as \u201cdistinctive features,\u201d suggests that humans can use dynamic information in the perception of static forms."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421006"
                        ],
                        "name": "R. Michalski",
                        "slug": "R.-Michalski",
                        "structuredName": {
                            "firstName": "Ryszard",
                            "lastName": "Michalski",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Michalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122137868"
                        ],
                        "name": "John R. Anderson",
                        "slug": "John-R.-Anderson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John R. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3065629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e",
            "isKey": false,
            "numCitedBy": 2796,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective. Research directions covered include: learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis."
            },
            "slug": "Machine-learning-an-artificial-intelligence-Michalski-Anderson",
            "title": {
                "fragments": [],
                "text": "Machine learning - an artificial intelligence approach"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective, including learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Symbolic computation"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719882"
                        ],
                        "name": "J. Houk",
                        "slug": "J.-Houk",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Houk",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Houk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40468115"
                        ],
                        "name": "Joel L. Davis",
                        "slug": "Joel-L.-Davis",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Davis",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3899043"
                        ],
                        "name": "D. Beiser",
                        "slug": "D.-Beiser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beiser",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beiser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 244
                            }
                        ],
                        "text": "2016; Doya 1999): the cortex to statically and dynamically store knowledge acquired by associative learning processes (Penhune & Steele 2012; Shadmehr & Krakauer 2008), the basal ganglia to learn to select information by reinforcement learning (Graybiel 2005; Houk et al. 1995), the cerebellum to implement fast time-scale computations possibly acquired with supervised learning (Kawato et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 141411992,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "188030804d0e5816762cca70e12cea9f18e0a182",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Dopamine Neurons, Organization of Strtosomal Modules, Mechanism of Responsiveness to Predictors of Reinforcement, Correspondence with the Theory of Adaptive Critics, Learning to Predict Primary Reinforcement, Learning Earlier Predictors of Reinforcement, Relation to the Actor-Critic Architecture, More Realistic Assumptions, Summary, Acknowledgments, References"
            },
            "slug": "A-Model-of-How-the-Basal-Ganglia-Generate-and-Use-Houk-Davis",
            "title": {
                "fragments": [],
                "text": "A Model of How the Basal Ganglia Generate and Use Neural Signals That Predict Reinforcement"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This chapter contains sections titled: Introduction, Dopamine Neurons, Organization of Strtosomal Modules, Mechanism of Responsiveness to Predictors of Reinforcement, Correspondence with the Theory of Adaptive Critics, and Relation to the Actor-Critic Architecture."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 179
                            }
                        ],
                        "text": "Providing a learner with different types of causal knowledge changes how they learn and generalize, as shown by changing the causal network that underlies the features of objects (Rehder, 2003; Rehder & Hastie, 2001) or the causal prescription for producing new examples of the concept (see Freyd, 1983, for an example related to the Characters Challenge)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "For example, the structure of the causal network underlying the features of a category influences how people categorize new examples (Rehder, 2003; Rehder & Hastie, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9701571,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2c77f1e39e8fbbe43549f8f15c64a31d3a9c52b7",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a theory of categorization that accounts for the effects of causal knowledge that relates the features of categories. According to causal-model theory, people explicitly represent the probabilistic causal mechanisms that link category features and classify objects by evaluating whether they were likely to have been generated by those mechanisms. In 3 experiments, participants were taught causal knowledge that related the features of a novel category. Causal-model theory provided a good quantitative account of the effect of this knowledge on the importance of both individual features and interfeature correlations to classification. By enabling precise model fits and interpretable parameter estimates, causal-model theory helps place the theory-based approach to conceptual representation on equal footing with the well-known similarity-based approaches."
            },
            "slug": "A-causal-model-theory-of-conceptual-representation-Rehder",
            "title": {
                "fragments": [],
                "text": "A causal-model theory of conceptual representation and categorization."
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A theory of categorization that accounts for the effects of causal knowledge that relates the features of categories and helps place the theory-based approach to conceptual representation on equal footing with the well-known similarity-based approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2464187"
                        ],
                        "name": "T. Lombrozo",
                        "slug": "T.-Lombrozo",
                        "structuredName": {
                            "firstName": "Tania",
                            "lastName": "Lombrozo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lombrozo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 81
                            }
                        ],
                        "text": "Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 192
                            }
                        ],
                        "text": "\u2026concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16561056,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5746668d3c1b5647f150acc26fa92740ea72fda1",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Explanation-and-categorization:-How-\u201cwhy\u201d-informs-Lombrozo",
            "title": {
                "fragments": [],
                "text": "Explanation and categorization: How \u201cwhy?\u201d informs \u201cwhat?\u201d"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153281777"
                        ],
                        "name": "D. Marr",
                        "slug": "D.-Marr",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Marr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144901036"
                        ],
                        "name": "H. Nishihara",
                        "slug": "H.-Nishihara",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Nishihara",
                            "middleNames": [
                                "Keith"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nishihara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43759520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdf9b8f4de001f5f37bc844efbce1210d581d599",
            "isKey": false,
            "numCitedBy": 2323,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The human visual process can be studied by examining the computational problems associated with deriving useful information from retinal images. In this paper, we apply this approach to the problem of representing three-dimensional shapes for the purpose of recognition. 1. Three criteria, accessibility, scope and uniqueness, and stability and sensitivity, are presented for judging the usefulness of a representation for shape recognition. 2. Three aspects of a representation\u2019s design are considered, (i) the representation\u2019s coordinate system, (ii) its primitives, which are the primary units of shape information used in the representation, and (iii) the organization the representation imposes on the information in its descriptions. 3. In terms of these design issues and the criteria presented, a shape representation for recognition should: (i) use an object-centred coordinate system, (ii) include volumetric primitives of varied sizes, and (iii) have a modular organization. A representation based on a shape\u2019s natural axes (for example the axes identified by a stick figure) follows directly from these choices. 4. The basic process for deriving a shape description in this representation must involve: (i) a means for identifying the natural axes of a shape in its image and (ii) a mechanism for transforming viewer-centred axis specifications to specifications in an object-centred coordinate system. 5. Shape recognition involves: (i) a collection of stored shape descriptions, and (ii) various indexes into the collection that allow a newly derived description to be associated with an appropriate stored description. The most important of these indexes allows shape recognition to proceed conservatively from the general to the specific based on the specificity of the information available from the image. 6. New constraints supplied by a conservative recognition process can be used to extract more information from the image. A relaxation process for carrying out this constraint analysis is described."
            },
            "slug": "Representation-and-recognition-of-the-spatial-of-Marr-Nishihara",
            "title": {
                "fragments": [],
                "text": "Representation and recognition of the spatial organization of three-dimensional shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The human visual process can be studied by examining the computational problems associated with deriving useful information from retinal images by applying the approach to the problem of representing three-dimensional shapes for the purpose of recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B. Biological Sciences"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024344857"
                        ],
                        "name": "G. Bi",
                        "slug": "G.-Bi",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "Bi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823913"
                        ],
                        "name": "M. Poo",
                        "slug": "M.-Poo",
                        "structuredName": {
                            "firstName": "Mu-Ming",
                            "lastName": "Poo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Poo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14280188,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "df1ec1bfb66ffa141bca936e8dbf9226378c77d1",
            "isKey": false,
            "numCitedBy": 1357,
            "numCiting": 204,
            "paperAbstract": {
                "fragments": [],
                "text": "Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type-specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing-dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks. When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased."
            },
            "slug": "Synaptic-modification-by-correlated-activity:-Bi-Poo",
            "title": {
                "fragments": [],
                "text": "Synaptic modification by correlated activity: Hebb's postulate revisited."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Spike timing-dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222423"
                        ],
                        "name": "A. Gopnik",
                        "slug": "A.-Gopnik",
                        "structuredName": {
                            "firstName": "Alison",
                            "lastName": "Gopnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gopnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3053914"
                        ],
                        "name": "A. Meltzoff",
                        "slug": "A.-Meltzoff",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Meltzoff",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Meltzoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 164
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 116887333,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9aac70efda696d0568bce1d9b2c03a27c6614710",
            "isKey": false,
            "numCitedBy": 1316,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The other Socratic method: Socrates's problem Augustine's problem a road map. Part 1 The theory theory: the scientist as child - but surely it can't really be a theory? a cognitive view of science, naturalistic epistemology and development - an evolutionary speculation, science as horticulture, objections - sociology, objections - timing and convergence, objections - magic, empirical advances, what is theory? structural features of theories, functional features of theories, dynamic features of theories, theories in childhood, theories as representations theories, modules, and empirical generalizations - modules, theories and development, modules and development, modularity in peripheral and central processing, empirical generalization - scripts, narratives, and nets, interactions among theories, modules, and empirical generalizations, nonconceptual development - information processing and social construction. Part 2 Evidence for the theory theory: the child's theory of appearances - the adult theory, the initial theory, the paradox of invisible objects, an alternative - a theory-change account, the nine-month-old's theory, the A-not-B error as an auxiliary hypothesis, the 18-month-old's theory, other evidence for the theory theory, semantic developments - from objects permanence to perspective taking, later semantic developments - \"gone\" and \"see\", conclusion the child's theory of action - the adult theory, the initial theory, the nine-month-old's theory, the 18-month-old's theory, other evidence for the theory theory, semantic development - \"no\", \"uh-oh\" and \"there\", later developments - from actions to desires, later semantic developments - \"want\", conclusion the child's theory of kinds - the adult theory, categories and kinds, the initial theory, the nine-month-old's theory, the 18-month-old's theory , other evidence for the theory theory, semantic development - the naming Spurt, later developments, conclusion. Part 3 Theories and language: language and thought - prerequisites, interactions, a theory-theory view, methodological issues - specificity and correlation, developmental relations between language and cognition, theories and constraints, crosslinguistic studies, individual-difference studies, conclusion the Darwinian conclusion - who's afraid of semantic holism? a developmental cognitive science, computational and neurological mechanisms, after Piaget, sailing in Neurath's boat."
            },
            "slug": "Words,-thoughts,-and-theories-Gopnik-Meltzoff",
            "title": {
                "fragments": [],
                "text": "Words, thoughts, and theories"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7829,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3347934"
                        ],
                        "name": "R. Siegler",
                        "slug": "R.-Siegler",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Siegler",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Siegler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72050797"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 78
                            }
                        ],
                        "text": "We conjecture that a competent Frostbite player can easily shift behavior appropriately, with little or no additional learning, and it is hard to imagine a way of doing that other than having a model-based planning approach in which the environment model can be modularly combined with arbitrary new\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 215
                            }
                        ],
                        "text": "There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al., 2009), to cues, to lists of rules (Siegler & Chen, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17772552,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "94ced766f3cf326a641c8c2d5ff42dafe80a2c04",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Trial-by-trial strategy assessments and a microgenetic design were used to examine 4- and 5-year-olds' learning of rules for solving balance scale problems. The design allowed us to examine simultaneously the contribution to rule learning of distal variables (qualities and knowledge with which children enter the learning situation) and proximal variables (processes that they execute during learning). Developmental differences in learning arose through two distal variables that were correlated with age--initial rule use and initial encoding-helping older children to execute several proximal processes--noticing the potential explanatory role of a key variable, formulating a more advanced rule, and generalizing and maintaining the rule. Joint consideration of distal and proximal influences seems likely to be generally useful for understanding learning and development."
            },
            "slug": "Developmental-Differences-in-Rule-Learning:-A-Siegler-Chen",
            "title": {
                "fragments": [],
                "text": "Developmental Differences in Rule Learning: A Microgenetic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Trial-by-trial strategy assessments and a microgenetic design were used to examine 4- and 5-year-olds' learning of rules for solving balance scale problems and joint consideration of distal and proximal influences seems likely to be generally useful for understanding learning and development."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683615"
                        ],
                        "name": "A. Bastos",
                        "slug": "A.-Bastos",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Bastos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bastos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145915477"
                        ],
                        "name": "W. Usrey",
                        "slug": "W.-Usrey",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Usrey",
                            "middleNames": [
                                "Martin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Usrey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27121461"
                        ],
                        "name": "Rick A Adams",
                        "slug": "Rick-A-Adams",
                        "structuredName": {
                            "firstName": "Rick",
                            "lastName": "Adams",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rick A Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324936"
                        ],
                        "name": "G. Mangun",
                        "slug": "G.-Mangun",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Mangun",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mangun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144677419"
                        ],
                        "name": "P. Fries",
                        "slug": "P.-Fries",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fries",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737497"
                        ],
                        "name": "Karl J. Friston",
                        "slug": "Karl-J.-Friston",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Friston",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl J. Friston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2024772,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "755bfd4f8060d5fcda64eaedb81a520ab7c8bdba",
            "isKey": false,
            "numCitedBy": 1558,
            "numCiting": 181,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Canonical-Microcircuits-for-Predictive-Coding-Bastos-Usrey",
            "title": {
                "fragments": [],
                "text": "Canonical Microcircuits for Predictive Coding"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144883814"
                        ],
                        "name": "E. Davis",
                        "slug": "E.-Davis",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923674"
                        ],
                        "name": "G. Marcus",
                        "slug": "G.-Marcus",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Marcus",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "And it has been difficult to learn neural-network-style representations that effortlessly generalize to new tasks that they were not trained on (see Davis & Marcus, 2015; Marcus, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13583137,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "9a522bdd86531839ff292d096e0c4050b787de02",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense."
            },
            "slug": "Commonsense-reasoning-and-commonsense-knowledge-in-Davis-Marcus",
            "title": {
                "fragments": [],
                "text": "Commonsense reasoning and commonsense knowledge in artificial intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153558474"
                        ],
                        "name": "A. Milner",
                        "slug": "A.-Milner",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Milner",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Milner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145989594"
                        ],
                        "name": "M. Goodale",
                        "slug": "M.-Goodale",
                        "structuredName": {
                            "firstName": "Melvyn",
                            "lastName": "Goodale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goodale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14969466,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "8c15631f7cca435149b2be850420a93665a49eea",
            "isKey": false,
            "numCitedBy": 4263,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Prologue 1. Introduction: vision from a biological point of view 2. Visual processing in the primate visual cortex 3. 'Cortical blindness' 4. Disorders of spatial perception and the visual control of action 5. Disorders of visual recognition 6. Dissociations between perception and action in normal subjects 7. Attention, consciousness, and the coordination of behaviour 8. Epilogue: twelve years on"
            },
            "slug": "The-visual-brain-in-action-Milner-Goodale",
            "title": {
                "fragments": [],
                "text": "The visual brain in action"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This chapter discusses vision from a biological point of view, attention, consciousness, and the coordination of behaviour in primate visual cortex, and discusses dissociations between perception and action in normal subjects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34943392"
                        ],
                        "name": "K. Clark",
                        "slug": "K.-Clark",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Clark",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 2
                            }
                        ],
                        "text": "& Clark, A. (2015) Words and the world: Predictive coding and the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11997077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3d84eb4d98de7805ff5cf7b5a3441ad31d4979a",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 238,
            "paperAbstract": {
                "fragments": [],
                "text": "Grover's quantum (search) algorithm exploits principles of quantum information theory and computation to surpass the strong Church\u2013Turing limit governing classical computers. The algorithm initializes a search field into superposed N (eigen)states to later execute nonclassical \u201csubroutines\u201d involving unitary phase shifts of measured states and to produce root-rate or quadratic gain in the algorithmic time (O(N1/2)) needed to find some \u201ctarget\u201d solution m. Akin to this fast technological search algorithm, single eukaryotic cells, such as differentiated neurons, perform natural quadratic speed-up in the search for appropriate store-operated Ca2+ response regulation of, among other processes, protein and lipid biosynthesis, cell energetics, stress responses, cell fate and death, synaptic plasticity, and immunoprotection. Such speed-up in cellular decision making results from spatiotemporal dynamics of networked intracellular Ca2+-induced Ca2+ release and the search (or signaling) velocity of Ca2+ wave propagation. As chemical processes, such as the duration of Ca2+ mobilization, become rate-limiting over interstore distances, Ca2+ waves quadratically decrease interstore-travel time from slow saltatory to fast continuous gradients proportional to the square-root of the classical Ca2+ diffusion coefficient, D1/2, matching the computing efficiency of Grover's quantum algorithm. In this Hypothesis and Theory article, I elaborate on these traits using a fire-diffuse-fire model of store-operated cytosolic Ca2+ signaling valid for glutamatergic neurons. Salient model features corresponding to Grover's quantum algorithm are parameterized to meet requirements for the Oracle Hadamard transform and Grover's iteration. A neuronal version of Grover's quantum algorithm figures to benefit signal coincidence detection and integration, bidirectional synaptic plasticity, and other vital cell functions by rapidly selecting, ordering, and/or counting optional response regulation choices."
            },
            "slug": "Basis-for-a-neuronal-version-of-Grover's-quantum-Clark",
            "title": {
                "fragments": [],
                "text": "Basis for a neuronal version of Grover's quantum algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A neuronal version of Grover's quantum algorithm figures to benefit signal coincidence detection and integration, bidirectional synaptic plasticity, and other vital cell functions by rapidly selecting, ordering, and/or counting optional response regulation choices."
            },
            "venue": {
                "fragments": [],
                "text": "Front. Mol. Neurosci."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 114
                            }
                        ],
                        "text": "In automatic speech recognition, Hidden Markov Models (HMMs) have been the leading approach since the late 1980s (Juang & Rabiner, 1990), yet this framework has been chipped away piece by piece and replaced with deep learning components (Hinton et al.,\nar X\niv :1\n60 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17743203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df682aa90fbbbf665a8b273a57ca87d6cea9ff99",
            "isKey": false,
            "numCitedBy": 1561,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of hidden Markov models for speech recognition has become predominant in the last several years, as evidenced by the number of published papers and talks at major speech conferences. The reasons this method has become so popular are the inherent statistical (mathematically precise) framework; the ease and availability of training algorithms for cstimating the parameters of the models from finite training sets of speech data; the flexibility of the resulting recognition system in which one can easily change the size, type, or architecture of the models to suit particular words, sounds, and so forth; and the ease of implementation of the overall recognition system. In this expository article, we address the role of statistical methods in this powerful technology as applied to speech recognition and discuss a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations."
            },
            "slug": "Hidden-Markov-Models-for-Speech-Recognition-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The role of statistical methods in this powerful technology as applied to speech recognition is addressed and a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4680033"
                        ],
                        "name": "M. Bouton",
                        "slug": "M.-Bouton",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Bouton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bouton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8953026,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "a1c77da415e5b7cdb957c00dc7df2ef7a7dfd327",
            "isKey": false,
            "numCitedBy": 1545,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "This article provides a selective review and integration of the behavioral literature on Pavlovian extinction. The first part reviews evidence that extinction does not destroy the original learning, but instead generates new learning that is especially context-dependent. The second part examines insights provided by research on several related behavioral phenomena (the interference paradigms, conditioned inhibition, and inhibition despite reinforcement). The final part examines four potential causes of extinction: the discrimination of a new reinforcement rate, generalization decrement, response inhibition, and violation of a reinforcer expectation. The data are consistent with behavioral models that emphasize the role of generalization decrement and expectation violation, but would be more so if those models were expanded to better accommodate the finding that extinction involves a context-modulated form of inhibitory learning."
            },
            "slug": "Context-and-behavioral-processes-in-extinction.-Bouton",
            "title": {
                "fragments": [],
                "text": "Context and behavioral processes in extinction."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Evidence that extinction does not destroy the original learning, but instead generates new learning that is especially context-dependent is reviewed, consistent with behavioral models that emphasize the role of generalization decrement and expectation violation."
            },
            "venue": {
                "fragments": [],
                "text": "Learning & memory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2778256"
                        ],
                        "name": "E. Tomai",
                        "slug": "E.-Tomai",
                        "structuredName": {
                            "firstName": "Emmett",
                            "lastName": "Tomai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tomai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1911547,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a6374d084df2791db52f0a5186666655e9145dfb",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Using Qualitative Reasoning for the Attribution of Moral Responsibility Emmett Tomai Ken Forbus Qualitative Reasoning Group, Northwestern University, 2133 Sheridan Rd, Evanston, IL 60208 {etomai,forbus}@northwestern.edu formal office. Blame is moral condemnation that follows from responsibility for a morally reprehensible outcome. Shaver\u2019s attribution process begins with a negative outcome and assigns responsibility to an involved agent by sequentially evaluating five dimensions: causality, intentionality, coercion, appreciation, and foreknowledge. Causal involvement in the negative outcome is a prerequisite for any responsibility to be assigned. Shaver characterizes intention as a scale of deliberateness with intentional at one end and involuntary at the other, such that the highest degree of intention should result in the strongest judgment of responsibility. Intention, however, can be moderated by coercion and appreciation. Coercion captures the force exerted by another agent which limits the available choices, from a social standpoint, for the agent in question. This could be through some direct threat or via an authority relationship. An agent who is coerced is assigned less responsibility than one who acts intentionally in the absence of coercion. Appreciation concerns the perceiver\u2019s judgment as to whether the agent in question has the capacity to understand that the outcome in question is morally wrong. If the agent does not have such capacity, they still bear some responsibility but are held exempt from blame. Foreknowledge is defined as the extent to which the agent was aware that an action would result in the outcome, prior to execution. Again, it is the perceiver\u2019s judgment of the knowledge the agent possessed that is evaluated. In the absence of intentionality, Shaver attributes responsibility based on foreknowledge. In Shaver\u2019s model foreknowledge may be what the agent is thought to know (epistemic) or what the perceiver thinks the agent should have known (expected). However, it says little about the contribution of expected foreknowledge. This is not surprising as his model focuses on the perception of the agent\u2019s deliberative process. Weiner\u2019s model [Weiner 1995], by contrast, focuses on attribution of responsibility in cases of achievement and failure. In the case where an agent has failed to have expected foreknowledge, this model predicts that the perception of causal controllability over that failure determines the degree of responsibility attributed. Blame in Shaver\u2019s model follows from responsibility unless there is a justification or excuse. Justification does not deny responsibility; instead it is an argument about why blame should not be assigned despite responsibility. An example would be when someone shot someone else dead, but did it in self-defense. Excuses deny responsibility by appealing the judgments of the dimensions (e.g. \u201cI didn\u2019t know\u201d, \u201cI didn\u2019t mean it\u201d). Successful intervention by an excuse alters the assignment of responsibility. Abstract We present a computational model, based on Attribution theory, of responsibility judgment for negative events. Our model uses Qualitative Process theory to reason over the continuous parameters involved in attribution, avoiding the need for ad-hoc assignment of quantitative values. Qualitative reasoning allows our model to infer relative amounts of responsibility for a situation in a manner that is consistent with relative amounts of blame attributed in a psychological experiment by Mao and Gratch [Mao & Gratch 2005]. Who is to blame? Bad things happen, and blame quickly follows. From the affairs of nations to personal misfortunes, accountability is an important part of how we understand the world around us. But how does one go from perceiving situations to judging responsibility? This question has been the topic of much research in social psychology. Recently, efforts have been made to create computational models that capture the process of responsibility judgment. This paper describes how Qualitative Process theory [Forbus 1984] can be used in such modeling. We briefly summarize aspects of Attribution theory relevant to responsibility and blame judgments, then discuss the Mao and Gratch computational model [Mao & Gratch 2005][Mao 2006]. We present an alternative model for attribution of blame based on QP theory, which we claim better represents the underlying theory. Experimental results using data collected by Mao show that our model captures that data better, and makes additional predictions. Attribution Theory The goal of Attribution theory [Heider 1958] is to identify the conditions that will lead a perceiver, through an attribution process, to attribute some behavior, event or outcome to an internal disposition of the agent involved, as opposed to an environmental condition. Attributions depend on the perceiver\u2019s knowledge. Attribution of blame has been addressed by Shaver [1985] and Weiner [1995]. Shaver distinguishes cause, responsibility and blameworthiness. For a given negative outcome, cause is defined as being an insufficient but necessary part of a condition, which is itself unnecessary but sufficient for that result. The theory only covers causes which represent human agency. Responsibility is \u201cmoral accountability\u201d, distinct from legal responsibility or the responsibilities of a"
            },
            "slug": "Using-Qualitative-Reasoning-for-the-Attribution-of-Tomai-Forbus",
            "title": {
                "fragments": [],
                "text": "Using Qualitative Reasoning for the Attribution of Moral Responsibility"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145461749"
                        ],
                        "name": "J. Power",
                        "slug": "J.-Power",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Power",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Power"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15897996"
                        ],
                        "name": "L. T. Thompson",
                        "slug": "L.-T.-Thompson",
                        "structuredName": {
                            "firstName": "Lucien",
                            "lastName": "Thompson",
                            "middleNames": [
                                "T"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. T. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40387289"
                        ],
                        "name": "J. Moyer",
                        "slug": "J.-Moyer",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Moyer",
                            "middleNames": [
                                "R."
                            ],
                            "suffix": "Jr"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3409440"
                        ],
                        "name": "J. Disterhoft",
                        "slug": "J.-Disterhoft",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Disterhoft",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Disterhoft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15243489,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "24ce2da9bfca749af57933611b1aab1ff893ca13",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "CA1 field potentials evoked by Schaffer collateral stimulation of hippocampal slices from trace-conditioned rabbits were compared with those from naive and pseudo-conditioned controls. Conditioned rabbits received 80 trace conditioning trials daily until reaching a criterion of 80% conditioned responses in a session. Hippocampal slices were prepared 1 or 24 h after reaching criterion (for trace-conditioned animals) or after a final unpaired stimulus session (for pseudo-conditioned animals); naive animals were untrained. Both somatic and dendritic field potentials were recorded in response to various stimulus durations. Recording and data reduction were performed blind to the conditioning state of the rabbit. The excitatory postsynaptic potential slope was greater in slices prepared from trace-conditioned animals killed 1 h after conditioning than in naive and pseudo-conditioned controls (repeated-measures analysis of variance, F = 4.250, P < 0.05). Associative learning specifically enhanced synaptic transmission between CA3 and CA1 immediately after training. This effect was not evident in the population field potential measured 24 h later."
            },
            "slug": "Enhanced-synaptic-transmission-in-CA1-hippocampus-Power-Thompson",
            "title": {
                "fragments": [],
                "text": "Enhanced synaptic transmission in CA1 hippocampus after eyeblink conditioning."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579894"
                        ],
                        "name": "B. Carpenter",
                        "slug": "B.-Carpenter",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Carpenter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carpenter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144389145"
                        ],
                        "name": "A. Gelman",
                        "slug": "A.-Gelman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28552618"
                        ],
                        "name": "M. Hoffman",
                        "slug": "M.-Hoffman",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hoffman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50403658"
                        ],
                        "name": "Daniel Lee",
                        "slug": "Daniel-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51024469"
                        ],
                        "name": "Ben Goodrich",
                        "slug": "Ben-Goodrich",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Goodrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Goodrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144227038"
                        ],
                        "name": "M. Betancourt",
                        "slug": "M.-Betancourt",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Betancourt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Betancourt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575536"
                        ],
                        "name": "Marcus A. Brubaker",
                        "slug": "Marcus-A.-Brubaker",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Brubaker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus A. Brubaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192564"
                        ],
                        "name": "Jiqiang Guo",
                        "slug": "Jiqiang-Guo",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48982216"
                        ],
                        "name": "Peter Li",
                        "slug": "Peter-Li",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47078183"
                        ],
                        "name": "A. Riddell",
                        "slug": "A.-Riddell",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Riddell",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Riddell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": "& Gentner, D. (2007) Nonintentional analogical inference in text comprehension."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7314923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97d9b9119bffc8b3bdd8859f88c52ead021a1b27",
            "isKey": false,
            "numCitedBy": 4290,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting."
            },
            "slug": "Stan:-A-Probabilistic-Programming-Language-Carpenter-Gelman",
            "title": {
                "fragments": [],
                "text": "Stan: A Probabilistic Programming Language"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Stan is a probabilistic programming language for specifying statistical models that provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler and an adaptive form of Hamiltonian Monte Carlo sampling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 179
                            }
                        ],
                        "text": "Somewhat surprisingly, the incorporation of attention has led to substantial performance gains in a variety of domains, including in machine translation (Bahdanau et al., 2015), object recognition (V. Mnih et al., 2014), and image caption generation (K. Xu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 139
                            }
                        ],
                        "text": "These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational optimization (Gregor et al., 2015; A. Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 193
                            }
                        ],
                        "text": "\u2026resolving the conflict between fast inference and structured representations, including Helmholtz-machine-style approximate inference in generative models (Dayan, Hinton, Neal, & Zemel, 1995; Hinton et al., 1995) and cooperation between model-free and model-based reinforcement learning systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In each case, the free combination of parts is not enough on its own: While compositionality and learning-to-learn can provide the parts for new ideas, causality provides the glue that gives them coherence and purpose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": true,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50222622"
                        ],
                        "name": "D. Hofstadter",
                        "slug": "D.-Hofstadter",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Hofstadter",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hofstadter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 199
                            }
                        ],
                        "text": "While compositionality and learning-to-learn fit naturally together, there are also forms of compositionality that rely less on previous learning, such as the bottom-up parts-based representation of Hoffman and Richards (1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144173622,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "65f872006a9df0ed7a7d522934b0e54aa03d6c2a",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nA bestselling collection of brilliant and quirky essays, on subjects ranging from biology to grammar to artificial intelligence, that are unified by one primary concern: the way people perceive and think."
            },
            "slug": "Metamagical-Themas:-Questing-for-the-Essence-of-and-Hofstadter",
            "title": {
                "fragments": [],
                "text": "Metamagical Themas: Questing for the Essence of Mind and Pattern"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922747"
                        ],
                        "name": "P. Frazier",
                        "slug": "P.-Frazier",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Frazier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frazier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765196"
                        ],
                        "name": "D. Kempe",
                        "slug": "D.-Kempe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kempe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kempe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633757"
                        ],
                        "name": "Robert D. Kleinberg",
                        "slug": "Robert-D.-Kleinberg",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert D. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 93
                            }
                        ],
                        "text": "More recent variants of the DQN have demonstrated superior performance (Schaul et al., 2015; Stadie et al., 2016; van Hasselt, Guez, & Silver, 2016; Wang et al., 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al., 2015) and 96% by using\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7893648,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "ea8a816f360e17048f00db780d63b9095b01f660",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a Bayesian multi-armed bandit (MAB) setting in which a principal seeks to maximize the sum of expected time-discounted rewards obtained by pulling arms, when the arms are actually pulled by selfish and myopic individuals. Since such individuals pull the arm with highest expected posterior reward (i.e., they always exploit and never explore), the principal must incentivize them to explore by offering suitable payments. Among others, this setting models crowdsourced information discovery and funding agencies incentivizing scientists to perform high-risk, high-reward research. We explore the tradeoff between the principal's total expected time-discounted incentive payments, and the total time-discounted rewards realized. Specifically, with a time-discount factor \u03b3 \u2208 (0,1), let OPT denote the total expected time-discounted reward achievable by a principal who pulls arms directly in a MAB problem, without having to incentivize selfish agents. We call a pair (\u03c1,b) \u2208 [0,1]2 consisting of a reward \u03c1 and payment b achievable if for every MAB instance, using expected time-discounted payments of at most b\u2022OPT, the principal can guarantee an expected time-discounted reward of at least \u03c1\u2022OPT. Our main result is an essentially complete characterization of achievable (payment, reward) pairs: if \u221ab+\u221a1-\u03c1>\u221a\u03b3, then (\u03c1,b) is achievable, and if \u221ab+\u221a1-\u03c1<\u221a\u03b3, then (\u03c1,b) is not achievable. In proving this characterization, we analyze so-called time-expanded policies, which in each step let the agents choose myopically with some probability p, and incentivize them to choose \"optimally\" with probability 1-p. The analysis of time-expanded policies leads to a question that may be of independent interest: If the same MAB instance (without selfish agents) is considered under two different time-discount rates \u03b3 > \u03b7, how small can the ratio of OPT\u03b7 to OPT\u03b3 be? We give a complete answer to this question, showing that OPT\u03b7 \u2265 (1-\u03b3)2/(1-\u03b7)2 \u2022 OPT\u03b3, and that this bound is tight."
            },
            "slug": "Incentivizing-exploration-Frazier-Kempe",
            "title": {
                "fragments": [],
                "text": "Incentivizing exploration"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A Bayesian multi-armed bandit (MAB) setting in which a principal seeks to maximize the sum of expected time-discounted rewards obtained by pulling arms, when the arms are actually pulled by selfish and myopic individuals is studied."
            },
            "venue": {
                "fragments": [],
                "text": "EC"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6226925"
                        ],
                        "name": "R. Baillargeon",
                        "slug": "R.-Baillargeon",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Baillargeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baillargeon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 166
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment and collisions (Baillargeon, 2004; Baillargeon, Li, Ng, & Yuan, 2009; Hespos & Baillargeon, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5634093,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "61075539298defdff11c0b5cabce6c76a9fd8d8e",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Investigations of infants' physical world over the past 20 years have revealed two main findings. First, even very young infants possess expectations about physical events. Second, these expectations undergo significant developments during the first year of life, as infants form event categories, such as occlusion, containment, and covering events, and identify the variables relevant for predicting outcomes in each category. A new account of infants' physical reasoning integrates these findings. Predictions from the account are examined in change-blindness and teaching experiments."
            },
            "slug": "Infants'-Physical-World-Baillargeon",
            "title": {
                "fragments": [],
                "text": "Infants' Physical World"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60565534,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69d7086300e7f5322c06f2f242a565b3a182efb5",
            "isKey": false,
            "numCitedBy": 4649,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Bill Baird { Publications References 1] B. Baird. Bifurcation analysis of oscillating neural network model of pattern recognition in the rabbit olfactory bulb. In D. 3] B. Baird. Bifurcation analysis of a network model of the rabbit olfactory bulb with periodic attractors stored by a sequence learning algorithm. 5] B. Baird. Bifurcation theory methods for programming static or periodic attractors and their bifurcations in dynamic neural networks."
            },
            "slug": "In-Advances-in-Neural-Information-Processing-Hanson",
            "title": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058739668"
                        ],
                        "name": "F. Crick",
                        "slug": "F.-Crick",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Crick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "It has long been argued, however, that backpropagation is not biologically plausible; as Crick (1989) famously pointed out, backpropagation seems to require that information be transmitted backwards along the axon, which does not fit with realistic models of neuronal function (although recent\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick, 1989; Fodor & Pylyshyn, 1988; Marcus, 1998, 2001; Minsky & Papert, 1969; Pinker & Prince, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5892527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66752d19577b2007d4b5e3a20c6ddeb8f1d1e600",
            "isKey": false,
            "numCitedBy": 646,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain. Unfortunately most of these neural nets are unrealistic in important respects."
            },
            "slug": "The-recent-excitement-about-neural-networks-Crick",
            "title": {
                "fragments": [],
                "text": "The recent excitement about neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain, but most of these neural nets are unrealistic in important respects."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48603437"
                        ],
                        "name": "A. Newell",
                        "slug": "A.-Newell",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Newell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Newell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62537074,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "547a664cf042af7ce4f171a65577441833ba673e",
            "isKey": false,
            "numCitedBy": 11582,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The aim of the book is to advance the understanding of how humans think. It seeks to do so by putting forth a theory of human problem solving, along with a body of empirical evidence that permits assessment of the theory. (Author)"
            },
            "slug": "Human-Problem-Solving-Newell",
            "title": {
                "fragments": [],
                "text": "Human Problem Solving"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The aim of the book is to advance the understanding of how humans think by putting forth a theory of human problem solving, along with a body of empirical evidence that permits assessment of the theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35219266"
                        ],
                        "name": "P. Baudis",
                        "slug": "P.-Baudis",
                        "structuredName": {
                            "firstName": "Petr",
                            "lastName": "Baudis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baudis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144663576"
                        ],
                        "name": "J. Gailly",
                        "slug": "J.-Gailly",
                        "structuredName": {
                            "firstName": "Jean-Loup",
                            "lastName": "Gailly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gailly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 54
                            }
                        ],
                        "text": "For evaluation, our model competes with GnuGo, Pachi [Baudis & Gailly (2012)] and Fuego [Enzenberger et al. (2010)]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17734019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e58dff9198c1f4327c2ee2e0753b642b552180b",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a state of the art implementation of the Monte Carlo Tree Search algorithm for the game of Go. Our Pachi software is currently one of the strongest open source Go programs, competing at the top level with other programs and playing evenly against advanced human players. We describe our implementation and choice of published algorithms as well as three notable original improvements: (1) an adaptive time control algorithm, (2) dynamic komi, and (3) the usage of the criticality statistic. We also present new methods to achieve efficient scaling both in terms of multiple threads and multiple machines in a cluster."
            },
            "slug": "PACHI:-State-of-the-Art-Open-Source-Go-Program-Baudis-Gailly",
            "title": {
                "fragments": [],
                "text": "PACHI: State of the Art Open Source Go Program"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A state of the art implementation of the Monte Carlo Tree Search algorithm for the game of Go and three notable original improvements: an adaptive time control algorithm, dynamic komi, and the usage of the criticality statistic are described."
            },
            "venue": {
                "fragments": [],
                "text": "ACG"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802148"
                        ],
                        "name": "S. Gelly",
                        "slug": "S.-Gelly",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Gelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 80
                            }
                        ],
                        "text": "Each of these components has made gains against artificial and real Go players (Gelly & Silver, 2008, 2011; Silver et al., 2016; Tian & Zhu, 2015), and the notion of combining pattern recognition and model-based search goes back decades in Go and other games."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18941952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c542aaafcf80a87b37ffa350344e65fe19b9c0ce",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Monte-Carlo-tree-search-and-rapid-action-value-in-Gelly-Silver",
            "title": {
                "fragments": [],
                "text": "Monte-Carlo tree search and rapid action value estimation in computer Go"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8093804"
                        ],
                        "name": "M. D. Jonge",
                        "slug": "M.-D.-Jonge",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Jonge",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. D. Jonge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8379654"
                        ],
                        "name": "R. Racine",
                        "slug": "R.-Racine",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Racine",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Racine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 125
                            }
                        ],
                        "text": "Learned behavior is rapidly reacquired after extinction (Bouton, 2004), whereas no such facilitation is observed for LTP (de Jonge & Racine, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13135507,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "d5ee1becdeaa30246b7f575d09eace9fe7f997b1",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-effects-of-repeated-induction-of-long-term-in-Jonge-Racine",
            "title": {
                "fragments": [],
                "text": "The effects of repeated induction of long-term potentiation in the dentate gyrus"
            },
            "venue": {
                "fragments": [],
                "text": "Brain Research"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100435"
                        ],
                        "name": "A. Alao",
                        "slug": "A.-Alao",
                        "structuredName": {
                            "firstName": "Adekola",
                            "lastName": "Alao",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Alao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 2
                            }
                        ],
                        "text": "& Sun, J. (2016) Deep residual learning for image recog-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 25
                            }
                        ],
                        "text": "Incorporating motivation (e.g., Maslow 1954; Sun 2016) in computational models of human intelligence can explain where goals come from."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37602536,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b085a54d6fe685bd4742d0c81f00ccc4f18ab285",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Do you ever know the book? Yeah, this is a very interesting book to read. As we told previously, reading is not kind of obligation activity to do when we have to obligate. Reading should be a habit, a good habit. By reading, you can open the new world and get the power from the world. Everything can be gained through the book. Well in brief, book is very powerful. As what we offer you right here, this an anatomy of the mind is as one of reading book for you."
            },
            "slug": "Anatomy-of-the-mind.-Alao",
            "title": {
                "fragments": [],
                "text": "Anatomy of the mind."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This an anatomy of the mind is as one of reading book for you, where by reading, you can open the new world and get the power from the world."
            },
            "venue": {
                "fragments": [],
                "text": "Psychiatric services"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2226641"
                        ],
                        "name": "C. Harley",
                        "slug": "C.-Harley",
                        "structuredName": {
                            "firstName": "Carolyn",
                            "lastName": "Harley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Harley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33512190,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4ee8db64f2f9543969a118266f3bf205c2f2c86a",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Noradrenergic-long-term-potentiation-in-the-dentate-Harley",
            "title": {
                "fragments": [],
                "text": "Noradrenergic long-term potentiation in the dentate gyrus."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in pharmacology"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144389145"
                        ],
                        "name": "A. Gelman",
                        "slug": "A.-Gelman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50403658"
                        ],
                        "name": "Daniel Lee",
                        "slug": "Daniel-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192564"
                        ],
                        "name": "Jiqiang Guo",
                        "slug": "Jiqiang-Guo",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Guo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18351694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e943ff3efefa3d8377c63184dec6c163b5dab0c",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users\u2019 and developers\u2019 perspectives and illustrate with a simple but nontrivial nonlinear regression example."
            },
            "slug": "Stan:-A-Probabilistic-Programming-Language-for-and-Gelman-Lee",
            "title": {
                "fragments": [],
                "text": "Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3517194"
                        ],
                        "name": "E. Capaldi",
                        "slug": "E.-Capaldi",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Capaldi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Capaldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 129
                            }
                        ],
                        "text": "There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt, 1958), Hebbian learning (Hebb, 1949), the BCM rule (Bienenstock, Cooper, & Munro, 1982), backpropagation (Rumelhart, Hinton, & Williams, 1986), the wake-sleep algorithm (Hinton, Dayan, Frey, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 24
                            }
                        ],
                        "text": "1958), Hebbian learning (Hebb, 1949), the BCM rule (Bienenstock, Cooper, & Munro, 1982), backpropagation (Rumelhart, Hinton, & Williams, 1986), the wake-sleep algorithm (Hinton, Dayan, Frey, & Neal, 1995), and contrastive divergence (Hinton, 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2085961,
            "fieldsOfStudy": [
                "Education",
                "Medicine"
            ],
            "id": "3cea0c3d350d78bd3d9bd557c1fe56c59c9bf0e2",
            "isKey": false,
            "numCitedBy": 4822,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd the organization of behavior as the choice of reading, you can find here."
            },
            "slug": "The-organization-of-behavior.-Capaldi",
            "title": {
                "fragments": [],
                "text": "The organization of behavior."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of applied behavior analysis"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312012"
                        ],
                        "name": "D. Osherson",
                        "slug": "D.-Osherson",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Osherson",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Osherson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107623826"
                        ],
                        "name": "E. Smith",
                        "slug": "E.-Smith",
                        "structuredName": {
                            "firstName": "EDWARD E.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 267
                            }
                        ],
                        "text": "\u2026concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We think it is helpful to address these points directly, to maximize the potential for moving forward together."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10482356,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "55090df49926417102b01385c320b2b271eec502",
            "isKey": false,
            "numCitedBy": 763,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-adequacy-of-prototype-theory-as-a-theory-of-Osherson-Smith",
            "title": {
                "fragments": [],
                "text": "On the adequacy of prototype theory as a theory of concepts"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069089245"
                        ],
                        "name": "Ryan",
                        "slug": "Ryan",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ryan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120534422"
                        ],
                        "name": "Deci",
                        "slug": "Deci",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Deci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1098145,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b55987b4cfff292dd121ee03c46b41f4f696136e",
            "isKey": false,
            "numCitedBy": 12876,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed. Copyright 2000 Academic Press."
            },
            "slug": "Intrinsic-and-Extrinsic-Motivations:-Classic-and-Ryan-Deci",
            "title": {
                "fragments": [],
                "text": "Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This review revisits the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory and discusses the relations of both classes of motives to basic human needs for autonomy, competence and relatedness."
            },
            "venue": {
                "fragments": [],
                "text": "Contemporary educational psychology"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 137
                            }
                        ],
                        "text": "Parallel to these developments, a radically different approach was being explored, based on neuron-like \u201csub-symbolic\u201d computations (e.g., Fukushima, 1980; Grossberg, 1976; Rosenblatt, 1958)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60159108,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Psychology"
            ],
            "id": "9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5",
            "isKey": false,
            "numCitedBy": 678,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network model, called a \u201cneocognitron\u201d, is proposed for a mechanism of visual pattern recognition. It is demonstrated by computer simulation that the neocognitron has characteristics similar to those of visual systems of vertebrates."
            },
            "slug": "Neocognitron:-A-Self-Organizing-Neural-Network-for-Fukushima-Miyake",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A neural network model, called a \u201cneocognitron\u201d, is proposed for a mechanism of visual pattern recognition that has characteristics similar to those of visual systems of vertebrates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50259673"
                        ],
                        "name": "M. Stacey",
                        "slug": "M.-Stacey",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Stacey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stacey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60490439,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "3a9d5418c3ddb6a83ffa7076004e0ba91aae6470",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientific Discovery: Computational Explorations of the Creative Processes. By Pat Langley, Herbert A. Simon, Gary L. Bradshaw and Jan M. Zytkow. Cambridge Mass., USA, The MIT Press, USD 13.50."
            },
            "slug": "Scientific-Discovery:-Computational-Explorations-of-Stacey",
            "title": {
                "fragments": [],
                "text": "Scientific Discovery: Computational Explorations of the Creative Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Scientific Discovery: Computational Explorations of the Creative Processes examines the role of language in the creative process and the role that language plays in the development of science."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860332"
                        ],
                        "name": "C. Berdahl",
                        "slug": "C.-Berdahl",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Berdahl",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berdahl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 443,
                                "start": 231
                            }
                        ],
                        "text": "However, similar reported strategies for machine architectures, algorithms, and performance demonstrate only marginal success when used as protocols to reach nearer cognitive-emotional humanness in trending social robot archetypes (Arbib & Fellous 2004; Asada 2015; Berdahl 2010; Di & Wu 2015; Han et al. 2013; Hiolle et al. 2014; Kaipa et al. 2010; McShea 2013; Read et al. 2010; Thomaz & Cakmak 2013; Wallach et al. 2010; Youyou et al. 2015), emphasizing serious need for improved adaptive quasimodel-free/-based neural nets, trainable distributed cognitionemotion mapping, and artificial personality trait parameterization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 528,
                                "start": 312
                            }
                        ],
                        "text": "Even simplistic artificial cognitive-emotional profiles and personalities thus effect varying control over acquisition and lean of machine domain-general/-specific knowledge, perception and expression of flat or excessive machine affect, and rationality and use of inferential machine attitudes/opinions/beliefs (Arbib & Fellous 2004; Asada 2015; Berdahl 2010; Cardon 2006; Davies 2016; Di & Wu 2015; Han et al. 2013; Hiolle et al. 2014; Kaipa et al. 2010; McShea 2013; Read et al. 2010; Wallach et al. 2010; Youyou et al. 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30309063,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "e67ea3fb09667ac65957034a04ddb164c32a9597",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-neural-network-model-of-Borderline-Personality-Berdahl",
            "title": {
                "fragments": [],
                "text": "A neural network model of Borderline Personality Disorder"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51975648"
                        ],
                        "name": "M. Baily",
                        "slug": "M.-Baily",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Baily",
                            "middleNames": [
                                "Neil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Baily"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964039"
                        ],
                        "name": "B. Bosworth",
                        "slug": "B.-Bosworth",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Bosworth",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bosworth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 154775159,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "0cb3cec547ee5a89ee10fd0d42cd1324a98e63a3",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of the US manufacturing sector over the last half-century displays two striking and somewhat contradictory features: 1) the growth of real output in the US manufacturing sector, measured by real value added, has equaled or exceeded that of total GDP, keeping the manufacturing share of the economy constant in price-adjusted terms; and 2) there is a long-standing decline in the share of total employment attributable to manufacturing. The persistence of these trends seems inconsistent with stories of a recent or sudden crisis in the US manufacturing sector. After all, as recently as 2010, the United States had the world's largest manufacturing sector measured by its valued-added, and while it has now been surpassed by China, the United States remains a very large manufacturer. On the other hand, there are some potential causes for concern. First, though manufacturing's output share of GDP has remained stable over 50 years, and manufacturing retains a reputation as a sector of rapid productivity improvements, this is largely due to the spectacular performance of one subsector of manufacturing: computers and electronics. Second, recently there has been a large drop in the absolute level of manufacturing employment that many find alarming. Third, the US manufacturing sector runs an enormous trade deficit, equaling $460 billion in 2012, which is also very concentrated in trade with Asia. Finally, we consider the future evolution of the manufacturing sector and its importance for the US economy. Many of the largest US corporations continue to shift their production facilities overseas. It is important to understand why the United States is not perceived to be an attractive base for their production."
            },
            "slug": "US-Manufacturing:-Understanding-Its-Past-and-Its-Baily-Bosworth",
            "title": {
                "fragments": [],
                "text": "US Manufacturing: Understanding Its Past and Its Potential Future"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060896"
                        ],
                        "name": "P. Winston",
                        "slug": "P.-Winston",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Winston",
                            "middleNames": [
                                "Henry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Winston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 106617047,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "a7eb50210a468d0878666e8f82fb55f2b179f802",
            "isKey": false,
            "numCitedBy": 1208,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D."
            },
            "slug": "Learning-Structural-Descriptions-From-Examples-Winston",
            "title": {
                "fragments": [],
                "text": "Learning Structural Descriptions From Examples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145485799"
                        ],
                        "name": "Chris Russell",
                        "slug": "Chris-Russell",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2699095"
                        ],
                        "name": "A. Dick",
                        "slug": "A.-Dick",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dick",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39699652"
                        ],
                        "name": "J. Bastian",
                        "slug": "J.-Bastian",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bastian",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bastian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153546277"
                        ],
                        "name": "D. Pooley",
                        "slug": "D.-Pooley",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Pooley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pooley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38229661"
                        ],
                        "name": "Lachlan Fleming",
                        "slug": "Lachlan-Fleming",
                        "structuredName": {
                            "firstName": "Lachlan",
                            "lastName": "Fleming",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lachlan Fleming"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3377447"
                        ],
                        "name": "L. Agapito",
                        "slug": "L.-Agapito",
                        "structuredName": {
                            "firstName": "Lourdes",
                            "lastName": "Agapito",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Agapito"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "Certainly one could argue that language should be included on any short list of key ingredients in human intelligence: For example, Mikolov et al. (2016) featured language prominently in their recent paper sketching challenge problems and a road map for AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6203141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "374b0aeb7a9a8309a92b48dbb43c075cdf34587e",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to recover the structure of a compound scene from multiple silhouettes. Structure is expressed as a collection of 3D primitives chosen from a predefined library, each with an associated pose. This has several advantages over a volume or mesh representation both for estimation and the utility of the recovered model. The main challenge in recovering such a model is the combinatorial number of possible arrangements of parts. We address this issue by exploiting the intrinsic structure and sparsity of the problem, and show that our method scales to scenes constructed from large libraries of parts."
            },
            "slug": "Part-based-modelling-of-compound-scenes-from-images-Hengel-Russell",
            "title": {
                "fragments": [],
                "text": "Part-based modelling of compound scenes from images"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work proposes a method to recover the structure of a compound scene from multiple silhouettes by exploiting the intrinsic structure and sparsity of the problem, and shows that the method scales to scenes constructed from large libraries of parts."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144676410"
                        ],
                        "name": "M. Halle",
                        "slug": "M.-Halle",
                        "structuredName": {
                            "firstName": "Morris",
                            "lastName": "Halle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Halle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144179113"
                        ],
                        "name": "K. Stevens",
                        "slug": "K.-Stevens",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stevens",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stevens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 205
                            }
                        ],
                        "text": "Many researchers have speculated about key features of human cognition that gives rise to language and other uniquely human modes of thought: Is it recursion, or some new kind of recursive structure building ability (Berwick & Chomsky, 2016; Hauser, Chomsky, & Fitch, 2002)?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 181
                            }
                        ],
                        "text": "\u201cAnalysis-by-synthesis\u201d theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel, 2010; Eden, 1962; Halle & Stevens, 1962; Neisser, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10846833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ff661af7f909f8a8644a0b5d445216c357f8f76",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A speech recognition model is proposed in which the transformation from an input speech signal into a sequence of phonemes is carried out largely through an active or feedback process. In this process, patterns are generated internally in the analyzer according to an adaptable sequence of instructions until a best match with the input signal is obtained. Details of the process are given, and the areas where further research is needed are indicated."
            },
            "slug": "Speech-recognition:-A-model-and-a-program-for-Halle-Stevens",
            "title": {
                "fragments": [],
                "text": "Speech recognition: A model and a program for research"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A speech recognition model is proposed in which the transformation from an input speech signal into a sequence of phonemes is carried out largely through an active or feedback process."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323727"
                        ],
                        "name": "M. Buscema",
                        "slug": "M.-Buscema",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Buscema",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buscema"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258380"
                        ],
                        "name": "W. Tastle",
                        "slug": "W.-Tastle",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Tastle",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Tastle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8225631"
                        ],
                        "name": "Stefano Terzi",
                        "slug": "Stefano-Terzi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Terzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Terzi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57157351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79e1d424f7b564f632e5f4fd18bbdcbb89a5b416",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "An innovative taxonomy for the classification of classifiers is presented. This new family of meta-classifiers called Meta-Net, having its foundation in the theory of independent judges, is introduced, defined, described, and shown to possess very good performance when compared to other known meta-classifiers."
            },
            "slug": "Meta-Net:-A-New-Meta-Classifier-Family-Buscema-Tastle",
            "title": {
                "fragments": [],
                "text": "Meta Net: A New Meta-Classifier Family"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An innovative taxonomy for the classification of classifiers called Meta-Net, having its foundation in the theory of independent judges, is introduced, defined, described, and shown to possess very good performance when compared to other known meta-classifiers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145902909"
                        ],
                        "name": "J. Davies",
                        "slug": "J.-Davies",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Davies",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Davies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 528,
                                "start": 312
                            }
                        ],
                        "text": "Even simplistic artificial cognitive-emotional profiles and personalities thus effect varying control over acquisition and lean of machine domain-general/-specific knowledge, perception and expression of flat or excessive machine affect, and rationality and use of inferential machine attitudes/opinions/beliefs (Arbib & Fellous 2004; Asada 2015; Berdahl 2010; Cardon 2006; Davies 2016; Di & Wu 2015; Han et al. 2013; Hiolle et al. 2014; Kaipa et al. 2010; McShea 2013; Read et al. 2010; Wallach et al. 2010; Youyou et al. 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 330,
                                "start": 278
                            }
                        ],
                        "text": "Moreover, this same, somewhat counterintuitive, problem in the authors\u2019 otherwise rational approach dangerously leaves unaddressed the major ethical and security issues of \u201cfree-willed\u201d personified artificial sentient agents, often popularized by fantasists and futurists alike (Bostrom 2014; Briegel 2012; Davies 2016; Fung 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4448193,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "3fb408fd4edd98552a8a9c9915fe8ec7d31a9147",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Concerns that artificial intelligence will pose a danger if it develops consciousness are misplaced, says Jim Davies."
            },
            "slug": "Program-good-ethics-into-artificial-intelligence-Davies",
            "title": {
                "fragments": [],
                "text": "Program good ethics into artificial intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Concerns that artificial intelligence will pose a danger if it develops consciousness are misplaced, says Jim Davies."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847175"
                        ],
                        "name": "M. Minsky",
                        "slug": "M.-Minsky",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Minsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Minsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2434678"
                        ],
                        "name": "S. Papert",
                        "slug": "S.-Papert",
                        "structuredName": {
                            "firstName": "Seymour",
                            "lastName": "Papert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Papert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick, 1989; Fodor & Pylyshyn, 1988; Marcus, 1998, 2001; Minsky & Papert, 1969; Pinker & Prince, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5400596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f74ded11f72099d16591a1191d72262ae6b5f14a",
            "isKey": false,
            "numCitedBy": 3040,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons"
            },
            "slug": "Perceptrons-an-introduction-to-computational-Minsky-Papert",
            "title": {
                "fragments": [],
                "text": "Perceptrons - an introduction to computational geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The aim of this book is to seek general results from the close study of abstract version of devices known as perceptrons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676083"
                        ],
                        "name": "H. Gray",
                        "slug": "H.-Gray",
                        "structuredName": {
                            "firstName": "Heather",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144470585"
                        ],
                        "name": "Kurt Gray",
                        "slug": "Kurt-Gray",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Gray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kurt Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810430"
                        ],
                        "name": "D. Wegner",
                        "slug": "D.-Wegner",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Wegner",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wegner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 127
                            }
                        ],
                        "text": "Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models (see Griffiths et al. [2012] for a review)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31773170,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b4f7cb021d130ca7f8949d53b746d60b216ce14c",
            "isKey": false,
            "numCitedBy": 998,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Participants compared the mental capacities of various human and nonhuman characters via online surveys. Factor analysis revealed two dimensions of mind perception, Experience (for example, capacity for hunger) and Agency (for example, capacity for self-control). The dimensions predicted different moral judgments but were both related to valuing of mind."
            },
            "slug": "Dimensions-of-Mind-Perception-Gray-Gray",
            "title": {
                "fragments": [],
                "text": "Dimensions of Mind Perception"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Factor analysis revealed two dimensions of mind perception, Experience and Agency, which predicted different moral judgments but were both related to valuing of mind."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46443210"
                        ],
                        "name": "S. Harkness",
                        "slug": "S.-Harkness",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Harkness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harkness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114453854"
                        ],
                        "name": "Marjolijn J M Blom",
                        "slug": "Marjolijn-J-M-Blom",
                        "structuredName": {
                            "firstName": "Marjolijn",
                            "lastName": "Blom",
                            "middleNames": [
                                "J",
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marjolijn J M Blom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152212206"
                        ],
                        "name": "Alfredo Oliva",
                        "slug": "Alfredo-Oliva",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alfredo Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6921716"
                        ],
                        "name": "Ughetta Moscardino",
                        "slug": "Ughetta-Moscardino",
                        "structuredName": {
                            "firstName": "Ughetta",
                            "lastName": "Moscardino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ughetta Moscardino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5944572"
                        ],
                        "name": "P. .. \u017bylicz",
                        "slug": "P.-..-\u017bylicz",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "\u017bylicz",
                            "middleNames": [
                                "Olaf"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. .. \u017bylicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071618895"
                        ],
                        "name": "Mois\u00e9s R\u00edos Berm\u00fadez",
                        "slug": "Mois\u00e9s-R\u00edos-Berm\u00fadez",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Berm\u00fadez",
                            "middleNames": [
                                "R\u00edos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mois\u00e9s R\u00edos Berm\u00fadez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144344443"
                        ],
                        "name": "Xin Feng",
                        "slug": "Xin-Feng",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423258705"
                        ],
                        "name": "Agnieszka Carrasco\u2010Zylicz",
                        "slug": "Agnieszka-Carrasco\u2010Zylicz",
                        "structuredName": {
                            "firstName": "Agnieszka",
                            "lastName": "Carrasco\u2010Zylicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agnieszka Carrasco\u2010Zylicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5945056"
                        ],
                        "name": "G. Axia",
                        "slug": "G.-Axia",
                        "structuredName": {
                            "firstName": "Giovanna",
                            "lastName": "Axia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Axia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6021939"
                        ],
                        "name": "C. Super",
                        "slug": "C.-Super",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Super",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Super"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 6
                            }
                        ],
                        "text": "[CDG] Super, C. M. & Harkness, S. (2002) Culture structures the environment for devel-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 8
                            }
                        ],
                        "text": "western cultures. Comparative Education 43(1):113\u201335. [JMC] Harlow, H. F. (1949) The formation of learning sets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54649153,
            "fieldsOfStudy": [
                "Sociology",
                "Education"
            ],
            "id": "fde713a628d1106def248785500d30c4e60219ab",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores teachers' ethnotheories of the \u2018ideal student\u2019 in five western societies: Italy, The Netherlands, Poland, Spain, and the US. Quantitative and qualitative methods are used to derive culture\u2010specific profiles of the \u2018ideal student\u2019 as described by kindergarten and primary school teachers in semi\u2010structured interviews (sample n's = 12 to 21). Discriminant function analysis shows that teachers' descriptor profiles can be correctly assigned to their own cultural group in up to 94% of all cases. Qualitative analysis of the interviews suggests both shared themes (e.g. motivation, independence) and culturally specific understandings of their meaning and significance. Contrary to the prevalent focus on cognitive qualities emphasized by western educational assessment practices, teachers in all the samples talked more about the importance of social intelligence and self\u2010regulation for success in school."
            },
            "slug": "Teachers'-ethnotheories-of-the-\u2018ideal-student\u2019-in-Harkness-Blom",
            "title": {
                "fragments": [],
                "text": "Teachers' ethnotheories of the \u2018ideal student\u2019 in five western cultures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152871994"
                        ],
                        "name": "J. Hamlin",
                        "slug": "J.-Hamlin",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hamlin",
                            "middleNames": [
                                "Kiley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hamlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5745658"
                        ],
                        "name": "K. Wynn",
                        "slug": "K.-Wynn",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Wynn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wynn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3594555"
                        ],
                        "name": "P. Bloom",
                        "slug": "P.-Bloom",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bloom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bloom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2377620,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "bdbb659cbdd311ee65d0e5c5f27e8e94f159e0fa",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous research has shown that 6-month-olds evaluate others on the basis of their social behaviors--they are attracted to prosocial individuals, and avoid antisocial individuals (Hamlin, Wynn & Bloom, 2007). The current studies investigate these capacities prior to 6 months of age. Results from two experiments indicate that even 3-month-old infants evaluate others based on their social behavior towards third parties, and that negative social information is developmentally privileged."
            },
            "slug": "Three-month-olds-show-a-negativity-bias-in-their-Hamlin-Wynn",
            "title": {
                "fragments": [],
                "text": "Three-month-olds show a negativity bias in their social evaluations."
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Results from two experiments indicate that even 3-month-old infants evaluate others based on their social behavior towards third parties, and that negative social information is developmentally privileged."
            },
            "venue": {
                "fragments": [],
                "text": "Developmental science"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24032547"
                        ],
                        "name": "H. Hashemi",
                        "slug": "H.-Hashemi",
                        "structuredName": {
                            "firstName": "Hosein",
                            "lastName": "Hashemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hashemi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17616836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18830db62961f5b3b1a0a554f4062c427b1d5555",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Time-frequency (TF) representations based on minimum mean cross-entropy (MMCE) solution, Bessel kernel, and generalized marginal page distribution are used as the input of clustering. It is shown that general trends of frequency changed with time are not the same for these transforms. The proposed method is based on the knowledge integration of TF transforms followed by a clustering data matrix and the derivation of fuzzy membership values. Based on the output of cluster membership values, the full bandwidth illumination (FBWI) index is defined as a tool for qualitative seismic interpretation. The method is applicable for studying the frequency behavior of reflectors in a reservoir for detecting fluid migration paths."
            },
            "slug": "Fuzzy-Clustering-of-Seismic-Sequences:-Segmentation-Hashemi",
            "title": {
                "fragments": [],
                "text": "Fuzzy Clustering of Seismic Sequences: Segmentation of Time-Frequency Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed method is based on the knowledge integration of TF transforms followed by a clustering data matrix and the derivation of fuzzy membership values, and the full bandwidth illumination (FBWI) index is defined as a tool for qualitative seismic interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5157152"
                        ],
                        "name": "T. Chouard",
                        "slug": "T.-Chouard",
                        "structuredName": {
                            "firstName": "Tanguy",
                            "lastName": "Chouard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Chouard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 156621825,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "f0d7327216d9edd010cfbecb3e486936a9b55a3b",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature reports from AlphaGo's victory in Seoul."
            },
            "slug": "The-Go-Files:-AI-computer-wraps-up-4-1-victory-Chouard",
            "title": {
                "fragments": [],
                "text": "The Go Files: AI computer wraps up 4-1 victory against human champion"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144967468"
                        ],
                        "name": "M. Eden",
                        "slug": "M.-Eden",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Eden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Eden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 169
                            }
                        ],
                        "text": "\u201cAnalysis-by-synthesis\u201d theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel, 2010; Eden, 1962; Halle & Stevens, 1962; Neisser, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5294048,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology",
                "Medicine"
            ],
            "id": "9c9129e491c2d2ab65159251d23c7ea566ccd56f",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwriting can be characterized as a sequence of basic strokes connected according to rule. Handwriting so generated approximates that of humans very closely. Such a matching process can be used as the fundamental principle in a handwriting recognizer."
            },
            "slug": "Handwriting-and-pattern-recognition-Eden",
            "title": {
                "fragments": [],
                "text": "Handwriting and pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Handwriting can be characterized as a sequence of basic strokes connected according to rule, and so generated approximates that of humans very closely."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323727"
                        ],
                        "name": "M. Buscema",
                        "slug": "M.-Buscema",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Buscema",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buscema"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258380"
                        ],
                        "name": "W. Tastle",
                        "slug": "W.-Tastle",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Tastle",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Tastle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258380"
                        ],
                        "name": "W. Tastle",
                        "slug": "W.-Tastle",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Tastle",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Tastle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 131
                            }
                        ],
                        "text": "\u201cdeep\u201d ANNs, mathematically very different from one another, on the same problem and to filter their results by means of a MetaNet (Buscema 1998; Buscema et al. 2010; 2013) that ignores their specific architectures, in terms of both prediction performance and biological plausibility."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17202009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e26b8dff3aafd3bd5e4c789e3fd438da2f6aa0e9",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A taxonomy for classifying classifiers is presented. A new meta-classifier, Meta-Consensus, with a foundation in both consensus theory and the theory of independent judges, is introduced."
            },
            "slug": "A-new-meta-classifier-Buscema-Tastle",
            "title": {
                "fragments": [],
                "text": "A new meta-classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new meta-classifier, Meta-Consensus, with a foundation in both consensus theory and the theory of independent judges, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2010 Annual Meeting of the North American Fuzzy Information Processing Society"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339997"
                        ],
                        "name": "S. Gelman",
                        "slug": "S.-Gelman",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Gelman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17427736,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f3d274c375639fb4abd90c33fa7dd2453a350859",
            "isKey": false,
            "numCitedBy": 1372,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "PART I: THE PHENOMENA PART II: MECHANISMS OF ACQUISITION PART III: IMPLICATIONS AND SPECULATIONS"
            },
            "slug": "The-Essential-Child:-Origins-of-Essentialism-in-Gelman",
            "title": {
                "fragments": [],
                "text": "The Essential Child: Origins of Essentialism in Everyday Thought"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51921883"
                        ],
                        "name": "Refractor",
                        "slug": "Refractor",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Refractor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Refractor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 85
                            }
                        ],
                        "text": "Understanding which details matter and which do not requires a computational theory (Marr, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208793436,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "c3a24b0b38922c4f3a825edb97cc470a4ca7af75",
            "isKey": false,
            "numCitedBy": 3113,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Vision-Refractor",
            "title": {
                "fragments": [],
                "text": "Vision"
            },
            "venue": {
                "fragments": [],
                "text": "The Lancet"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116163503"
                        ],
                        "name": "A. Antunes",
                        "slug": "A.-Antunes",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Antunes",
                            "middleNames": [
                                "Cesar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Antunes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123797461,
            "fieldsOfStudy": [
                "Education",
                "Sociology"
            ],
            "id": "5827b0a11123e04477a0a390ee31c82417d41423",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This article discusses the exercise of democracy and citizenship within school as the condition for everyone to be the subject of the educational process. Considering the educational discourse is favorable to the practice of democracy, when attempting to respond to a series of questions raised here we see how distant we are from living its essential aspects. The text also brings contributions on how to educate for and by citizenship within the school context."
            },
            "slug": "Democracia-e-Cidadania-na-Escola:-Do-Discurso-\u00e0-Antunes",
            "title": {
                "fragments": [],
                "text": "Democracia e Cidadania na Escola: Do Discurso \u00e0 Pr\u00e1tica"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155783"
                        ],
                        "name": "C. Robert",
                        "slug": "C.-Robert",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Robert",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One possibility is that intuitive psychology is simply cues \u201call the way down\u201d (Schlottmann, Cole, Watts, & White, 2013; Scholl & Gao, 2013), though this would require more and more cues as the scenarios become more complex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One limitation is that PhysNet currently requires extensive training \u2013 between 100,000 and 200,000 scenes \u2013 to learn judgments for just a single task (will the tower fall?) on a narrow range of scenes (towers with two to four cubes)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2235,
                                "start": 95
                            }
                        ],
                        "text": "With the full 924 hours of unique experience and additional replay, the DQN achieved less than 10% of human-level performance during a controlled test session (see DQN in Fig. 3). More recent variants of the DQN perform better, and can even outperform the human tester (Schaul et al. 2016; Stadie et al. 2016; van Hasselt et al. 2016; Wang et al. 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al. 2016), and 172% by using smarter replay and more efficient parameter sharing (Wang et al. 2016) (see DQN+ and DQN++ in Fig. 3).(3) But they require a lot of experience to reach this level. The learning curve for the model of Wang et al. (2016) shows performance is approximately 44% after 200 hours, 8% after 100 hours, and less than 2% after 5 hours (which is close to random play, approximately 1.5%). The differences between the human and machine learning curves suggest that they may be learning different kinds of knowledge, using different learning mechanisms, or both. The contrast becomes even more dramatic if we look at the very earliest stages of learning. Although both the original DQN and these more recent variants require multiple hours of experience to perform reliably better than random play, even non-professional humans can grasp the basics of the game after just a few minutes of play. We speculate that people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below. Although novice players may make some mistakes, such as inferring that fish are harmful rather than helpful, they can learn to play better than chance within a few minutes. If humans are able to first watch an expert playing for a few minutes, they can learn even faster. In informal experiments with two of the authors playing Frostbite on a Javascript emulator (http:// www.virtualatari.org/soft.php?soft=Frostbite), after watching videos of expert play on YouTube for just 2 minutes, we found that we were able to reach scores comparable to or better than the human expert reported in Mnih et al. (2015) after at most 15 to 20 minutes of total practice."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One indicator\nof richness is the variety of functions that these models support (A. B. Markman & Ross, 2003; Solomon, Medin, & Lynch, 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 39
                            }
                        ],
                        "text": "ences of the United States of America 110(45):18327\u201332. [arBML, ED] Baudi\u0161, P. & Gailly, J.-l. (2012) PACHI: State of the art open source Go program."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One way to examine the differences is by considering the amount of experience required for learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 703,
                                "start": 95
                            }
                        ],
                        "text": "With the full 924 hours of unique experience and additional replay, the DQN achieved less than 10% of human-level performance during a controlled test session (see DQN in Fig. 3). More recent variants of the DQN perform better, and can even outperform the human tester (Schaul et al. 2016; Stadie et al. 2016; van Hasselt et al. 2016; Wang et al. 2016), reaching 83% of the professional gamer\u2019s score by incorporating smarter experience replay (Schaul et al. 2016), and 172% by using smarter replay and more efficient parameter sharing (Wang et al. 2016) (see DQN+ and DQN++ in Fig. 3).(3) But they require a lot of experience to reach this level. The learning curve for the model of Wang et al. (2016) shows performance is approximately 44% after 200 hours, 8% after 100 hours, and less than 2% after 5 hours (which is close to random play, approximately 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1839,
                                "start": 144
                            }
                        ],
                        "text": "In individual sessions lasting no longer than 5 minutes, author TDU obtained scores of 3520 points after approximately 5 minutes of gameplay, 3510 points after 10 minutes, and 7810 points after 15 minutes. Author JBT obtained 4060 after approximately 5 minutes of gameplay, 4920 after 10 to 15 minutes, and 6710 after no more than 20 minutes. TDU and JBT each watched approximately 2 minutes of expert play on YouTube (e.g., https://www.youtube.com/watch?v=ZpUFztf9Fjc, but there are many similar examples that can be found in a YouTube search). 5. Although connectionist networks have been used to model the general transition that children undergo between the ages of 3 and 4 regarding false belief (e.g., Berthiaume et al. 2013), we are referring here to scenarios, which require inferring goals, utilities, and relations. 6. We must be careful here about what \u201csimple\u201d means. An inductive bias may appear simple in the sense that we can compactly describe it, but it may require complex computation (e.g., motion analysis, parsing images into objects, etc.) just to produce its inputs in a suitable form. 7. A new approach using convolutional \u201cmatching networks\u201d achieves good one-shot classification performance when discriminating between characters from different alphabets (Vinyals et al. 2016). It has not yet been directly compared with BPL, which was evaluated on one-shot classification with characters from the same alphabet. 8. Deep convolutional neural network classifiers have error rates approximately five times higher than those of humans when pre-trained with five alphabets (23% versus 4% error), and two to three times higher when pre-training on six times as much data (30 alphabets) (Lake et al. 2015a). The current need for extensive pre-training is illustrated for deep generative models by Rezende et al. (2016), who present extensions of the DRAW architecture capable of one-shot learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One of the main motivations for using neural networks in machine vision and speech systems is to respond as quickly as the brain does."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2155,
                                "start": 166
                            }
                        ],
                        "text": "\u201cAnalysis-by-synthesis\u201d theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel 2010; Eden 1962; Halle & Stevens 1962; Neisser 1966). Relating data to their causal source provides strong priors for perception and learning, as well as a richer basis for generalizing in new ways and to new tasks. The canonical examples of this approach are speech and visual perception. For example, Liberman et al. (1967) argued that the richness of speech perception is best explained by inverting the production plan, at the level of vocal tract movements, to explain the large amounts of acoustic variability and the blending of cues across adjacent phonemes. As discussed, causality does not have to be a literal inversion of the actual generative mechanisms, as proposed in the motor theory of speech. For the BPL of learning handwritten characters, causality is operationalized by treating concepts as motor programs, or abstract causal descriptions of how to produce examples of the concept, rather than concrete configurations of specific muscles (Fig. 5A). Causality is an important factor in the model\u2019s success in classifying and generating new examples after seeing just a single example of a new concept (Lake et al. 2015a) (Fig. 5B). Causal knowledge has also been shown to influence how people learn new concepts; providing a learner with different types of causal knowledge changes how he or she learns and generalizes. For example, the structure of the causal network underlying the features of a category influences how people categorize new examples (Rehder 2003; Rehder & Hastie 2001). Similarly, as related to the Characters Challenge, the way people learn to write a novel handwritten character influences later perception and categorization (Freyd 1983; 1987). To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick, whereas other equally applicable features wash away (Murphy & Medin 1985). Borrowing examples from Murphy and Medin (1985), the feature \u201cflammable\u201d is more closely attached to wood than money because of the underlying causal roles of the concepts, even though Lake et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 44
                            }
                        ],
                        "text": "of Sciences of the United States of America 108(suppl 2):10918\u201325. [MHT] Braud, R., Mostafaoui, G., Karaouzene, A. & Gaussier, P. (2014). Simulating the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 2
                            }
                        ],
                        "text": "2010; Tenenbaum et al. 2011). One way people acquire this prior knowledge is through \u201clearning-to-learn,\u201d a term introduced by Harlow (1949) and closely related to the machine learning notions of \u201ctransfer learning,\u201d \u201cmultitask learning,\u201d and \u201crepresentation learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One implication of amortization is that solutions to different problems will become correlated due to the sharing of amortized computations; some evidence for inferential correlations in humans was reported by Gershman and Goodman (2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One way people acquire this prior knowledge is through \u201clearning-to-learn,\u201d a term introduced by Harlow (1949) and closely related to the machine learning notions of \u201ctransfer learning\u201d, \u201cmultitask learning\u201d or \u201crepresentation learning.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 9
                            }
                        ],
                        "text": "PLoS One 10(7):e0126020. [MBu] Buscema, M., Tastle, W. J. & Terzi, S. (2013) Meta net: A new meta-classifier family."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4051,
                                "start": 145
                            }
                        ],
                        "text": "The basic system always learned from 30 million games, but it played against successively stronger versions of itself, effectively learning from 100 million or more games altogether (D. Silver, personal communication, 2017). In contrast, Lee has probably played around 50,000 games in his entire life. Looking at numbers like these, it is impressive that Lee can even compete with AlphaGo. What would it take to build a professional-level Go AI that learns from only 50,000 games? Perhaps a system that combines the advances of AlphaGo with some of the complementary ingredients for intelligence we argue for here would be a route to that end. Artificial intelligence could also gain much by trying to match the learning speed and flexibility of normal human Go players. People take a long time to master the game of Go, but as with the Frostbite and Characters challenges (sects. 3.1 and 3.2), humans can quickly learn the basics of the game through a combination of explicit instruction, watching others, and experience. Playing just a few games teaches a human enough to beat someone who has just learned the rules but never played before. Could AlphaGo model these earliest stages of real human learning curves? Human Go players can also adapt what they have learned to innumerable game variants. The Wikipedia page \u201cGo variants\u201d describes versions such as playing on bigger or smaller board sizes (ranging from 9 \u00d7 9 to 38 \u00d7 38, not just the usual 19 \u00d7 19 board), or playing on boards of different shapes and connectivity structures (rectangles, triangles, hexagons, even a map of the English city Milton Keynes). The board can be a torus, a mobius strip, a cube, or a diamond lattice in three dimensions. Holes can be cut in the board, in regular or irregular ways. The rules can be adapted to what is known as First Capture Go (the first player to capture a stone wins), NoGo (the player who avoids capturing any enemy stones longer wins), or Time Is Money Go (players begin with a fixed amount of time and at the end of the game, the number of seconds remaining on each player\u2019s clock is added to his or her score). Players may receive bonuses for creating certain stone patterns or capturing territory near certain landmarks. There could be four or more players, competing individually or in teams. In each of these variants, effective play needs to change from the basic game, but a skilled player can adapt, and does not simply have to relearn the game from scratch. Could AlphaGo quickly adapt to new variants of Go? Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes (Sermanet et al. 2014), the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people. Many of the variants described above would require significant reprogramming and retraining, directed by the smart humans who programmed AlphaGo, not the system itself. As impressive as AlphaGo is in beating the world\u2019s best players at the standard game \u2013 and it is extremely impressive \u2013 the fact that it cannot even conceive of these variants, let alone adapt to them autonomously, is a sign that it does not understand the game as humans do. Human players can Figure 7. An AI system for playing Go, combining a deep convolutional network (ConvNet) and model-based search through MonteCarlo Tree Search (MCTS). (A) The ConvNet on its own can be used to predict the next k moves given the current board. (B) A search tree with the current board state as its root and the current \u201cwin/total\u201d statistics at each node. A newMCTS rollout selects moves along the tree according to the MCTS policy (red arrows) until it reaches a new leaf (red circle), where the next move is chosen by the ConvNet. From there, play proceeds until the game\u2019s end according to a pre-defined default policy based on the Pachi program (Baudi\u0161 & Gailly 2012), itself based on MCTS. (C) The end-game result of the new leaf is used to update the search tree. Adapted from Tian and Zhu (2016) with permission."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One boundary condition on this flexibility is the fact that the skills become \u201chabitized\u201d with routine application, possibly reflecting a shift from model-based to model-free control."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "One worthy goal would be to build an AI system that beats a world-class player with the amount and kind of training human champions receive \u2013 rather than overpowering them with Google-scale computational resources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 490,
                                "start": 166
                            }
                        ],
                        "text": "\u201cAnalysis-by-synthesis\u201d theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel 2010; Eden 1962; Halle & Stevens 1962; Neisser 1966). Relating data to their causal source provides strong priors for perception and learning, as well as a richer basis for generalizing in new ways and to new tasks. The canonical examples of this approach are speech and visual perception. For example, Liberman et al. (1967) argued that the richness of speech perception is best explained by inverting the production plan, at the level of vocal tract movements, to explain the large amounts of acoustic variability and the blending of cues across adjacent phonemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63827220,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "e0fe9b2f77288bc5e6f778611a49e62e98231f8c",
            "isKey": true,
            "numCitedBy": 514,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Superintelligence:-Paths,-Dangers,-Strategies-Robert",
            "title": {
                "fragments": [],
                "text": "Superintelligence: Paths, Dangers, Strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 801,
                                "start": 201
                            }
                        ],
                        "text": "A recent article shows that one can outperformDQNs early in learning (and make non-trivial generalizations) with an \u201cepisodic controller\u201d that chooses actions based on memory and simple interpolation (Blundell et al. 2016). Although it is unclear if the DQN also memorizes action sequences, an alternative \u201chuman starts\u201d metric provides a stronger test of generalization (van Hasselt et al. 2016), evaluating the algorithms on a wider variety of start states and levels that are sampled from human play. It would be preferable to compare people and algorithms on the human starts metric, but most learning curves to date have only been reported using standard test performance, which starts the game from the beginning with some added jitter. 4. More precisely, the human expert in Mnih et al. (2015) scored an average of 4335 points across 30 game sessions of up to 5 minutes of play."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 7
                            }
                        ],
                        "text": "[aBML] Hofstadter, D. R. (2001) Epilogue: Analogy as the core of cognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 674,
                                "start": 185
                            }
                        ],
                        "text": "This agnosticism implicitly licenses a modeling approach in which detailed, domain-specific information can be imparted to an agent directly, an approach for which some of the authors\u2019 Bayesian Program Learning (BPL) work is emblematic. The two domains Lake and colleagues focus most upon \u2013 physics and theory of mind \u2013 are amenable to such an approach, in that these happen to be fields for which mature scientific disciplines exist. This provides unusually rich support for hand design of cognitive models. However, it is not clear that such hand design will be feasible in other more idiosyncratic domains where comparable scaffolding is unavailable. Lake et al. (2015a) were able to extend the approach to Omniglot characters by intuiting a suitable (stroke-based) model, but are we in a position to build comparably detailed domain models for such things as human dialogue and architecture? What about Japanese cuisine or ice skating? Even video-game play appears daunting, when one takes into account the vast amount of semantic knowledge that is plausibly relevant (knowledge about igloos, ice floes, cold water, polar bears, video-game levels, avatars, lives, points, and so forth)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 453,
                                "start": 0
                            }
                        ],
                        "text": "But will such knowledge really be embedded in \u201cintuitive theories\u201dofphysics or psychology?This commentary argues that there is a paradox at the heart of the \u201cintuitive theory\u201d viewpoint, that has bedevilled analytic philosophy and symbolic artificial intelligence: human knowledge is both (1) extremely sparse and (2) self-contradictory (e.g., Oaksford & Chater 1991). The sparseness of intuitive knowledge is exemplified in Rozenblit and Keil\u2019s (2002) discussion of the \u201cillusion of explanatory depth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 0
                            }
                        ],
                        "text": "Between the publication of Silver et al. (2016) and facing world champion Lee Sedol, AlphaGo was iteratively retrained several times in this way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2252,
                                "start": 185
                            }
                        ],
                        "text": "This agnosticism implicitly licenses a modeling approach in which detailed, domain-specific information can be imparted to an agent directly, an approach for which some of the authors\u2019 Bayesian Program Learning (BPL) work is emblematic. The two domains Lake and colleagues focus most upon \u2013 physics and theory of mind \u2013 are amenable to such an approach, in that these happen to be fields for which mature scientific disciplines exist. This provides unusually rich support for hand design of cognitive models. However, it is not clear that such hand design will be feasible in other more idiosyncratic domains where comparable scaffolding is unavailable. Lake et al. (2015a) were able to extend the approach to Omniglot characters by intuiting a suitable (stroke-based) model, but are we in a position to build comparably detailed domain models for such things as human dialogue and architecture? What about Japanese cuisine or ice skating? Even video-game play appears daunting, when one takes into account the vast amount of semantic knowledge that is plausibly relevant (knowledge about igloos, ice floes, cold water, polar bears, video-game levels, avatars, lives, points, and so forth). In short, it is not clear that detailed knowledge engineering will be realistically attainable in all areas we will want our agents to tackle. Given this observation, it would appear most promising to focus our efforts on developing learning systems that can be flexibly applied across a wide range of domains, without an unattainable overhead in terms of a priori knowledge. Encouraging this view, the recent machine learning literature offers many examples of learning systems conquering tasks that had long eluded more hand-crafted approaches, including object recognition, speech recognition, speech generation, language translation, and (significantly) game play (Silver et al. 2016). In many cases, such successes have depended on large amounts of training data, and have implemented an essentially model-free approach. However, a growing volume of work suggests that flexible, domain-general learning can also be successful on tasks where training data are scarcer and where model-based inference is important. For example, Rezende and colleagues (2016) reported a deep generative model that produces plausible novel instances of Omniglot characters after one presentation of a model character, going a significant distance toward answering Lake\u2019s \u201cCharacter Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Hofstadter (1985) argued that the problem of recognizing characters in all of the ways people do \u2013 both handwritten and printed \u2013 contains most, if not all, of the fundamental challenges of AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3106,
                                "start": 185
                            }
                        ],
                        "text": "This agnosticism implicitly licenses a modeling approach in which detailed, domain-specific information can be imparted to an agent directly, an approach for which some of the authors\u2019 Bayesian Program Learning (BPL) work is emblematic. The two domains Lake and colleagues focus most upon \u2013 physics and theory of mind \u2013 are amenable to such an approach, in that these happen to be fields for which mature scientific disciplines exist. This provides unusually rich support for hand design of cognitive models. However, it is not clear that such hand design will be feasible in other more idiosyncratic domains where comparable scaffolding is unavailable. Lake et al. (2015a) were able to extend the approach to Omniglot characters by intuiting a suitable (stroke-based) model, but are we in a position to build comparably detailed domain models for such things as human dialogue and architecture? What about Japanese cuisine or ice skating? Even video-game play appears daunting, when one takes into account the vast amount of semantic knowledge that is plausibly relevant (knowledge about igloos, ice floes, cold water, polar bears, video-game levels, avatars, lives, points, and so forth). In short, it is not clear that detailed knowledge engineering will be realistically attainable in all areas we will want our agents to tackle. Given this observation, it would appear most promising to focus our efforts on developing learning systems that can be flexibly applied across a wide range of domains, without an unattainable overhead in terms of a priori knowledge. Encouraging this view, the recent machine learning literature offers many examples of learning systems conquering tasks that had long eluded more hand-crafted approaches, including object recognition, speech recognition, speech generation, language translation, and (significantly) game play (Silver et al. 2016). In many cases, such successes have depended on large amounts of training data, and have implemented an essentially model-free approach. However, a growing volume of work suggests that flexible, domain-general learning can also be successful on tasks where training data are scarcer and where model-based inference is important. For example, Rezende and colleagues (2016) reported a deep generative model that produces plausible novel instances of Omniglot characters after one presentation of a model character, going a significant distance toward answering Lake\u2019s \u201cCharacter Challenge.\u201d Lake et al. call attention to this model\u2019s \u201cneed for extensive pre-training.\u201d However, it is not clear why their preinstalled model is to be preferred over knowledge acquired through pre-training. In weighing this point, it is important to note that the human modeler, to furnish the BPL architecture with its \u201cstart-up software,\u201d must draw on his or her own large volume of prior experience. In this sense, the resulting BPL model is dependent on the human designer\u2019s own \u201cpre-training.\u201d A more significant aspect of the Rezende model is that it can be applied without change to very different domains, as Rezende and colleagues (2016) demonstrate through experiments on human facial images."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45905019,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "694d016d2bc00e9511f26f7b0914b513b2b7bda2",
            "isKey": true,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Metamagical-themas:-Questing-for-the-essence-of-and-Mullin",
            "title": {
                "fragments": [],
                "text": "Metamagical themas: Questing for the essence of mind and patterns"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 166
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions (Baillargeon 2004; Baillargeon et al. 2009; Hespos & Baillargeon 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 7
                            }
                        ],
                        "text": "[aBML] Baillargeon, R., Li, J., Ng, W. & Yuan, S. (2009) An account of infants physical"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 2
                            }
                        ],
                        "text": "& Baillargeon, R. (2008) Young infants\u2019 actions reveal their developing knowledge of support variables: Converging evidence for violation-of-expecta-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5067,
                                "start": 167
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions (Baillargeon 2004; Baillargeon et al. 2009; Hespos & Baillargeon 2008). There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al. 2009), to cues, to lists of rules (Siegler & Chen 1998). A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine, the kind of simulators that power modern-day animations and games (Bates et al. 2015; Battaglia et al. 2013; Gerstenberg et al. 2015; Sanborn et al. 2013). According to this hypothesis, people reconstruct a perceptual scene using internal representations of the objects and their physically relevant properties (such as mass, elasticity, and surface friction) and forces acting on objects (such as gravity, friction, or collision impulses). Relative to physical ground truth, the intuitive physical state representation is approximate and probabilistic, and oversimplified and incomplete in many ways. Still, it is rich enough to support mental simulations that can predict how objects will move in the immediate future, either on their own or in responses to forces we might apply. This \u201cintuitive physics engine\u201d approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues. For example, (Fig. 4), a physics-engine reconstruction of a tower of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al. 2013), as well as simpler kinds of physical predictions that have been studied in infants (T\u00e9gl\u00e1s et al. 2011). Simulation-based models can also capture how people make hypothetical or counterfactual predictions: What would happen if certain blocks were taken away, more blocks were added, or the table supporting the tower was jostled? What if certain blocks were glued together, or attached to the table surface? What if the blocks were made of different materials (Styrofoam, lead, ice)? What if the blocks of one color were much heavier than those of other colors? Each of these physical judgments may require new features or new training for a pattern recognition account to work at the same level as the model-based simulator. What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems? Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion (Buckingham & Shultz 2000). However, these networks do not attempt to work with complex scenes as input, or a wide range of scenarios and judgments as in Figure 4. A recent paper from Facebook AI researchers (Lerer et al. 2016) represents an exciting step in this direction. Lerer et al. (2016) trained a deep convolutional network-based system (PhysNet) to predict the stability of block towers from simulated images similar to those in Figure 4A, but with much simpler configurations of two, three, or four cubical blocks stacked vertically. Impressively, PhysNet generalized to simple real images of block towers, matching human performance on these images, meanwhile exceeding human performance on synthetic images. Human and PhysNet confidence were also correlated across towers, although not as strongly as for the approximate probabilistic simulation models and experiments of Battaglia et al. (2013). One limitation is that PhysNet currently requires extensive training \u2013 between 100,000 and 200,000 scenes \u2013 to learn judgments for just a single task (will the tower fall?) on a narrow range of scenes (towers with two to four cubes). It has been shown to generalize, but also only in limited ways (e.g., from towers of two and three cubes to towers of four cubes). In contrast, people require far less experience to perform any particular task, and can generalize to many novel judgments and complex scenes with no new training required (although they receive large amounts of physics experience through interacting with the world more generally). Could deep learning systems such as PhysNet capture this flexibility, without explicitly Figure 4. The intuitive physics-engine approach to scene understanding, illustrated through tower stability. (A) The engine takes in inputs through perception, language, memory, and other faculties. It then constructs a physical scene with objects, physical properties, and forces; simulates the scene\u2019s development over time; and hands the output to other reasoning systems. (B) Many possible \u201ctweaks\u201d to the input can result in very different scenes, requiring the potential discovery, training, and evaluation of new features for each tweak. Adapted from Battaglia et al. (2013). Lake et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3747,
                                "start": 167
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions (Baillargeon 2004; Baillargeon et al. 2009; Hespos & Baillargeon 2008). There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al. 2009), to cues, to lists of rules (Siegler & Chen 1998). A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine, the kind of simulators that power modern-day animations and games (Bates et al. 2015; Battaglia et al. 2013; Gerstenberg et al. 2015; Sanborn et al. 2013). According to this hypothesis, people reconstruct a perceptual scene using internal representations of the objects and their physically relevant properties (such as mass, elasticity, and surface friction) and forces acting on objects (such as gravity, friction, or collision impulses). Relative to physical ground truth, the intuitive physical state representation is approximate and probabilistic, and oversimplified and incomplete in many ways. Still, it is rich enough to support mental simulations that can predict how objects will move in the immediate future, either on their own or in responses to forces we might apply. This \u201cintuitive physics engine\u201d approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues. For example, (Fig. 4), a physics-engine reconstruction of a tower of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al. 2013), as well as simpler kinds of physical predictions that have been studied in infants (T\u00e9gl\u00e1s et al. 2011). Simulation-based models can also capture how people make hypothetical or counterfactual predictions: What would happen if certain blocks were taken away, more blocks were added, or the table supporting the tower was jostled? What if certain blocks were glued together, or attached to the table surface? What if the blocks were made of different materials (Styrofoam, lead, ice)? What if the blocks of one color were much heavier than those of other colors? Each of these physical judgments may require new features or new training for a pattern recognition account to work at the same level as the model-based simulator. What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems? Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion (Buckingham & Shultz 2000). However, these networks do not attempt to work with complex scenes as input, or a wide range of scenarios and judgments as in Figure 4. A recent paper from Facebook AI researchers (Lerer et al. 2016) represents an exciting step in this direction. Lerer et al. (2016) trained a deep convolutional network-based system (PhysNet) to predict the stability of block towers from simulated images similar to those in Figure 4A, but with much simpler configurations of two, three, or four cubical blocks stacked vertically. Impressively, PhysNet generalized to simple real images of block towers, matching human performance on these images, meanwhile exceeding human performance on synthetic images. Human and PhysNet confidence were also correlated across towers, although not as strongly as for the approximate probabilistic simulation models and experiments of Battaglia et al. (2013). One limitation is that PhysNet currently requires extensive training \u2013 between 100,000 and 200,000 scenes \u2013 to learn judgments for just a single task (will the tower fall?) on a narrow range of scenes (towers with two to four cubes)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 166
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment and collisions (Baillargeon, 2004; Baillargeon, Li, Ng, & Yuan, 2009; Hespos & Baillargeon, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3134,
                                "start": 167
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions (Baillargeon 2004; Baillargeon et al. 2009; Hespos & Baillargeon 2008). There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al. 2009), to cues, to lists of rules (Siegler & Chen 1998). A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine, the kind of simulators that power modern-day animations and games (Bates et al. 2015; Battaglia et al. 2013; Gerstenberg et al. 2015; Sanborn et al. 2013). According to this hypothesis, people reconstruct a perceptual scene using internal representations of the objects and their physically relevant properties (such as mass, elasticity, and surface friction) and forces acting on objects (such as gravity, friction, or collision impulses). Relative to physical ground truth, the intuitive physical state representation is approximate and probabilistic, and oversimplified and incomplete in many ways. Still, it is rich enough to support mental simulations that can predict how objects will move in the immediate future, either on their own or in responses to forces we might apply. This \u201cintuitive physics engine\u201d approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues. For example, (Fig. 4), a physics-engine reconstruction of a tower of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al. 2013), as well as simpler kinds of physical predictions that have been studied in infants (T\u00e9gl\u00e1s et al. 2011). Simulation-based models can also capture how people make hypothetical or counterfactual predictions: What would happen if certain blocks were taken away, more blocks were added, or the table supporting the tower was jostled? What if certain blocks were glued together, or attached to the table surface? What if the blocks were made of different materials (Styrofoam, lead, ice)? What if the blocks of one color were much heavier than those of other colors? Each of these physical judgments may require new features or new training for a pattern recognition account to work at the same level as the model-based simulator. What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems? Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion (Buckingham & Shultz 2000). However, these networks do not attempt to work with complex scenes as input, or a wide range of scenarios and judgments as in Figure 4. A recent paper from Facebook AI researchers (Lerer et al. 2016) represents an exciting step in this direction. Lerer et al. (2016) trained a deep convolutional network-based system (PhysNet) to predict the stability of block towers from simulated images similar to those in Figure 4A, but with much simpler configurations of two, three, or four cubical blocks stacked vertically."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Infants\u2019 physical world. Current Directions in Psychological Science 13:89\u201394"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 33
                            }
                        ],
                        "text": "Topics in Cognitive Science 8(2):492\u2013502. [P-YO] Palmer, S. (1999) Vision science: Photons to phenomenology."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2042,
                                "start": 32
                            }
                        ],
                        "text": "The network was trained to play 49 classic Atari games, proposed as a test domain for reinforcement learning (Bellemare et al. 2013), impressively achieving human-level performance or above on 29 of the games. It did, however, have particular trouble with Frostbite and other games that required temporally extended planning strategies. In Frostbite, players control an agent (Frostbite Bailey) tasked with constructing an igloo within a time limit. The igloo is built piece by piece as the agent jumps on ice floes in water (Fig. 2A\u2013C). The challenge is that the ice floes are in constant motion (moving either left or right), and ice floes only contribute to the construction of the igloo if they are visited in an active state (white, rather than blue). The agent may also earn extra points by gathering fish while avoiding a number of fatal hazards (falling in the water, snow geese, polar bears, etc.). Success in this game requires a temporally extended plan to ensure the agent can accomplish a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal. Ultimately, once all of the pieces of the igloo are in place, the agent must proceed to the igloo and complete the level before time expires (Fig. 2C). The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning [Watkins & Dayan 1992]). These components allow the network to map sensory inputs (frames of pixels) onto a policy over a small set of actions, and both the mapping and the policy are trained to optimize long-term cumulative reward (the game score). The network embodies the strongly empiricist approach characteristic of most connectionist models: very little is built into the network apart from the assumptions about image structure inherent in convolutional networks, so the network has to essentially learn a visual and conceptual system from scratch for each new game. In Mnih et al. (2015), the network architecture and hyper-parameters were fixed, but the network was trained anew for each game, meaning the visual system and the policy are highly specialized for the games it was trained on."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 6
                            }
                        ],
                        "text": "61(1):49\u201373. [P-YO] Baranes, A. F., Oudeyer, P. Y. & Gottlieb, J. (2014) The effects of task difficulty,"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2840,
                                "start": 32
                            }
                        ],
                        "text": "The network was trained to play 49 classic Atari games, proposed as a test domain for reinforcement learning (Bellemare et al. 2013), impressively achieving human-level performance or above on 29 of the games. It did, however, have particular trouble with Frostbite and other games that required temporally extended planning strategies. In Frostbite, players control an agent (Frostbite Bailey) tasked with constructing an igloo within a time limit. The igloo is built piece by piece as the agent jumps on ice floes in water (Fig. 2A\u2013C). The challenge is that the ice floes are in constant motion (moving either left or right), and ice floes only contribute to the construction of the igloo if they are visited in an active state (white, rather than blue). The agent may also earn extra points by gathering fish while avoiding a number of fatal hazards (falling in the water, snow geese, polar bears, etc.). Success in this game requires a temporally extended plan to ensure the agent can accomplish a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal. Ultimately, once all of the pieces of the igloo are in place, the agent must proceed to the igloo and complete the level before time expires (Fig. 2C). The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning [Watkins & Dayan 1992]). These components allow the network to map sensory inputs (frames of pixels) onto a policy over a small set of actions, and both the mapping and the policy are trained to optimize long-term cumulative reward (the game score). The network embodies the strongly empiricist approach characteristic of most connectionist models: very little is built into the network apart from the assumptions about image structure inherent in convolutional networks, so the network has to essentially learn a visual and conceptual system from scratch for each new game. In Mnih et al. (2015), the network architecture and hyper-parameters were fixed, but the network was trained anew for each game, meaning the visual system and the policy are highly specialized for the games it was trained on. More recent work has shown how these game-specific networks can share visual features (Rusu et al. 2016) or be used to train a multitask network (Parisotto et al. 2016), achieving modest benefits of transfer when learning to play new games. Although it is interesting that the DQN learns to play games at human-level performance while assuming very little prior knowledge, the DQN may be learning to play Frostbite and other games in a very different way than people do. One way to examine the differences is by considering the amount of experience required for learning. In Mnih et al. (2015), the DQN was compared with a professional gamer who received approximately 2 hours of practice on each of the 49 Atari games (although he or she likely had prior experience with some of the games)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical mechanics definition of insight"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "The standard benchmark is theMixedNational Institute of Standards and Technology (MNIST) data set for digit recognition, which involves classifying images of digits into the categories \u20180\u2019 to \u20189\u2019 (LeCun et al. 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 2
                            }
                        ],
                        "text": ", Lee, D. & Guo, J. (2015) Stan: A probabilistic programming language for Bayesian inference and optimization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 247
                            }
                        ],
                        "text": "With a large amount of training data available, many algorithms achieve respectable performance, including K-nearest neighbors (5% test error), support vector machines (about 1% test error), and convolutional neural networks (below 1% test error; LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "\u2026context of learning new handwritten characters or learning to play Frostbite, the MNIST benchmark includes 6000 examples of each handwritten digit (LeCun et al., 1998), and the DQN of V. Mnih et al. (2015) played each Atari video game for approximately 924 hours of unique training experience\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "In the context of learning new, handwritten characters or learning to play Frostbite, the MNIST benchmark includes 6,000 examples of each handwritten digit (LeCun et al. 1998), and the DQN of Mnih et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 245
                            }
                        ],
                        "text": "With a large amount of training data available, many algorithms achieve respectable performance, including Knearest neighbors (5% test error), support vector machines (about 1% test error), and convolutional neural networks (below 1% test error [LeCun et al. 1998])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Computation 1:541\u201351"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2847,
                                "start": 13
                            }
                        ],
                        "text": "For example, Fornito et al. (2016) proposed a mechanism of deflection routing (which is used to reroute signals around damaged or congested nodes), to explain changes in functional connectivity following focal lesions. Nevertheless, functional demands in human cognitive systems appear to require a dynamic mechanism that could resemble a packet-switched system (Schlegel et al. 2015). As Lake et al. note, the abilities of brains to (1) grow and develop over time and (2) flexibly, creatively, and quickly adapt to new events are essential to their function. Packet switching as a general strategy may be more compatible with these requirements than alternative architectures. In terms of growth, the number of Internet hosts \u2013 each of which can potentially communicate with any other within milliseconds \u2013 has increased without major disruption over a few decades, to surpass thenumberofneurons in the cortex ofmanyprimates including the macaque (Fasolo 2011). This growth has also been much faster than the growth of the message-switched U.S. Postal Service (Giambene 2005; U.S. Postal Service 2016). Cortical neurons, like Internet hosts, are separated by relatively short network distances, and have the potential for communication along many possible routes within milliseconds. Communication principles that allowed for the rapid rise and sustained development of the packet-switched Internet may provide insights relevant to understanding how evolution and development conspire to generate intelligent brains. In terms of adapting quickly to new situations, Lake et al. point out that a fully trained artificial neural network generally cannot take on new or different tasks without substantial retraining and reconfiguration. Perhaps this is not so much a problem of computation, but rather one of routing: in neural networks, one commonly employs a fixed routing system, all-to-all connectivity between layers, and feedback only between adjacent layers. These features may make such systems well suited to learning a particular input space, but ill suited to flexible processing and efficient handling of new circumstances. Although a packet-switched routing protocol would not necessarily improve current deep learning systems, it may be better suited to modeling approaches that more closely approximate cortical networks\u2019 structure and function. Unlike most deep learning networks, the brain appears to largely show dynamic routing, sparse connectivity, and feedback among many hierarchical levels. Including such features in computational models may better approximate and explain biological function, which could in turn spawn better AI. Progress in understanding routing in the brain is already being made through simulations of dynamic signal flow on brain-like networks and in studies of brains themselves. Mi\u0161ic \u0301 et al. (2014) have investigated howMarkovian queuing networks (a form of messageswitched architecture) with primate brain-like connectivity could take advantage of small-world and rich-club topologies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3299,
                                "start": 13
                            }
                        ],
                        "text": "For example, Fornito et al. (2016) proposed a mechanism of deflection routing (which is used to reroute signals around damaged or congested nodes), to explain changes in functional connectivity following focal lesions. Nevertheless, functional demands in human cognitive systems appear to require a dynamic mechanism that could resemble a packet-switched system (Schlegel et al. 2015). As Lake et al. note, the abilities of brains to (1) grow and develop over time and (2) flexibly, creatively, and quickly adapt to new events are essential to their function. Packet switching as a general strategy may be more compatible with these requirements than alternative architectures. In terms of growth, the number of Internet hosts \u2013 each of which can potentially communicate with any other within milliseconds \u2013 has increased without major disruption over a few decades, to surpass thenumberofneurons in the cortex ofmanyprimates including the macaque (Fasolo 2011). This growth has also been much faster than the growth of the message-switched U.S. Postal Service (Giambene 2005; U.S. Postal Service 2016). Cortical neurons, like Internet hosts, are separated by relatively short network distances, and have the potential for communication along many possible routes within milliseconds. Communication principles that allowed for the rapid rise and sustained development of the packet-switched Internet may provide insights relevant to understanding how evolution and development conspire to generate intelligent brains. In terms of adapting quickly to new situations, Lake et al. point out that a fully trained artificial neural network generally cannot take on new or different tasks without substantial retraining and reconfiguration. Perhaps this is not so much a problem of computation, but rather one of routing: in neural networks, one commonly employs a fixed routing system, all-to-all connectivity between layers, and feedback only between adjacent layers. These features may make such systems well suited to learning a particular input space, but ill suited to flexible processing and efficient handling of new circumstances. Although a packet-switched routing protocol would not necessarily improve current deep learning systems, it may be better suited to modeling approaches that more closely approximate cortical networks\u2019 structure and function. Unlike most deep learning networks, the brain appears to largely show dynamic routing, sparse connectivity, and feedback among many hierarchical levels. Including such features in computational models may better approximate and explain biological function, which could in turn spawn better AI. Progress in understanding routing in the brain is already being made through simulations of dynamic signal flow on brain-like networks and in studies of brains themselves. Mi\u0161ic \u0301 et al. (2014) have investigated howMarkovian queuing networks (a form of messageswitched architecture) with primate brain-like connectivity could take advantage of small-world and rich-club topologies. Complementing this work, Sizemore et al. (2016) have shown that the abundance of weakly interconnected brain regions suggests a prominent role for parallel processing, which would be well suited to dynamic routing. Using algebraic topology, Sizemore et al. (2016) provide evidence that human brains show loops of converging or diverging signal flow (see also Granger 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 13
                            }
                        ],
                        "text": "For example, Fornito et al. (2016) proposed a mechanism of deflection routing (which is used to reroute signals around damaged or congested nodes), to explain changes in functional connectivity following focal lesions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3462,
                                "start": 13
                            }
                        ],
                        "text": "For example, Fornito et al. (2016) proposed a mechanism of deflection routing (which is used to reroute signals around damaged or congested nodes), to explain changes in functional connectivity following focal lesions. Nevertheless, functional demands in human cognitive systems appear to require a dynamic mechanism that could resemble a packet-switched system (Schlegel et al. 2015). As Lake et al. note, the abilities of brains to (1) grow and develop over time and (2) flexibly, creatively, and quickly adapt to new events are essential to their function. Packet switching as a general strategy may be more compatible with these requirements than alternative architectures. In terms of growth, the number of Internet hosts \u2013 each of which can potentially communicate with any other within milliseconds \u2013 has increased without major disruption over a few decades, to surpass thenumberofneurons in the cortex ofmanyprimates including the macaque (Fasolo 2011). This growth has also been much faster than the growth of the message-switched U.S. Postal Service (Giambene 2005; U.S. Postal Service 2016). Cortical neurons, like Internet hosts, are separated by relatively short network distances, and have the potential for communication along many possible routes within milliseconds. Communication principles that allowed for the rapid rise and sustained development of the packet-switched Internet may provide insights relevant to understanding how evolution and development conspire to generate intelligent brains. In terms of adapting quickly to new situations, Lake et al. point out that a fully trained artificial neural network generally cannot take on new or different tasks without substantial retraining and reconfiguration. Perhaps this is not so much a problem of computation, but rather one of routing: in neural networks, one commonly employs a fixed routing system, all-to-all connectivity between layers, and feedback only between adjacent layers. These features may make such systems well suited to learning a particular input space, but ill suited to flexible processing and efficient handling of new circumstances. Although a packet-switched routing protocol would not necessarily improve current deep learning systems, it may be better suited to modeling approaches that more closely approximate cortical networks\u2019 structure and function. Unlike most deep learning networks, the brain appears to largely show dynamic routing, sparse connectivity, and feedback among many hierarchical levels. Including such features in computational models may better approximate and explain biological function, which could in turn spawn better AI. Progress in understanding routing in the brain is already being made through simulations of dynamic signal flow on brain-like networks and in studies of brains themselves. Mi\u0161ic \u0301 et al. (2014) have investigated howMarkovian queuing networks (a form of messageswitched architecture) with primate brain-like connectivity could take advantage of small-world and rich-club topologies. Complementing this work, Sizemore et al. (2016) have shown that the abundance of weakly interconnected brain regions suggests a prominent role for parallel processing, which would be well suited to dynamic routing. Using algebraic topology, Sizemore et al. (2016) provide evidence that human brains show loops of converging or diverging signal flow (see also Granger 2006). In terms of neurophysiology, Briggs and Usrey (2007) have shown that corticothalamic networks can pass signals in a loop in just 37 milliseconds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3083,
                                "start": 13
                            }
                        ],
                        "text": "For example, Fornito et al. (2016) proposed a mechanism of deflection routing (which is used to reroute signals around damaged or congested nodes), to explain changes in functional connectivity following focal lesions. Nevertheless, functional demands in human cognitive systems appear to require a dynamic mechanism that could resemble a packet-switched system (Schlegel et al. 2015). As Lake et al. note, the abilities of brains to (1) grow and develop over time and (2) flexibly, creatively, and quickly adapt to new events are essential to their function. Packet switching as a general strategy may be more compatible with these requirements than alternative architectures. In terms of growth, the number of Internet hosts \u2013 each of which can potentially communicate with any other within milliseconds \u2013 has increased without major disruption over a few decades, to surpass thenumberofneurons in the cortex ofmanyprimates including the macaque (Fasolo 2011). This growth has also been much faster than the growth of the message-switched U.S. Postal Service (Giambene 2005; U.S. Postal Service 2016). Cortical neurons, like Internet hosts, are separated by relatively short network distances, and have the potential for communication along many possible routes within milliseconds. Communication principles that allowed for the rapid rise and sustained development of the packet-switched Internet may provide insights relevant to understanding how evolution and development conspire to generate intelligent brains. In terms of adapting quickly to new situations, Lake et al. point out that a fully trained artificial neural network generally cannot take on new or different tasks without substantial retraining and reconfiguration. Perhaps this is not so much a problem of computation, but rather one of routing: in neural networks, one commonly employs a fixed routing system, all-to-all connectivity between layers, and feedback only between adjacent layers. These features may make such systems well suited to learning a particular input space, but ill suited to flexible processing and efficient handling of new circumstances. Although a packet-switched routing protocol would not necessarily improve current deep learning systems, it may be better suited to modeling approaches that more closely approximate cortical networks\u2019 structure and function. Unlike most deep learning networks, the brain appears to largely show dynamic routing, sparse connectivity, and feedback among many hierarchical levels. Including such features in computational models may better approximate and explain biological function, which could in turn spawn better AI. Progress in understanding routing in the brain is already being made through simulations of dynamic signal flow on brain-like networks and in studies of brains themselves. Mi\u0161ic \u0301 et al. (2014) have investigated howMarkovian queuing networks (a form of messageswitched architecture) with primate brain-like connectivity could take advantage of small-world and rich-club topologies. Complementing this work, Sizemore et al. (2016) have shown that the abundance of weakly interconnected brain regions suggests a prominent role for parallel processing, which would be well suited to dynamic routing."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fundamentals of brain network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 213
                            }
                        ],
                        "text": "These models have been used to explain the dynamics of human learning-to-learn in many areas of cognition,\nincluding word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 130
                            }
                        ],
                        "text": "Hierarchical Bayesian models operating over probabilistic programs (Goodman et al., 2008; Lake, Salakhutdinov, & Tenenbaum, 2015; Tenenbaum et al., 2011) are equipped to deal with theorylike structures and rich causal representations of the world, yet there are formidable algorithmic challenges for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 212
                            }
                        ],
                        "text": "These models have been used to explain the dynamics of human learning-to-learn in many areas of cognition, including word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 162
                            }
                        ],
                        "text": "When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference (Geman et al. 1992; Griffiths et al. 2010; Tenenbaum et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 66
                            }
                        ],
                        "text": "Hierarhical Bayesian models operating over probabilistic programs (Goodman et al. 2008; Lake et al. 2015a; Tenenbaum et al. 2011) are equipped to deal with theory-like structures and rich causal representations of the world, yet there are formidable algorithmic challenges for efficient inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to grow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3943,
                                "start": 0
                            }
                        ],
                        "text": "But a broader view of what a \u201cmodel\u201d is, is needed. In most of the examples discussed in the target article, a \u201cmodel\u201d is a generative system that synthesizes a specified output. For example, the target article discusses a system built by Lake et al. (2015a) that learns to recognize handwritten characters from one or two examples, by modeling the sequence of strokes that produced them. The result is impressive, but the approach \u2013 identifying elements from a small class of items based on a reconstruction of how something might be generated \u2013 does not readily generalize in many other situations. Consider, for example, how one might recognize a cat, a cartoon of a cat, a painting of a cat, a marble sculpture of a cat, or a cloud that happens to look like a cat. The causal processes that generated each of these are very different; and yet a person familiar with cats will recognize any of these depictions, even if they know little of the causal processes underlying sculpture or the formation of clouds. Conversely, the differences between the causal processes that generate a cat and those that generate a dog are understood imperfectly, even by experts in developmental biology, and hardly at all by laypeople. Yet even children can readily distinguish dogs from cats. Likewise, where children learn to recognize letters significantly before they can write them at all well,(1) it seems doubtful that models of how an image is synthesized, play any necessary role in visual recognition even of letters, let alone of more complex entities. Lake et al.\u2019s results are technically impressive, but may tell us little about object recognition in general. The discussion of physical reasoning here, which draws on studies such as Battaglia et al. (2013), Gerstenberg et al. (2015), and Sanborn et al. (2013), may be similarly misleading. The target article argues that the cognitive processes used for human physical reasoning are \u201cintuitive physics engines,\u201d similar to the simulators used in scientific computation and computer games. But, as we have argued elsewhere (Davis & Marcus 2014; 2016), this model of physical reasoning is much too narrow, both for AI and for cognitive modeling. First, simulation engines require both a precise predictive theory of the domain and a geometrically and physically precise description of the situation. Human reasoners, by contrast, can deal with information that is radically incomplete. For example, if you are carrying a number of small creatures in a closed steel box, you can predict that as long as the box remains completely closed, the creatures will remain inside. This prediction can be made without knowing anything about the creatures and the way they move, without knowing the initial positions or shapes of the box or the creatures, and without knowing the trajectory of the box. Second, simulation engines predict how a system will develop by tracing its state in detail over a sequence of closely spaced instances. For example, Battaglia et al. (2013) use an existing physics engine to model how humans reason about an unstable tower of blocks collapsing to the floor. The physics engine generates a trace of the exact positions, velocities of every block, and the forces between them, at a sequence of instants a fraction of a second apart. There is no evidence that humans routinely generate comparably detailed traces or even that they are capable of doing so. Conversely, people are capable of predicting characteristics of an end state for problems where it is impossible to predict the intermediate states in detail, as the example of the creatures in the box illustrates. Third, there is extensive evidence that in many cases where the actual physics is simple, humans make large, systematic errors. For example, a gyroscope or a balance beam constructed of solid parts is governed by the identical physics as the falling tower of blocks studied in Battaglia et al. (2013); the physical interactions and their analysis are much simpler for these than for the tower of blocks, and the physics engine that Battaglia et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3338,
                                "start": 156
                            }
                        ],
                        "text": "This function hierarchy provides an efficient description of higher-level functions, such as a hierarchy of parts for describing complex objects or scenes (Bienenstock et al. 1997). Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives, just as the mind can think an infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor 1975; Fodor & Pylyshyn 1988; Marcus 2001; Piantadosi 2011). Compositionality has been broadly influential in both AI and cognitive science, especially as it pertains to theories of object recognition, conceptual representation, and language. Here, we focus on compositional representations of object concepts for illustration. Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman 1987; Hummel & Biederman 1992; Marr & Nishihara 1978; van den Hengel et al. 2015; Winston 1975). For instance, the novel two-wheeled vehicle in Figure 1B might be represented as two wheels connected by a platform, which provides the base for a post, which holds the handlebars, and so on. Parts can themselves be composed of subparts, forming a \u201cpartonomy\u201d of part-whole relationships (Miller & Johnson-Laird 1976; Tversky & Hemenway 1984). In the novel vehicle example, the parts and relations can be shared and re-used from existing related concepts, such as cars, scooters, motorcycles, and unicycles. Because the parts and relations are themselves a product of previous learning, their facilitation of the construction of new models is also an example of learning-to-learn, another ingredient that is covered below. Although compositionality and learning-to-learn fit naturally together, there are also forms of compositionality that rely less on previous learning, such as the bottom-up, parts-based representation of Hoffman and Richards (1984). Learning models of novel handwritten characters can be operationalized in a similar way. Handwritten characters are inherently compositional, where the parts are pen strokes, and relations describe how these strokes connect to each other. Lake et al. (2015a) modeled these parts using an additional layer of compositionality, where parts are complex movements created from simpler sub-part movements. New characters can be constructed by Figure 5. A causal, compositional model of handwritten characters. (A) New types are generated compositionally by choosing primitive actions (color coded) from a library (i), combining these sub-parts (ii) to make parts (iii), and combining parts with relations to define simple programs (iv). These programs can create different tokens of a concept (v) that are rendered as binary images (vi). (B) Probabilistic inference allows the model to generate new examples from just one example of a new concept; shown here in a visual Turing test. An example image of a new concept is shown above each pair of grids. One grid was generated by nine people and the other is nine samples from the BPL model. Which grid in each pair (A or B) was generated by the machine? Answers by row: 1,2;1,1. Adapted from Lake et al. (2015a). Lake et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 0
                            }
                        ],
                        "text": "But a broader view of what a \u201cmodel\u201d is, is needed. In most of the examples discussed in the target article, a \u201cmodel\u201d is a generative system that synthesizes a specified output. For example, the target article discusses a system built by Lake et al. (2015a) that learns to recognize handwritten characters from one or two examples, by modeling the sequence of strokes that produced them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2080,
                                "start": 156
                            }
                        ],
                        "text": "This function hierarchy provides an efficient description of higher-level functions, such as a hierarchy of parts for describing complex objects or scenes (Bienenstock et al. 1997). Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives, just as the mind can think an infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor 1975; Fodor & Pylyshyn 1988; Marcus 2001; Piantadosi 2011). Compositionality has been broadly influential in both AI and cognitive science, especially as it pertains to theories of object recognition, conceptual representation, and language. Here, we focus on compositional representations of object concepts for illustration. Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman 1987; Hummel & Biederman 1992; Marr & Nishihara 1978; van den Hengel et al. 2015; Winston 1975). For instance, the novel two-wheeled vehicle in Figure 1B might be represented as two wheels connected by a platform, which provides the base for a post, which holds the handlebars, and so on. Parts can themselves be composed of subparts, forming a \u201cpartonomy\u201d of part-whole relationships (Miller & Johnson-Laird 1976; Tversky & Hemenway 1984). In the novel vehicle example, the parts and relations can be shared and re-used from existing related concepts, such as cars, scooters, motorcycles, and unicycles. Because the parts and relations are themselves a product of previous learning, their facilitation of the construction of new models is also an example of learning-to-learn, another ingredient that is covered below. Although compositionality and learning-to-learn fit naturally together, there are also forms of compositionality that rely less on previous learning, such as the bottom-up, parts-based representation of Hoffman and Richards (1984). Learning models of novel handwritten characters can be operationalized in a similar way."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2340,
                                "start": 156
                            }
                        ],
                        "text": "This function hierarchy provides an efficient description of higher-level functions, such as a hierarchy of parts for describing complex objects or scenes (Bienenstock et al. 1997). Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives, just as the mind can think an infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor 1975; Fodor & Pylyshyn 1988; Marcus 2001; Piantadosi 2011). Compositionality has been broadly influential in both AI and cognitive science, especially as it pertains to theories of object recognition, conceptual representation, and language. Here, we focus on compositional representations of object concepts for illustration. Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman 1987; Hummel & Biederman 1992; Marr & Nishihara 1978; van den Hengel et al. 2015; Winston 1975). For instance, the novel two-wheeled vehicle in Figure 1B might be represented as two wheels connected by a platform, which provides the base for a post, which holds the handlebars, and so on. Parts can themselves be composed of subparts, forming a \u201cpartonomy\u201d of part-whole relationships (Miller & Johnson-Laird 1976; Tversky & Hemenway 1984). In the novel vehicle example, the parts and relations can be shared and re-used from existing related concepts, such as cars, scooters, motorcycles, and unicycles. Because the parts and relations are themselves a product of previous learning, their facilitation of the construction of new models is also an example of learning-to-learn, another ingredient that is covered below. Although compositionality and learning-to-learn fit naturally together, there are also forms of compositionality that rely less on previous learning, such as the bottom-up, parts-based representation of Hoffman and Richards (1984). Learning models of novel handwritten characters can be operationalized in a similar way. Handwritten characters are inherently compositional, where the parts are pen strokes, and relations describe how these strokes connect to each other. Lake et al. (2015a) modeled these parts using an additional layer of compositionality, where parts are complex movements created from simpler sub-part movements."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1278,
                                "start": 0
                            }
                        ],
                        "text": "Beyond concept learning, people also understand scenes by building causal models. Human-level scene understanding involves composing a story that explains the perceptual observations, drawing upon and integrating the ingredients of intuitive physics, intuitive psychology, and compositionality. Perception without these ingredients, and absent the causal glue that binds them, can lead to revealing errors. Consider image captions generated by a deep neural network (Fig. 6) (Karpathy & Fei-Fei 2017). In many cases, the network gets the key objects in a scene correct, but fails to understand the physical forces at work, the mental states of the people, or the causal relationships between the objects. In other words, it does not build the right causal model of the data. There have been steps toward deep neural networks and related approaches that learn causal models. Lopez-Paz et al. (2015) introduced a discriminative, data-driven framework for distinguishing the direction of causality from examples. Although it outperforms existing methods on various causal prediction tasks, it is unclear how to apply the approach to inferring rich hierarchies of latent causal variables, as needed for the Frostbite Challenge and especially the Characters Challenge. Graves (2014) learned a generative model of cursive handwriting using a recurrent neural network trained on handwriting data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3015,
                                "start": 0
                            }
                        ],
                        "text": "But a broader view of what a \u201cmodel\u201d is, is needed. In most of the examples discussed in the target article, a \u201cmodel\u201d is a generative system that synthesizes a specified output. For example, the target article discusses a system built by Lake et al. (2015a) that learns to recognize handwritten characters from one or two examples, by modeling the sequence of strokes that produced them. The result is impressive, but the approach \u2013 identifying elements from a small class of items based on a reconstruction of how something might be generated \u2013 does not readily generalize in many other situations. Consider, for example, how one might recognize a cat, a cartoon of a cat, a painting of a cat, a marble sculpture of a cat, or a cloud that happens to look like a cat. The causal processes that generated each of these are very different; and yet a person familiar with cats will recognize any of these depictions, even if they know little of the causal processes underlying sculpture or the formation of clouds. Conversely, the differences between the causal processes that generate a cat and those that generate a dog are understood imperfectly, even by experts in developmental biology, and hardly at all by laypeople. Yet even children can readily distinguish dogs from cats. Likewise, where children learn to recognize letters significantly before they can write them at all well,(1) it seems doubtful that models of how an image is synthesized, play any necessary role in visual recognition even of letters, let alone of more complex entities. Lake et al.\u2019s results are technically impressive, but may tell us little about object recognition in general. The discussion of physical reasoning here, which draws on studies such as Battaglia et al. (2013), Gerstenberg et al. (2015), and Sanborn et al. (2013), may be similarly misleading. The target article argues that the cognitive processes used for human physical reasoning are \u201cintuitive physics engines,\u201d similar to the simulators used in scientific computation and computer games. But, as we have argued elsewhere (Davis & Marcus 2014; 2016), this model of physical reasoning is much too narrow, both for AI and for cognitive modeling. First, simulation engines require both a precise predictive theory of the domain and a geometrically and physically precise description of the situation. Human reasoners, by contrast, can deal with information that is radically incomplete. For example, if you are carrying a number of small creatures in a closed steel box, you can predict that as long as the box remains completely closed, the creatures will remain inside. This prediction can be made without knowing anything about the creatures and the way they move, without knowing the initial positions or shapes of the box or the creatures, and without knowing the trajectory of the box. Second, simulation engines predict how a system will develop by tracing its state in detail over a sequence of closely spaced instances. For example, Battaglia et al. (2013) use an existing physics engine to model how humans reason about an unstable tower of blocks collapsing to the floor."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 474,
                                "start": 90
                            }
                        ],
                        "text": "An autonomous agent needs good models, but it also needs to know how to make use of them (Botvinick & Cohen 2014), especially in settings where task goals may vary over time. This point also favors a learning and agent-based approach, because it allows control structures to co-evolve with internal models, maximizing their compatibility. Though efforts to capitalize on these advantages in practice are only in their infancy, recent work from Hamrick and colleagues (2017), which simultaneously trained an internal model and a corresponding set of control functions, provides a case study of how this might work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 898,
                                "start": 0
                            }
                        ],
                        "text": "Beyond concept learning, people also understand scenes by building causal models. Human-level scene understanding involves composing a story that explains the perceptual observations, drawing upon and integrating the ingredients of intuitive physics, intuitive psychology, and compositionality. Perception without these ingredients, and absent the causal glue that binds them, can lead to revealing errors. Consider image captions generated by a deep neural network (Fig. 6) (Karpathy & Fei-Fei 2017). In many cases, the network gets the key objects in a scene correct, but fails to understand the physical forces at work, the mental states of the people, or the causal relationships between the objects. In other words, it does not build the right causal model of the data. There have been steps toward deep neural networks and related approaches that learn causal models. Lopez-Paz et al. (2015) introduced a discriminative, data-driven framework for distinguishing the direction of causality from examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2705,
                                "start": 0
                            }
                        ],
                        "text": "Beyond concept learning, people also understand scenes by building causal models. Human-level scene understanding involves composing a story that explains the perceptual observations, drawing upon and integrating the ingredients of intuitive physics, intuitive psychology, and compositionality. Perception without these ingredients, and absent the causal glue that binds them, can lead to revealing errors. Consider image captions generated by a deep neural network (Fig. 6) (Karpathy & Fei-Fei 2017). In many cases, the network gets the key objects in a scene correct, but fails to understand the physical forces at work, the mental states of the people, or the causal relationships between the objects. In other words, it does not build the right causal model of the data. There have been steps toward deep neural networks and related approaches that learn causal models. Lopez-Paz et al. (2015) introduced a discriminative, data-driven framework for distinguishing the direction of causality from examples. Although it outperforms existing methods on various causal prediction tasks, it is unclear how to apply the approach to inferring rich hierarchies of latent causal variables, as needed for the Frostbite Challenge and especially the Characters Challenge. Graves (2014) learned a generative model of cursive handwriting using a recurrent neural network trained on handwriting data. Although it synthesizes impressive examples of handwriting in various styles, it requires a large training corpus and has not been applied to other tasks. The DRAW network performs both recognition and generation of handwritten digits using recurrent neural networks with a window of attention, producing a limited circular area of the image at each time step (Gregor et al. 2015). A more recent variant of DRAW was applied to generating examples of a novel character from just a single training example (Rezende et al. 2016). The model demonstrates an impressive ability to make plausible generalizations that go beyond the training examples, yet it generalizes too broadly in other cases, in ways that are not especially human-like. It is not clear that it could yet pass any of the \u201cvisual Turing tests\u201d in Lake et al. (2015a) (Fig. 5B), although we hope DRAW-style networks will continue to be extended and enriched, and could be made to pass these tests. Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process. An attentional window is only a crude approximation of the true causal process of drawing with a pen, and in Rezende et al. (2016) the attentional window is not pen-like at all, although a more accurate pen model could be incorporated."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kokinov, pp. 499\u2013538"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 37
                            }
                        ],
                        "text": ", 1995) and variational optimization (Gregor et al., 2015; A. Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density estimation (Kulkarni, Kohli, et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 159
                            }
                        ],
                        "text": "In generative neural network models, attention has been used to concentrate on generating particular regions of the image rather than the whole image at once (Gregor et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "\u2026in various ways, for example, using paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational optimization (Gregor et al., 2015; A. Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density estimation (Kulkarni, Kohli, et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 206
                            }
                        ],
                        "text": "The DRAW network performs both recognition and generation of handwritten digits using recurrent neural networks with an attentional window, producing a limited circular area of the image at each time step (Gregor et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "neural networks with an attentional window, producing a limited circular area of the image at each time step (Gregor et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DRAW: A Recurrent"
            },
            "venue": {
                "fragments": [],
                "text": "Unbounded Memory"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 573,
                                "start": 185
                            }
                        ],
                        "text": "In this line of thinking, computational models have been built and used to improve human robot interaction and communication, in particular through the notion of learning by imitation (Breazeal & Scassellati 2002; Lopes & Santos-Victor 2007). Furthermore, some studies embedded machines with computational models using an adequate action-perception loop and showed that some complex social competencies such as immediate imitation (present in early human development) could emerge through sensorimotor ambiguities as proposed in Gaussier et al. (1998), Nagai et al. (2011), and Braud et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 552,
                                "start": 185
                            }
                        ],
                        "text": "In this line of thinking, computational models have been built and used to improve human robot interaction and communication, in particular through the notion of learning by imitation (Breazeal & Scassellati 2002; Lopes & Santos-Victor 2007). Furthermore, some studies embedded machines with computational models using an adequate action-perception loop and showed that some complex social competencies such as immediate imitation (present in early human development) could emerge through sensorimotor ambiguities as proposed in Gaussier et al. (1998), Nagai et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 7
                            }
                        ],
                        "text": "& Kim, B. (2017) A roadmap for a rigorous science of interpretability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "B. (2010) Probabilistic models of cognition: Exploring representations and inductive"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 10
                            }
                        ],
                        "text": ", Monroe, B. M., Brownstein, A. L., Yang, Y., Chopra, G. & Miller, L. C. (2010) A neural network model of the structure and dynamics of human per-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 598,
                                "start": 185
                            }
                        ],
                        "text": "In this line of thinking, computational models have been built and used to improve human robot interaction and communication, in particular through the notion of learning by imitation (Breazeal & Scassellati 2002; Lopes & Santos-Victor 2007). Furthermore, some studies embedded machines with computational models using an adequate action-perception loop and showed that some complex social competencies such as immediate imitation (present in early human development) could emerge through sensorimotor ambiguities as proposed in Gaussier et al. (1998), Nagai et al. (2011), and Braud et al. (2014). This kind of model allows future machines to better generalize their learning and to acquire new social skills."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "B. & Fogel, L. J. (1995) Evolution and computational intelligence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 0
                            }
                        ],
                        "text": "B., Feldman, J. & Griffiths, T. L. (2008) A rational analysis of rule-based concept learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "B. (2007) Learning overhypotheses with hierarchical Bayesian models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 101
                            }
                        ],
                        "text": "Generalization from such experiences, to deal with new cases, can be extremely flexible and abstract (Hofstadter 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Epilogue: Analogy as the core of cognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7433481"
                        ],
                        "name": "Onur Teymur",
                        "slug": "Onur-Teymur",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Teymur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Onur Teymur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90421119"
                        ],
                        "name": "K. Zygalakis",
                        "slug": "K.-Zygalakis",
                        "structuredName": {
                            "firstName": "Kostas",
                            "lastName": "Zygalakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Zygalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476875"
                        ],
                        "name": "B. Calderhead",
                        "slug": "B.-Calderhead",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Calderhead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Calderhead"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 34
                            }
                        ],
                        "text": ", 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 245
                            }
                        ],
                        "text": "Much larger random-access memories can be implemented using Memory Networks which automatically embed and store each incoming piece of information in memory, especially useful for question answering tasks and other aspects of language modeling (Sukhbaatar et al., 2015; Weston et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 200
                            }
                        ],
                        "text": "plemented using Memory Networks which automatically embed and store each incoming piece of information in memory, especially useful for question answering tasks and other aspects of language modeling (Sukhbaatar et al., 2015; Weston et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 162
                            }
                        ],
                        "text": "\u2026Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland, McNaughton, & O\u2019Reilly, 1995; V. Mnih et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195992338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4fd9f07f40e8935139edfdac3b8cf64c7a4e351",
            "isKey": true,
            "numCitedBy": 109,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Advances-in-Neural-Information-Processing-Systems-Teymur-Zygalakis",
            "title": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 29"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4269482"
                        ],
                        "name": "H. Harlow",
                        "slug": "H.-Harlow",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Harlow",
                            "middleNames": [
                                "Frederick"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Harlow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 1
                            }
                        ],
                        "text": "06581. Available at: http://arxiv.org/abs/1511.06581. [aBML] Ward, T. B. (1994) Structured imagination: The role of category structure in"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 488,
                                "start": 237
                            }
                        ],
                        "text": "In automatic speech recognition, hidden Markov models (HMMs) have been the leading approach since the late 1980s (Juang & Rabiner 1990), yet this framework has been chipped away piece by piece and replaced with deep learning components (Hinton et al. 2012). Now, the leading approaches to speech recognition are fully neural network systems (Graves et al. 2013; Hannun et al. 2014). Ideas from deep learning have also been applied to learning complex control problems. Mnih et al. (2015) combined ideas from deep learning and reinforcement learning to make a \u201cdeep reinforcement learning\u201d algorithm that learns to play large classes of simple video games from just frames of pixels and the game BEHAVIORAL AND BRAIN SCIENCES (2017), Page 1 of 72 doi:10."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27210261,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3b93c41bd18ed90793f38fa34a2c7dbe48270e82",
            "isKey": true,
            "numCitedBy": 89,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-and-satiation-of-response-in-intrinsically-Harlow",
            "title": {
                "fragments": [],
                "text": "Learning and satiation of response in intrinsically motivated complex puzzle performance by monkeys."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of comparative and physiological psychology"
            },
            "year": 1950
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 213
                            }
                        ],
                        "text": "These models have been used to explain the dynamics of human learning-to-learn in many areas of cognition,\nincluding word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 212
                            }
                        ],
                        "text": "These models have been used to explain the dynamics of human learning-to-learn in many areas of cognition, including word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 130
                            }
                        ],
                        "text": "Hierarchical Bayesian models operating over probabilistic programs (Goodman et al., 2008; Lake, Salakhutdinov, & Tenenbaum, 2015; Tenenbaum et al., 2011) are equipped to deal with theorylike structures and rich causal representations of the world, yet there are formidable algorithmic challenges for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 67
                            }
                        ],
                        "text": "Hierarchical Bayesian models operating over probabilistic programs (Goodman et al., 2008; Lake, Salakhutdinov, & Tenenbaum, 2015; Tenenbaum et al., 2011) are equipped to deal with theory-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to Grow a Mind"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 173
                            }
                        ],
                        "text": "One important shortcoming of cognitive functionalism is its failure to acknowledge that the same behavior/function may be caused by different representations and mechanisms (Block 1978; Hanson 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 6
                            }
                        ],
                        "text": "[LRC] Hanson, S. J. & Burr, D. J., (1990) What connectionist models learn: Toward a theory"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 2
                            }
                        ],
                        "text": "& Hanson, S. J. (2016) Deep learning and attentional bias in human"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some comments and variations on back-propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 674,
                                "start": 185
                            }
                        ],
                        "text": "This agnosticism implicitly licenses a modeling approach in which detailed, domain-specific information can be imparted to an agent directly, an approach for which some of the authors\u2019 Bayesian Program Learning (BPL) work is emblematic. The two domains Lake and colleagues focus most upon \u2013 physics and theory of mind \u2013 are amenable to such an approach, in that these happen to be fields for which mature scientific disciplines exist. This provides unusually rich support for hand design of cognitive models. However, it is not clear that such hand design will be feasible in other more idiosyncratic domains where comparable scaffolding is unavailable. Lake et al. (2015a) were able to extend the approach to Omniglot characters by intuiting a suitable (stroke-based) model, but are we in a position to build comparably detailed domain models for such things as human dialogue and architecture? What about Japanese cuisine or ice skating? Even video-game play appears daunting, when one takes into account the vast amount of semantic knowledge that is plausibly relevant (knowledge about igloos, ice floes, cold water, polar bears, video-game levels, avatars, lives, points, and so forth)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2252,
                                "start": 185
                            }
                        ],
                        "text": "This agnosticism implicitly licenses a modeling approach in which detailed, domain-specific information can be imparted to an agent directly, an approach for which some of the authors\u2019 Bayesian Program Learning (BPL) work is emblematic. The two domains Lake and colleagues focus most upon \u2013 physics and theory of mind \u2013 are amenable to such an approach, in that these happen to be fields for which mature scientific disciplines exist. This provides unusually rich support for hand design of cognitive models. However, it is not clear that such hand design will be feasible in other more idiosyncratic domains where comparable scaffolding is unavailable. Lake et al. (2015a) were able to extend the approach to Omniglot characters by intuiting a suitable (stroke-based) model, but are we in a position to build comparably detailed domain models for such things as human dialogue and architecture? What about Japanese cuisine or ice skating? Even video-game play appears daunting, when one takes into account the vast amount of semantic knowledge that is plausibly relevant (knowledge about igloos, ice floes, cold water, polar bears, video-game levels, avatars, lives, points, and so forth). In short, it is not clear that detailed knowledge engineering will be realistically attainable in all areas we will want our agents to tackle. Given this observation, it would appear most promising to focus our efforts on developing learning systems that can be flexibly applied across a wide range of domains, without an unattainable overhead in terms of a priori knowledge. Encouraging this view, the recent machine learning literature offers many examples of learning systems conquering tasks that had long eluded more hand-crafted approaches, including object recognition, speech recognition, speech generation, language translation, and (significantly) game play (Silver et al. 2016). In many cases, such successes have depended on large amounts of training data, and have implemented an essentially model-free approach. However, a growing volume of work suggests that flexible, domain-general learning can also be successful on tasks where training data are scarcer and where model-based inference is important. For example, Rezende and colleagues (2016) reported a deep generative model that produces plausible novel instances of Omniglot characters after one presentation of a model character, going a significant distance toward answering Lake\u2019s \u201cCharacter Challenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 25
                            }
                        ],
                        "text": "As recent work has shown (Andrychowicz et al. 2016; Denil et al. 2016; Duan et al. 2016; Hochreiter et al. 2001; Santoro et al. 2016; Wang et al. 2017), this learning-to-learn mechanism can allow agents to adapt rapidly to new problems, providing a novel route to install prior knowledge through learning, rather than by hand."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3106,
                                "start": 185
                            }
                        ],
                        "text": "This agnosticism implicitly licenses a modeling approach in which detailed, domain-specific information can be imparted to an agent directly, an approach for which some of the authors\u2019 Bayesian Program Learning (BPL) work is emblematic. The two domains Lake and colleagues focus most upon \u2013 physics and theory of mind \u2013 are amenable to such an approach, in that these happen to be fields for which mature scientific disciplines exist. This provides unusually rich support for hand design of cognitive models. However, it is not clear that such hand design will be feasible in other more idiosyncratic domains where comparable scaffolding is unavailable. Lake et al. (2015a) were able to extend the approach to Omniglot characters by intuiting a suitable (stroke-based) model, but are we in a position to build comparably detailed domain models for such things as human dialogue and architecture? What about Japanese cuisine or ice skating? Even video-game play appears daunting, when one takes into account the vast amount of semantic knowledge that is plausibly relevant (knowledge about igloos, ice floes, cold water, polar bears, video-game levels, avatars, lives, points, and so forth). In short, it is not clear that detailed knowledge engineering will be realistically attainable in all areas we will want our agents to tackle. Given this observation, it would appear most promising to focus our efforts on developing learning systems that can be flexibly applied across a wide range of domains, without an unattainable overhead in terms of a priori knowledge. Encouraging this view, the recent machine learning literature offers many examples of learning systems conquering tasks that had long eluded more hand-crafted approaches, including object recognition, speech recognition, speech generation, language translation, and (significantly) game play (Silver et al. 2016). In many cases, such successes have depended on large amounts of training data, and have implemented an essentially model-free approach. However, a growing volume of work suggests that flexible, domain-general learning can also be successful on tasks where training data are scarcer and where model-based inference is important. For example, Rezende and colleagues (2016) reported a deep generative model that produces plausible novel instances of Omniglot characters after one presentation of a model character, going a significant distance toward answering Lake\u2019s \u201cCharacter Challenge.\u201d Lake et al. call attention to this model\u2019s \u201cneed for extensive pre-training.\u201d However, it is not clear why their preinstalled model is to be preferred over knowledge acquired through pre-training. In weighing this point, it is important to note that the human modeler, to furnish the BPL architecture with its \u201cstart-up software,\u201d must draw on his or her own large volume of prior experience. In this sense, the resulting BPL model is dependent on the human designer\u2019s own \u201cpre-training.\u201d A more significant aspect of the Rezende model is that it can be applied without change to very different domains, as Rezende and colleagues (2016) demonstrate through experiments on human facial images."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to learn using gradient"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 118
                            }
                        ],
                        "text": "These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational optimization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 139
                            }
                        ],
                        "text": "These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational optimization (Gregor et al., 2015; A. Mnih & Gregor, 2014; Rezende, Mohamed, & Wierstra, 2014) or nearest-neighbor density\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 201
                            }
                        ],
                        "text": "This section discusses possible paths towards resolving the conflict between fast inference and structured representations, including Helmholtz-machine-style approximate inference in generative models (Dayan, Hinton, Neal, & Zemel, 1995; Hinton et al., 1995) and cooperation between model-free and model-based reinforcement learning systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 193
                            }
                        ],
                        "text": "\u2026resolving the conflict between fast inference and structured representations, including Helmholtz-machine-style approximate inference in generative models (Dayan, Hinton, Neal, & Zemel, 1995; Hinton et al., 1995) and cooperation between model-free and model-based reinforcement learning systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The \u201cwake-sleep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 98
                            }
                        ],
                        "text": "Moreover, we are beginning to understand how such methods could be implemented in neural circuits (Buesing et al. 2011; Huang & Rao 2014; Pecevski et al. 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2195,
                                "start": 99
                            }
                        ],
                        "text": "Moreover, we are beginning to understand how such methods could be implemented in neural circuits (Buesing et al. 2011; Huang & Rao 2014; Pecevski et al. 2011).(9) Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning. When the hypothesis space is vast, and only a few hypotheses are consistent with the data, how can good models be discovered without exhaustive search? In at least some domains, people may not have an especially clever solution to this problem, instead grappling with the full combinatorial complexity of theory learning (Ullman et al. 2012b). Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problemsolving (Sternberg & Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987). Discovering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of \u201cAha!\u201d moments: He or she will learn that jumping on ice floes causes them to change color, that changing the color of ice floes causes an igloo to be constructed piece-by-piece, that birds make him or her lose points, that fish make him or her gain points, that he or she can change the direction of ice floes at the cost of one igloo piece, and so on. These little fragments of a \u201cFrostbite theory\u201d are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme. Similarly, as described in the Characters Challenge, people can quickly infer motor programs to draw a new character in a similarly guided processes. For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection. Schulz (2012b) has suggested that abstract structural properties of problems contain information about the abstract forms of their solutions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2669,
                                "start": 99
                            }
                        ],
                        "text": "Moreover, we are beginning to understand how such methods could be implemented in neural circuits (Buesing et al. 2011; Huang & Rao 2014; Pecevski et al. 2011).(9) Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning. When the hypothesis space is vast, and only a few hypotheses are consistent with the data, how can good models be discovered without exhaustive search? In at least some domains, people may not have an especially clever solution to this problem, instead grappling with the full combinatorial complexity of theory learning (Ullman et al. 2012b). Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problemsolving (Sternberg & Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987). Discovering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of \u201cAha!\u201d moments: He or she will learn that jumping on ice floes causes them to change color, that changing the color of ice floes causes an igloo to be constructed piece-by-piece, that birds make him or her lose points, that fish make him or her gain points, that he or she can change the direction of ice floes at the cost of one igloo piece, and so on. These little fragments of a \u201cFrostbite theory\u201d are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme. Similarly, as described in the Characters Challenge, people can quickly infer motor programs to draw a new character in a similarly guided processes. For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection. Schulz (2012b) has suggested that abstract structural properties of problems contain information about the abstract forms of their solutions. Even without knowing the answer to the question, \u201cWhere is the deepest point in the Pacific Ocean?\u201d one still knows that the answer must be a location on a map. The answer \u201c20 inches\u201d to the question, \u201cWhat year was Lincoln born?\u201d can be invalidated a priori, even without knowing the correct answer. In recent experiments, Tsividis et al. (2015) found that children can use high-level abstract features of a domain to guide hypothesis selection, by reasoning about distributional properties like the ratio of seeds to flowers, and dynamical properties like periodic or monotonic relationships between causes and effects (see also Magid et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural dynamics as sampling: A"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2438157"
                        ],
                        "name": "H. Wellman",
                        "slug": "H.-Wellman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Wellman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339997"
                        ],
                        "name": "S. Gelman",
                        "slug": "S.-Gelman",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Gelman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 243
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 197732848,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "a26aac4e6f7574ee542709ccae42301b76eedd46",
            "isKey": false,
            "numCitedBy": 493,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Knowledge-acquisition-in-foundational-domains.-Wellman-Gelman",
            "title": {
                "fragments": [],
                "text": "Knowledge acquisition in foundational domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68992212"
                        ],
                        "name": "\u4e2d\u57a3 \u6052\u592a\u90ce",
                        "slug": "\u4e2d\u57a3-\u6052\u592a\u90ce",
                        "structuredName": {
                            "firstName": "\u4e2d\u57a3",
                            "lastName": "\u6052\u592a\u90ce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u4e2d\u57a3 \u6052\u592a\u90ce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "[NK] Dick, P. K. (1968) Do androids dream of electric sheep?Del Ray-Ballantine."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 47
                            }
                        ],
                        "text": "Dick\u2019s story, Do Androids Dream Electric Sheep (Dick 1968), or better yet, watch Ridley Scott\u2019s film Blade Runner (Scott 2007) based on Dick\u2019s story."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 193258233,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "352aeb4f4e2893cbd6e86b45ecb04ba20d4fcd50",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u90ca\u5916\u306e\u672a\u6765\u50cf-:-Do-Androids-Dream-of-Electric-\u6052\u592a\u90ce",
            "title": {
                "fragments": [],
                "text": "\u90ca\u5916\u306e\u672a\u6765\u50cf : Do Androids Dream of Electric Sheep?\u306b\u304a\u3051\u308b\u6d88\u8cbb\u6587\u5316\u30fb\u74b0\u5883\u6b63\u7fa9"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82762395"
                        ],
                        "name": "P. Jucevi\u010dien\u0117",
                        "slug": "P.-Jucevi\u010dien\u0117",
                        "structuredName": {
                            "firstName": "Palmira",
                            "lastName": "Jucevi\u010dien\u0117",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jucevi\u010dien\u0117"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20598157"
                        ],
                        "name": "Robertas Jucevicius",
                        "slug": "Robertas-Jucevicius",
                        "structuredName": {
                            "firstName": "Robertas",
                            "lastName": "Jucevicius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robertas Jucevicius"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 155837612,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "4f2d56b54c542b9ab2c8dc062302104069554a3e",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-does-it-mean-to-be-smart-Jucevi\u010dien\u0117-Jucevicius",
            "title": {
                "fragments": [],
                "text": "What does it mean to be smart"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34240308"
                        ],
                        "name": "Mirko Farina",
                        "slug": "Mirko-Farina",
                        "structuredName": {
                            "firstName": "Mirko",
                            "lastName": "Farina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirko Farina"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 158
                            }
                        ],
                        "text": "Their unique forms of imitation and readiness to learn from teachers suggest means by which humans can accumulate and exploit an \u201cinformational commonwealth\u201d (Kiraly et al. 2013; Sterelny 2012; 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 151725552,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "01e0e141729c4dc3b174419d5b28e9613da74b3a",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Evolved-Apprentice-Farina",
            "title": {
                "fragments": [],
                "text": "The Evolved Apprentice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923920"
                        ],
                        "name": "B. Scholl",
                        "slug": "B.-Scholl",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Scholl",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Scholl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152163976"
                        ],
                        "name": "Tao Gao",
                        "slug": "Tao-Gao",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Gao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "One possibility is that intuitive psychology is simply cues \u201call the way down\u201d (Schlottmann, Cole, Watts, & White, 2013; Scholl & Gao, 2013), though this would require more and more cues as the scenarios become more complex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 151676045,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "255523d8ef1b2d2c5e22ec050e3640afdb91f385",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perceiving-Animacy-and-Intentionality-Scholl-Gao",
            "title": {
                "fragments": [],
                "text": "Perceiving Animacy and Intentionality"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6226925"
                        ],
                        "name": "R. Baillargeon",
                        "slug": "R.-Baillargeon",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Baillargeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baillargeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155870416"
                        ],
                        "name": "Jie Li",
                        "slug": "Jie-Li",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061155205"
                        ],
                        "name": "Weiting Ng",
                        "slug": "Weiting-Ng",
                        "structuredName": {
                            "firstName": "Weiting",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiting Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3385453"
                        ],
                        "name": "Sylvia Yuan",
                        "slug": "Sylvia-Yuan",
                        "structuredName": {
                            "firstName": "Sylvia",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sylvia Yuan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 166
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions (Baillargeon 2004; Baillargeon et al. 2009; Hespos & Baillargeon 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 160
                            }
                        ],
                        "text": "There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al., 2009), to cues, to lists of rules (Siegler & Chen, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 159
                            }
                        ],
                        "text": "There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al. 2009), to cues, to lists of rules (Siegler & Chen 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 148428931,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "830997fdc451b3f0fb5b1348335d0822b12535c5",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Account-of-Infants'-Physical-Reasoning-Baillargeon-Li",
            "title": {
                "fragments": [],
                "text": "An Account of Infants' Physical Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "105062139"
                        ],
                        "name": "D. Swanson",
                        "slug": "D.-Swanson",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Swanson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Swanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061632675"
                        ],
                        "name": "L. Gray",
                        "slug": "L.-Gray",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Gray",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 147560887,
            "fieldsOfStudy": [
                "History",
                "Sociology"
            ],
            "id": "b0d04415aecbcee25748c58811e8b6a60d493382",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-Language-Harris-Swanson",
            "title": {
                "fragments": [],
                "text": "Foundations of Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1940
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403310"
                        ],
                        "name": "R. Schank",
                        "slug": "R.-Schank",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Schank",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "xplicitly referenced human cognition, and even published papers in cognitive psychology journals (e.g., 6 Bobrow &amp; Winograd, 1977; Hayes-Roth &amp; Hayes-Roth, 1979; Winograd, 1972). For example, Schank (1972), writing in the journal Cognitive Psychology, declared that We hope to be able to build a program that can learn, as a child does, how to do what we have described in this paper instead of being spoo"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "For example, Schank (1972), writing in the journal Cognitive Psychology, declared that\nWe hope to be able to build a program that can learn, as a child does, how to do what we have described in this paper instead of being spoon-fed the tremendous information necessary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145237497,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "2932a16f87dd9bad2cc59145a8263239c6a9cfcc",
            "isKey": false,
            "numCitedBy": 1031,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conceptual-dependency:-A-theory-of-natural-language-Schank",
            "title": {
                "fragments": [],
                "text": "Conceptual dependency: A theory of natural language understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "77045719"
                        ],
                        "name": "Harlow Hf",
                        "slug": "Harlow-Hf",
                        "structuredName": {
                            "firstName": "Harlow",
                            "lastName": "Hf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harlow Hf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "t be achievable if we use the human reinforcement learning systems as guidance. Intrinsic motivation also plays an important role in human learning and behavior (Berlyne, 1966; Deci &amp; Ryan, 1975; Harlow, 1950). While much of the previous discussion assumes the standard view of behavior as seeking to maximize reward and minimize punishment, all externally provided rewards are reinterpreted according to the "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 210219317,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "619ae16ee5fdf8a044b3ca211caec540bdb2a5c9",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-and-satiation-of-response-in-intrinsically-Hf",
            "title": {
                "fragments": [],
                "text": "Learning and satiation of response in intrinsically motivated complex puzzle performance by monkeys."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88215038"
                        ],
                        "name": "S. Brison",
                        "slug": "S.-Brison",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Brison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143165809,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "3e557968d8df23d52b1fe37c1f118c35b832f8a2",
            "isKey": false,
            "numCitedBy": 1934,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Intentional-Stance-Brison",
            "title": {
                "fragments": [],
                "text": "The Intentional Stance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749765"
                        ],
                        "name": "J. Rehling",
                        "slug": "J.-Rehling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Rehling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rehling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50222622"
                        ],
                        "name": "D. Hofstadter",
                        "slug": "D.-Hofstadter",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Hofstadter",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hofstadter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2379247"
                        ],
                        "name": "Robert L. Goldstone",
                        "slug": "Robert-L.-Goldstone",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Goldstone",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert L. Goldstone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 278
                            }
                        ],
                        "text": "As illustrated in Figure 1-iv, novel vehicles can be created as a combination of parts from existing vehicles, and similarly novel characters can be constructed from the parts of stylistically similar characters, or familiar characters can be re-conceptualized in novel styles (Rehling, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 240
                            }
                        ],
                        "text": "vehicles can be created as a combination of parts from existing vehicles, and similarly novel characters can be constructed from the parts of stylistically similar characters, or familiar characters can be re-conceptualized in novel styles (Rehling, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143159861,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "62af5f8ad29cae6803b31d06131ad88607033e17",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Letter-spirit-(part-two):-modeling-creativity-in-a-Rehling-Hofstadter",
            "title": {
                "fragments": [],
                "text": "Letter spirit (part two): modeling creativity in a visual domain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111068109"
                        ],
                        "name": "Jennie Hill",
                        "slug": "Jennie-Hill",
                        "structuredName": {
                            "firstName": "Jennie",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennie Hill"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 119
                            }
                        ],
                        "text": "But if the Netsilik Inuit people taught them to you, your chances of surviving a winter would be dramatically improved (Lambert 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 142656852,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "4a9f95b1a2a508eb6d87b785f9e31f3581651045",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Gates-of-Hell:-Sir-John-Franklin's-Tragic-Quest-Hill",
            "title": {
                "fragments": [],
                "text": "The Gates of Hell: Sir John Franklin's Tragic Quest for the North West Passage (review)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810353"
                        ],
                        "name": "E. Markman",
                        "slug": "E.-Markman",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Markman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Markman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 147
                            }
                        ],
                        "text": "\u2026especially in the context of learning the meanings of words in their native language (Carey & Bartlett, 1978; Landau, Smith, & Jones, 1988; E. M. Markman, 1989; Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002; F. Xu & Tenenbaum, 2007, although see Horst and Samuelson 2008 regarding\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "A similar sentiment was expressed by Minsky (1974): \u201cI draw no boundary between a theory of human thinking and a scheme for making an intelligent machine; no purpose would be served by separating these today since neither domain has theories good enough to explain\u2014or to produce\u2014enough mental capacity\u201d (p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 140272371,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "69cbc2632087f31bf9796ae445269ad74c0a3df7",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Categorization-and-naming-in-children-Markman",
            "title": {
                "fragments": [],
                "text": "Categorization and naming in children"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38807526"
                        ],
                        "name": "J. Means",
                        "slug": "J.-Means",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Means",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Means"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 128621242,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "15b8504195577f85a1f3776875128bc020722904",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Charts-of-the-Atmosphere-Means",
            "title": {
                "fragments": [],
                "text": "Charts of the Atmosphere"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1911
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46551443"
                        ],
                        "name": "C. Daly",
                        "slug": "C.-Daly",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Daly",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Daly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 217
                            }
                        ],
                        "text": "Many researchers have speculated about key features of human cognition that gives rise to language and other uniquely human modes of thought: Is it recursion, or some new kind of recursive structure building ability (Berwick & Chomsky, 2016; Hauser, Chomsky, & Fitch, 2002)?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125486395,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "d91f32cc375581a1209c6521f57059288d5ac29b",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Why-Only-Us-Language-and-Evolution-Daly",
            "title": {
                "fragments": [],
                "text": "Why Only Us? Language and Evolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925596"
                        ],
                        "name": "R. Lewin",
                        "slug": "R.-Lewin",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Lewin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lewin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031435"
                        ],
                        "name": "B. Regine",
                        "slug": "B.-Regine",
                        "structuredName": {
                            "firstName": "Birute",
                            "lastName": "Regine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Regine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124869527,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "008ea818e42e2853fd8cb99a48a3605df686e323",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MASTERING-THE-GAME-Lewin-Regine",
            "title": {
                "fragments": [],
                "text": "MASTERING THE GAME"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084582"
                        ],
                        "name": "G. Reeke",
                        "slug": "G.-Reeke",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Reeke",
                            "middleNames": [
                                "N."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Reeke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "The best way to capture these structural features is to imagine the brain as a society of agents (Minsky 1986), very heterogeneous and communicating through their common neural base by means of shared protocols, much like the Internet."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124268831,
            "fieldsOfStudy": [
                "Psychology",
                "Mathematics"
            ],
            "id": "5d68d1462966920e1c67702f47314e6403f7ab62",
            "isKey": false,
            "numCitedBy": 2058,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-society-of-mind-Reeke",
            "title": {
                "fragments": [],
                "text": "The society of mind"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6318597"
                        ],
                        "name": "M. Churchland",
                        "slug": "M.-Churchland",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Churchland",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Churchland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575774"
                        ],
                        "name": "J. Cunningham",
                        "slug": "J.-Cunningham",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cunningham",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cunningham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2852334"
                        ],
                        "name": "Matthew T. Kaufman",
                        "slug": "Matthew-T.-Kaufman",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kaufman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew T. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7312194"
                        ],
                        "name": "J. Foster",
                        "slug": "J.-Foster",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Foster",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961004"
                        ],
                        "name": "P. Nuyujukian",
                        "slug": "P.-Nuyujukian",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Nuyujukian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nuyujukian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 87726185,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6864a093cc10057d8bfcc87ccd8cfca67f1ee0fe",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-population-dynamics-during-Churchland-Cunningham",
            "title": {
                "fragments": [],
                "text": "Neural population dynamics during"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089978"
                        ],
                        "name": "D. Damen",
                        "slug": "D.-Damen",
                        "structuredName": {
                            "firstName": "Dima",
                            "lastName": "Damen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Damen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64711781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd116435b6f93e803e8db708ad4d0bce71499982",
            "isKey": false,
            "numCitedBy": 1554,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Vision-and-Pattern-Recognition-(CVPR)-Damen-Hogg",
            "title": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition (CVPR)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46421372"
                        ],
                        "name": "M. Abbas",
                        "slug": "M.-Abbas",
                        "structuredName": {
                            "firstName": "Mudassar",
                            "lastName": "Abbas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Abbas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6245351"
                        ],
                        "name": "J. Kivinen",
                        "slug": "J.-Kivinen",
                        "structuredName": {
                            "firstName": "Jyri",
                            "lastName": "Kivinen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2785022"
                        ],
                        "name": "T. Raiko",
                        "slug": "T.-Raiko",
                        "structuredName": {
                            "firstName": "Tapani",
                            "lastName": "Raiko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Raiko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63450092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1aeb751d9cefc9d784d8862562f5a3fe15821ae",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "International-Conference-on-Learning-(ICLR)-Abbas-Kivinen",
            "title": {
                "fragments": [],
                "text": "International Conference on Learning Representations (ICLR)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143915473"
                        ],
                        "name": "Francisco C\u00e2mara Pereira",
                        "slug": "Francisco-C\u00e2mara-Pereira",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C\u00e2mara"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francisco C\u00e2mara Pereira"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63333359,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "b8bf7eef968b1071026525b8256738758ec6085e",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Creativity-and-Artificial-Intelligence-Pereira",
            "title": {
                "fragments": [],
                "text": "Creativity and Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40481777"
                        ],
                        "name": "E. Hunt",
                        "slug": "E.-Hunt",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Hunt",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hunt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 107
                            }
                        ],
                        "text": "Consider for example a network that learns to predict the trajectories of several balls bouncing in a box (Kodratoff & Michalski, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64164169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fe9f988dae545d2b588e8bb8a4a25992d702ed",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-learning:-An-artificial-intelligence-(vol.-Hunt",
            "title": {
                "fragments": [],
                "text": "Machine learning: An artificial intelligence approach (vol. 2): R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.). Los Alton, CA: Morgan Kaufmann, 1986. Pp. x + 738. $39.95"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8255594"
                        ],
                        "name": "L. Thurstone",
                        "slug": "L.-Thurstone",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Thurstone",
                            "middleNames": [
                                "Leon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Thurstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62691684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b08ce3d11f6c1dbefa2f69a724b30200aaa87274",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-learning-curve-equation-Thurstone",
            "title": {
                "fragments": [],
                "text": "The learning curve equation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1919
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111936591"
                        ],
                        "name": "Stephen B. Johnson",
                        "slug": "Stephen-B.-Johnson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Johnson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen B. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59852125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d29e011b097309db36193b046a14d9373b2eda7",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Development-of-object-perception-Johnson",
            "title": {
                "fragments": [],
                "text": "Development of object perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2252285"
                        ],
                        "name": "E. Spelke",
                        "slug": "E.-Spelke",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Spelke",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Spelke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704065"
                        ],
                        "name": "D. Gentner",
                        "slug": "D.-Gentner",
                        "structuredName": {
                            "firstName": "Dedre",
                            "lastName": "Gentner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gentner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115377287"
                        ],
                        "name": "S. Goldin-Meadow",
                        "slug": "S.-Goldin-Meadow",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Goldin-Meadow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Goldin-Meadow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57886386,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "e6ebd66dfd6c5cd8cee0be6745ca2c4d7b3a47e0",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-makes-us-smart-Core-knowledge-and-natural-Spelke-Gentner",
            "title": {
                "fragments": [],
                "text": "What makes us smart? Core knowledge and natural language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6568267"
                        ],
                        "name": "D. Berlyne",
                        "slug": "D.-Berlyne",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Berlyne",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Berlyne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "arriage of exibility and eciency might be achievable if we use the human reinforcement learning systems as guidance. Intrinsic motivation also plays an important role in human learning and behavior (Berlyne, 1966; Deci &amp; Ryan, 1975; Harlow, 1950). While much of the previous discussion assumes the standard view of behavior as seeking to maximize reward and minimize punishment, all externally provided rewar"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45801379,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "5bfa93bf67b8371ca82bd9ba177b0bd5dc34fc43",
            "isKey": false,
            "numCitedBy": 931,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Curiosity-and-exploration.-Berlyne",
            "title": {
                "fragments": [],
                "text": "Curiosity and exploration."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31489430"
                        ],
                        "name": "A. Liberman",
                        "slug": "A.-Liberman",
                        "structuredName": {
                            "firstName": "Alvin",
                            "lastName": "Liberman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Liberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145549314"
                        ],
                        "name": "F. Cooper",
                        "slug": "F.-Cooper",
                        "structuredName": {
                            "firstName": "Franklin",
                            "lastName": "Cooper",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51072738"
                        ],
                        "name": "D. Shankweiler",
                        "slug": "D.-Shankweiler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Shankweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shankweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403941699"
                        ],
                        "name": "M. Studdert-Kennedy",
                        "slug": "M.-Studdert-Kennedy",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Studdert-Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Studdert-Kennedy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42296795,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "id": "e09c410be443fa9189bb448cf8150e984bed1b91",
            "isKey": false,
            "numCitedBy": 3393,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perception-of-the-speech-code.-Liberman-Cooper",
            "title": {
                "fragments": [],
                "text": "Perception of the speech code."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4269482"
                        ],
                        "name": "H. Harlow",
                        "slug": "H.-Harlow",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Harlow",
                            "middleNames": [
                                "Frederick"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Harlow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One way people acquire this prior knowledge is through \u201clearning-to-learn,\u201d a term introduced by Harlow (1949) and closely related to the machine learning notions of \u201ctransfer learning\u201d, \u201cmultitask learning\u201d or \u201crepresentation learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "One way people acquire this prior knowledge is through \u201clearning-to-learn,\u201d a term introduced by Harlow (1949) and closely related to the machine learning notions of \u201ctransfer learning\u201d, \u201cmultitask learning\u201d or \u201crepresentation learning.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22804426,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "d69a3ca39f86625cd6ac5050d7599033881e3be8",
            "isKey": false,
            "numCitedBy": 1722,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-formation-of-learning-sets.-Harlow",
            "title": {
                "fragments": [],
                "text": "The formation of learning sets."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3544623"
                        ],
                        "name": "P. Redgrave",
                        "slug": "P.-Redgrave",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Redgrave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Redgrave"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38756714"
                        ],
                        "name": "K. Gurney",
                        "slug": "K.-Gurney",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gurney",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gurney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144673330,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "3bf6af7c8b42a205e4c9efd247317f7c2105a7bc",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "An influential concept in contemporary computational neuroscience is the reward prediction error hypothesis of phasic dopaminergic function. It maintains that midbrain dopaminergic neurons signal the occurrence of unpredicted reward, which is used in appetitive learning to reinforce existing actions that most often lead to reward. However, the availability of limited afferent sensory processing and the precise timing of dopaminergic signals suggest that they might instead have a central role in identifying which aspects of context and behavioural output are crucial in causing unpredicted events."
            },
            "slug": "The-short-latency-dopamine-signal:-a-role-in-novel-Redgrave-Gurney",
            "title": {
                "fragments": [],
                "text": "The short-latency dopamine signal: a role in discovering novel actions?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work has suggested that the availability of limited afferent sensory processing and the precise timing of dopaminergic signals suggest that they might instead have a central role in identifying which aspects of context and behavioural output are crucial in causing unpredicted events."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36064516"
                        ],
                        "name": "J. Purdy",
                        "slug": "J.-Purdy",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Purdy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Purdy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 148
                            }
                        ],
                        "text": "These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey 2009; Jackendoff 2003; Kemp 2007; O\u2019Donnell 2015; Pinker 2007; Xu & Tenenbaum 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60842494,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "847c46b370d705090c99a52a6f4c0a3559aa72d0",
            "isKey": false,
            "numCitedBy": 605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Foundations-of-Language-Purdy",
            "title": {
                "fragments": [],
                "text": "Foundations of Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113979381"
                        ],
                        "name": "Xing Hao",
                        "slug": "Xing-Hao",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8273966"
                        ],
                        "name": "Guigang Zhang",
                        "slug": "Guigang-Zhang",
                        "structuredName": {
                            "firstName": "Guigang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guigang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118869556"
                        ],
                        "name": "Shang Ma",
                        "slug": "Shang-Ma",
                        "structuredName": {
                            "firstName": "Shang",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shang Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1779661,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "isKey": false,
            "numCitedBy": 30723,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-Learning-Hao-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Semantic Comput."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099770357"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 106
                            }
                        ],
                        "text": "2016), as well as models of causal reasoning and learning built on the theory of causal Bayesian networks (Goodman et al. 2011; Griffiths & Tenenbaum 2005, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1287578,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "fed164e6d8573ac28b35fb016c1e4865a491fd90",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Theory-Mahadevan",
            "title": {
                "fragments": [],
                "text": "Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384194899"
                        ],
                        "name": "P. Johnson-Laird",
                        "slug": "P.-Johnson-Laird",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Johnson-Laird",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Johnson-Laird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60904510,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "65fddecba2be00e3c31870a72585981760deeaa1",
            "isKey": false,
            "numCitedBy": 2079,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Language-and-Perception-Miller-Johnson-Laird",
            "title": {
                "fragments": [],
                "text": "Language and Perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 204
                            }
                        ],
                        "text": "\u201cAnalysis-by-synthesis\u201d theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel, 2010; Eden, 1962; Halle & Stevens, 1962; Neisser, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7532458,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c4649ab18d0d60911620f1ce0d4d15fd85f4aef9",
            "isKey": false,
            "numCitedBy": 953,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cognitive-Psychology-Koehn",
            "title": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of GIS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2683931"
                        ],
                        "name": "J. Freyd",
                        "slug": "J.-Freyd",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Freyd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Freyd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 226
                            }
                        ],
                        "text": "Similarly, as related to the Characters Challenge, the way people learn to write a novel handwritten character \u2013 in other words, the causal prescription for producing new examples \u2013 influences later perception and categorization (Freyd, 1983, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26599790,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "427256332d5234c9a43d71b04fbfe4533c97aeb6",
            "isKey": false,
            "numCitedBy": 510,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-mental-representations.-Freyd",
            "title": {
                "fragments": [],
                "text": "Dynamic mental representations."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Emotions: From brain to robot. Trends in Cognitive Science 8(12):554\u201361"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Point: Should childhood vaccination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 322
                            }
                        ],
                        "text": "The flexibility of the social inference machinery in humans turns small signals into weighty observations: Even for young children, ambiguous word-learning events become informative through social reasoning (Frank & Goodman 2014), nonobvious causal action sequences become \u201cthe way you do it\u201d when presented pedagogically (Buchsbaum et al. 2011), and complex machines can become single-function tools when a learner is taught just one function (Bonawitz et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Children\u2019s imitation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 233
                            }
                        ],
                        "text": "For example (Figure 4), a physics-engine reconstruction of a tower of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al., 2013) as well as simpler kinds of physical predictions that have been studied in infants (T\u00e9gl\u00e1s et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 168
                            }
                        ],
                        "text": "\u2026of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al., 2013) as well as simpler kinds of physical predictions that have been studied in infants (Te\u0301gla\u0301s et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 164
                            }
                        ],
                        "text": "Human and PhysNet confidence were also correlated across towers, although not as strongly as for the approximate probabilistic simulation models and experiments of Battaglia et al. (2013)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simulation as an engine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "A similar sentiment was expressed by Minsky (1974): \u201cI draw no boundary between a theory of human thinking and a scheme for making an intelligent machine; no purpose would be served by separating these today since neither domain has theories good enough to explain\u2014or to produce\u2014enough mental capacity\u201d (p."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Biological Sciences 200(1140):269\u201394"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume I"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume I"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson &\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 93
                            }
                        ],
                        "text": "Beyond classification, concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Humans predict liquid"
            },
            "venue": {
                "fragments": [],
                "text": "Ad hoc categories. Memory & Cognition,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026can make meaningful generalizations from very sparse data, especially in the context of learning the meanings of words in their native language (Carey & Bartlett, 1978; Landau, Smith, & Jones, 1988; E. M. Markman, 1989; Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002; F. Xu &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acquiring a single new word. Papers and Reports on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "One possibility is that intuitive psychology is simply cues \u201call the way down\u201d (Schlottmann, Cole, Watts, & White, 2013; Scholl & Gao, 2013), though this would require more and more cues as the scenarios become more complex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Perceiving Animacy and Intentionality: Visual Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Difficulty leading interpersonal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inexperienced newborn chicks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Turing Machines. Retrieved from http:// arxiv.org/abs"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Turing Machines. Retrieved from http:// arxiv.org/abs"
            },
            "year": 1410
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 81
                            }
                        ],
                        "text": "Intrinsic motivation also plays an important role in human learning and behavior (Berlyne, 1966; Deci & Ryan, 1975; Harlow, 1950)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A constructivist connectionist model"
            },
            "venue": {
                "fragments": [],
                "text": "Curiosity and exploration. Science,"
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 114
                            }
                        ],
                        "text": "In automatic speech recognition, Hidden Markov Models (HMMs) have been the leading approach since the late 1980s (Juang & Rabiner, 1990), yet this framework has been chipped away piece by piece and replaced with deep learning components (Hinton et al.,\nar X\niv :1\n60 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models for speech"
            },
            "venue": {
                "fragments": [],
                "text": "recognition. Technometric,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 23
                            }
                        ],
                        "text": "In recent experiments, Tsividis, Tenenbaum, and Schulz (2015) found that children can use high-level abstract features of a domain to guide hypothesis selection, by reasoning about distributional properties like the ratio of seeds to flowers, and dynamical properties like periodic or monotonic\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constraints on hypothesis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "One-year-old infants use https:/www.cambridge.org/core/terms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 166
                            }
                        ],
                        "text": "By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment and collisions (Baillargeon, 2004; Baillargeon, Li, Ng, & Yuan, 2009; Hespos & Baillargeon, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Infants' physical world doi: 10.1111/j.0963-7214 An account of infants physical reasoning. Learning and the infant mind"
            },
            "venue": {
                "fragments": [],
                "text": "Current Directions in Psychological Science"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 494,
                                "start": 301
                            }
                        ],
                        "text": "In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks (Anselmi et al. 2016; Baxter 2000; Bottou 2014; Lopez-Paz et al. 2016; Rusu et al. 2016; Salakhutdinov et al. 2011; Srivastava & Salakhutdinov, 2013; Torralba et al. 2007; Zeiler & Fergus 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sharing visual features"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Psychological Bulletin 141:786\u2013811"
            },
            "venue": {
                "fragments": [],
                "text": "Semantic cognition. MIT Press. [aBML] Rogoff,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 16
                            }
                        ],
                        "text": "(See Shallice & Cooper [2011] for an extended review of relevant evidence and Fox et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 16
                            }
                        ],
                        "text": "(See Shallice & Cooper [2011] for an extended review of relevant evidence and Fox et al. [2013] and Cooper [2016], for detailed suggestions for the potential organisation of higher-level modulatory systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Executive functions and the generation of \u201crandom"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "The underlying cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "In contrast, while representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bootstrapping and the origin of concepts"
            },
            "venue": {
                "fragments": [],
                "text": "Daedalus"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unifying distillation and privileged"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 45
                            }
                        ],
                        "text": "Turing pictured the child\u2019s mind as a notebook with \u201crather little mechanism and lots of blank sheets,\u201d and the mind of a child-machine as filling in the notebook by responding to rewards and punishments, similar to reinforcement learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "Alan Turing suspected that it is easier to build and educate a child-machine than try to fully capture adult human cognition (Turing, 1950)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computing Machine and Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "MIND, LIX"
            },
            "year": 1950
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 286
                            }
                        ],
                        "text": "\u2026seems to require that information be transmitted backwards along the axon, which does not fit with realistic models of neuronal function (although recent models circumvent this problem in various ways Liao, Leibo, & Poggio, 2015; Lillicrap, Cownden, Tweed, & Akerman, 2014; Scellier & Bengio, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 112
                            }
                        ],
                        "text": "This sort of richer feedback can easily be incorporated into neural networks, and doing so can enhance learning (G\u00fcl\u00e7ehre and Bengio 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledge matters: Importance of prior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2050,
                                "start": 211
                            }
                        ],
                        "text": "discuss these kinds of breakthroughs as a big step for artificial intelligence, but raise the question how we can build machines that learn like people? We can find an indication in a survey of mind perception (Gray et al. 2007), which is the \u201camount of mind\u201d people are willing to attribute to others. Participants judged machines to be high on agency but low on experience. We attribute this to the fact that computers are trained on individual tasks, often involving a single modality such as vision or speech, or a single context such as classifying traffic signs, as opposed to interpreting spoken and gestured utterances. In contrast, for people, the \u201cworld\u201d essentially appears as a multimodal stream of stimuli, which unfold over time. Therefore, we suggest that the next paradigm shift in intelligent machines will have to include processing the \u201cworld\u201d through lifelong and crossmodal learning. This is important because people develop problem-solving capabilities, including language processing, over their life span and via interaction with the environment and other people (Elman 1993, Christiansen and Chater 2016). In addition, the learning is embodied, as developing infants have a body-rational view of the world, but also seem to apply general problem-solving strategies to a wide range of quite different tasks (Cangelosi and Schlesinger 2015). Hence, we argue that the proposed principles or \u201cstart-up software\u201d are coupled tightly with general learning mechanisms in the brain. We argue that these conditions inherently enable the development of distributed representations of knowledge. For example, in our research, we found that architectural mechanisms, like different timings in the information processing in the cortex, foster compositionality that in turn enables both the development of more complex body actions and the development of language competence from primitives (Heinrich 2016). These kinds of distributed representations are coherent with the cognitive science on embodied cognition. Lakoff and Johnson (2003), for example, argue that people describe personal relationships in terms of the physical sensation of temperature."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hybrid computing using a"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 272
                            }
                        ],
                        "text": "Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman, 1987; Hummel & Biederman, 1992; Marr & Nishihara, 1978; van den Hengel et al., 2015; Winston, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 38
                            }
                        ],
                        "text": "Many seemingly well-accepted ideas regarding neural computation are in fact biologically dubious, or uncertain at best \u2013 and thus should not disqualify cognitive ingredients that pose challenges for implementation within that approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning structural descriptions from examples The psychology of computer vision"
            },
            "venue": {
                "fragments": [],
                "text": "Learning structural descriptions from examples The psychology of computer vision"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prioritized Experience Replay. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "venue": {
                "fragments": [],
                "text": "Prioritized Experience Replay. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "year": 1511
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 57
                            }
                        ],
                        "text": "imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 192
                            }
                        ],
                        "text": "\u2026concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explanation and categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Intelligence (Vol"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prioritized Experience Replay. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "venue": {
                "fragments": [],
                "text": "Prioritized Experience Replay. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "year": 1511
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 57
                            }
                        ],
                        "text": "imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 192
                            }
                        ],
                        "text": "\u2026concepts support prediction (Murphy & Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman & Makin, 1998), imagination (Jern & Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams & Lombrozo, 2010), and composition (Murphy, 1988; Osherson & Smith, 1981)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explanation and categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Intelligence (Vol"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scribner&E. Souberman, pp. 79\u201391"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A canonical theory of dynamic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 228
                            }
                        ],
                        "text": "\u2026master language, and they provide the building\n8Michael Jordan made this point forcefully in his 2015 speech accepting the Rumelhart Prize.\nblocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 148
                            }
                        ],
                        "text": "These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The acquisition of inductive constraints (Unpublished doctoral dissertation)"
            },
            "venue": {
                "fragments": [],
                "text": "MIT."
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How important is weight symmetry"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "Alan Turing suspected that it is easier to build and educate a child-machine than try to fully capture adult human cognition (Turing, 1950)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computing Machine and Intelligence. MIND, LIX"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Closures and cavities"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Q-learning. Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 154
                            }
                        ],
                        "text": "First, we argue that human-like machines need to decide and act in transparent ways, such that humans can readily understand how their decisions are made (see Arnold & Scheutz 2016; Indurkhya & Misztal-Radecka 2016; Mittelstadt et al. 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "generalization: A unifying view. Machine Learning 1:47\u201380"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Gallistel and Matzel (2013) have persuasively argued that the critical interstimulus interval for LTP is orders of magnitude smaller than the intervals that are behaviorally relevant in most forms of learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The neuroscience of learning: beyond"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 107
                            }
                        ],
                        "text": "1995), the cerebellum to implement fast time-scale computations possibly acquired with supervised learning (Kawato et al. 2011; Wolpert et al. 1998), and the limbic brain structures interfacing the brain to the body and generating motivations, emotions, and the value of things (Mirolli et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cerebellar supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 55
                            }
                        ],
                        "text": "Having underlying hybrid neural embodied architectures (Wermter et al. 2005) will support horizontal and vertical transfer and integration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards biomimetic neural"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intrinsic motivation. Wiley Online Library"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Directed Actions of Self-propelled Objects (Vol"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 34
                            }
                        ],
                        "text": ", 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Bahdanau, Cho, & Bengio, 2015; V. Mnih, Heess, Graves, & Kavukcuoglu, 2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland, McNaughton, & O\u2019Reilly, 1995; V. Mnih et\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, Grefenstette et al. (2015) showed how simple sequence-to-sequence prediction tasks can be solved using differentiable variants of traditional computing elements such as stacks and queues."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to Transduce"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep Reinforcement Learning with Double Qlearning"
            },
            "venue": {
                "fragments": [],
                "text": "Thirtieth Conference on Artificial Intelligence (AAAI)"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "The underlying cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linguistic theory and psychological reality (pp"
            },
            "venue": {
                "fragments": [],
                "text": "Papers and Reports on Child"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thinking, fast and slow. Macmillan"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 151
                            }
                        ],
                        "text": "For example, mechanical legs reproducing essential properties of human leg morphology generate human-like gaits on mild slopes without any computation (Collins et al. 2005), showing the guiding role of morphology in infant learning of locomotion (Oudeyer 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient bipedal robots"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 135
                            }
                        ],
                        "text": "1998), and the limbic brain structures interfacing the brain to the body and generating motivations, emotions, and the value of things (Mirolli et al. 2010; Mogenson et al. 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The roles of the amygdala"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 148
                            }
                        ],
                        "text": "\u2026techniques for model building and selection (Grosse, Salakhutdinov, Freeman, & Tenenbaum, 2012), and probabilistic\n1In their influential textbook, Russell and Norvig (2003) state that \u201cThe quest for \u2018artificial flight\u2019 succeeded when the Wright brothers and others stopped imitating birds and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ImageNet large scale visual recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Upper Saddle River,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Similarity and numerical equivalence: Appearances"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The arcade learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems. [MB,"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "\u2026generic networks is also the best current approach for object recognition (He et al., 2015; Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2014), where the high-level feature representations of these convolutional nets have also been used to predict patterns of neural\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 190
                            }
                        ],
                        "text": "In the years since, convnets continue to dominate, recently approaching human-level performance on some object recognition benchmarks (He, Zhang, Ren, & Sun, 2015; Russakovsky et al., 2015; Szegedy et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Going Deeper with Convolutions. arXiv preprint"
            },
            "venue": {
                "fragments": [],
                "text": "Going Deeper with Convolutions. arXiv preprint"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 99
                            }
                        ],
                        "text": "There has been recent work on other types of tasks, including learning generative models of images (Denton et al. 2015; Gregor et al. 2015), caption generation (Karpathy & Fei-Fei 2017; Vinyals et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep generative image"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 505,
                                "start": 343
                            }
                        ],
                        "text": "The \u2018child as scientist\u2019 proposal further views the process of learning itself as also scientist-like, with recent experiments showing that children seek out new data to distinguish between hypotheses, isolate variables, test causal hypotheses, make use of the data-generating process in drawing conclusions, and learn selectively from others (Cook, Goodman, & Schulz, 2011; Gweon et al., 2010; L. E. Schulz, Gopnik, & Glymour, 2007; Stahl & Feigenson, 2015; Tsividis, Gershman, Tenenbaum, & Schulz, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 164
                            }
                        ],
                        "text": "\u2026variables, test causal hypotheses, make use of the data-generating process in drawing conclusions, and learn selectively from others (Cook, Goodman, & Schulz, 2011; Gweon et al., 2010; L. E. Schulz, Gopnik, & Glymour, 2007; Stahl & Feigenson, 2015; Tsividis, Gershman, Tenenbaum, & Schulz, 2013)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Infants consider both the sample"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 152
                            }
                        ],
                        "text": "For example, mechanical legs reproducing essential properties of human leg morphology generate human-like gaits on mild slopes without any computation (Collins et al. 2005), showing the guiding role of morphology in infant learning of locomotion (Oudeyer 2016). Yamada et al. (2010) developed a series of models showing that hand-face touch behaviours in the foetus and hand looking in the infant self-organize through interaction of a non-uniform physical distribution of proprioceptive sensors across the body with basic neural plasticity loops."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Undecidability and opacity of metacognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 494,
                                "start": 301
                            }
                        ],
                        "text": "In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks (Anselmi et al. 2016; Baxter 2000; Bottou 2014; Lopez-Paz et al. 2016; Rusu et al. 2016; Salakhutdinov et al. 2011; Srivastava & Salakhutdinov, 2013; Torralba et al. 2007; Zeiler & Fergus 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unifying distillation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cognitive Psychology 27:1\u201340"
            },
            "venue": {
                "fragments": [],
                "text": "[KBC] Weizenbaum, J"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 206
                            }
                        ],
                        "text": "The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cognitive Psychology 27:1\u201340"
            },
            "venue": {
                "fragments": [],
                "text": "[KBC] Weizenbaum, J"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision network account of reasoning about other peoples"
            },
            "venue": {
                "fragments": [],
                "text": "choices. Cognition,"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2016) How language programs the mind"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech recognition with deep recurrent neu45"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural dynamics as sampling: a model 68"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "One way people acquire this prior knowledge is through \u201clearning-to-learn,\u201d a term introduced by Harlow (1949) and closely related to the machine learning notions of \u201ctransfer learning\u201d, \u201cmultitask learning\u201d or \u201crepresentation learning.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The formation of learning sets. Psychological Review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences (PNAS),"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "\u2026through the sharing of features between the models learned for old objects (or old tasks) and the models learned for new objects (or new tasks) (Anselmi et al., 2016; Baxter, 2000; Bottou, 2014; Lopez-Paz, Bottou, Scholko\u0308pf, & Vapnik, 2016; Salakhutdinov, Torralba, & Tenenbaum, 2011;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 494,
                                "start": 301
                            }
                        ],
                        "text": "In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks (Anselmi et al. 2016; Baxter 2000; Bottou 2014; Lopez-Paz et al. 2016; Rusu et al. 2016; Salakhutdinov et al. 2011; Srivastava & Salakhutdinov, 2013; Torralba et al. 2007; Zeiler & Fergus 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of invariant representations. Theoretical Computer Science 633:112\u201321"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "One-shot learning with a hierarchical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intrinsic motivation systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "One-year-old infants use https://doi.org/10.1017/S0140525X16001837"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 274
                            }
                        ],
                        "text": "When a new object type such as a bear is introduced, as in the later levels of Frostbite (Figure 2D), a network endowed with intuitive physics would also have an easier time adding this object type to its knowledge (the challenge of adding new objects was also discussed in Marcus, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 171
                            }
                        ],
                        "text": "And it has been difficult to learn neural-network-style representations that effortlessly generalize to new tasks that they were not trained on (see Davis & Marcus, 2015; Marcus, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 138
                            }
                        ],
                        "text": "For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick, 1989; Fodor & Pylyshyn, 1988; Marcus, 1998, 2001; Minsky & Papert, 1969; Pinker & Prince, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "probabilistic programming platform with programmable inference. arXiv preprint arXiv:1404.0099"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep speech: Scaling up"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How transferable are features"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning as accumulation: A reexamination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 37
                            }
                        ],
                        "text": "A similar sentiment was expressed by Minsky (1974):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A framework for representing knowledge. MIT-AI Laboratory Memo 306"
            },
            "venue": {
                "fragments": [],
                "text": "A framework for representing knowledge. MIT-AI Laboratory Memo 306"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Show and tell: A neural image"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 24
                            }
                        ],
                        "text": "In The Society of Mind, Minsky (1986) argued that the human brain is more similar to a complex society of diverse neural networks, than to a large, single one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building machines that learn and think like people"
            },
            "venue": {
                "fragments": [],
                "text": "BEHAVIORAL AND BRAIN SCIENCES,"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026master language, and they provide the building\n8Michael Jordan made this point forcefully in his 2015 speech accepting the Rumelhart Prize.\nblocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 148
                            }
                        ],
                        "text": "These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Stuff of Thought"
            },
            "venue": {
                "fragments": [],
                "text": "The Stuff of Thought"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 164
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Words, Thoughts, and Theories. Mind: A Quarterly Review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026master language, and they provide the building\n8Michael Jordan made this point forcefully in his 2015 speech accepting the Rumelhart Prize.\nblocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 148
                            }
                        ],
                        "text": "These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey, 2009; Jackendoff, 2003; Kemp, 2007; O\u2019Donnell, 2015; Pinker, 2007; F. Xu & Tenenbaum, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Stuff of Thought"
            },
            "venue": {
                "fragments": [],
                "text": "The Stuff of Thought"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 164
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Words, Thoughts, and Theories. Mind: A Quarterly Review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 223
                            }
                        ],
                        "text": "\u2026variables, test causal hypotheses, make use of the data-generating process in drawing conclusions, and learn selectively from others (Cook, Goodman, & Schulz, 2011; Gweon et al., 2010; L. E. Schulz, Gopnik, & Glymour, 2007; Stahl & Feigenson, 2015; Tsividis, Gershman, Tenenbaum, & Schulz, 2013)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Observing the unexpected enhances infants"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Domain-specific perceptual causality"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026can make meaningful generalizations from very sparse data, especially in the context of learning the meanings of words in their native language (Carey & Bartlett, 1978; Landau, Smith, & Jones, 1988; E. M. Markman, 1989; Smith, Jones, Landau, Gershkoff-Stowe, & Samuelson, 2002; F. Xu &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acquiring a single new word. Papers and Reports"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Roadmap towards Machine Intelligence. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "venue": {
                "fragments": [],
                "text": "A Roadmap towards Machine Intelligence. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "year": 1511
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Early in development, humans have a foundational understanding of several core domains (Spelke, 2003; Spelke & Kinzler, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Core knowledge. Attention and performance"
            },
            "venue": {
                "fragments": [],
                "text": "Core knowledge. Attention and performance"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic backpropagation and approxi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Turing Machines. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Turing Machines. arXiv preprint. Retrieved from http://arxiv.org/abs"
            },
            "year": 1410
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Essential Child"
            },
            "venue": {
                "fragments": [],
                "text": "The Essential Child"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The handbook of child psychology (pp. 523\u2013573)"
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 136
                            }
                        ],
                        "text": "In the interest of brevity, we do not discuss here another important vein of work linking neural circuits to variational approximations (Bastos et al., 2012), which have received less attention in the psychological literature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Canonical microcircuits for predictive"
            },
            "venue": {
                "fragments": [],
                "text": "coding. Neuron,"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 330,
                                "start": 278
                            }
                        ],
                        "text": "Moreover, this same, somewhat counterintuitive, problem in the authors\u2019 otherwise rational approach dangerously leaves unaddressed the major ethical and security issues of \u201cfree-willed\u201d personified artificial sentient agents, often popularized by fantasists and futurists alike (Bostrom 2014; Briegel 2012; Davies 2016; Fung 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robots with heart. Scientific American 313(5):60\u201363"
            },
            "venue": {
                "fragments": [],
                "text": "[KBC] Funke, J"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cognitive control over learning: Creating"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bridging levels of analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-task sequence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition (CVPR) (pp"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Biological movements look uniform: evidence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to Share Visual Appearance"
            },
            "venue": {
                "fragments": [],
                "text": "Models. IEEE Transactions on Pattern Analysis and Machine Intelligence,"
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparative Education 43(1):113\u201335"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to Share Visual Appearance"
            },
            "venue": {
                "fragments": [],
                "text": "Models. IEEE Transactions on Pattern Analysis and Machine Intelligence,"
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparative Education 43(1):113\u201335"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with Hierarchical-Deep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 243
                            }
                        ],
                        "text": "\u2026cognitive representations can be understood as \u2018intuitive theories\u2019, with a causal structure resembling a scientific theory (Carey, 2004, 2009; Gopnik et al., 2004; Gopnik & Meltzoff, 1999; Gweon, Tenenbaum, & Schulz, 2010; L. Schulz, 2012; H. Wellman & Gelman, 1998; H. M. Wellman & Gelman, 1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledge acquisition in foundational domains. In The handbook of child psychology (pp. 523\u2013573)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 202
                            }
                        ],
                        "text": "For example, the structure of early infant vocal development self-organizes spontaneously from such intrinsically motivated exploration, in interaction with the physical properties of the vocal systems (Moulin-Frier et al. 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organization of early"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 2
                            }
                        ],
                        "text": ", Barry, C. & Burgess, N. (2010) Evidence for grid cells in a human memory network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cooperate without looking: Why"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep learning with double Q-learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Receptive fields of single neurons in the cat\u2019s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A constructivist connectionist model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fetus and infant developmental"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "The motor behavior directly expresses the state of mind of the partner (Marin et al. 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpersonal motor coordination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 111
                            }
                        ],
                        "text": "While techniques for handling variable sized inputs in convnets may help for playing on different board sizes (Sermanet et al., 2014), the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people do."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 163
                            }
                        ],
                        "text": "Could AlphaGo quickly adapt to new variants of Go? Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes (Sermanet et al. 2014), the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience 5(7):532\u201346"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eilan, pp. 43\u201370"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 54
                            }
                        ],
                        "text": "Emotions are the framework in which cognition happens (e.g., Bach 2009; D\u00f6rner 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 367,
                                "start": 334
                            }
                        ],
                        "text": "Namely, goals come from specific needs, for example, from existential needs such as hunger or pain avoidance; sexual needs; the social need for affiliation, to be together with other people; the need for certainty related to unpredictability of the environment; and the need for competence related to ineffective coping with problems (D\u00f6rner 2001; D\u00f6rner & G\u00fcss 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bauplan f\u00fcr eine Seele [Blueprint for a soul"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 83
                            }
                        ],
                        "text": "Now, the leading approaches to speech recognition are fully neural network systems (Graves et al. 2013; Hannun et al. 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech recognition with deep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Words and the world: Predictive coding and the languageperception-cognition interface"
            },
            "venue": {
                "fragments": [],
                "text": "Current Directions in Psychological Science"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2016) Russell\u2019s paradox. In: The Stanford encyclopedia"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 23
                            }
                        ],
                        "text": "In recent experiments, Tsividis, Tenenbaum, and Schulz (2015) found that children can use high-level abstract features of a domain to guide hypothesis selection, by reasoning about distributional properties like the ratio of seeds to flowers, and dynamical properties like periodic or monotonic\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constraints on hypothesis selection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 153
                            }
                        ],
                        "text": "Somewhat surprisingly, the incorporation of attention has led to substantial performance gains in a variety of domains, including in machine translation (Bahdanau et al., 2015), object recognition (V."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 154
                            }
                        ],
                        "text": "Somewhat surprisingly, the incorporation of attention has led to substantial performance gains in a variety of domains, including in machine translation (Bahdanau et al., 2015), object recognition (V. Mnih et al., 2014), and image caption generation (K. Xu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation by Jointly Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The scope and limits of simulation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "One possibility is that intuitive psychology is simply cues \u201call the way down\u201d (Schlottmann, Cole, Watts, & White, 2013; Scholl & Gao, 2013), though this would require more and more cues as the scenarios become more complex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Perceiving Animacy and Intentionality: Visual Processing or Higher-Level Judgment? Social perception: Detection and interpretation of animacy, agency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 83
                            }
                        ],
                        "text": "In particular, the phasic firing of midbrain dopaminergic neurons is qualitatively (Schultz et al. 1997) and quantitatively (Bayer & Glimcher 2005) consistent with the reward prediction error that drives updating of model-free value estimates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neural substrate of prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with unsupervised auxiliary"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long-term recurrent convolutional networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1050,
                                "start": 67
                            }
                        ],
                        "text": "Hierarhical Bayesian models operating over probabilistic programs (Goodman et al. 2008; Lake et al. 2015a; Tenenbaum et al. 2011) are equipped to deal with theory-like structures and rich causal representations of the world, yet there are formidable algorithmic challenges for efficient inference. Computing a probability distribution over an entire space of programs is usually intractable, and often even finding a single high-probability program poses an intractable search problem. In contrast, whereas representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces. A complete account of learning and inference must explain how the brain does so much with limited computational resources (Gershman et al. 2015; Vul et al. 2014). Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models (see Griffiths et al. [2012] for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pragmatic language interpretation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 183
                            }
                        ],
                        "text": "These developments are also part of a broader trend towards \u201cdifferentiable programming,\u201d the incorporation of classic programming data structures into gradient-based learning systems (Dalrmple, 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 80
                            }
                        ],
                        "text": "Another avenue for potential integration is through differentiable programming (Dalrmple, 2016) \u2013 by ensuring that the program-like hypotheses are differentiable and thus learnable via gradient descent \u2013 a possibility discussed in the concluding section."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differentiable Programming. Retrieved from https://www.edge.org/ response-detail"
            },
            "venue": {
                "fragments": [],
                "text": "Differentiable Programming. Retrieved from https://www.edge.org/ response-detail"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamics of learning in deep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2062,
                                "start": 0
                            }
                        ],
                        "text": "Lake et al. discuss these kinds of breakthroughs as a big step for artificial intelligence, but raise the question how we can build machines that learn like people? We can find an indication in a survey of mind perception (Gray et al. 2007), which is the \u201camount of mind\u201d people are willing to attribute to others. Participants judged machines to be high on agency but low on experience. We attribute this to the fact that computers are trained on individual tasks, often involving a single modality such as vision or speech, or a single context such as classifying traffic signs, as opposed to interpreting spoken and gestured utterances. In contrast, for people, the \u201cworld\u201d essentially appears as a multimodal stream of stimuli, which unfold over time. Therefore, we suggest that the next paradigm shift in intelligent machines will have to include processing the \u201cworld\u201d through lifelong and crossmodal learning. This is important because people develop problem-solving capabilities, including language processing, over their life span and via interaction with the environment and other people (Elman 1993, Christiansen and Chater 2016). In addition, the learning is embodied, as developing infants have a body-rational view of the world, but also seem to apply general problem-solving strategies to a wide range of quite different tasks (Cangelosi and Schlesinger 2015). Hence, we argue that the proposed principles or \u201cstart-up software\u201d are coupled tightly with general learning mechanisms in the brain. We argue that these conditions inherently enable the development of distributed representations of knowledge. For example, in our research, we found that architectural mechanisms, like different timings in the information processing in the cortex, foster compositionality that in turn enables both the development of more complex body actions and the development of language competence from primitives (Heinrich 2016). These kinds of distributed representations are coherent with the cognitive science on embodied cognition. Lakoff and Johnson (2003), for example, argue that people describe personal relationships in terms of the physical sensation of temperature."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Goldilocks effect: Human"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 326,
                                "start": 223
                            }
                        ],
                        "text": "While neural networks can learn multiple mappings or tasks with the same set of stimuli \u2013 adapting their outputs depending on a specified goal \u2013 these models require substantial training or reconfiguration to add new tasks (e.g., Collins & Frank, 2013; Eliasmith et al., 2012; Rougier, Noelle, Braver, Cohen, & O\u2019Reilly, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A large-scale model of the functioning"
            },
            "venue": {
                "fragments": [],
                "text": "brain. Science,"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining modality specific deep"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Communication efficiency and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "teleological representations of actions productively"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science,"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 241,
            "methodology": 61,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 524,
        "totalPages": 53
    },
    "page_url": "https://www.semanticscholar.org/paper/Building-machines-that-learn-and-think-like-people-Lake-Ullman/7260c0692f8d265e11c4e9c4c8ef4c185bd587ad?sort=total-citations"
}