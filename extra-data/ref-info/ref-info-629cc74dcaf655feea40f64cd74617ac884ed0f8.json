{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 743435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611dac316bf03112c778cf7365d08e4a9d171876",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases."
            },
            "slug": "Learning-Probabilistic-Relational-Models-Getoor",
            "title": {
                "fragments": [],
                "text": "Learning Probabilistic Relational Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model and shows how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065321832"
                        ],
                        "name": "L. Ngo",
                        "slug": "L.-Ngo",
                        "structuredName": {
                            "firstName": "Liem",
                            "lastName": "Ngo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ngo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681109"
                        ],
                        "name": "P. Haddawy",
                        "slug": "P.-Haddawy",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Haddawy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haddawy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16738647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10b56711c11cee5c388e826ad9f9e7ff9d3281d8",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a probabilistic logic programming framework that allows the representation of conditional probabilities. While conditional probabilities are the most commonly used method for representing uncertainty in probabilistic expert systems, they have been largely neglected by work in quantitative logic programming. We define a fixpoint theory, declarative semantics, and proof procedure for the new class of probabilistic logic programs. Compared to other approaches to quantitative logic programming, we provide a true probabilistic framework with potential applications in probabilistic expert systems and decision support systems. We also discuss the relationship between such programs and Bayesian networks, thus moving toward a unification of two major approaches to automated reasoning."
            },
            "slug": "Probabilistic-Logic-Programming-and-Bayesian-Ngo-Haddawy",
            "title": {
                "fragments": [],
                "text": "Probabilistic Logic Programming and Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work defines a fixpoint theory, declarative semantics, and proof procedure for the new class of probabilistic logic programs, and discusses the relationship between such programs and Bayesian networks, thus moving toward a unification of two major approaches to automated reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "ASIAN"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3587277"
                        ],
                        "name": "N. Zhang",
                        "slug": "N.-Zhang",
                        "structuredName": {
                            "firstName": "Nevin",
                            "lastName": "Zhang",
                            "middleNames": [
                                "Lianwen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143715817"
                        ],
                        "name": "D. Poole",
                        "slug": "D.-Poole",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Poole",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Poole"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2978086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1b8a329df99d7f26fcd99ae49fd2654cdddafb6",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The general problem of computing poste rior probabilities in Bayesian networks is NP hard Cooper However e cient algorithms are often possible for particular applications by exploiting problem struc tures It is well understood that the key to the materialization of such a possibil ity is to make use of conditional indepen dence and work with factorizations of joint probabilities rather than joint probabilities themselves Di erent exact approaches can be characterized in terms of their choices of factorizations We propose a new approach which adopts a straightforward way for fac torizing joint probabilities In comparison with the clique tree propagation approach our approach is very simple It allows the pruning of irrelevant variables it accommo dates changes to the knowledge base more easily it is easier to implement More importantly it can be adapted to utilize both intercausal independence and condi tional independence in one uniform frame work On the other hand clique tree prop agation is better in terms of facilitating pre computations"
            },
            "slug": "A-simple-approach-to-Bayesian-network-computations-Zhang-Poole",
            "title": {
                "fragments": [],
                "text": "A simple approach to Bayesian network computations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new approach is proposed which adopts a straightforward way for facilitating joint probabilities and allows the pruning of irrelevant variables and changes to the knowledge base more easily, and can be adapted to utilize both intercausal independence and condi tional independence in one uniform frame work."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143715817"
                        ],
                        "name": "D. Poole",
                        "slug": "D.-Poole",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Poole",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Poole"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11707680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9ef7893ae7ee6826a43b5f58365092194c0e213",
            "isKey": false,
            "numCitedBy": 644,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-Horn-Abduction-and-Bayesian-Networks-Poole",
            "title": {
                "fragments": [],
                "text": "Probabilistic Horn Abduction and Bayesian Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691828"
                        ],
                        "name": "Joseph Y. Halpern",
                        "slug": "Joseph-Y.-Halpern",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Halpern",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Y. Halpern"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16514075,
            "fieldsOfStudy": [
                "Computer Science",
                "Philosophy"
            ],
            "id": "71f4bdd0b59d68076419d81cca12dda49edd3b81",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Analysis-of-First-Order-Logics-of-Probability-Halpern",
            "title": {
                "fragments": [],
                "text": "An Analysis of First-Order Logics of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 6,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Graphical-Models-for-Machine-Learning-and-Digital-Frey/629cc74dcaf655feea40f64cd74617ac884ed0f8?sort=total-citations"
}