{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144477659"
                        ],
                        "name": "J. Hammer",
                        "slug": "J.-Hammer",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4658767"
                        ],
                        "name": "Junghoo Cho",
                        "slug": "Junghoo-Cho",
                        "structuredName": {
                            "firstName": "Junghoo",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junghoo Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308846"
                        ],
                        "name": "R. Aranha",
                        "slug": "R.-Aranha",
                        "structuredName": {
                            "firstName": "Rohan",
                            "lastName": "Aranha",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Aranha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143965506"
                        ],
                        "name": "Arturo Crespo",
                        "slug": "Arturo-Crespo",
                        "structuredName": {
                            "firstName": "Arturo",
                            "lastName": "Crespo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arturo Crespo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Early approaches to wrapping Web sites were based on manual techniques [2, 9, 17, 4, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1733803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aac5b7d3127605318708de9496617a79e10dd21b",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a configurable tool for extracting semistructured data from a set of HTML pages andfor converting the extracted information into database objects. The input to the extractor is adeclarative specification that states where the data of interest is located on the HTML pages, andhow the data should be packaged into objects. We have implemented the Web extractor usingthe Python programming language stressing efficiency and ease-of-use. We also describe variousways of improving the functionality of our current prototype. The prototype is installed andrunning in the TSIMMIS testbed as part of a DARPA I3 (Intelligent Integration of Information)technology demonstration where it is used for extracting weather data form various WWW sites."
            },
            "slug": "Extracting-Semistructured-Information-from-the-Web.-Hammer-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Extracting Semistructured Information from the Web."
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A configurable tool for extracting semistructured data from a set of HTML pages and for converting the extracted information into database objects and various ways of improving the functionality of the current prototype are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1396785771"
                        ],
                        "name": "B. Ribeiro-Neto",
                        "slug": "B.-Ribeiro-Neto",
                        "structuredName": {
                            "firstName": "Berthier",
                            "lastName": "Ribeiro-Neto",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ribeiro-Neto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764577"
                        ],
                        "name": "Alberto H. F. Laender",
                        "slug": "Alberto-H.-F.-Laender",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Laender",
                            "middleNames": [
                                "H.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto H. F. Laender"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690426"
                        ],
                        "name": "A. D. Silva",
                        "slug": "A.-D.-Silva",
                        "structuredName": {
                            "firstName": "Altigran",
                            "lastName": "Silva",
                            "middleNames": [
                                "Soares",
                                "da"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Silva"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Experimental\nresults on real-life data-intensive Web sites con-\nfirm the feasibility of the approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8819185,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f3eb796827690a0be107633230be623e563c702",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an innovative approach to extracting semi-structured data from Web sources. The idea is to collect a couple of example objects from the user and to use this information to extract new objects from new pages or texts. To perform the extraction of new objects, we introduce a bottom-up extration strategy and, through experimentation, demonstrate that it works quite effectively with distinct Web sources, even if only a few examples are provided by the user."
            },
            "slug": "Extracting-semi-structured-data-through-examples-Ribeiro-Neto-Laender",
            "title": {
                "fragments": [],
                "text": "Extracting semi-structured data through examples"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "An innovative approach to extracting semi-structured data from Web sources by introducing a bottom-up extration strategy and demonstrating that it works quite effectively with distinct Web sources, even if only a few examples are provided by the user."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441542"
                        ],
                        "name": "G. Huck",
                        "slug": "G.-Huck",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Huck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Huck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137629"
                        ],
                        "name": "P. Fankhauser",
                        "slug": "P.-Fankhauser",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Fankhauser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fankhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751802"
                        ],
                        "name": "K. Aberer",
                        "slug": "K.-Aberer",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Aberer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Aberer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720826"
                        ],
                        "name": "E. Neuhold",
                        "slug": "E.-Neuhold",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Neuhold",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Neuhold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2538164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62c9b4c353afad20f9a1f1d59060066d5aceb708",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Jedi (Java based Extraction and Dissemination of Information) is a lightweight tool for the creation of wrappers and mediators to extract, combine, and reconcile information from several independent information sources. For wrappers it uses attributed grammars, which are evaluated with a fault-tolerant parsing strategy to cope with ambiguous grammars and irregular sources. For mediation it uses a simple generic object-model that can be extended with Java-libraries for specific models such as HTML, XML or the relational model. This paper describes the architecture of Jedi, and then focuses on Jedi's wrapper generator."
            },
            "slug": "Jedi:-extracting-and-synthesizing-information-from-Huck-Fankhauser",
            "title": {
                "fragments": [],
                "text": "Jedi: extracting and synthesizing information from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The architecture of Jedi is described, and then the focus is on Jedi's wrapper generator."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 3rd IFCIS International Conference on Cooperative Information Systems (Cat. No.98EX122)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2832139"
                        ],
                        "name": "B. Adelberg",
                        "slug": "B.-Adelberg",
                        "structuredName": {
                            "firstName": "Brad",
                            "lastName": "Adelberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Adelberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15329383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30cad07f9c314777641b9d1bfa60eb0c7bf58fac",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Often interesting structured or semistructured data is not in database systems but in HTML pages, text files, or on paper. The data in these formats is not usable by standard query processing engines and hence users need a way of extracting data from these sources into a DBMS or of writing wrappers around the sources. This paper describes NoDoSE, the Northwestern Document Structure Extractor, which is an interactive tool for semi-automatically determining the structure of such documents and then extracting their data. Using a GUI, the user hierarchically decomposes the file, outlining its interesting regions and then describing their semantics. This task is expedited by a mining component that attempts to infer the grammar of the file from the information the user has input so far. Once the format of a document has been determined, its data can be extracted into a number of useful forms. This paper describes both the NoDoSE architecture, which can be used as a test bed for structure mining algorithms in general, and the mining algorithms that have been developed by the author. The prototype, which is written in Java, is described and experiences parsing a variety of documents are reported."
            },
            "slug": "NoDoSE\u2014a-tool-for-semi-automatically-extracting-and-Adelberg",
            "title": {
                "fragments": [],
                "text": "NoDoSE\u2014a tool for semi-automatically extracting structured and semistructured data from text documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "NoDoSE, the Northwestern Document Structure Extractor, is described, which is an interactive tool for semi-automatically determining the structure of such documents and then extracting their data."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "(iii) Differently from roadRunner and Stalker, Wien is unable to handle optional fields, and therefore fails on samples 13, 14 and 15."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "(v) Both Wien and Stalker cannot handle nested structures, and therefore they fail on PharmaWeb, the only class whose pages contain a list of lists (nest equals 2); on the contrary, roadRunner correctly discovers the nesting and generates the wrapper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 113
                            }
                        ],
                        "text": "These works have attacked the wrapper generation problem under various perspectives, going from machine\u2013learning [18, 12, 10, 14] to data mining [1, 16] and conceptual modeling [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "(i) While Wien and Stalker generated their wrappers by examining a number of labeled examples, and therefore the systems had a precise knowledge of the target schema, roadRunner did not have any a priori knowledge about the organization of the pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 46
                            }
                        ],
                        "text": "To compare our results with those of Wien and Stalker, Table B in Figure 6 reports a number of elements with respect to 5 page classes for which experimental results were known in the literature [12, 14]; the original test samples for classes 11 to 15 have been downloaded from RISE (http://www.isi.edu/~ muslea/RISE), a repository of information sources from data extraction projects."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 287
                            }
                        ],
                        "text": "(ii) Although computing times refer to different machines, it can still be seen that, in those cases in which all three systems are able to generate a wrapper (11 and 12), CPU times used by roadRunner are orders of magnitude lower than those needed to learn the wrapper both by Wien and Stalker."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 302
                            }
                        ],
                        "text": "Figure 6 actually contains two tables; while Table A refers to experiments we have conducted independently, in Table B we compare for 5 page classes our results with those of other data extraction systems for which experimental results are available in the literature, namely Wien [13, 12] and Stalker [14], two wrapper generation systems based on a machine learning approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 146
                            }
                        ],
                        "text": "In essence, in these cases all relevant data are correctly extracted, but according to a schema that is not\n1Since for some of the pages Wien and Stalker consider only a portion of the HTML code, to have comparable results when needed we have restricted our analysis to those portions only.\nthe one expected after looking at the logical organization of the pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 417,
                                "start": 410
                            }
                        ],
                        "text": "Table B contains the following elements: (i) site from which the pages were taken, and number of samples; (ii) description of the target schema, i.e., number of attributes (pcd), level of nesting (nest), whether the pages contain optional\nelements (opt), and whether attributes may occur in different orders (ord); (iii) results: results obtained by the three systems, with computing times; times for Wien and Stalker refer to CPU times used during the learning.1\nA few things are worth noting here with respect to the expressive power of the various systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 195
                            }
                        ],
                        "text": "To compare our results with those of Wien and Stalker, Table B in Figure 6 reports a number of elements with respect to 5 page classes for which experimental results were known in the literature [12, 14]; the original test samples for classes 11 to 15 have been downloaded from RISE (http://www."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 5
                            }
                        ],
                        "text": "(iv) Stalker has more considerable expressive power since it can handle disjunctive patterns; this allows for treating attributes that appear in various orders, like in Address Finder (14); being limited to union\u2013free patterns, roadRunner fails in cases like this."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3514097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cc263c84b85027164bd39db169f5d5959ef6822",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques."
            },
            "slug": "A-hierarchical-approach-to-wrapper-induction-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "A hierarchical approach to wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can handle information sources that could not be wrapped by existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145887845"
                        ],
                        "name": "D. M. Campbell",
                        "slug": "D.-M.-Campbell",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Campbell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Campbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158090475"
                        ],
                        "name": "Y. S. Jiang",
                        "slug": "Y.-S.-Jiang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Jiang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. S. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742211"
                        ],
                        "name": "Stephen W. Liddle",
                        "slug": "Stephen-W.-Liddle",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Liddle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen W. Liddle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144259075"
                        ],
                        "name": "D. Quass",
                        "slug": "D.-Quass",
                        "structuredName": {
                            "firstName": "Dallan",
                            "lastName": "Quass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Quass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985589"
                        ],
                        "name": "Randy D. Smith",
                        "slug": "Randy-D.-Smith",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Smith",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randy D. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1589294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3ea8d41b57e74cf5c4815fbd20982719dea768f",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Electronically available data on the Web is exploding at an ever increasing pace. Much of this data is unstructured, which makes searching hard and traditional database querying impossible. Many Web documents, however, contain an abundance of recognizable constants that together describe the essence of a document\u2019s content. For these kinds of data-rich documents (e.g., advertisements, movie reviews, weather reports, travel information, sports summaries, financial statements, obituaries, and many others) we can apply a conceptual-modeling approach to extract and structure data. The approach is based on an ontology \u2013 a conceptual model instance \u2013 that describes the data of interest, including relationships, lexical appearance, and context keywords. By parsing the ontology, we can automatically produce a database scheme and recognizers for constants and keywords, and then invoke routines to recognize and extract data from unstructured documents and structure it according to the generated database scheme. Experiments show that it is possible to achieve good recall and precision ratios for documents that are rich in recognizable constants and narrow in ontological breadth."
            },
            "slug": "A-Conceptual-Modeling-Approach-to-Extracting-Data-Embley-Campbell",
            "title": {
                "fragments": [],
                "text": "A Conceptual-Modeling Approach to Extracting Data from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments show that it is possible to achieve good recall and precision ratios for documents that are rich in recognizable constants and narrow in ontological breadth."
            },
            "venue": {
                "fragments": [],
                "text": "ER"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741158"
                        ],
                        "name": "P. Atzeni",
                        "slug": "P.-Atzeni",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Atzeni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Atzeni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Note that, although nested types and UFREs are far from catching the full diversity of structures present in HTML pages, they have been shown [ 3 ] to be a promising abstraction for describing the structure of pages in fairly regular Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15682860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c16d2261375e0b26e918a4d9d5160aa9d775e2a",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper discusses the issue of views in the Web context. We introduce a set of languages for managing and restructuring data coming from the World Wide Web. We present a specific data model, called the ARANEUS Data Model, inspired to the structures typically present in Web sites. The model allows us to describe the scheme of a Web hypertext, in the spirit of databases. Based on the data model, we develop two languages to support a sophisticate view definition process: the first, called ULIXES, is used to build database views of the Web, which can then be analyzed and integrated using database techniques; the second, called PENELOPE, allows the definition of derived Web hypertexts from relational views. This can be used to generate hypertextual views over the Web."
            },
            "slug": "To-Weave-the-Web-Atzeni-Mecca",
            "title": {
                "fragments": [],
                "text": "To Weave the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A set of languages for managing and restructuring data coming from the World Wide Web is introduced, inspired to the structures typically present in Web sites, and a specific data model, called the ARANEUS Data Model, is presented, to describe the scheme of a Web hypertext, in the spirit of databases."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34607455"
                        ],
                        "name": "Chun-Nan Hsu",
                        "slug": "Chun-Nan-Hsu",
                        "structuredName": {
                            "firstName": "Chun-Nan",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Nan Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094164369"
                        ],
                        "name": "Ming-Tzung Dung",
                        "slug": "Ming-Tzung-Dung",
                        "structuredName": {
                            "firstName": "Ming-Tzung",
                            "lastName": "Dung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Tzung Dung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 113
                            }
                        ],
                        "text": "These works have attacked the wrapper generation problem under various perspectives, going from machine\u2013learning [18, 12, 10, 14] to data mining [1, 16] and conceptual modeling [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17895561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9478265afd280486299a5b8f1dbaaf6769422de",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generating-Finite-State-Transducers-for-Data-from-Hsu-Dung",
            "title": {
                "fragments": [],
                "text": "Generating Finite-State Transducers for Semi-Structured Data Extraction from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "(iii) Differently from roadRunner and Stalker, Wien is unable to handle optional fields, and therefore fails on samples 13, 14 and 15."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "(v) Both Wien and Stalker cannot handle nested structures, and therefore they fail on PharmaWeb, the only class whose pages contain a list of lists (nest equals 2); on the contrary, roadRunner correctly discovers the nesting and generates the wrapper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 113
                            }
                        ],
                        "text": "These works have attacked the wrapper generation problem under various perspectives, going from machine\u2013learning [18, 12, 10, 14] to data mining [1, 16] and conceptual modeling [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "(i) While Wien and Stalker generated their wrappers by examining a number of labeled examples, and therefore the systems had a precise knowledge of the target schema, roadRunner did not have any a priori knowledge about the organization of the pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "To compare our results with those of Wien and Stalker, Table B in Figure 6 reports a number of elements with respect to 5 page classes for which experimental results were known in the literature [12, 14]; the original test samples for classes 11 to 15 have been downloaded from RISE (http://www.isi.edu/~ muslea/RISE), a repository of information sources from data extraction projects."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 278
                            }
                        ],
                        "text": "(ii) Although computing times refer to different machines, it can still be seen that, in those cases in which all three systems are able to generate a wrapper (11 and 12), CPU times used by roadRunner are orders of magnitude lower than those needed to learn the wrapper both by Wien and Stalker."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 281
                            }
                        ],
                        "text": "Figure 6 actually contains two tables; while Table A refers to experiments we have conducted independently, in Table B we compare for 5 page classes our results with those of other data extraction systems for which experimental results are available in the literature, namely Wien [13, 12] and Stalker [14], two wrapper generation systems based on a machine learning approach."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "In essence, in these cases all relevant data are correctly extracted, but according to a schema that is not\n1Since for some of the pages Wien and Stalker consider only a portion of the HTML code, to have comparable results when needed we have restricted our analysis to those portions only.\nthe one expected after looking at the logical organization of the pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 405,
                                "start": 401
                            }
                        ],
                        "text": "Table B contains the following elements: (i) site from which the pages were taken, and number of samples; (ii) description of the target schema, i.e., number of attributes (pcd), level of nesting (nest), whether the pages contain optional\nelements (opt), and whether attributes may occur in different orders (ord); (iii) results: results obtained by the three systems, with computing times; times for Wien and Stalker refer to CPU times used during the learning.1\nA few things are worth noting here with respect to the expressive power of the various systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 195
                            }
                        ],
                        "text": "To compare our results with those of Wien and Stalker, Table B in Figure 6 reports a number of elements with respect to 5 page classes for which experimental results were known in the literature [12, 14]; the original test samples for classes 11 to 15 have been downloaded from RISE (http://www."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11075952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f052f40a3307de1e45e11a3007a7552b36ebfc8",
            "isKey": true,
            "numCitedBy": 641,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wrapper-induction:-Efficiency-and-expressiveness-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Wrapper induction: Efficiency and expressiveness"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741158"
                        ],
                        "name": "P. Atzeni",
                        "slug": "P.-Atzeni",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Atzeni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Atzeni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15285677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e46905d83ee82fc5ce9f2af6c76157f49a2cef79",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper develops Editor, a language for manipulating semistructured documents, such as those typically available on the Web. Editor programs are based on two simple ideas, taken from text editors: \u201csearch\u201d instructions are used to select regions of interest in a document, and \u201ccut & paste\u201d instructions to restructure them. We study the expressive power and the complexity of these programs. We show that they are computationally complete, in the sense that any computable document restructuring can be expressed in Editor. We also study the complexity of a safe subclass of programs, showing that it captures exactly the class of polynomial-time restructurings. The language has been implemented in Java and is currently used in the Araneus project as a basis for a wrapper-generation toolkit."
            },
            "slug": "Cut-and-paste-Mecca-Atzeni",
            "title": {
                "fragments": [],
                "text": "Cut and paste"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The paper develops Editor, a language for manipulating semistructured documents, such as those typically available on the Web, that is computationally complete, in the sense that any computable document restructuring can be expressed in Editor."
            },
            "venue": {
                "fragments": [],
                "text": "PODS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1883948"
                        ],
                        "name": "A. Sahuguet",
                        "slug": "A.-Sahuguet",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Sahuguet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sahuguet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3181471"
                        ],
                        "name": "Fabien Azavant",
                        "slug": "Fabien-Azavant",
                        "structuredName": {
                            "firstName": "Fabien",
                            "lastName": "Azavant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabien Azavant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Early approaches to wrapping Web sites were based on manual techniques [2, 9,  17 , 4, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6535760,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b165e57590208418c90f63de9cf11b097eb5ec9",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present the World-Wide WebWrapper Factory (W4F), a Java toolkit to generate wrappers for Web data sources. Some key features of W4F are an expressive language to extract information from HTML pages in a structured way, a mapping to export it as XML documents and some visual tools to assist the user during wrapper creation. Moreover, the entire description of wrappers is fully declarative. As an illustration, we demonstrate how to use W4F to create XML gateways, that serve transparently and onthe-fly HTML pages as XML documents with their DTDs. Comments Postprint version. Copyright ACM, 1999. This is the author\u2019s version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in WebDB '99. This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/db_research/24"
            },
            "slug": "Web-Ecology:-Recycling-HTML-Pages-as-XML-Documents-Sahuguet-Azavant",
            "title": {
                "fragments": [],
                "text": "Web Ecology: Recycling HTML Pages as XML Documents Using W4F"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This paper presents the World-Wide WebWrapper Factory (W4F), a Java toolkit to generate wrappers for Web data sources, an expressive language to extract information from HTML pages in a structured way, a mapping to export it as XML documents and some visual tools to assist the user during wrapper creation."
            },
            "venue": {
                "fragments": [],
                "text": "WebDB"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Early approaches to wrapping Web sites were based on manual techniques [2, 9, 17, 4, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14968522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5ef65defc1abd0f5db2e88081bd30f994c879e7",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Grammars-Have-Exceptions-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "Grammars Have Exceptions"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710280"
                        ],
                        "name": "S. Grumbach",
                        "slug": "S.-Grumbach",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Grumbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grumbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16484726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "780c11402e2393bea169192d13b68d8d20a27fd5",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of rediscovering the schema of nested relations that have been encoded as strings for storage purposes. We consider various classes of encoding functions, and consider the markup encodings, which allow to find the schema without knowledge of the encoding function, under reasonable assumptions on the input data. Depending upon the encoding of empty sets, we propose two polynomial on-line algorithms (with different buffer size) solving the schema finding problem. We also prove that with a high probability, both algorithms find the schema after examining a fixed number of tuples, thus leading in practice to a linear time behavior with respect to the database size for wrapping the data. Finally, we show that the proposed techniques are well-suited for practical applications, such as structuring and wrapping HTML pages and Web sites."
            },
            "slug": "In-Search-of-the-Lost-Schema-Grumbach-Mecca",
            "title": {
                "fragments": [],
                "text": "In Search of the Lost Schema"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Depending upon the encoding of empty sets, two polynomial on-line algorithms are proposed for solving the schema finding problem, and it is proved that with a high probability, both algorithms find the schema after examining a fixed number of tuples, thus leading in practice to a linear time behavior with respect to the database size for wrapping the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICDT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47494927"
                        ],
                        "name": "L. Pitt",
                        "slug": "L.-Pitt",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Pitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pitt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Grammar inference is a well known and extensively studied problem (for a survey of the literature see, for example, [15])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9379037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8132b4bb210169c80c1f5d6ce0051accb25fcaf7",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys recent results concerning the inference of deterministic finite automata (DFAs). The results discussed determine the extent to which DFAs can be feasibly inferred, and highlight a number of interesting approaches in computational learning theory."
            },
            "slug": "Inductive-Inference,-DFAs,-and-Computational-Pitt",
            "title": {
                "fragments": [],
                "text": "Inductive Inference, DFAs, and Computational Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results discussed determine the extent to which DFAs can be feasibly inferred, and highlight a number of interesting approaches in computational learning theory."
            },
            "venue": {
                "fragments": [],
                "text": "AII"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Finding one solution to match(w, s) corresponds to finding one visit for the AND-OR tree [19] shown in Figure 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1028275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b886f2c097b635ee9550ca29fff7dcbbb7727ff7",
            "isKey": false,
            "numCitedBy": 5912,
            "numCiting": 271,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a survey of Artifici'al Intelligence (AI). It divides the field into four cor~ topics (embodying the base fo\u00b7r a science of intelligence) and eight applications topics (in which research has been contributing to core ideas).. The paper discusses the history, the major landmarks, and some of the controversies in each of these twelve topics. Each topic is represented by a chart citing the major references. These references are contained in an extensive bibliography. The paper concludes with a discussion of some of the criticisms of 'AI and with some predictions about the course of future research."
            },
            "slug": "Artificial-Intelligence-Nilsson",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The history, the major landmarks, and some of the controversies in each of these twelve topics are discussed, as well as some predictions about the course of future research."
            },
            "venue": {
                "fragments": [],
                "text": "IFIP Congress"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144342820"
                        ],
                        "name": "E. M. Gold",
                        "slug": "E.-M.-Gold",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Gold",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. M. Gold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 417,
                                "start": 414
                            }
                        ],
                        "text": "However, regular grammar inference is a hard problem: first, it is known from Gold\u2019s works [6] that regular grammars cannot be correctly identified from positive examples alone; also, even in the presence of both positive and negative examples, there exists no efficient learning algorithm for identifying the minimum state deterministic finite state automaton that is consistent with an arbitrary set of examples [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8943792,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9dfa951bec812bd7b8c905c587bca50b7883a10f",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Complexity-of-Automaton-Identification-from-Given-Gold",
            "title": {
                "fragments": [],
                "text": "Complexity of Automaton Identification from Given Data"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144342820"
                        ],
                        "name": "E. M. Gold",
                        "slug": "E.-M.-Gold",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Gold",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. M. Gold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "However, regular grammar inference is a hard problem: first, it is known from Gold\u2019s works [6] that regular grammars cannot be correctly identified from positive examples alone; also, even in the presence of both positive and negative examples, there exists no efficient learning algorithm for identifying the minimum state deterministic finite state automaton that is consistent with an arbitrary set of examples [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12438987,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "20cc59e8879305cbe18409c77464eff272e1cf55",
            "isKey": false,
            "numCitedBy": 3486,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Language-Identification-in-the-Limit-Gold",
            "title": {
                "fragments": [],
                "text": "Language Identification in the Limit"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Early approaches to wrapping Web sites were based on manual techniques [ 2 , 9, 17, 4, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33775467,
            "fieldsOfStudy": [],
            "id": "2de80febcc2ec053c78f0786fe899a1e52083253",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cut and Paste"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Smith A conceptual - modeling approach to extracting data from the web Language identification in the limit"
            },
            "venue": {
                "fragments": [],
                "text": "Information Systems"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting semistructured data through examples Learning information extraction rules for semistructured and free text"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting semistructured data through examples Web ecology : Recycling HTML pages as XML documents using W 4 F Learning information extraction rules for semistructured and free text"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 4,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/RoadRunner:-Towards-Automatic-Data-Extraction-from-Crescenzi-Mecca/3dd1f9f7795b31493d98d9f260d37aad07550f6e?sort=total-citations"
}