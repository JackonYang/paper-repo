{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "Graphical models with undirected connections will require the separate estimation of a single constant as in [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "1 MNIST digits In our first experiment we used a deep belief network (DBN) taken from [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "The difference of about 2 nats shows that the variational bound in [1] was rather tight, although a very small improvement of the DBN over the RBM is now revealed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "obtained as a byproduct of the greedy learning procedure, and an AIS estimate of the model\u2019s partition function Z, [1] proposed obtaining an estimate of a variational lower bound: log P (v) \u2265 \u2211"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Our current work is motivated by recent work on evaluating RBMs and their generalization to Deep Belief Networks (DBNs) [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 458722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08d0ea90b53aba0008d25811268fe46562cfb38c",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data."
            },
            "slug": "On-the-quantitative-analysis-of-deep-belief-Salakhutdinov-Murray",
            "title": {
                "fragments": [],
                "text": "On the quantitative analysis of deep belief networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and a novel AIS scheme for comparing RBM's with different architectures is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089732"
                        ],
                        "name": "S. Chib",
                        "slug": "S.-Chib",
                        "structuredName": {
                            "firstName": "Siddhartha",
                            "lastName": "Chib",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chib"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "This trivial identity suggests a family of estimators introduced by Chib [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "(8)\nThis trivial identity suggests a family of estimators introduced by Chib [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "We start with the simplest Chib-inspired estimator based on equations (8,9,11)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "There are also well-known problems with the Chib approach [14], to which we will return."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Neal noted that Chibs method will return incorrect answers in cases where the Markov chain does not mix well amongst modes [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "There are technical difficulties with the original Chib-style approach applied to Metropolis\u2013Hastings and continuous latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Chib and Jeliazkov implicitly took out the h\u2217=h point from all of their integrals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Moreover the approach above is not what Chib recommended."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "It is worth emphasizing that we have only outlined the simplest possible scheme inspired by Chib\u2019s general approach."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2915703,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f09b90206271d48a1ed0a3638e9d69bddc2117ee",
            "isKey": true,
            "numCitedBy": 1941,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In the context of Bayes estimation via Gibbs sampling, with or without data augmentation, a simple approach is developed for computing the marginal density of the sample data (marginal likelihood) given parameter draws from the posterior distribution. Consequently, Bayes factors for model comparisons can be routinely computed as a by-product of the simulation. Hitherto, this calculation has proved extremely challenging. Our approach exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density. This simple identity holds for any parameter value. An estimate of the posterior density is shown to be available if all complete conditional densities used in the Gibbs sampler have closed-form expressions. To improve accuracy, the posterior density is estimated at a high density point, and the numerical standard error of resulting estimate is derived. The ideas are applied to probit regression and finite mixture models."
            },
            "slug": "Marginal-Likelihood-from-the-Gibbs-Output-Chib",
            "title": {
                "fragments": [],
                "text": "Marginal Likelihood from the Gibbs Output"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work exploits the fact that the marginal density can be expressed as the prior times the likelihood function over the posterior density, so that Bayes factors for model comparisons can be routinely computed as a by-product of the simulation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143754991"
                        ],
                        "name": "Vibhav Gogate",
                        "slug": "Vibhav-Gogate",
                        "structuredName": {
                            "firstName": "Vibhav",
                            "lastName": "Gogate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vibhav Gogate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1916113"
                        ],
                        "name": "Bozhena Bidyuk",
                        "slug": "Bozhena-Bidyuk",
                        "structuredName": {
                            "firstName": "Bozhena",
                            "lastName": "Bidyuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bozhena Bidyuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751239"
                        ],
                        "name": "R. Dechter",
                        "slug": "R.-Dechter",
                        "structuredName": {
                            "firstName": "Rina",
                            "lastName": "Dechter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dechter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Therefore the methods discussed in [18] could be used to bound the probability of accidentally over-estimating a test set probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2111476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92ce55064806fe8f972632a7f02ce9850633295e",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Computing the probability of evidence even with known error bounds is NP-hard. In this paper we address this hard problem by settling on an easier problem. We propose an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error. Our proposed approximation is a randomized importance sampling scheme that uses the Markov inequality. However, a straight-forward application of the Markov inequality may lead to poor lower bounds. We therefore propose several heuristic measures to improve its performance in practice. Empirical evaluation of our scheme with state-of-the-art lower bounding schemes reveals the promise of our approach."
            },
            "slug": "Studies-in-Lower-Bounding-Probabilities-of-Evidence-Gogate-Bidyuk",
            "title": {
                "fragments": [],
                "text": "Studies in Lower Bounding Probabilities of Evidence using the Markov Inequality"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error, and proposes a randomized importance sampling scheme that uses the Markov inequality."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089732"
                        ],
                        "name": "S. Chib",
                        "slug": "S.-Chib",
                        "structuredName": {
                            "firstName": "Siddhartha",
                            "lastName": "Chib",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chib"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226535"
                        ],
                        "name": "Ivan Jeliazkov",
                        "slug": "Ivan-Jeliazkov",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Jeliazkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Jeliazkov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "In fact, [11] explicitly favors a more elaborate procedure involving sampling from a sequence of distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44046690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e33e3ce9541f261dd1d1c35190cf191347c1c15e",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This article provides a framework for estimating the marginal likelihood for the purpose of Bayesian model comparisons. The approach extends and completes the method presented in Chib (1995) by overcoming the problems associated with the presence of intractable full conditional densities. The proposed method is developed in the context of MCMC chains produced by the Metropolis\u2013Hastings algorithm, whose building blocks are used both for sampling and marginal likelihood estimation, thus economizing on prerun tuning effort and programming. Experiments involving the logit model for binary data, hierarchical random effects model for clustered Gaussian data, Poisson regression model for clustered count data, and the multivariate probit model for correlated binary data, are used to illustrate the performance and implementation of the method. These examples demonstrate that the method is practical and widely applicable."
            },
            "slug": "Marginal-Likelihood-From-the-Metropolis\u2013Hastings-Chib-Jeliazkov",
            "title": {
                "fragments": [],
                "text": "Marginal Likelihood From the Metropolis\u2013Hastings Output"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The proposed method is developed in the context of MCMC chains produced by the Metropolis\u2013Hastings algorithm, whose building blocks are used both for sampling and marginal likelihood estimation, thus economizing on prerun tuning effort and programming."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40014328"
                        ],
                        "name": "P. Moral",
                        "slug": "P.-Moral",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Moral",
                            "middleNames": [
                                "Del"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moral"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701800"
                        ],
                        "name": "A. Doucet",
                        "slug": "A.-Doucet",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Doucet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doucet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12074789,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "843ce16f3235593bed044ceee9e4e0a937b57a2e",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary.\u2002 We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference."
            },
            "slug": "Sequential-Monte-Carlo-samplers-Moral-Doucet",
            "title": {
                "fragments": [],
                "text": "Sequential Monte Carlo samplers"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Details of training can be found in [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "The raw image intensities were preprocessed and whitened as described in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2054939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80c330eee12decb84aaebcc85dc7ce414134ad61",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images."
            },
            "slug": "Modeling-image-patches-with-a-directed-hierarchy-of-Osindero-Hinton",
            "title": {
                "fragments": [],
                "text": "Modeling image patches with a directed hierarchy of Markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets is described and it is shown that this type of model is good at capturing the statistics of patches of natural images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "As an example we give a partial review of Annealed Importance Sampling (AIS) [7], a special case of a larger family of Sequential Monte Carlo (SMC) methods (see, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "AIS used a hand-tuned temperature schedule designed to equalize the variance of the intermediate log weights [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11112994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f59406cce55c7bb9a78521bd14755a0db0aee7d",
            "isKey": false,
            "numCitedBy": 1212,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Simulated annealing\u2014moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\u2014has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers."
            },
            "slug": "Annealed-importance-sampling-Neal",
            "title": {
                "fragments": [],
                "text": "Annealed importance sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler, which can be seen as a generalization of a recently-proposed variant of sequential importance sampling."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31935178"
                        ],
                        "name": "M. Newton",
                        "slug": "M.-Newton",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Newton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Newton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7496107,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0aa8774e218fafa445a986715f5b40124b7ae6a5",
            "isKey": false,
            "numCitedBy": 1450,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling-importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR-adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second-order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models."
            },
            "slug": "Approximate-Bayesian-inference-With-the-Weighted-Newton",
            "title": {
                "fragments": [],
                "text": "Approximate Bayesian-inference With the Weighted Likelihood Bootstrap"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764140"
                        ],
                        "name": "F. Bartolucci",
                        "slug": "F.-Bartolucci",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Bartolucci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bartolucci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156283"
                        ],
                        "name": "L. Scaccia",
                        "slug": "L.-Scaccia",
                        "structuredName": {
                            "firstName": "Luisa",
                            "lastName": "Scaccia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Scaccia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145827252"
                        ],
                        "name": "A. Mira",
                        "slug": "A.-Mira",
                        "structuredName": {
                            "firstName": "Antonietta",
                            "lastName": "Mira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17601589,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c3beb1622a48d7fe17a1b085a6aa86ea42eb3be2",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a class of estimators of the Bayes factor which is based on an extension of the bridge sampling identity of Meng & Wong (1996) and makes use of the output of the reversible jump algorithm of Green (1995). Within this class we give the optimal estimator and also a suboptimal one which may be simply computed on the basis of the acceptance probabilities used within the reversible jump algorithm for jumping between models. The proposed estimators are very easily computed and lead to a substantial gain of efficiency in estimating the Bayes factor over the standard estimator based on the reversible jump output. This is illustrated through a series of Monte Carlo simulations involving a linear and a logistic regression model. Copyright 2006, Oxford University Press."
            },
            "slug": "Efficient-Bayes-factor-estimation-from-the-jump-Bartolucci-Scaccia",
            "title": {
                "fragments": [],
                "text": "Efficient Bayes factor estimation from the reversible jump output"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "Our presentation is for general latent variable models, however for a running example, we use DBNs (see section 4 and [2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "4 Deep Belief Networks In this section we provide a brief overview of Deep Belief Networks (DBNs), recently introduced by [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13413,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831395"
                        ],
                        "name": "H. Wallach",
                        "slug": "H.-Wallach",
                        "structuredName": {
                            "firstName": "Hanna",
                            "lastName": "Wallach",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wallach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 181
                            }
                        ],
                        "text": "Despite these problems the estimator has received significant attention in statistics, and has been used for evaluating latent variable models in recent machine learning literature [5, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1174898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56702c8ddc72ba45eaad4e5b6d44afe86b8a4a9d",
            "isKey": false,
            "numCitedBy": 1083,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the \"bag-of-words\" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful."
            },
            "slug": "Topic-modeling:-beyond-bag-of-words-Wallach",
            "title": {
                "fragments": [],
                "text": "Topic modeling: beyond bag-of-words"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model is explored."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "For comparison, we also trained square ICA and a mixture of factor analyzers (MFA) using code from [16, 17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18270595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09ef86868035bbfd4803a9e1c98640804bf8f4a4",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing di erent local factor models in di erent regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation{Maximization algorithm for tting the parameters of this mixture of factor analyzers."
            },
            "slug": "The-EM-algorithm-for-mixtures-of-factor-analyzers-Ghahramani-Hinton",
            "title": {
                "fragments": [],
                "text": "The EM algorithm for mixtures of factor analyzers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents an exact Expectation{Maximization algorithm for determining the parameters of this mixture of factor analyzers which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7585417,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a912be1031c08cb21d5bf65a7240d3ca2aee51eb",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (\u2018exclusive\u2019 versus \u2018inclusive\u2019 Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals."
            },
            "slug": "Divergence-measures-and-message-passing-Minka",
            "title": {
                "fragments": [],
                "text": "Divergence measures and message passing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143802492"
                        ],
                        "name": "C. Ritter",
                        "slug": "C.-Ritter",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ritter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31781461"
                        ],
                        "name": "M. Tanner",
                        "slug": "M.-Tanner",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Tanner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Equations (9, 11) have been used in schemes for monitoring the convergence of Gibbs samplers [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121168762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8515a311f9bcf0e8a8588b589d9cd82b0e44022",
            "isKey": false,
            "numCitedBy": 478,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The article briefly reviews the history, literature, and form of the Gibbs sampler. An importance sampling device is proposed for converting the output of the Gibbs sampler to a sample from the exact posterior. This Gibbs stopper technique is also useful for assessing convergence of the Gibbs sampler for moderate sized problems. Also presented is an approach for implementing the Gibbs sampler in nonconjugate situations. The basic idea is to approximate the true cdf of each conditional distribution by a piecewise linear function and then sample from the approximation. Questions relating to the number of nodes in the approximation, gap size between successive nodes, and the treatment of unbounded intervals for a given conditional are discussed. The methodology is illustrated using a genetic linkage model, a nonlinear regression model, and the Cox model."
            },
            "slug": "Facilitating-the-Gibbs-Sampler:-The-Gibbs-Stopper-Ritter-Tanner",
            "title": {
                "fragments": [],
                "text": "Facilitating the Gibbs Sampler: The Gibbs Stopper and the Griddy-Gibbs Sampler"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An importance sampling device is proposed for converting the output of the Gibbs Sampler to a sample from the exact posterior, and an approach for implementing the Gibbs sampler in nonconjugate situations is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145827252"
                        ],
                        "name": "A. Mira",
                        "slug": "A.-Mira",
                        "structuredName": {
                            "firstName": "Antonietta",
                            "lastName": "Mira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145668185"
                        ],
                        "name": "G. Nicholls",
                        "slug": "G.-Nicholls",
                        "structuredName": {
                            "firstName": "Geoff",
                            "lastName": "Nicholls",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nicholls"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16435140,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "822684f0665aca9ad21bc38ff15752c72f1ee561",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Bridge estimation, as described by Meng and Wong in 1996, is used to estimate the value taken by a probability density at a point in the state space. When the normalisation of the prior density is known, this value may be used to estimate a Bayes factor. It is shown that the multi-block Metropolis-Hastings estimators of Chib and Jeliazkov (2001) are bridge estimators. This identification leads to more efficient estimators for the quantity of interest."
            },
            "slug": "Bridge-estimation-of-the-probability-density-at-a-Mira-Nicholls",
            "title": {
                "fragments": [],
                "text": "Bridge estimation of the probability density at a point"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 181
                            }
                        ],
                        "text": "Despite these problems the estimator has received significant attention in statistics, and has been used for evaluating latent variable models in recent machine learning literature [5, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11408454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ecc5ffeae38689dd2fe6ed4c32a6745744d7641",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively."
            },
            "slug": "Integrating-Topics-and-Syntax-Griffiths-Steyvers",
            "title": {
                "fragments": [],
                "text": "Integrating Topics and Syntax"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work presents a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "For comparison, we also trained square ICA and a mixture of factor analyzers (MFA) using code from [16, 17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16135158,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8d946c3eb1d1db376a89ad9342282163b5ae0930",
            "isKey": false,
            "numCitedBy": 5791,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon's information-theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably."
            },
            "slug": "Fast-and-robust-fixed-point-algorithms-for-analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Fast and robust fixed-point algorithms for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Using maximum entropy approximations of differential entropy, a family of new contrast (objective) functions for ICA enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neal . Annealed importance sampling"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Computing"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "Specifically, the variance of the estimator is an \u03b1-divergence between the distributions [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Divergence measures and message passing. TR-2005-173"
            },
            "venue": {
                "fragments": [],
                "text": "Microsoft Research,"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Evaluating-probabilities-under-high-dimensional-Murray-Salakhutdinov/4341446db90f569e689cddf2b08a5093a5bb83ae?sort=total-citations"
}