{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705268"
                        ],
                        "name": "D. Achlioptas",
                        "slug": "D.-Achlioptas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Achlioptas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Achlioptas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137076"
                        ],
                        "name": "Frank McSherry",
                        "slug": "Frank-McSherry",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "McSherry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank McSherry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Throwing away individual entries [7] or entire rows [8, 9, 10] of the kernel matrix lowers the storage and computational cost of operating on the kernel matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 147
                            }
                        ],
                        "text": "These approximations either preserve the separability of the data [8], or produce good low-rank or sparse approximations of the true kernel matrix [7, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "The evaluation of the kernel function can be sped up using linear random projections [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1821854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5e12dd2604cb7cc9cde165509ca8db764784688",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function k by a \"randomized kernel\" which behaves like k in expectation."
            },
            "slug": "Sampling-Techniques-for-Kernel-Methods-Achlioptas-McSherry",
            "title": {
                "fragments": [],
                "text": "Sampling Techniques for Kernel Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "All three randomized techniques for speeding up Kernel Principal Component Analysis on three levels can be viewed as instantiations of the following idea: replace the kernel function k by a \"randomized kernel\" which behaves like k in expectation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15307365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "174cecadda4ecbcebaf575bc0d5c4b3fd233fd12",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Random projection is a simple technique that has had a number of applications in algorithm design. In the context of machine learning, it can provide insight into questions such as why is a learning problem easier if data is separable by a large margin? and in what sense is choosing a kernel much like choosing a set of features? This talk is intended to provide an introduction to random projection and to survey some simple learning algorithms and other applications to learning based on it. I will also discuss how, given a kernel as a black-box function, we can use various forms of random projection to extract an explicit small feature space that captures much of what the kernel is doing. This talk is based in large part on work in [BB05, BBV04] joint with Nina Balcan and Santosh Vempala."
            },
            "slug": "Random-Projection,-Margins,-Kernels,-and-Blum",
            "title": {
                "fragments": [],
                "text": "Random Projection, Margins, Kernels, and Feature-Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is discussed how, given a kernel as a black-box function, the authors can use various forms of random projection to extract an explicit small feature space that captures much of what the kernel is doing."
            },
            "venue": {
                "fragments": [],
                "text": "SLSFS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103897"
                        ],
                        "name": "Changjiang Yang",
                        "slug": "Changjiang-Yang",
                        "structuredName": {
                            "firstName": "Changjiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changjiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719541"
                        ],
                        "name": "R. Duraiswami",
                        "slug": "R.-Duraiswami",
                        "structuredName": {
                            "firstName": "Ramani",
                            "lastName": "Duraiswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duraiswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Further, the quality of the Hermite or Taylor approximation that these methods rely on degrades exponentially with the dimensionality of the dataset [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5809817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "303600138609133e1cf558fca94514d9c19470d2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The computation and memory required for kernel machines with N training samples is at least O(N2). Such a complexity is significant even for moderate size problems and is prohibitive for large datasets. We present an approximation technique based on the improved fast Gauss transform to reduce the computation to O(N). We also give an error bound for the approximation, and provide experimental results on the UCI datasets."
            },
            "slug": "Efficient-Kernel-Machines-Using-the-Improved-Fast-Yang-Duraiswami",
            "title": {
                "fragments": [],
                "text": "Efficient Kernel Machines Using the Improved Fast Gauss Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An approximation technique based on the improved fast Gauss transform to reduce the computation to O(N) is presented and an error bound for the approximation is given."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2290924"
                        ],
                        "name": "D. Mazzoni",
                        "slug": "D.-Mazzoni",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Mazzoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mazzoni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 234
                            }
                        ],
                        "text": "One way to take advantage of these linear training algorithms for training nonlinear machines is to approximately factor the kernel matrix and to treat the columns of the factor matrix as features in a linear machine (see for example [4])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13351718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e43f51ac91781b18f5cb426db5c8b7cfb3de002f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (and other kernel machines) offer robust modern machine learning methods for nonlinear classification. However, relative to other alternatives (such as linear methods, decision trees and neural networks), they can be orders of magnitude slower at query-time. Unlike existing methods that attempt to speedup querytime, such as reduced set compression (e.g. (Burges, 1996)) and anytime bounding (e.g. (DeCoste, 2002), we propose a new and efficient approach based on treating the kernel machine classifier as a special form of k nearest-neighbor. Our approach improves upon a traditional k-NN by determining at query-time a good k for each query, based on pre-query analysis guided by the original robust kernel machine. We demonstrate effectiveness on high-dimensional benchmark MNIST data, observing a greater than 100- fold reduction in the number of SVs required per query (amortized over all 45 pairwise MNIST digit classifiers), with no extra test errors (in fact, it happens to make 4 fewer)."
            },
            "slug": "Fast-Query-Optimized-Kernel-Machine-Classification-DeCoste-Mazzoni",
            "title": {
                "fragments": [],
                "text": "Fast Query-Optimized Kernel Machine Classification Via Incremental Approximate Nearest Support Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a new and efficient approach based on treating the kernel machine classifier as a special form of k nearest-neighbor by determining at query-time a good k for each query, based on pre-query analysis guided by the original robust kernel machine."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807998"
                        ],
                        "name": "I. Tsang",
                        "slug": "I.-Tsang",
                        "structuredName": {
                            "firstName": "Ivor",
                            "lastName": "Tsang",
                            "middleNames": [
                                "Wai-Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Tsang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145193332"
                        ],
                        "name": "J. Kwok",
                        "slug": "J.-Kwok",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kwok",
                            "middleNames": [
                                "Tin-Yau"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kwok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061914"
                        ],
                        "name": "Pak-Ming Cheung",
                        "slug": "Pak-Ming-Cheung",
                        "structuredName": {
                            "firstName": "Pak-Ming",
                            "lastName": "Cheung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pak-Ming Cheung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "We focus our comparisons against the Core Vector Machine [14] because it was shown in [14] to be both faster and more accurate than other known approaches for training kernel machines, including, in most cases, random sampling of datapoints [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "The experiments were conducted on the five standard large-scale datasets evaluated in [14], excluding the synthetic datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14500006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8608a48f6a5850259afb036b70fd289b4c174cc6",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard SVM training has O(m3) time and O(m2) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such \"approximateness\" in this paper. We first show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efficient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and real-world data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about five million training patterns, in only 1.4 seconds on a 3.2GHz Pentium--4 PC."
            },
            "slug": "Core-Vector-Machines:-Fast-SVM-Training-on-Very-Tsang-Kwok",
            "title": {
                "fragments": [],
                "text": "Core Vector Machines: Fast SVM Training on Very Large Data Sets"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper shows that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry and obtains provably approximately optimal solutions with the idea of core sets, and proposes the proposed Core Vector Machine (CVM) algorithm, which can be used with nonlinear kernels and has a time complexity that is linear in m."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5155714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "427b168f490b56716f22b129ac93aba5425ea08f",
            "isKey": false,
            "numCitedBy": 2113,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "slug": "Training-linear-SVMs-in-linear-time-Joachims",
            "title": {
                "fragments": [],
                "text": "Training linear SVMs in linear time"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n)) for ordinal regression problems and several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738441"
                        ],
                        "name": "P. Drineas",
                        "slug": "P.-Drineas",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Drineas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Drineas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143884206"
                        ],
                        "name": "Michael W. Mahoney",
                        "slug": "Michael-W.-Mahoney",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mahoney",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael W. Mahoney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215012,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "11e6b5a30a921e6028662105148fac41a76f0500",
            "isKey": false,
            "numCitedBy": 935,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O(n 3 ), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n x n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the form G k = CW + k C T , where C is a matrix consisting of a small number c of columns of G and W k is the best rank-k approximation to W, the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let \u2225.\u2225 2 and \u2225.\u2225 F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let G k be the best rank-k approximation to G. We prove that by choosing O(k/\u2208 4 ) columns \u2225G-CW + k C T \u2225 \u03be \u2264 \u2225G - G k \u2225 \u03be + \u2208 n \u03a3 i=1 G 2 ii , both in expectation and with high probability, for both \u03be = 2, F, and for all k: 0 < k < rank(W). This approximation can be computed using O(n) additional space and time, after making two passes over the data from external storage."
            },
            "slug": "On-the-Nystr\u00f6m-Method-for-Approximating-a-Gram-for-Drineas-Mahoney",
            "title": {
                "fragments": [],
                "text": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm to compute an easily-interpretable low-rank approximation to an n x n Gram matrix G such that computations of interest may be performed more rapidly."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1530308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8aaff7e62c4ce8724298a276a56ac519c804bb74",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) problem. This paper proposes an algorithm for training SVMs: Sequential Minimal Optimization, or SMO. SMO breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable. Thus, SMO does not require a numerical QP library. SMO's computation time is dominated by evaluation of the kernel, hence kernel optimizations substantially quicken SMO. For the MNIST database, SMO is 1.7 times as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be 1500 times faster than the PCG chunking algorithm."
            },
            "slug": "Using-Analytic-QP-and-Sparseness-to-Speed-Training-Platt",
            "title": {
                "fragments": [],
                "text": "Using Analytic QP and Sparseness to Speed Training of Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm for training SVMs: Sequential Minimal Optimization, or SMO, which breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable and does not require a numerical QP library."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685767"
                        ],
                        "name": "Ga\u00eblle Loosli",
                        "slug": "Ga\u00eblle-Loosli",
                        "structuredName": {
                            "firstName": "Ga\u00eblle",
                            "lastName": "Loosli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ga\u00eblle Loosli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794818"
                        ],
                        "name": "S. Canu",
                        "slug": "S.-Canu",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Canu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Canu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "Even on the original ordering, perturbing the CVM\u2019s regularization parameter by a mere 15% yields 49% error rate on the test set [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11023643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8acdddd4b43099f897620bd7682b9c4a74bfb54e",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recently published paper in JMLR, Tsang et al. (2005) present an algorithm for SVM called Core Vector Machines (CVM) and illustrate its performances through comparisons with other SVM solvers. After reading the CVM paper we were surprised by some of the reported results. In order to clarify the matter, we decided to reproduce some of the experiments. It turns out that to some extent, our results contradict those reported. Reasons of these different behaviors are given through the analysis of the stopping criterion."
            },
            "slug": "Comments-on-the-\"Core-Vector-Machines:-Fast-SVM-on-Loosli-Canu",
            "title": {
                "fragments": [],
                "text": "Comments on the \"Core Vector Machines: Fast SVM Training on Very Large Data Sets\""
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It turns out that to some extent, the results contradict those reported in the CVM paper, and some of the experiments are reproduced to clarify the matter."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416039"
                        ],
                        "name": "Yirong Shen",
                        "slug": "Yirong-Shen",
                        "structuredName": {
                            "firstName": "Yirong",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yirong Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Fast nearest neighbor lookup with KD-Trees has been used to approximate multiplication with the kernel matrix, and in turn, a variety of other operations [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1190939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb00902d67f2fe224721b8a54c816e72883d28d2",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The computation required for Gaussian process regression with n training examples is about O(n3) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression."
            },
            "slug": "Fast-Gaussian-Process-Regression-using-KD-Trees-Shen-Ng",
            "title": {
                "fragments": [],
                "text": "Fast Gaussian Process Regression using KD-Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5106399"
                        ],
                        "name": "M. Ferris",
                        "slug": "M.-Ferris",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ferris",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ferris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882062"
                        ],
                        "name": "T. Munson",
                        "slug": "T.-Munson",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Munson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Munson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 126
                            }
                        ],
                        "text": "On the other hand, linear machines can be trained very quickly on large datasets when the dimensionality of the data is small [1, 2, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13563302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20f6ac9427d700e48a5025c9c43e5b6d20a2a79d",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of interior-point methods for solving quadratic programming problems with a small number of linear constraints, where the quadratic term consists of a low-rank update to a positive semidefinite matrix. Several formulations of the support vector machine fit into this category. An interesting feature of these particular problems is the volume of data, which can lead to quadratic programs with between 10 and 100 million variables and, if written explicitly, a dense Q matrix. Our code is based on OOQP, an object-oriented interior-point code, with the linear algebra specialized for the support vector machine application. For the targeted massive problems, all of the data is stored out of core and we overlap computation and input/output to reduce overhead. Results are reported for several linear support vector machine formulations demonstrating that the method is reliable and scalable."
            },
            "slug": "Interior-Point-Methods-for-Massive-Support-Vector-Ferris-Munson",
            "title": {
                "fragments": [],
                "text": "Interior-Point Methods for Massive Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work investigates the use of interior-point methods for solving quadratic programming problems with a small number of linear constraints, where the quadratics term consists of a low-rank update to a positive semidefinite matrix."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "We focus our comparisons against the Core Vector Machine [14] because it was shown in [14] to be both faster and more accurate than other known approaches for training kernel machines, including, in most cases, random sampling of datapoints [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 52
                            }
                        ],
                        "text": "Throwing away individual entries [7] or entire rows [8, 9, 10] of the kernel matrix lowers the storage and computational cost of operating on the kernel matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "These approximations either preserve the separability of the data [8], or produce good low-rank or sparse approximations of the true kernel matrix [7, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11506680,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "eeeec164d5a5501bd76b71828378646836511483",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Random projection is a powerful technique behind compressive sensing and matrix completion. When n points in some high dimensional space are randomly projected down to O(log n 2) dimensions, with large probability the pairwise squared distances between the points change by a factor of no more than 1 \u00b1. Note the original dimensionality does not matter. This is a statistical property based on concentration of measure. It is useful as an efficient dimension reduction tool. The following theorem considers projecting a random vector onto a fixed subspace, which is equivalent to projecting a fixed vector onto a random subspace. Lemma 1 Let Y \u2208 R d be chosen uniformly from the surface of the d-dimensional sphere. Let Z = (Y 1 , Y 2 ,. .. , Y k) be the projection onto the first k coordinates, where k < d. Then for any \u03b1 < 1 and \u03b2 > 1, P r d k Z 2 \u2264 \u03b1 \u2264 exp k 2 (1 \u2212 \u03b1 + log \u03b1) (1) P r d k Z 2 \u2265 \u03b2 \u2264 exp k 2 (1 \u2212 \u03b2 + log \u03b2). (2) With this, one can prove the Johnson-Lindenstrauss Lemma."
            },
            "slug": "Random-Projection-Zhu",
            "title": {
                "fragments": [],
                "text": "Random Projection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The following theorem considers projecting a random vectors onto a fixed subspace, which is equivalent to projecting a fixed vector onto a random sub space, and proves the Johnson-Lindenstrauss Lemma."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709874"
                        ],
                        "name": "A. Frieze",
                        "slug": "A.-Frieze",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Frieze",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Frieze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144632403"
                        ],
                        "name": "R. Kannan",
                        "slug": "R.-Kannan",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kannan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kannan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 52
                            }
                        ],
                        "text": "Throwing away individual entries [7] or entire rows [8, 9, 10] of the kernel matrix lowers the storage and computational cost of operating on the kernel matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 147
                            }
                        ],
                        "text": "These approximations either preserve the separability of the data [8], or produce good low-rank or sparse approximations of the true kernel matrix [7, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2483891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbc531f6ca5e83c6fa507bdf6399ecf76ef2e614",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In several applications, the data consists of an m/spl times/n matrix A and it is of interest to find an approximation D of a specified rank k to A where, k is much smaller than m and n. Traditional methods like the Singular Value Decomposition (SVD) help us find the \"best\" such approximation. However, these methods take time polynomial in m, n which is often too prohibitive. In this paper, we develop an algorithm which is qualitatively faster provided we may sample the entries of the matrix according to a natural probability distribution. Indeed, in the applications such sampling is possible. Our main result is that we can find the description of a matrix D* of rank at most k so that /spl par/A-D*/spl par//sub F//spl les/min/D,rank(D)/spl les/k/spl par/A-D/spl par//sub F/+/spl epsiv//spl par/A/spl par//sub F/ holds with probability at least 1-/spl delta/. (For any matrix M, /spl par/M/spl par//sub F//sup 2/ denotes the sum of the squares of all the entries of M.) The algorithm takes time polynomial in k, 1//spl epsiv/, log(1//spl delta/) only, independent of m, n."
            },
            "slug": "Fast-Monte-Carlo-algorithms-for-finding-low-rank-Frieze-Kannan",
            "title": {
                "fragments": [],
                "text": "Fast Monte-Carlo algorithms for finding low-rank approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper develops an algorithm which is qualitatively faster provided the entries of the matrix are sampled according to a natural probability distribution and the algorithm takes time polynomial in k, 1//spl epsiv/, log(1//spl delta/) only, independent of m, n."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 39th Annual Symposium on Foundations of Computer Science (Cat. No.98CB36280)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145658292"
                        ],
                        "name": "Andrew Cotter",
                        "slug": "Andrew-Cotter",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Cotter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Cotter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 126
                            }
                        ],
                        "text": "On the other hand, linear machines can be trained very quickly on large datasets when the dimensionality of the data is small [1, 2, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53306004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9691f67f5075bde2fd70da0135a4a70f25ef042b",
            "isKey": false,
            "numCitedBy": 2003,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\\epsilon}$$ is $${\\tilde{O}(1 / \\epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\\Omega(1 / \\epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/\u03bb, where \u03bb is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\\tilde{O}(d/(\\lambda \\epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods."
            },
            "slug": "Pegasos:-primal-estimated-sub-gradient-solver-for-Shalev-Shwartz-Singer",
            "title": {
                "fragments": [],
                "text": "Pegasos: primal estimated sub-gradient solver for SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines, which is particularly well suited for large text classification problems, and demonstrates an order-of-magnitude speedup over previous SVM learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755208"
                        ],
                        "name": "F. Cucker",
                        "slug": "F.-Cucker",
                        "structuredName": {
                            "firstName": "Felipe",
                            "lastName": "Cucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Cucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34911188"
                        ],
                        "name": "S. Smale",
                        "slug": "S.-Smale",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8188805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b2dd79083a74699e4e0509ac3f0a8a302b4eabe",
            "isKey": false,
            "numCitedBy": 1461,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "(1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In"
            },
            "slug": "On-the-mathematical-foundations-of-learning-Cucker-Smale",
            "title": {
                "fragments": [],
                "text": "On the mathematical foundations of learning"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference) and relations of the theory of learning to the mainstream of mathematics are emphasized."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50012345"
                        ],
                        "name": "W. Rudin",
                        "slug": "W.-Rudin",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Rudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rudin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "The following classical theorem from harmonic analysis provides the key insight behind this transformation: Theorem 1 (Bochner [15])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120596714,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7838d046f296235cb0bbab0a190d539e8debb25a",
            "isKey": false,
            "numCitedBy": 2292,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the late 1950s, many of the more refined aspects of Fourier analysis were transferred from their original settings (the unit circle, the integers, the real line) to arbitrary locally compact abelian (LCA) groups. Rudin's book, published in 1962, was the first to give a systematic account of these developments and has come to be regarded as a classic in the field. The basic facts concerning Fourier analysis and the structure of LCA groups are proved in the opening chapters, in order to make the treatment relatively self-contained."
            },
            "slug": "Fourier-Analysis-on-Groups-Rudin",
            "title": {
                "fragments": [],
                "text": "Fourier Analysis on Groups"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 52
                            }
                        ],
                        "text": "Throwing away individual entries [7] or entire rows [8, 9, 10] of the kernel matrix lowers the storage and computational cost of operating on the kernel matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 147
                            }
                        ],
                        "text": "These approximations either preserve the separability of the data [8], or produce good low-rank or sparse approximations of the true kernel matrix [7, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9308117,
            "fieldsOfStudy": [],
            "id": "bbc531f6ca5e83c6fa507bdf6399ecf76ef2e614",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Monte-Carlo algorithms for finding low-rank approximations"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 39th Annual Symposium on Foundations of Computer Science (Cat. No.98CB36280)"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 160
                            }
                        ],
                        "text": "These methods iteratively update a subset of the kernel machine\u2019s coefficients using coordinate ascent until KKT conditions are satisfied to within a tolerance [5, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using sparseness and analytic QP to speed training of Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems (NIPS)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "The feature map we present in Section 4 is reminiscent of KD-trees in that it partitions the input space using multi-resolution axis-aligned grids similar to those developed in [13] for embedding linear assignment problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "The probability that two points x and y fall in the same bin in this grid is max ( 0, 1\u2212 |x\u2212y| \u03b4 ) [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast image retrieval via embeddings"
            },
            "venue": {
                "fragments": [],
                "text": "International Workshop on Statistical and Computational Theories of Vision"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Random-Features-for-Large-Scale-Kernel-Machines-Rahimi-Recht/7a59fde27461a3ef4a21a249cc403d0d96e4a0d7?sort=total-citations"
}