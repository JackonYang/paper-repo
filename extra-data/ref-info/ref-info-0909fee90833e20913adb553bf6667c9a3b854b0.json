{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There are several existing schemes for decomposing the larger problem of wrapping websites into a series of binary extraction problems [22,  14 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17582165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53880036fb85cc737103c480c613e1912c416010",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an extensible architecture which allows wrapper-learning systems to be easily constructed and tuned. In this architecture the bias of the wrapper-learning system is encoded as an ordered set of \u201cbuilders\u201d, each associated with some restricted extraction language L. To implement a new builder it is only necessary to implement a small set of core operations for L. Builders can also be constructed by combining other builders. A single master learning algorithm which invokes the builders handles most of the real work of learning. The learning system described here is fully implemented, and is part of an \u201cindustrial-strength\u201d wrapper-learning system which has been used to extract job postings from more than 500 sites."
            },
            "slug": "A-structured-wrapper-induction-system-for-from-Cohen",
            "title": {
                "fragments": [],
                "text": "A structured wrapper induction system for extracting information from semi-structured documents"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An extensible architecture which allows wrapper-learning systems to be easily constructed and tuned is proposed, and is part of an \u201cindustrial-strength\u201d wrapper- learning system which has been used to extract job postings from more than 500 sites."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113533823"
                        ],
                        "name": "Wei Fan",
                        "slug": "Wei-Fan",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Fan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15679108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e7cfed8815cce163efac9d17b1109849c050c6b",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Page-Independent-Heuristics-for-Extracting-Cohen-Fan",
            "title": {
                "fragments": [],
                "text": "Learning Page-Independent Heuristics for Extracting Data from Web Pages"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lesion studies show that the more exotic builders do indeed improve performance on complex wrapper-learning tasks, and experiments on artificial data suggest that the system has broader coverage and a faster learning rate than two earlier wrapper-learning systems, WEIN [ 16 ] and STALKER [21, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "WEIN [ 16 ] is an earlier wrapper-learning system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Wrapper learning is the problem of learning website wrappers from examples [ 16 , 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11075952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f052f40a3307de1e45e11a3007a7552b36ebfc8",
            "isKey": true,
            "numCitedBy": 641,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wrapper-induction:-Efficiency-and-expressiveness-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Wrapper induction: Efficiency and expressiveness"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 278
                            }
                        ],
                        "text": "Lesion studies show that the more exotic builders do indeed improve performance \non complex wrapper-learning tasks, and experiments on arti.cial data suggest that the system has broader \ncoverage and a faster learning rate than two earlier wrapper-learning systems, WEIN [16] and STALKER \n[21, 22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 109
                            }
                        ],
                        "text": "S1 Okra 3335 46 1 1 S2 BigBook 4299 \n274 8 6 S3 AddressFinder 57  1 S4 QuoteServer 22  4 Table 1: Comparison of STALKER, WEIN, and WL2 \n."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "STALKER [21] is a wrapper-learning system \nwhich learns wrappers expressed as landmark au\u00adtomata ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 27
                            }
                        ],
                        "text": "Note that neither WIEN nor STALKER successfully learns \nwrap\u00adpers for problems S3 and S4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "STALKER [21] is a wrapper-learning system which learns wrappers expressed as \u201clandmark automata\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "The value of k shown in the column labeled STALKER( ) of Table 1 shows the number \nof examples required for STALKER to achieve 97% accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[21] provide a detailed comparison of STALKER and WIEN on a set of four sample wrapperlearning problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 287
                            }
                        ],
                        "text": "Lesion studies show that the more exotic builders do indeed improve performance on complex wrapper-learning tasks, and experiments on artificial data suggest that the system has broader coverage and a faster learning rate than two earlier wrapper-learning systems, WEIN [16] and STALKER [21, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "In the experiments of Muslea et al., STALKER is repeat\u00adedly run on a sample of k labeled records, \nfor k =1,2,..,10, and then tested on all remaining labeled records."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 52
                            }
                        ],
                        "text": "Muslea et al. [21] provide a detailed comparison of STALKER \nand WIEN on a set of four sample wrapper\u00adlearning problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "STALKER( ) WL2("
                    },
                    "intents": []
                }
            ],
            "corpusId": 3514097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cc263c84b85027164bd39db169f5d5959ef6822",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques."
            },
            "slug": "A-hierarchical-approach-to-wrapper-induction-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "A hierarchical approach to wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can handle information sources that could not be wrapped by existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6286860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddc502b6c0d08fefe5b77639e4737cd8c7bce25c",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present general-purpose methods for recognizing certain types of structure in HTML documents. The methods are implemented using WHIRL, a \"soft\" logic that incorporates a notion of textual similarity developed in the information retrieval community. In an experimental evaluation on 82 Web pages, the structure ranked first by our method is \"meaningful\"--i.e., a structure that was used in a hand-coded \"wrapper\", or extraction program, for the page-nearly 70% of the time. This improves on a value of 50% obtained by an earlier method. With appropriate background information, the structure-recognition methods we describe can also be used to learn a wrapper from examples, or for maintaining a wrapper as a Web page changes format. In these settings, the top-ranked structure is meaningful nearly 85% of the time."
            },
            "slug": "Recognizing-Structure-in-Web-Pages-using-Similarity-Cohen",
            "title": {
                "fragments": [],
                "text": "Recognizing Structure in Web Pages using Similarity Queries"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The structure-recognition methods described can be used to learn a wrapper from examples, or for maintaining a wrapper as a Web page changes format, and in these settings, the top-ranked structure is meaningful nearly 85% of the time."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of recent extraction systems work by generating and classifying candidate spans (e.g., [9,  10 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17531530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce2347d8fbeb3689a989ee13e7e0b4da50a6994a",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in machine learning for information extraction has focused on two distinct sub-problems: the conventional problem of filling template slots from natural language text, and the problem of wrapper induction, learning simple extraction procedures (\u201cwrappers\u201d) for highly structured text such as Web pages produced by CGI scripts. For suitably regular domains, existing wrapper induction algorithms can efficiently learn wrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. Boosting is a technique for improving the performance of a simple machine learning algorithm by repeatedly applying it to the training set with different example weightings. We describe an algorithm that learns simple, low-coverage wrapper-like extraction patterns, which we then apply to conventional information extraction problems using boosting. The result is BWI, a trainable information extraction system with a strong precision bias and F1 performance better than state-of-the-art techniques in many domains."
            },
            "slug": "Boosted-Wrapper-Induction-Freitag-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Boosted Wrapper Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes an algorithm that learns simple, low-coverage wrapper-like extraction patterns, which it then applies to conventional information extraction problems using boosting, resulting in BWI, a trainable information extraction system with a strong precision bias and F1 performance better than state-of-the-art techniques in many domains."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of recent extraction systems work by generating and classifying candidate spans (e.g., [ 9 , 10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16677640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29c99d263b5e05aae6bb96f004f025dcc9b5caae",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction IE is the problem of lling out pre de ned structured sum maries from text documents We are in terested in performing IE in non traditional domains where much of the text is often ungrammatical such as electronic bulletin board posts and Web pages We suggest that the best approach is one that takes into ac count many di erent kinds of information and argue for the suitability of a multistrat egy approach We describe learners for IE drawn from three separate machine learning paradigms rote memorization term space text classi cation and relational rule induc tion By building regression models mapping from learner con dence to probability of cor rectness and combining probabilities appro priately it is possible to improve extraction accuracy over that achieved by any individ ual learner We describe three di erent mul tistrategy approaches Experiments on two IE domains a collection of electronic seminar announcements from a university computer science department and a set of newswire ar ticles describing corporate acquisitions from the Reuters collection demonstrate the e ec tiveness of all three approaches"
            },
            "slug": "Multistrategy-Learning-for-Information-Extraction-Freitag",
            "title": {
                "fragments": [],
                "text": "Multistrategy Learning for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is possible to improve extraction accuracy over that achieved by any individ ual learner by building regression models mapping from learner con dence to probability of cor rectness and combining probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729294"
                        ],
                        "name": "Boris Chidlovskii",
                        "slug": "Boris-Chidlovskii",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Chidlovskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Chidlovskii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10802500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c33cb402b17404106b78d2e004ae989ca61d68b3",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern agent and mediator systems communicate to a multitude of Web information providers to better satisfy t he user requests. They use wrappers to extract relevant informatio n fr m HTML pages and annotate it with user-defined labels. A number of approaches exploit the regularity in page structures to i nduce instances of wrapper classes. The power of a class is crucial; a more powerful class permits to successfully wrap more sites. In t his work, we use the grammatical inference theory to develop a powerfu l w apper class based on the k-reversible grammars. We also address the sample labeling problem and show how the label conflicts can m ke the wrapper inference impossible. We propose the label norm alization method in order to discard the label conflicts and induce partial wrappers."
            },
            "slug": "Wrapper-generation-by-reversible-grammar-induction-Chidlovskii",
            "title": {
                "fragments": [],
                "text": "Wrapper generation by -reversible grammar induction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses the grammatical inference theory to develop a powerfu l w apper class based on the k-reversible grammars, and proposes the label norm alization method in order to discard the label conflicts and induce partial wrappers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is advantageous in wrapper-learning since when a site changes format, it is usually the case that old rules will simply fail to extract any data; this simplifies the process of \u201cregression testing\u201d for wrappers [ 15 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5876617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d060dec4bf295ba459b197288e05807bbd5c51a5",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on Internet information integration assumes a library of wrappers, specialized information extraction procedures. Maintaining wrappers is difficult, because the formatting regularities on which they rely often change. The wrapper verification problem is to determine whether a wrapper is correct. Standard regression testing approaches are inappropriate, because both the formatting regularities and a site's underlying content may change. Wei ntroduce RAPTURE, a fully-implemented, domain-independenvt erification algorithm. RAPTURE uses well-motivated heuristics to compute the similarity between a wrapper's expected and observed output. Experiments with 27 actual Internet sites show a substantial performance improvement over standard regression testing."
            },
            "slug": "Regression-testing-for-wrapper-maintenance-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Regression testing for wrapper maintenance"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "RAPTURE is a fully-implemented, domain-independenvt erification algorithm that uses well-motivated heuristics to compute the similarity between a wrapper's expected and observed output."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2801554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "855eaab4505fea76ab21402839670a483e0ae339",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of simpler extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER requires up to two orders of magnitude fewer examples than other algorithms. Furthermore, STALKER can wrap information sources that could not be wrapped by existing inductive techniques."
            },
            "slug": "Hierarchical-Wrapper-Induction-for-Semistructured-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "Hierarchical Wrapper Induction for Semistructured Information Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can wrap information sources that could not be wrapped by existing inductive techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Autonomous Agents and Multi-Agent Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34607455"
                        ],
                        "name": "Chun-Nan Hsu",
                        "slug": "Chun-Nan-Hsu",
                        "structuredName": {
                            "firstName": "Chun-Nan",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Nan Hsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7276869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52a3bc5971752b7bd66b63dad1f0040e5b4540d2",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents SoftMealy, a novel Web wrapper representation formalism. This representation is based on a finite-state transducer (FST) and contextual rules, which allow a wrapper to wrap semistructured Web pages containing missing attributes, multiple attribute values, variant attribute permutations, exceptions and typos, the features that no previous work can handle. A SoftMealy wrapper can be learned from labeled example items using a simple induction algorithm. Learnability analysis shows that SoftMealy scales well with the number of attributes and the number of different attribute permutations. Experimental results show that the learning algorithm can learn correct wrappers for a wide range of Web pages with a handful of examples and generalize well over unseen pages and structural patterns."
            },
            "slug": "Initial-Results-on-Wrapping-Semistructured-Web-with-Hsu",
            "title": {
                "fragments": [],
                "text": "Initial Results on Wrapping Semistructured Web Pages with Finite-State Transducers and Contextual Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that the learning algorithm can learn correct wrappers for a wide range of Web pages with a handful of examples and generalize well over unseen pages and structural patterns."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1883948"
                        ],
                        "name": "A. Sahuguet",
                        "slug": "A.-Sahuguet",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Sahuguet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sahuguet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3181471"
                        ],
                        "name": "Fabien Azavant",
                        "slug": "Fabien-Azavant",
                        "structuredName": {
                            "firstName": "Fabien",
                            "lastName": "Azavant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabien Azavant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This representation is used by a handful of wrapper learning systems [7, 6] and many wrapper programming languages (e.g., [ 27 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7252520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba1b6020d376dd28d4b1d3598c16e9477f379113",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web has become a major conduit to information repositories of all kinds. Today, more than 80% of information published on the Web is generated by underlying databases (however access is granted through a Web gateway using forms as a query language and HTML as a display vehicle) and this proportion keeps increasing. But Web data sources also consist of standalone HTML pages hand-coded by individuals, that provide very useful information such as reviews, digests, links, etc. As for the information that also exists in underlying databases, the HTML interface is often the only one available for many would-be clients."
            },
            "slug": "Building-Light-Weight-Wrappers-for-Legacy-Web-Using-Sahuguet-Azavant",
            "title": {
                "fragments": [],
                "text": "Building Light-Weight Wrappers for Legacy Web Data-Sources Using W4F"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The Web has become a major conduit to information repositories of all kinds, but Web data sources also consist of standalone HTML pages hand-coded by individuals, that provide very useful information such as reviews, digests, links, etc."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729294"
                        ],
                        "name": "Boris Chidlovskii",
                        "slug": "Boris-Chidlovskii",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Chidlovskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Chidlovskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707113"
                        ],
                        "name": "Jon Ragetli",
                        "slug": "Jon-Ragetli",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Ragetli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Ragetli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696030"
                        ],
                        "name": "M. de Rijke",
                        "slug": "M.-de-Rijke",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "de Rijke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. de Rijke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11329852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eae95b854e459fd910cd0812d2052c9b3ce02b23",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "To facilitate effective search on the World Wide Web, meta search engines have been developed which do not search the Web themselves, but use available search engines to find the required information. By means of wrappers, meta search engines retrieve information from the pages returned by search engines. We present an approach to automatically create such wrappers by means of an incremental grammar induction algorithm. The algorithm uses an adaptation of the string edit distance. Our method performs well; it is quick, can be used for several types of result pages and requires a minimal amount of user interaction."
            },
            "slug": "Wrapper-Generation-via-Grammar-Induction-Chidlovskii-Ragetli",
            "title": {
                "fragments": [],
                "text": "Wrapper Generation via Grammar Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents an approach to automatically create wrappers by means of an incremental grammar induction algorithm that uses an adaptation of the string edit distance to create such wrappers."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21897,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 122
                            }
                        ],
                        "text": "In particular, the approach of defining bias via a set of builders is reminiscent of earlier ILP work in declarative bias [5, 1]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18553524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b9c3f82a0c0fd62f8ae527126b118890cfd452d",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Grammatically-Biased-Learning:-Learning-Logic-Using-Cohen",
            "title": {
                "fragments": [],
                "text": "Grammatically Biased Learning: Learning Logic Programs Using an Explicit Antecedent Description Language"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145501780"
                        ],
                        "name": "Matthew F. Hurst",
                        "slug": "Matthew-F.-Hurst",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hurst",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew F. Hurst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": ") For more detailed discussion refer to Hurst [13] or Wang [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 102
                            }
                        ],
                        "text": "A Flexible Learning System for Wrapping Tables and Lists in HTML Documents * William W. Cohen Matthew \nHurst Lee S. Jensen WhizBang Labs WhizBang Labs WhizBang Labs 4616 Henry Street 4616 Henry Street 3210 \nN. Canyon Road Pittsburgh, PA 15213 Pittsburgh, PA 15213 Provo, UT 84604 wcohen@whizbang.com mhurst@whizbang.com \nlee.jensen@whizbang.com ABSTRACT A program that makes an existing website look like a database is called \na wrapper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 38
                            }
                        ],
                        "text": "For more detailed discussion refer to Hurst [13] or Wang [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "[13] M. Hurst."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5481713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ba28bb5c79f9115b3f3b62593feedf1d73b3027",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis looks at the issues relating to the development of technology capable of processing tables as they appear in textual documents so that their contents may be accessed and further interpreted by standard information extraction and natural language processing systems. The thesis offers a formal description of the table and the description and evaluation of a system which provides instances of that model for table examples. There are three parts to the thesis. The first looks at tables in general terms, suggests where their complexities are to be found, and reviews the literature dealing with research into tables in other fields. The second part introduces a layered model of the table and provides some notational equipment for encoding tables in these component layers. The final part discusses the design, implementation and evaluation of a system which produces an instance of the model for the tables found in a document. It also discusses the design and collection of a corpus of tables used for the training and evaluation of the system. The thesis catalogues a laxge number of phenomena discovered in the corpus collected during the research and provides appropriate terminology."
            },
            "slug": "The-interpretation-of-tables-in-texts-Hurst",
            "title": {
                "fragments": [],
                "text": "The interpretation of tables in texts"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This thesis looks at the issues relating to the development of technology capable of processing tables as they appear in textual documents so that their contents may be accessed and further interpreted by standard information extraction and natural language processing systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2992420"
                        ],
                        "name": "D. Raggett",
                        "slug": "D.-Raggett",
                        "structuredName": {
                            "firstName": "Dave",
                            "lastName": "Raggett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Raggett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59905338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55b0a1558dcb30d41e408fe2196139d3e1b5875b",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "When editing HTML it's easy to make mistakes. Wouldn't it be nice if there was a simple way to fix these mistakes automatically and tidy up sloppy editing into nicely layed out markup? Well now there is! Dave Raggett's HTML TIDY is a free utility for doing just that. It also works great on the atrociously hard to read markup generated by specialized HTML editors and conversion tools, and can help you identify where you need to pay further attention on making your pages more accessible to people with disabilities. Tidy is able to fix up a wide range of problems and to bring to your attention things that you need to work on yourself. Each item found is listed with the line number and column so that you can see where the problem lies in your markup. Tidy won't generate a cleaned up version when there are problems that it can't be sure of how to handle. These are logged as \"errors\" rather than \"warnings\". Dave Raggett has now passed the baton for maintaining Tidy to a group of volunteers working together as part of the open source community at Source Forge. The source code continues to be available under an open source license, and you are encouraged to pass on bug reports and enhancement requests at http://tidy.sourceforge.net. If you find HTML Tidy useful and you would like to say thanks, then please send me a (paper) postcard or other souvenir from the area in which you live along with a few words on what you are using Tidy for. It will be fun to map out where Tidy users are to be found! My postal address is given at the end of this file. The W3C public email list devoted to HTML Tidy is: <html-tidy@w3.org>. To subscribe send an email to html-tidy-request@w3.org with the word subscribe in the subject line (include the word unsubscribe if you want to unsubscribe). The archive for this list is accessible online. If you would like to contact the developers, or you just want to submit an enhancement request or a bug report, please visit http://tidy.sourceforge.net. Tidy can now perform wonders on HTML saved from Microsoft Word 2000! Word bulks out HTML files with stuff for round-tripping presentation between HTML and Word. If you are more concerned about using HTML on the Web, check out Tidy's \"Word-2000\" config option! Of course Tidy does a good job on Word'97 files as well! Tidy features in an article by Scott Nesbitt on webreview.com, and more recently on Dave Central's Best of Linux, and as tool of the month on Unix Review by Joe Brockmeier, who writes: \"One thing I love about the UNIX philosophy is the idea that each program should do one job and do it really well. There are zillions of small tools for UNIX-type OSes that make life much easier and are hugely useful, but they don't necessarily get written about. They certainly don't receive the same kind of coverage that Apache and Sendmail receive. One of my favorites, HTML Tidy, is a tool for HTML/Web development that I think will interest a lot of folks. HTML Tidy cleans up HTML produced by WYSIWYG editors and such.\" Tidy is available as a downloadable binary, as source code (ANSI C), or as an online service at W3C, Info Network, HTML Help's site Valet and other sites."
            },
            "slug": "Clean-Up-Your-Web-Pages-with-HTML-TIDY-Raggett",
            "title": {
                "fragments": [],
                "text": "Clean Up Your Web Pages with HTML TIDY"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398564680"
                        ],
                        "name": "R. Cameron-Jones",
                        "slug": "R.-Cameron-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Cameron-Jones",
                            "middleNames": [
                                "Mike"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cameron-Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "There are several \ndi.erences between this learning algo\u00adrithm and FOIL."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "The algorithm is based on FOIL [24, 26] and learns \na DNF expression, the primitive elements of which are predicates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Builders are also used to gener\u00adate primitive predicates in the learnConjunction \nfunction, instead of instead of testing all possible primitive predi\u00adcates as FOIL does."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "FOIL: A midterm report."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "The pred\u00adicate choices \nmade in the inner loop are guided by the same information-gain metric used in FOIL."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 31
                            }
                        ],
                        "text": "The algorithm is based on FOIL [24, 26] and learns a DNF expression, the primitive elements of which are predicates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "This is useful \nsince there are some languages that are di.cult to learn using FOIL s top-down approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "As in FOIL, the outer loop of the learning \nalgorithm (the learnPredicate function) is a set\u00adcovering algorithm, which repeatedly learns a single \nrule p (actually a conjunction of builder-produced predicates) that covers some positive data from the \ntraining set, and then re\u00admoves the data covered by p."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16455032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2cf9ae81b38f7637899f19274226e623f9b153b",
            "isKey": true,
            "numCitedBy": 667,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "FOIL is a learning system that constructs Horn clause programs from examples. This paper summarises the development of FOIL from 1989 up to early 1993 and evaluates its effectiveness on a non-trivial sequence of learning tasks taken from a Prolog programming text. Although many of these tasks are handled reasonably well, the experiment highlights some weaknesses of the current implementation. Areas for further research are identified."
            },
            "slug": "FOIL:-A-Midterm-Report-Quinlan-Cameron-Jones",
            "title": {
                "fragments": [],
                "text": "FOIL: A Midterm Report"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper summarises the development of FOIL from 1989 up to early 1993 and evaluates its effectiveness on a non-trivial sequence of learning tasks taken from a Prolog programming text."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 106
                            }
                        ],
                        "text": "We explored several learning \nalgorithms including multino\u00admial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision \ntree learner modeled after C4.5 [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 113
                            }
                        ],
                        "text": "We explored several learning algorithms including multinomial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision tree learner modeled after C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 14
                            }
                        ],
                        "text": "Of these, the Winnow classi.er performs the best with a precision \nof 1.00, a recall 0.922, and an F-measure of 0.959.4  4.3.2 Exploiting Table Context Table classi.cation \nis not only the .rst step in table pro\u00adcessing: it is also useful in itself."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dace286582d91916fe470d08f30381cf453f20",
            "isKey": true,
            "numCitedBy": 1612,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space."
            },
            "slug": "Learning-Quickly-When-Irrelevant-Attributes-Abound:-Littlestone",
            "title": {
                "fragments": [],
                "text": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7510969"
                        ],
                        "name": "S. DeRose",
                        "slug": "S.-DeRose",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "DeRose",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. DeRose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69034572"
                        ],
                        "name": "I. Corp",
                        "slug": "I.-Corp",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Corp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Corp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59749372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5bec75eec18c27c58baf2e97bc4feb967f70bf0",
            "isKey": false,
            "numCitedBy": 1984,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "XPath is a language for addressing parts of an XML document, designed to be used by both XSLT and XPointer. Status of this document This document has been reviewed by W3C Members and other interested parties and has been endorsed by the Director as a W3C Recommendation. It is a stable document and may be used as reference material or cited as a normative reference from other documents. W3C's role in making the Recommendation is to draw attention to the specification and to promote its widespread deployment. This enhances the functionality and interoperability of the Web. XML Path Language (XPath) http://www.w3.org/TR/1999/REC-xpath-19991116 (1 of 30) [7/19/2001 5:31:03 PM] The list of known errors in this specification is available at http://www.w3.org/1999/11/REC-xpath-19991116-errata. Comments on this specification may be sent to www-xpath-comments@w3.org; archives of the comments are available. The English version of this specification is the only normative version. However, for translations of this document, see http://www.w3.org/Style/XSL/translations.html. A list of current W3C Recommendations and other technical documents can be found at http://www.w3.org/TR. This specification is joint work of the XSL Working Group and the XML Linking Working Group and so is part of the W3C Style activity and of the W3C XML activity."
            },
            "slug": "XML-Path-Language-(XPath)-Version-1.0-DeRose-Corp",
            "title": {
                "fragments": [],
                "text": "XML Path Language (XPath) Version 1.0"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "XPath is a language for addressing parts of an XML document, designed to be used by both XSLT and XPointer, and has been endorsed by the Director as a W3C Recommendation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108023096"
                        ],
                        "name": "Xinxin Wang",
                        "slug": "Xinxin-Wang",
                        "structuredName": {
                            "firstName": "Xinxin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinxin Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16895319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb8327c5b091ea26e42ed924e25a02c564998f19",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation investigates the composition of high-quality tables with the use of electronic tools. A generic model is designed to support the different stages of tabular composition, including the editing of logical structure, the specification of layout structure, and the formatting of concrete tables. The model separates table's logical structure from its layout structure, which consists of tabular topology and typographic style. The notion of an abstract table, which describes the logical relationships among tabular items, is formally defined and a set of logical operations is proposed to manipulate tables based on these logical relationships. An abstract table can be visualized through a layout structure specified by a set of topological rules, which determine the relative placement of tabular items in two dimensions, and a set of style rules, which determine the final appearance of different items. The absolute placement of a concrete table can be automatically generated by applying a layout specification to an abstract line. An NP-complete problem arises in the formatting process that uses automatic line breaking and determines the physical dimension of a table to satisfy user-specified size constraints. An algorithm has been designed to solve the formatting problem in polynomial time for typical tables. Based on the tabular model, a prototype tabular composition system has been implemented in a UNIX, X Windows environment. This prototype provides an interactive interface to edit the logical structure, the topology and the styles of tables. It allows us to manipulate tables based on the logical relationships tabular items, regardless of where the items are placed in the layout structure, and capable of presenting a table in different topologies and styles so that we can select a high-quality layout structure."
            },
            "slug": "Tabular-Abstraction,-Editing,-and-Formatting-Wang",
            "title": {
                "fragments": [],
                "text": "Tabular Abstraction, Editing, and Formatting"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This dissertation investigates the composition of high-quality tables with the use of electronic tools using a generic model designed to support the different stages of tabular composition, including the editing of logical structure, the specification of layout structure, and the formatting of concrete tables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34695622"
                        ],
                        "name": "H. Ad\u00e9",
                        "slug": "H.-Ad\u00e9",
                        "structuredName": {
                            "firstName": "Hilde",
                            "lastName": "Ad\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ad\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743332"
                        ],
                        "name": "M. Bruynooghe",
                        "slug": "M.-Bruynooghe",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Bruynooghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bruynooghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12907113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fadf308845ca275406b02b32319e18b1d568ddf",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A comparative study is presented of language biases employed in specific-to-general learning systems within the Inductive Logic Programming (ILP) paradigm. More specifically, we focus on the biases employed in three well known systems: CLINT, GOLEM and ITOU, and evaluate both conceptually and empirically their strengths and weaknesses. The evaluation is carried out within the generic framework of the NINA system, in which bias is a parameter. Two different types of biases are considered: syntactic bias, which defines the set of well-formed clauses, and semantic bias, which imposes restrictions on the behaviour of hypotheses or clauses. NINA is also able to shift its bias (within a predefined series of biases), whenever its current bias is insufficient for finding complete and consistent concept definitions. Furthermore, a new formalism for specifying the syntactic bias of inductive logic programming systems is introduced."
            },
            "slug": "Declarative-bias-for-specific-to-general-ILP-Ad\u00e9-Raedt",
            "title": {
                "fragments": [],
                "text": "Declarative bias for specific-to-general ILP systems"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A comparative study is presented of language biases employed in specific-to-general learning systems within the Inductive Logic Programming (ILP) paradigm, focusing on three well known systems: CLINT, GOLEM and ITOU, and evaluating both conceptually and empirically their strengths and weaknesses."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "We explored several learning \nalgorithms including multino\u00admial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision \ntree learner modeled after C4.5 [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 74
                            }
                        ],
                        "text": "We explored several learning algorithms including multinomial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision tree learner modeled after C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7311285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04ce064505b1635583fa0d9cc07cac7e9ea993cc",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size."
            },
            "slug": "A-comparison-of-event-models-for-naive-bayes-text-McCallum-Nigam",
            "title": {
                "fragments": [],
                "text": "A comparison of event models for naive bayes text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi -variateBernoulli model at any vocabulary size."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1998"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We explored several learning algorithms including multinomial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18,  2 ], and a decision tree learner modeled after C4.5 [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195325954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f23d52268e53f9ea81cc6b367eac55f38090257",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-Support-for-Winnow-and-Weighted-Majority-Blum",
            "title": {
                "fragments": [],
                "text": "Empirical Support for Winnow and Weighted-Majority Based Algorithms: Results on a Calendar Scheduling Domain"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 106
                            }
                        ],
                        "text": "We explored several learning \nalgorithms including multino\u00admial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision \ntree learner modeled after C4.5 [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 113
                            }
                        ],
                        "text": "We explored several learning algorithms including multinomial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision tree learner modeled after C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 14
                            }
                        ],
                        "text": "Of these, the Winnow classi.er performs the best with a precision \nof 1.00, a recall 0.922, and an F-measure of 0.959.4  4.3.2 Exploiting Table Context Table classi.cation \nis not only the .rst step in table pro\u00adcessing: it is also useful in itself."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6242854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62c15846197d81d7e3ada08fa80e6adf89ff83a3",
            "isKey": true,
            "numCitedBy": 193,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes experimental results on using Winnow and Weighted-Majority based algorithms on a real-world calendar scheduling domain. These two algorithms have been highly studied in the theoretical machine learning literature. We show here that these algorithms can be quite competitive practically, outperforming the decision-tree approach currently in use in the Calendar Apprentice system in terms of both accuracy and speed. One of the contributions of this paper is a new variant on the Winnow algorithm (used in the experiments) that is especially suited to conditions with string-valued classifications, and we give a theoretical analysis of its performance. In addition we show how Winnow can be applied to achieve a good accuracy/coverage tradeoff and explore issues that arise such as concept drift. We also provide an analysis of a policy for discarding predictors in Weighted-Majority that allows it to speed up as it learns."
            },
            "slug": "Empirical-Support-for-Winnow-and-Weighted-Majority-Blum",
            "title": {
                "fragments": [],
                "text": "Empirical Support for Winnow and Weighted-Majority Algorithms: Results on a Calendar Scheduling Domain"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new variant on the Winnow algorithm is created that is especially suited to conditions with string-valued classifications, and an analysis of a policy for discarding predictors in Weighted-Majority that allows it to speed up as it learns."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "There are several \ndi.erences between this learning algo\u00adrithm and FOIL."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "The algorithm is based on FOIL [24, 26] and learns \na DNF expression, the primitive elements of which are predicates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Builders are also used to gener\u00adate primitive predicates in the learnConjunction \nfunction, instead of instead of testing all possible primitive predi\u00adcates as FOIL does."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "FOIL: A midterm report."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "The pred\u00adicate choices \nmade in the inner loop are guided by the same information-gain metric used in FOIL."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 31
                            }
                        ],
                        "text": "The algorithm is based on FOIL [24, 26] and learns a DNF expression, the primitive elements of which are predicates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "This is useful \nsince there are some languages that are di.cult to learn using FOIL s top-down approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "As in FOIL, the outer loop of the learning \nalgorithm (the learnPredicate function) is a set\u00adcovering algorithm, which repeatedly learns a single \nrule p (actually a conjunction of builder-produced predicates) that covers some positive data from the \ntraining set, and then re\u00admoves the data covered by p."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6746439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "554f3b32b956035fbfabba730c6f0300d6955dce",
            "isKey": true,
            "numCitedBy": 780,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describesfoil, a system that learns Horn clauses from data expressed as relations.foil is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature."
            },
            "slug": "Learning-logical-definitions-from-relations-Quinlan",
            "title": {
                "fragments": [],
                "text": "Learning logical definitions from relations"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "foil is a system that learns Horn clauses from data expressed as relations, based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 84
                            }
                        ],
                        "text": "We explored several learning \nalgorithms including multino\u00admial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision \ntree learner modeled after C4.5 [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "We explored several learning algorithms including multinomial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision tree learner modeled after C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 574041,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656859af2ed88cfa23f2bd063c1816a8fc04c47e",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes the use of maximum entropy techniques for text classification. Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation. The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform. Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform. The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm. In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document. In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse. Much future work remains, but the results indicate that maximum entropy is a promising technique for text classification."
            },
            "slug": "Using-Maximum-Entropy-for-Text-Classification-Nigam-Lafferty",
            "title": {
                "fragments": [],
                "text": "Using Maximum Entropy for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper uses maximum entropy techniques for text classification by estimating the conditional distribution of the class variable given the document by comparing accuracy to naive Bayes and showing that maximum entropy is sometimes significantly better, but also sometimes worse."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "The hybrid top-down/bottom-up learning algorithm is also broadly similar to some earlier ILP systems like CHILL [30]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 106
                            }
                        ],
                        "text": "The hybrid top-down/bottom-up learning algorithm is also \nbroadly similar to some earlier ILP systems like CHILL [30]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7879170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21917201dd4c2c44252318e230820a05573cc446",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for constructing deterministic Prolog parsers from corpora of parsed sentences. Our approach uses recent machine learning methods for inducing Prolog rules from examples (inductive logic programming). We discuss several advantages of this method compared to recent statistical methods and present results on learning complete parsers from portions of the ATIS corpus."
            },
            "slug": "Inducing-Deterministic-Prolog-Parsers-from-A-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Inducing Deterministic Prolog Parsers from Treebanks: A Machine Learning Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper presents a method for constructing deterministic Prolog parsers from corpora of parsed sentences, using recent machine learning methods for inducing Prolog rules from examples (inductive logic programming)."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740042"
                        ],
                        "name": "L. D. Raedt",
                        "slug": "L.-D.-Raedt",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Raedt",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Raedt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "Many of the ideas used in this learning system are adapted from work in inductive logic \nprogramming (ILP) [20, 8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "The approach taken here avoids the computational \ncomplex\u00adities involved in ILP, while keeping much of the expressive power."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "In particular, the approach of de.ning bias via a set of builders is reminiscent \nof earlier ILP work in declarative bias [5, 1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "Declarative bias for general-to-speci.c ILP systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 106
                            }
                        ],
                        "text": "Many of the ideas used in this learning system are adapted from work in inductive logic programming (ILP) [20, 8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The hybrid top-down/bottom-up learning algorithm is also \nbroadly similar to some earlier ILP systems like CHILL [30]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16762143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f47f99a16f60d649f7d3c1c6a26c6eff68e2e3da",
            "isKey": true,
            "numCitedBy": 1754,
            "numCiting": 178,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inductive-Logic-Programming:-Theory-and-Methods-Muggleton-Raedt",
            "title": {
                "fragments": [],
                "text": "Inductive Logic Programming: Theory and Methods"
            },
            "venue": {
                "fragments": [],
                "text": "J. Log. Program."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "We explored several learning \nalgorithms including multino\u00admial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision \ntree learner modeled after C4.5 [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 74
                            }
                        ],
                        "text": "We explored several learning algorithms including multinomial Naive Bayes [17, 19], Maximum Entropy [23], Winnow [18, 2], and a decision tree learner modeled after C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32800624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44e915a220ce74badf755aae870fa0b69ee2b82a",
            "isKey": false,
            "numCitedBy": 2252,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents."
            },
            "slug": "Naive-(Bayes)-at-Forty:-The-Independence-Assumption-Lewis",
            "title": {
                "fragments": [],
                "text": "Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval, and some of the variations used for text retrieval and classification are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47934783"
                        ],
                        "name": "J. Davenport",
                        "slug": "J.-Davenport",
                        "structuredName": {
                            "firstName": "Janina",
                            "lastName": "Davenport",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Davenport"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 220072940,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5864512d1e03cb9072ec623c688336f1d8b5e9a3",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editor-Davenport",
            "title": {
                "fragments": [],
                "text": "Editor"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Dietetic Association"
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tag each cell with (some aspects) of its role in the table. @BULLET Currently, \" cut-in cells"
            },
            "venue": {
                "fragments": [],
                "text": "Tag each cell with (some aspects) of its role in the table. @BULLET Currently, \" cut-in cells"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inducing deterministic Prolog parsers from  September 19, 2003 9:19 WSPC/Trim Size: 9in x 6in for Review Volume ws-chj 26 Cohen et al treebanks: a machine learning approach"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Twelfth National Conference on Artificial Intelligence, Seattle, Washington,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classify HTML tables nodes as \" data tables \" or \" non-data tables"
            },
            "venue": {
                "fragments": [],
                "text": "Classify HTML tables nodes as \" data tables \" or \" non-data tables"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 122
                            }
                        ],
                        "text": "In particular, the approach of defining bias via a set of builders is reminiscent of earlier ILP work in declarative bias [5, 1]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Declarative bias for general-to-specific ILP systems"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Render each data table"
            },
            "venue": {
                "fragments": [],
                "text": "Render each data table"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Declarative bias for general-tospecific ILP systems"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Find the logical cells of the table"
            },
            "venue": {
                "fragments": [],
                "text": "Find the logical cells of the table"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Construct geometric model of table: an integer grid, with each logical cell having co-ordinates on the grid"
            },
            "venue": {
                "fragments": [],
                "text": "Construct geometric model of table: an integer grid, with each logical cell having co-ordinates on the grid"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HTML 4.01 specification"
            },
            "venue": {
                "fragments": [],
                "text": "HTML 4.01 specification"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fancy Builders: Understanding Table Rendering"
            },
            "venue": {
                "fragments": [],
                "text": "Fancy Builders: Understanding Table Rendering"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "9:19 WSPC/Trim Size: 9in x 6in for Review Volume ws-chj"
            },
            "venue": {
                "fragments": [],
                "text": "9:19 WSPC/Trim Size: 9in x 6in for Review Volume ws-chj"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On 339 examples, precision/recall of 1.00/0.92 with Winnow and features"
            },
            "venue": {
                "fragments": [],
                "text": "On 339 examples, precision/recall of 1.00/0.92 with Winnow and features"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 10,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 43,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-flexible-learning-system-for-wrapping-tables-and-Cohen-Hurst/0909fee90833e20913adb553bf6667c9a3b854b0?sort=total-citations"
}