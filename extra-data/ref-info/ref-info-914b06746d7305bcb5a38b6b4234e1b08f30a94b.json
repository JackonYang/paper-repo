{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73158308"
                        ],
                        "name": "M. Strauss",
                        "slug": "M.-Strauss",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Strauss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Strauss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "These models have been successful in reducing the perplexity of the text considerably, and [5] also reports a positive effect on the word recognition rate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Several adaptive language models have been proposed recently [3, 4, 5, 6], which use caching of the partially dictated document, and interpolate a dynamic component based on the cache with the static component."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "This was done to facilitate a more equitable comparison with the results reported in [5]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11601499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0687165a9f0360bde0469fd401d966540e0897c3",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In the case of a trigram language model, the probability of the next word conditioned on the previous two words is estimated from a large corpus of text. The resulting static trigram language model (STLM) has fixed probabilities that are independent of the document being dictated. To improve the language model (LM), one can adapt the probabilities of the trigram language model to match the current document more closely. The partially dictated document provides significant clues about what words are more likely to be used next. Of many methods that can be used to adapt the LM, we describe in this paper a simple model based on the trigram frequencies estimated from the partially dictated document. We call this model a cache trigram language model (CTLM) since we are caching the recent history of words. We have found that the CTLM reduces the perplexity of a dictated document by 23%. The error rate of a 20,000-word isolated word recognizer decreases by about 5% at the beginning of a document and by about 24% after a few hundred words."
            },
            "slug": "A-Dynamic-Language-Model-for-Speech-Recognition-Jelinek-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "A Dynamic Language Model for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This model is called a cache trigram language model (CTLM) since it is caching the recent history of words and it is found that the CTLM reduces the perplexity of a dictated document by 23%."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Several adaptive language models have been proposed recently [3, 4, 5, 6], which use caching of the partially dictated document, and interpolate a dynamic component based on the cache with the static component."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60782193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8363d2b7935a1bc1e6b91d51a617666f0450c21b",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The language model part of a speech recognition system provides information about the probabilities of word sequences. The probabilities are estimated beforehand from a large set of training data, so that the language model does not reflect any short-term fluctuations in word use. In order to enable the adaptation to those fluctuations, we added a dynamic component, the cache memory, which uses the word frequencies of the recent past to update the static word probabilities. Compared to a usual bigram language model we achieved an improvement of perplexity of 8% and 23%, respectively, depending on the heterogeneity of the data."
            },
            "slug": "Statistical-Language-Modelling-Using-a-Cache-Memory-Essen-Ney",
            "title": {
                "fragments": [],
                "text": "Statistical Language Modelling Using a Cache Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A dynamic component is added to the language model, the cache memory, which uses the word frequencies of the recent past to update the static word probabilities, and achieves an improvement of perplexity of 8% and 23%, respectively, depending on the heterogeneity of the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This can be done in two different ways: \u2022 By using a POS-based trigger model, in the spirit of [ 4 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Several adaptive language models have been proposed recently [3,  4 , 5, 6], which use caching of the partially dictated document, and interpolate a dynamic component based on the cache with the static component."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31924166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. >"
            },
            "slug": "A-Cache-Based-Natural-Language-Model-for-Speech-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "A Cache-Based Natural Language Model for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented and contains a 3g-gram component of the traditional type."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Several adaptive language models have been proposed recently [3, 4, 5, 6], which use caching of the partially dictated document, and interpolate a dynamic component based on the cache with the static component."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14679951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "343c8af478f7703459b0e390e888efe723f15e31",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes two complementary models that represent dependencies between words in local and non-local contexts. The type of local dependencies considered are sequences of part of speech categories for words. The non-local context of word dependency considered here is that of word recurrence, which is typical in a text. Both are models of phenomena that are to a reasonable extent domain independent, and thus are useful for doing prediction in systems using large vocabularies."
            },
            "slug": "Probabilistic-Models-of-Short-and-Long-Distance-in-Kupiec",
            "title": {
                "fragments": [],
                "text": "Probabilistic Models of Short and Long Distance Word Dependencies in Running Text"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Two complementary models that represent dependencies between words in local and non-local contexts are described, which are useful for doing prediction in systems using large vocabularies."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52075048"
                        ],
                        "name": "P. Strevens",
                        "slug": "P.-Strevens",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Strevens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Strevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 220781652,
            "fieldsOfStudy": [
                "Education",
                "Linguistics"
            ],
            "id": "09830e7210f5e9d4f204ebad2a2a8def4e9de9f1",
            "isKey": false,
            "numCitedBy": 4172,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "over native-speaking users of English. Secondly, the numerical preponderance of non-native speakers means that it is their communication which is increasing more rapidly and thus dominating the development and evolution of English. Thirdly, it is therefore becoming inescapably necessary for native speakers to accept unfamiliarities in the effective use of English. Fourthly, acceptance of these unfamiliarities will be easier if there is a basis for understanding them."
            },
            "slug": "Iii-Strevens",
            "title": {
                "fragments": [],
                "text": "Iii"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 61
                            }
                        ],
                        "text": "Several adaptive language models have been proposed recently [3, 4, 5, 6], which use caching of the partially dictated document, and interpolate a dynamic component based on the cache with the static component."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "This can be done in two different ways: \u2022 By using a POS-based trigger model, in the spirit of [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Cache-BasedNatural Language Model for Speech Recognition,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. PatternAnalysis and Machine Intelligence, vol"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 5,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 7,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Improvements-in-Stochastic-Language-Modeling-Rosenfeld-Huang/914b06746d7305bcb5a38b6b4234e1b08f30a94b?sort=total-citations"
}