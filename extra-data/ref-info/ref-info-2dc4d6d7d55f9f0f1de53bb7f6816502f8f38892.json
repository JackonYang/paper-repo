{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1117961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fd2df4f095e5764c4d26990625f75139d849da4",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a large family of Boltzmann machines that can be trained by standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results on the problems of N-bit parity and the detection of hidden symmetries."
            },
            "slug": "Learning-in-Boltzmann-Trees-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning in Boltzmann Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A large family of Boltzmann machines that can be trained by standard gradient descent, which can have one or more layers of hidden units, with tree-like connectivity, are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "In the framework of Boltzmann learning (Williams & Hinton, 1990), the data for our problem consist of sequences of states over the visible units; the goal is to nd the weights (A ii 0; B ij ; i ) that maximize the likelihood of the observed data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117771711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4725bea4175f9a3912188aca3c4f25c66469fea1",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mean-field-networks-that-learn-to-discriminate-Williams-Hinton",
            "title": {
                "fragments": [],
                "text": "Mean field networks that learn to discriminate temporally distorted strings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 160
                            }
                        ],
                        "text": "For Boltzmann machines in general, neither the E-step nor the M-step can be done exactly; one must estimate the necessary statistics by Monte Carlo simulation (Ackley et al., 1985) or mean--eld theory (Peterson & Anderson, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3396,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 152
                            }
                        ],
                        "text": ".2 LOOPS AND LONG-TERM DEPENDENCIES Another shortcoming of rst-order HMMs is that they cannot exhibit long-term dependencies between the hidden states (Juang et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 44
                            }
                        ],
                        "text": "An important problem in speech recognition (Juang et al., 1991) is how to \\combine feature sets with fundamentally diierent time scales.\""
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 174
                            }
                        ],
                        "text": "Suppose we have trained two separate HMMs on sequences of spectral and prosodic features, knowing that the diierent features \\may not warrant a single, uniied Markov chain\" (Juang et al., 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17743203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df682aa90fbbbf665a8b273a57ca87d6cea9ff99",
            "isKey": true,
            "numCitedBy": 1561,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of hidden Markov models for speech recognition has become predominant in the last several years, as evidenced by the number of published papers and talks at major speech conferences. The reasons this method has become so popular are the inherent statistical (mathematically precise) framework; the ease and availability of training algorithms for cstimating the parameters of the models from finite training sets of speech data; the flexibility of the resulting recognition system in which one can easily change the size, type, or architecture of the models to suit particular words, sounds, and so forth; and the ease of implementation of the overall recognition system. In this expository article, we address the role of statistical methods in this powerful technology as applied to speech recognition and discuss a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations."
            },
            "slug": "Hidden-Markov-Models-for-Speech-Recognition-Juang-Rabiner",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The role of statistical methods in this powerful technology as applied to speech recognition is addressed and a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716907"
                        ],
                        "name": "W. Byrne",
                        "slug": "W.-Byrne",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Byrne",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Byrne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 130553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11fcb64e0a9e0f4eb06da9379e428a4a9654f1e3",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a Boltzmann machine with hidden units is appropriately treated in information geometry using the information divergence and the technique of alternating minimization. The resulting algorithm is shown to be closely related to gradient descent Boltzmann machine learning rules, and the close relationship of both to the EM algorithm is described. An iterative proportional fitting procedure for training machines without hidden units is described and incorporated into the alternating minimization algorithm."
            },
            "slug": "Alternating-minimization-and-Boltzmann-machine-Byrne",
            "title": {
                "fragments": [],
                "text": "Alternating minimization and Boltzmann machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The resulting algorithm is shown to be closely related to gradient descent Boltzmann machine learning rules, and the close relationship of both to the EM algorithm is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11627878"
                        ],
                        "name": "N. Sourlas",
                        "slug": "N.-Sourlas",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Sourlas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sourlas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4358269,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e10110742b747306b1e1642105b44adf5e1cc0b",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "DURING the transmission of information, errors may occur because of the presence of noise, such as thermal noise in electronic signals or interference with other sources of radiation. One wants to recover the information with the minimum error possible. In theory this is possible by increasing the power of the emitter source. But as the cost is proportional to the energy fed into the channel, it costs less to code the message before sending it, thus including redundant 'coding' bits, and to decode at the end. Coding theory provides rigorous bounds on the cost-effectiveness of any code. The explicit codes proposed so far for practical applications do not saturate these bounds; that is, they do not achieve optimal cost-efficiency. Here we show that theoretical models of magnetically disordered materials (spin glasses) provide a new class of error-correction codes. Their cost performance can be calculated using the methods of statistical mechanics, and is found to be excellent. These models can, under certain circumstances, constitute the first known codes to saturate Shannon's well-known cost-performance bounds."
            },
            "slug": "Spin-glass-models-as-error-correcting-codes-Sourlas",
            "title": {
                "fragments": [],
                "text": "Spin-glass models as error-correcting codes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that theoretical models of magnetically disordered materials (spin glasses) provide a new class of error-correction codes that can, under certain circumstances, constitute the first known codes to saturate Shannon's well-known cost-performance bounds."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": false,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110869996"
                        ],
                        "name": "James R. Anderson",
                        "slug": "James-R.-Anderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 202
                            }
                        ],
                        "text": "For Boltzmann machines in general, neither the E-step nor the M-step can be done exactly; one must estimate the necessary statistics by Monte Carlo simulation (Ackley et al., 1985) or mean--eld theory (Peterson & Anderson, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3851750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607802a4067cb7738bac85d3ca3386f859e637b9",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Mean-Field-Theory-Learning-Algorithm-for-Neural-Peterson-Anderson",
            "title": {
                "fragments": [],
                "text": "A Mean Field Theory Learning Algorithm for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 202
                            }
                        ],
                        "text": "For Boltzmann machines in general, neither the E-step nor the M-step can be done exactly; one must estimate the necessary statistics by Monte Carlo simulation (Ackley et al., 1985) or mean--eld theory (Peterson & Anderson, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Mean Field Theory Learning Algorithm for Neural Networks, Complex Systems 1:995-1019"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Figure 5: (a) Looped network. (b) Log-likelihood versus epoch for a looped network with 4-state hidden units, 6-state visible units, and 100 hidden{visible unit pairs"
            },
            "venue": {
                "fragments": [],
                "text": "Figure 5: (a) Looped network. (b) Log-likelihood versus epoch for a looped network with 4-state hidden units, 6-state visible units, and 100 hidden{visible unit pairs"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 215
                            }
                        ],
                        "text": "In fact, the two weights in series exert the same innuence as a single eeective weight, given by e A ii 00 = X i 0 e A (1) ii 0 +A (2) i 0 i 00 +B i 0 : (11) 2 A related method, the transfer matrix, is described by Stolorz (1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Links Between Dynamic Programming and Statistical Physics for Heterogeneous Systems, JPL/Caltech preprint"
            },
            "venue": {
                "fragments": [],
                "text": "Links Between Dynamic Programming and Statistical Physics for Heterogeneous Systems, JPL/Caltech preprint"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. Nat. Acad"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Nat. Acad"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "This example represents the simplest possible Boltzmann \\chain\", one that is essentially equivalent to a rstorder HMM unfolded in time (MacKay, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Equivalence of Boltzmann Chains and Hidden Markov Models , submitted to Neural Comp"
            },
            "venue": {
                "fragments": [],
                "text": "Equivalence of Boltzmann Chains and Hidden Markov Models , submitted to Neural Comp"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Models for Speech Recognition, Technometrics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "This example represents the simplest possible Boltzmann \\chain\", one that is essentially equivalent to a rstorder HMM unfolded in time (MacKay, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "An interpretation of these extra degrees of freedom is given by MacKay (1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Equivalence of Boltzmann Chains and Hidden Markov Models, submitted to Neural Compo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 215
                            }
                        ],
                        "text": "In fact, the two weights in series exert the same innuence as a single eeective weight, given by e A ii 00 = X i 0 e A (1) ii 0 +A (2) i 0 i 00 +B i 0 : (11) 2 A related method, the transfer matrix, is described by Stolorz (1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Links Between Dynamic Programming and Statistical Physics for Heterogeneous Systems, JPL/Caltech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. Nat . Acad. Sci. (USA) 91"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Nat . Acad. Sci. (USA) 91"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of Markov Processes, Inequalities 3:1-8"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sci. (USA)"
            },
            "venue": {
                "fragments": [],
                "text": "Sci. (USA)"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Boltzmann-Chains-and-Hidden-Markov-Models-Saul-Jordan/2dc4d6d7d55f9f0f1de53bb7f6816502f8f38892?sort=total-citations"
}