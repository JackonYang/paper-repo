{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 25151650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c19c8c1d6778a16f8b27beac4d9c6a55357580",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars. When an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory, the combination can be interpreted as a neural net pushdown automata. The neural net finite state machine is given the primitives, push and POP, and is able to read the top of the stack. Through a gradient descent learning rule derived from the common error function, the hybrid network learns to effectively use the stack actions to manipulate the stack memory and to learn simple contextfree grammars."
            },
            "slug": "Higher-Order-Recurrent-Networks-and-Grammatical-Giles-Sun",
            "title": {
                "fragments": [],
                "text": "Higher Order Recurrent Networks and Grammatical Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars and can be interpreted as a neural net pushdown automata."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749125"
                        ],
                        "name": "Colin Giles",
                        "slug": "Colin-Giles",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Giles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107456756"
                        ],
                        "name": "D. Chen",
                        "slug": "D.-Chen",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107321237"
                        ],
                        "name": "C. B. Miller",
                        "slug": "C.-B.-Miller",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. B. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108324032"
                        ],
                        "name": "H. Chen",
                        "slug": "H.-Chen",
                        "structuredName": {
                            "firstName": "H.-H.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148800040"
                        ],
                        "name": "G. Sun",
                        "slug": "G.-Sun",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Sun",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109251232"
                        ],
                        "name": "Y. Lee",
                        "slug": "Y.-Lee",
                        "structuredName": {
                            "firstName": "Y.-C.",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61528525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea0fe8c2a77c84f9bbb172a9f65f2cb9ba9d24d",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that a recurrent, second-order neural network using a real-time, feedforward training algorithm readily learns to infer regular grammars from positive and negative string training samples. Numerous simulations which show the effect of initial conditions, training set size and order, and neuron architecture are presented. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. The authors discuss a quantization algorithm for dynamically extracting finite-state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. It is then shown through simulations that many of the neural net state machines are dynamically stable and correctly classify long unseen strings.<<ETX>>"
            },
            "slug": "Second-order-recurrent-neural-networks-for-Giles-Chen",
            "title": {
                "fragments": [],
                "text": "Second-order recurrent neural networks for grammatical inference"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "It is shown that a recurrent, second-order neural network using a real-time, feedforward training algorithm readily learns to infer regular grammars from positive and negative string training samples and many of the neural net state machines are dynamically stable and correctly classify long unseen strings."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537431"
                        ],
                        "name": "Axel Cleeremans",
                        "slug": "Axel-Cleeremans",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Cleeremans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Cleeremans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403615640"
                        ],
                        "name": "D. Servan-Schreiber",
                        "slug": "D.-Servan-Schreiber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Servan-Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Servan-Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7741931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd46c1b5948abe04e565a8bae6454da63a1b021e",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "slug": "Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber",
            "title": {
                "fragments": [],
                "text": "Finite State Automata and Simple Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A network architecture introduced by Elman (1988) for predicting successive elements of a sequence and shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3832,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9858,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18612099"
                        ],
                        "name": "B. Ladendorf",
                        "slug": "B.-Ladendorf",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Ladendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ladendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091434"
                        ],
                        "name": "G. Kuhn",
                        "slug": "G.-Kuhn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kuhn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222252385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bed713424942fa3cb4e8916d2220c1e0a2b534d",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A complete gradient optimization technique for connectionist networks with recurrent links is presented. A truncated gradient technique is available in the literature. A network with recurrent links for discrimination of /b/, /d/, and /g/ in the context of following /i/, /a/, or /u/ is designed. The performance of this network as optimized with either of the two techniques is then compared. It will be shown that the complete gradient is exact and simpler, and it will be demonstrated that it leads to superior performance."
            },
            "slug": "Complete-gradient-optimization-of-a-recurrent-to-Watrous-Ladendorf",
            "title": {
                "fragments": [],
                "text": "Complete gradient optimization of a recurrent network applied to /b/,/d/,/g/ discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It will be shown that the complete gradient is exact and simpler, and it will be demonstrated that it leads to superior performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of finite automata from examples using hill-climbing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complete gradient opti"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Fourth Int. Cog. Sci. Conf.,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 112
                            }
                        ],
                        "text": "Recognizers of finite-state languages have also been induced using first-order recurrent connectionist networks (Elman 1990; Williams and Zipser 1988; Cleeremans et al. 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 65
                            }
                        ],
                        "text": "were obtained by training the network to predict the next symbol (Cleeremans et al. 1989; Williams and Zipser 1988), rather than by training the network to accept or reject strings of different lengths."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finite state auElman, J"
            },
            "venue": {
                "fragments": [],
                "text": "L. 1990. Finding structure in time. Cog. Sci. 14, 179-212. Giles, C. L., Chen,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 73
                            }
                        ],
                        "text": "The networks were optimized by gradient descent using the BFGS algorithm (Luenberger 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linear and Nonlinear Programming, 2nd ed"
            },
            "venue": {
                "fragments": [],
                "text": "AddisonWesley, Reading, MA."
            },
            "year": 1984
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 10,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Induction-of-Finite-State-Languages-Using-Recurrent-Watrous-Kuhn/a64ca771a733d58dcbf8f7a3fe65a09310424bf8?sort=total-citations"
}