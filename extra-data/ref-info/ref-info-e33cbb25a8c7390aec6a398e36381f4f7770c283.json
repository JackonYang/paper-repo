{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Subsequent work combined the two approaches by using state-of-the-art, discriminative ly trained (DT) speaker-dependent features as input to the DBN-DNN [24], but this produced little further improvement , probably because the hidden layers of the DBN-DNN were already doing quite a good job of progressively elimina ting speaker differences [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13908978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be53d4def5e0601f2416e9345babc7ef1b30a664",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Belief Networks (DBNs) are multi-layer generative models. They can be trained to model windows of coefficients extracted from speech and they discover multiple layers of features that capture the higher-order statistical structure of the data. These features can be used to initialize the hidden units of a feed-forward neural network that is then trained to predict the HMM state for the central frame of the window. Initializing with features that are good at generating speech makes the neural network perform much better than initializing with random weights. DBNs have already been used successfully for phone recognition with input coefficients that are MFCCs or filterbank outputs [1, 2]. In this paper, we demonstrate that they work even better when their inputs are speaker adaptive, discriminative features. On the standard TIMIT corpus, they give phone error rates of 19.6% using monophone HMMs and a bigram language model and 19.4% using monophone HMMs and a trigram language model."
            },
            "slug": "Deep-Belief-Networks-using-discriminative-features-Mohamed-Sainath",
            "title": {
                "fragments": [],
                "text": "Deep Belief Networks using discriminative features for phone recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Deep Belief Networks work even better when their inputs are speaker adaptive, discriminative features, and on the standard TIMIT corpus, they give phone error rates of 19.6% using monophone HMMs and a bigram language model."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9530137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2b62f77cb2864e465aa60bca6c26bb1d2f84963",
            "isKey": false,
            "numCitedBy": 1641,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models."
            },
            "slug": "Acoustic-Modeling-Using-Deep-Belief-Networks-Mohamed-Dahl",
            "title": {
                "fragments": [],
                "text": "Acoustic Modeling Using Deep Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [3] NOvEMbER 2012\nGMMs have a number of advantages that make them suitable for modeling the probability distributions over vectors of input features that are associated with each state of an HMM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 68
                            }
                        ],
                        "text": "For multiclass classification,\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [4] NOvEMbER 2012\noutput unit j converts its total input, x j , into a class probability, p j , by using the \u201csoftmax\u201d nonlinearity\n( )\n( ) , exp exp p x x j k\nk\nj = / (2)\nwhere k is an index over all classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] showed that a DBN-DNN acoustic model outperformed the best published recognition results on TIMIT at about the same time as Sainath et."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 238
                            }
                        ],
                        "text": "Before this work, DNN-HMM acoustic models had only been trained with up to 48 h of data [44] and hundreds of tied triphone states as targets, whereas this work used over 300 h of training data and thou-\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [10] NOvEMbER 2012\nsands of tied triphone states as targets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [5] NOvEMbER 2012\nWe start by describing an approximate learning algorithm for a restricted Boltzmann machine (RBM) which consists of a layer of stochastic binary \u201cvisible\u201d units that represent binary input data connected to a layer of stochastic binary hidden units that learn to model significant nonindependencies between the visible units [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 579,
                                "start": 571
                            }
                        ],
                        "text": "MethOd per\nCD-HMM [26] 27.3%\nAugMenteD ConDitionAl RAnDoM FielDs [26] 26.6%\nRAnDoMly initiAlizeD ReCuRRent neuRAl nets [27] 26.1%\nBAyesiAn tRipHone gMM-HMM [28] 25.6%\nMonopHone HtMs [29] 24.8%\nHeteRogeneous ClAssiFieRs [30] 24.4%\nMonopHone RAnDoMly initiAlizeD Dnns (six lAyeRs) [13] 23.4%\nMonopHone DBn-Dnns (six lAyeRs) [13] 22.4%\nMonopHone DBn-Dnns witH MMi tRAining [31] 22.1%\ntRipHone gMM-HMMs Dt w/ BMMi [32] 21.7%\nMonopHone DBn-Dnns on FBAnk (eigHt lAyeRs) [13] 20.7%\nMonopHone MCRBM-DBn-Dnns on FBAnk (Five lAyeRs) [33] 20.5%\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [9] NOvEMbER 2012\nalso offering noise robustness due to the band-limited nature of the filters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "In this article, we review exploratory experiments on the TIMIT <AU: can TIMIT be spelled out?> database [12], [13] that were used to demonstrate the power of this two-stage training procedure for acoustic modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 346,
                                "start": 338
                            }
                        ],
                        "text": "Fortunately, the time that a DNN-HMM system requires to recognize 1 s of speech can be reduced from 1.6 s to 210 ms, without decreasing recognition accuracy, by quantizing the weights down to 8 b and using the very fast SIMD primitives for fixed-point computation that are provided by a modern x86 cen-\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [12] NOvEMbER 2012\ntral processing unit [49]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [7] NOvEMbER 2012\nThe DBN-DNNs that worked best on the TIMIT data formed the starting point for subsequent experiments on much more challenging large vocabulary tasks that were too computationally intensive to allow extensive exploration of variations in the architecture of the neural network, the representation of the acoustic input, or the training procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 237
                            }
                        ],
                        "text": "50 h 430 h\nlvcsr stAge gMM-hMM bAseline Ae-bn\ngMM/hMM bAseline Ae-bn\nFsA 24.8 20.6 20.2 17.6\n+fBMMi 20.7 19.0 17.7 16.6\n+BMMi 19.6 18.1 16.5 15.8 +MllR 18.8 17.5 16.0 15.5\nMoDel CoMBinAtion 16.4 15.0\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [14] NOvEMbER 2012\nsystems that make use of phonological features in the full detection-based framework discussed in [65].\nsUMMAry And fUtUre directiOns When GMMs were first used for acoustic modeling, they were trained as generative models using the EM algorithm, and it was some time before researchers showed that significant gains could be achieved by a subsequent stage of discriminative training using an objective function more closely related to the ultimate goal of an ASR system [7], [68]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "The two conditional distributions required for CD1 learning are\nv( )p h b v wlogisticj j i i ij i ; v = +c m/ (14)\nh( ) ,Np h a h wj i i j ij j\ni 2; v v= +c m/ , (15)\nIE\nEE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [6] NOvEMbER 2012\nwhere ( , )N 2n v is a Gaussian."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [8] NOvEMbER 2012\nusing such sequence classification criteria with shallow neural networks has already been shown by [36]\u2013[38]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 137
                            }
                        ],
                        "text": "He is the general chair of the International Conference on Acoustics, Speech, and Signal Processing\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [15] NOvEMbER 2012\n(ICASSP) 2013."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 993,
                                "start": 985
                            }
                        ],
                        "text": "Model combination was then used to combine results from the [tAble 2] cOMpAring five different dbn-dnn AcOUstic MOdels with twO strOng gMM-hMM bAseline systeMs thAt Are dt. si trAining On 309 h Of dAtA And single-pAss decOding were Used fOr All MOdels except fOr the gMM-hMM systeM shOwn On the lAst rOw which Used sA trAining with 2,000 h Of dAtA And MUltipAss decOding inclUding hypOtheses cOMbinAtiOn. in the tAble, \u201c40 Mix\u201d MeAns A MixtUre Of 40 gAUssiAns per hMM stAte And \u201c15.2 nz\u201d MeAns 15.2 MilliOn, nOnzerO weights. wers in % Are shOwn fOr twO sepArAte test sets, hUb500-swb And rt03s-fsh. wer MOdeling techniqUe #pArAMs [106] hUb5\u201900-swb rt03s-fsh gMM, 40 Mix Dt 309H si 29.4 23.6 27.4 nn 1 HiDDen-lAyeR # 4,634 units 43.6 26.0 29.4 + 2 # 5 neigHBoRing FRAMes 45.1 22.4 25.7 DBn-Dnn 7 HiDDen lAyeRs # 2,048 units 45.1 17.1 19.6 + upDAteD stAte AlignMent 45.1 16.4 18.6 + spARsiFiCAtion 15.2 nz 16.1 18.5 gMM 72 Mix Dt 2000H sA 102.4 17.1 18.6\nIE\nEE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [11] NOvEMbER 2012\nGMM-HMM system with the DNN-HMM hybrid, using the SCARF <AU: plesae spell out SCARF> framework [47]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 76
                            }
                        ],
                        "text": "He is currently an associate editor of\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [16] NOvEMbER 2012\nIEEE Transactions on Audio, Speech, and Language Processing.\nreferences [1] J. Baker, L. Deng, J. Glass, S. Khudanpur, Chin Hui Lee, N. Morgan, and D. O\u2019Shaughnessy, \u201cDevelopments and directions in speech recognition and understanding, part 1,\u201d IEEE Signal Processing Mag., vol. 26, no. 3, pp. 75\u201380, May 2009."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [12] NOvEMbER 2012 tral processing unit [49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [17] NOvEMbER 2012\n[52] L. Deng, D. Yu, and J. Platt, \u201cScalable stacking and learning for building deep architectures,\u201d in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [13] NOvEMbER 2012\nHere we describe a class of methods where neural networks are used to provide the feature vectors that the GMM in a GMMHMM system is trained to model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": "IE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [2] NOvEMbER 2012\nDigital Object Identifier 10.1109/MSP.2012.2205597\n\u00a9 xxxxx\n[Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, vincent vanhoucke, Patrick Nguyen, Tara Sainath, and brian Kingsbury]\nDate of publication:\n1053-5888/12/$31.00\u00a92012IEEE"
                    },
                    "intents": []
                }
            ],
            "corpusId": 131773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) have been the state-of-the-art techniques for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the research community for deeper models that are capable of modeling the many types of variability present in the speech generation p r cess. Deep Belief Networks (DBNs) have recently proved to be very effective fo r a variety of machine learning problems and this paper applies DBNs to acous ti modeling. On the standard TIMIT corpus, DBNs consistently outperform ot her techniques and the best DBN achieves a phone error rate (PER) of 23.0% on the T IMIT core test set."
            },
            "slug": "Deep-Belief-Networks-for-phone-recognition-Mohamed-Dahl",
            "title": {
                "fragments": [],
                "text": "Deep Belief Networks for phone recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deep Belief Networks (DBNs) have recently proved to be very effective in a variety of machine learning problems and this paper applies DBNs to acous ti modeling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745455"
                        ],
                        "name": "D. Ellis",
                        "slug": "D.-Ellis",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ellis",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ellis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109947002"
                        ],
                        "name": "Sangita Sharma",
                        "slug": "Sangita-Sharma",
                        "structuredName": {
                            "firstName": "Sangita",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangita Sharma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "Instead of replacing the coefficients usually modeled by GMM s, neural networks can also be used to provide additional features for the GMM to model [8], [9], [63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": ", MFCCs) with \u201ctandem\u201d or bottleneck features generated using neura l networks [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5807992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e9082caea65c76bfd23b8763872804473ee7872",
            "isKey": false,
            "numCitedBy": 805,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov model speech recognition systems typically use Gaussian mixture models to estimate the distributions of decorrelated acoustic feature vectors that correspond to individual subword units. By contrast, hybrid connectionist-HMM systems use discriminatively-trained neural networks to estimate the probability distribution among subword units given the acoustic observations. In this work we show a large improvement in word recognition performance by combining neural-net discriminative feature processing with Gaussian-mixture distribution modeling. By training the network to generate the subword probability posteriors, then using transformations of these estimates as the base features for a conventionally-trained Gaussian-mixture based system, we achieve relative error rate reductions of 35% or more on the multicondition Aurora noisy continuous digits task."
            },
            "slug": "Tandem-connectionist-feature-extraction-for-HMM-Hermansky-Ellis",
            "title": {
                "fragments": [],
                "text": "Tandem connectionist feature extraction for conventional HMM systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A large improvement in word recognition performance is shown by combining neural-net discriminative feature processing with Gaussian-mixture distribution modeling."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 305
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, b ut even with this constraint it was impossible to train all possible combinations of number of hidden layers [ 1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 118
                            }
                        ],
                        "text": "Using alignments from a baseline system, [32] trained a DBN-DNN acoustic model on 50 h of data from the 1996 and 1997 English Broadcast News Speech Corpora [37]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Using alignments from a baseline system, [32] trained a DBNDNN acoustic model on 50 hours of data from the 1996 and 1997 English Broadcast News Speech Corpora [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "In addition to the DBN-DNN trai ning, sequence level discriminative fine-tuning of the neural network was performed using MMI, similar to the m thod proposed in [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 343,
                                "start": 339
                            }
                        ],
                        "text": "However, it has long been known that sequence classification criteria, whic h are more directly correlated with the overall word or phone error rate, can be very helpful in improving recogniti o accuracy [7], [35] and the benefit of using such sequence classification criteria with shallow neural netwo rks has already been shown by [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14733612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2443dc59cf3d6cc1deba6d3220d61664b1a7eada",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Acoustic models used in hidden Markov model/neural-network (HMM/NN) speech recognition systems are usually trained with a frame-based cross-entropy error criterion. In contrast, Gaussian mixture HMM systems are discriminatively trained using sequence-based criteria, such as minimum phone error or maximum mutual information, that are more directly related to speech recognition accuracy. This paper demonstrates that neural-network acoustic models can be trained with sequence classification criteria using exactly the same lattice-based methods that have been developed for Gaussian mixture HMMs, and that using a sequence classification criterion in training leads to considerably better performance. A neural network acoustic model with 153K weights trained on 50 hours of broadcast news has a word error rate of 34.0% on the rt04 English broadcast news test set. When this model is trained with the state-level minimum Bayes risk criterion, the rt04 word error rate is 27.7%."
            },
            "slug": "Lattice-based-optimization-of-sequence-criteria-for-Kingsbury",
            "title": {
                "fragments": [],
                "text": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper demonstrates that neural-network acoustic models can be trained with sequence classification criteria using exactly the same lattice-based methods that have been developed for Gaussian mixture HMMs, and that using a sequence classification criterion in training leads to considerably better performance."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [11] NOvEMbER 2012 GMM-HMM system with the DNN-HMM hybrid, using the SCARF <AU: plesae spell out SCARF> framework [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[47] used this model to obtain approximately 5,870 h of aligned training data for a DBN-DNN acoustic model that predicts the 7,969 HMM state posteriors from the acoustic input."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13521651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7% absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 4.7% absolute. Maximum Mutual Information (MMI) fine tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the first dataset and 0.5% and 0.9% absolute on the second dataset."
            },
            "slug": "Application-of-Pretrained-Deep-Neural-Networks-to-Jaitly-Nguyen",
            "title": {
                "fragments": [],
                "text": "Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reports results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously, and outperforms the best Gaussian Mixture Model Hidden Markov Model baseline."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316518"
                        ],
                        "name": "Yasser Hifny",
                        "slug": "Yasser-Hifny",
                        "structuredName": {
                            "firstName": "Yasser",
                            "lastName": "Hifny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasser Hifny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "AugMenteD ConDitionAl RAnDoM FielDs [26] 26."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 39
                            }
                        ],
                        "text": "MethOd per\nCD-HMM [26] 27.3%\nAugMenteD ConDitionAl RAnDoM FielDs [26] 26.6%\nRAnDoMly initiAlizeD ReCuRRent neuRAl nets [27] 26.1%\nBAyesiAn tRipHone gMM-HMM [28] 25.6%\nMonopHone HtMs [29] 24.8%\nHeteRogeneous ClAssiFieRs [30] 24.4%\nMonopHone RAnDoMly initiAlizeD Dnns (six lAyeRs) [13] 23.4%\nMonopHone DBn-Dnns (six lAyeRs) [13] 22.4%\nMonopHone DBn-Dnns witH MMi tRAining [31] 22.1%\ntRipHone gMM-HMMs Dt w/ BMMi [32] 21.7%\nMonopHone DBn-Dnns on FBAnk (eigHt lAyeRs) [13] 20.7%\nMonopHone MCRBM-DBn-Dnns on FBAnk (Five lAyeRs) [33] 20.5%\nIE EE\nPr oo\nf\nIEEE SIGNAL PROCESSING MAGAZINE [9] NOvEMbER 2012\nalso offering noise robustness due to the band-limited nature of the filters."
                    },
                    "intents": []
                }
            ],
            "corpusId": 951317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df5b82595a29724467a98eed4d7e2a45e804579e",
            "isKey": true,
            "numCitedBy": 94,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "Acoustic modeling based on hidden Markov models (HMMs) is employed by state-of-the-art stochastic speech recognition systems. Although HMMs are a natural choice to warp the time axis and model the temporal phenomena in the speech signal, their conditional independence properties limit their ability to model spectral phenomena well. In this paper, a new acoustic modeling paradigm based on augmented conditional random fields (ACRFs) is investigated and developed. This paradigm addresses some limitations of HMMs while maintaining many of the aspects which have made them successful. In particular, the acoustic modeling problem is reformulated in a data driven, sparse, augmented space to increase discrimination. Acoustic context modeling is explicitly integrated to handle the sequential phenomena of the speech signal. We present an efficient framework for estimating these models that ensures scalability and generality. In the TIMIT phone recognition task, a phone error rate of 23.0% was recorded on the full test set, a significant improvement over comparable HMM-based systems."
            },
            "slug": "Speech-Recognition-Using-Augmented-Conditional-Hifny-Renals",
            "title": {
                "fragments": [],
                "text": "Speech Recognition Using Augmented Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new acoustic modeling paradigm based on augmented conditional random fields (ACRFs) is investigated and developed, which addresses some limitations of HMMs while maintaining many of the aspects which have made them successful."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46299348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a6ae3d667a5c2601c1852a0753c8b1c749fec1e",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes, and evaluates on a large scale, the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models (HMMs). This paper concentrates on the maximum mutual information estimation (MMIE) criterion which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data. These experiments represent the largest-scale application of discriminative training techniques for speech recognition of which the authors are aware. Details are given of the MMIE lattice-based implementation used with the extended Baum-Welch algorithm, which makes training of such large systems computationally feasible. Techniques for improving generalization using acoustic scaling and weakened language models are discussed. The overall technique has allowed the estimation of triphone and quinphone HMM parameters which has led to significant reductions in word error rate for the transcription of conversational telephone speech relative to our best systems trained using maximum likelihood estimation (MLE). This is in contrast to some previous studies, which have concluded that there is little benefit in using discriminative training for the most difficult large vocabulary speech recognition tasks. The lattice MMIE-based discriminative training scheme is also shown to out-perform the frame discrimination technique. Various properties of the lattice-based MMIE training scheme are investigated including comparisons of different lattice processing strategies (full search and exact-match) and the effect of lattice size on performance. Furthermore a scheme based on the linear interpolation of the MMIE and MLE objective functions is shown to reduce the danger of over-training. It is shown that HMMs trained with MMIE benefit as much as MLE-trained HMMs from applying model adaptation using maximum likelihood linear regression (MLLR). This has allowed the straightforward integration of MMIE-trained HMMs into complex multi-pass systems for transcription of conversational telephone speech and has contributed to our MMIE-trained systems giving the lowest word error rates in both the 2000 and 2001 NIST Hub5 evaluations."
            },
            "slug": "Large-scale-discriminative-training-of-hidden-for-Woodland-Povey",
            "title": {
                "fragments": [],
                "text": "Large scale discriminative training of hidden Markov models for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that HMMs trained with MMIE benefit as much as MLE-trained HMMs from applying model adaptation using maximum likelihood linear regression (MLLR), which has allowed the straightforward integration of MMIe- trained HMMs into complex multi-pass systems for transcription of conversational telephone speech."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14862572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6658bbf68995731b2083195054ff45b4eca38b3a",
            "isKey": false,
            "numCitedBy": 2677,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively."
            },
            "slug": "Context-Dependent-Pre-Trained-Deep-Neural-Networks-Dahl-Yu",
            "title": {
                "fragments": [],
                "text": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output that can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393667185"
                        ],
                        "name": "Ossama Abdel-Hamid",
                        "slug": "Ossama-Abdel-Hamid",
                        "structuredName": {
                            "firstName": "Ossama",
                            "lastName": "Abdel-Hamid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ossama Abdel-Hamid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36357862"
                        ],
                        "name": "Hui Jiang",
                        "slug": "Hui-Jiang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145047294"
                        ],
                        "name": "Gerald Penn",
                        "slug": "Gerald-Penn",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Penn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald Penn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The work reported in [34] applied local convolutional filters with max-pooling to the frequency rather than time dimension of the spectrogram."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] only used weight-sharing and max-pooling across nearby frequencies because, unlike features that occur at different positions in images, acoustic features occurring at very different frequencies are very different."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10042024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9f4bf3bfe133e1c70f6b60654c238b677c66d0",
            "isKey": false,
            "numCitedBy": 805,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNN) have showed success in achieving translation invariance for many image processing tasks. The success is largely attributed to the use of local filtering and max-pooling in the CNN architecture. In this paper, we propose to apply CNN to speech recognition within the framework of hybrid NN-HMM model. We propose to use local filtering and max-pooling in frequency domain to normalize speaker variance to achieve higher multi-speaker speech recognition performance. In our method, a pair of local filtering layer and max-pooling layer is added at the lowest end of neural network (NN) to normalize spectral variations of speech signals. In our experiments, the proposed CNN architecture is evaluated in a speaker independent speech recognition task using the standard TIMIT data sets. Experimental results show that the proposed CNN method can achieve over 10% relative error reduction in the core TIMIT test sets when comparing with a regular NN using the same number of hidden layers and weights. Our results also show that the best result of the proposed CNN model is better than previously published results on the same TIMIT test sets that use a pre-trained deep NN model."
            },
            "slug": "Applying-Convolutional-Neural-Networks-concepts-to-Abdel-Hamid-Mohamed",
            "title": {
                "fragments": [],
                "text": "Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed CNN architecture is applied to speech recognition within the framework of hybrid NN-HMM model to use local filtering and max-pooling in frequency domain to normalize speaker variance to achieve higher multi-speaker speech recognition performance."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376916"
                        ],
                        "name": "G. Flammia",
                        "slug": "G.-Flammia",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Flammia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Flammia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688499"
                        ],
                        "name": "R. Kompe",
                        "slug": "R.-Kompe",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Kompe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kompe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 333
                            }
                        ],
                        "text": "However, it has long been known that sequence classification criteria, whic h are more directly correlated with the overall word or phone error rate, can be very helpful in improving recogniti o accuracy [7], [35] and the benefit of using such sequence classification criteria with shallow neural netwo rks has already been shown by [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 894840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "An original method for integrating artificial neural networks (ANN) with hidden Markov models (HMM) is proposed. ANNs are suitable for performing phonetic classification, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported.<<ETX>>"
            },
            "slug": "Global-optimization-of-a-neural-network-hidden-Bengio-Mori",
            "title": {
                "fragments": [],
                "text": "Global optimization of a neural network-hidden Markov model hybrid"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An original method for integrating artificial neural networks (ANN) with hidden Markov models (HMM) with results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61058350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d82e058a5c40954b8f5db170a298a889a254c37",
            "isKey": false,
            "numCitedBy": 1409,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing."
            },
            "slug": "Connectionist-Speech-Recognition:-A-Hybrid-Approach-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "Connectionist Speech Recognition: A Hybrid Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2130362,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "90b63e917d5737b06357d50aa729619e933d9614",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date."
            },
            "slug": "Phone-Recognition-with-the-Mean-Covariance-Machine-Dahl-Ranzato",
            "title": {
                "fragments": [],
                "text": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work uses the mean-covariance restricted Boltzmann machine (mcRBM) to learn features of speech data that serve as input into a standard DBN, and achieves a phone error rate superior to all published results on speaker-independent TIMIT to date."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is because temporal variations in speech can be partially handled by the dynamic programing procedure in the HMM component and those aspects of temporal variation that cannot be adequately handled by the HMM can be addressed more explicitly and effectively by hidden trajectory models [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[40] L."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14421525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62c87f843ae5c1ce7972d7cdcd227e3ec3fe5417",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling dynamic structure of speech is a novel paradigm in speech recognition research within the generative modeling framework, and it offers a potential to overcome limitations of the current hidden Markov modeling approach. Analogous to structured language models where syntactic structure is exploited to represent long-distance relationships among words , the structured speech model described in this paper makes use of the dynamic structure in the hidden vocal tract resonance space to characterize long-span contextual influence among phonetic units. A general overview is provided first on hierarchically classified types of dynamic speech models in the literature. A detailed account is then given for a specific model type called the hidden trajectory model, and we describe detailed steps of model construction and the parameter estimation algorithms. We show how the use of resonance target parameters and their temporal filtering enables joint modeling of long-span coarticulation and phonetic reduction effects. Experiments on phonetic recognition evaluation demonstrate superior recognizer performance over a modern hidden Markov model-based system. Error analysis shows that the greatest performance gain occurs within the sonorant speech class"
            },
            "slug": "Structured-speech-modeling-Deng-Yu",
            "title": {
                "fragments": [],
                "text": "Structured speech modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows how the use of resonance target parameters and their temporal filtering enables joint modeling of long-span coarticulation and phonetic reduction effects and demonstrates superior recognizer performance over a modern hidden Markov model-based system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145047294"
                        ],
                        "name": "Gerald Penn",
                        "slug": "Gerald-Penn",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Penn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald Penn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 346,
                                "start": 342
                            }
                        ],
                        "text": "Subsequent work combined the two approaches by using state-of-the-art, discriminative ly trained (DT) speaker-dependent features as input to the DBN-DNN [24], but this produced little further improvement , probably because the hidden layers of the DBN-DNN were already doing quite a good job of progressively elimina ting speaker differences [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2432232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06c152df89ca6a1f1b8f8e139ddda82cd4539415",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Belief Networks (DBNs) are a very competitive alternative to Gaussian mixture models for relating states of a hidden Markov model to frames of coefficients derived from the acoustic input. They are competitive for three reasons: DBNs can be fine-tuned as neural networks; DBNs have many non-linear hidden layers; and DBNs are generatively pre-trained. This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN."
            },
            "slug": "Understanding-how-Deep-Belief-Networks-perform-Mohamed-Hinton",
            "title": {
                "fragments": [],
                "text": "Understanding how Deep Belief Networks perform acoustic modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the Dbns that preserves the similarity structure of the feature vector at multiple scales."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745715"
                        ],
                        "name": "F. Seide",
                        "slug": "F.-Seide",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Seide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Seide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155118983"
                        ],
                        "name": "Gang Li",
                        "slug": "Gang-Li",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109030402"
                        ],
                        "name": "Xie Chen",
                        "slug": "Xie-Chen",
                        "structuredName": {
                            "firstName": "Xie",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xie Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Further study [45] suggests that feature-engineering techniques such as HLDA and VTLN, commonly used in GMMHMMs, are more helpful for shallow neural nets than for DBN-DNNs, presumably because DBN-DNNs are able to learn appropriate features in their lower layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 299
                            }
                        ],
                        "text": "Purely discriminative training of the whole DNN from random initial weights works much better than had been thought, provided the scales of the initial weights are set carefully, a large amount of labeled training data is available, and minibatch sizes over training epochs are set appropr ia te ly [45] , [53] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 274
                            }
                        ],
                        "text": "This type of discriminative pretraining works well in practice, approaching the accuracy achieved by generative DBN pretraining and further improvement can be achieved by stopping the discriminative pretraining after a single epoch instead of multiple epochs as reported in [45]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9933050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third\u2014from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%\u2014using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers."
            },
            "slug": "Feature-engineering-in-Context-Dependent-Deep-for-Seide-Li",
            "title": {
                "fragments": [],
                "text": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This work investigates the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective to reduce the word error rate for speaker-independent transcription of phone calls."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 305
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, b ut even with this constraint it was impossible to train all possible combinations of number of hidden layers [ 1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Speech is produced by modulating a relatively small number of parameters of a dy namical system [10], [11] and this implies that its true underlying structure is much lower-dimensional than is imm ediately apparent in a window that contains hundreds of coefficients."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12350627,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d63225540b0c8337b8b58ad7260684ba0e6dc6c0",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "A statistical generative model for the speech process is described that embeds a substantially richer structure than the HMM currently in predominant use for automatic speech recognition. This switching dynamic-system model generalizes and integrates the HMM and the piece-wise stationary nonlinear dynamic system (state- space) model. Depending on the level and the nature of the switching in the model design, various key properties of the speech dynamics can be naturally represented in the model. Such properties include the temporal structure of the speech acoustics, its causal articulatory movements, and the control of such movements by the multidimensional targets correlated with the phonological (symbolic) units of speech in terms of overlapping articulatory features."
            },
            "slug": "Switching-Dynamic-System-Models-for-Speech-and-Deng",
            "title": {
                "fragments": [],
                "text": "Switching Dynamic System Models for Speech Articulation and Acoustics"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A statistical generative model for the speech process is described that embeds a substantially richer structure than the HMM currently in predominant use for automatic speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10473793,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0191c9b53a99942a9b4ec39dc30489e41c7aaa1",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, Deep Belief Networks (DBNs) have been proposed for phone recognition and were found to achieve highly competitive performance. In the original DBNs, only framelevel information was used for training DBN weights while it has been known for long that sequential or full-sequence information can be helpful in improving speech recognition accuracy. In this paper we investigate approaches to optimizing the DBN weights, state-to-state transition parameters, and language model scores using the sequential discriminative training criterion. We describe and analyze the proposed training algorithm and strategy, and discuss practical issues and how they affect the final results. We show that the DBNs learned using the sequence-based training criterion outperform those with frame-based criterion using both threelayer and six-layer models, but the optimization procedure for the deeper DBN is more difficult for the former criterion."
            },
            "slug": "Investigation-of-full-sequence-training-of-deep-for-Mohamed-Yu",
            "title": {
                "fragments": [],
                "text": "Investigation of full-sequence training of deep belief networks for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the DBNs learned using the sequence-based training criterion outperform those with frame-based criterion using both threelayer and six-layer models, but the optimization procedure for the deeper DBN is more difficult for the former criterion."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[10] L."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Speech is produced by modulating a relatively small number of parameters of a dynamical system [10], [11] and this implies that its true underlying structure is much lower-dimensional than is immediately apparent in a window that contains hundreds of coefficients."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "MOdeling techniqUe #pArAMs [10] hUb5\u201900-swb rt03s-fsh"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [10] NOvEMbER 2012 sands of tied triphone states as targets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16224396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15754483b95c5386fb145751ad3b054761207451",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Major speech production models from speech science literature and a number of popular statistical \u201cgenerative\u201d models of speech used in speech technology are surveyed. Strengths and weaknesses of these two styles of speech models are analyzed, pointing to the need to integrate the respective strengths while eliminating the respective weaknesses. As an example, a statistical task-dynamic model of speech production is described, motivated by the original deterministic version of the model and targeted for integrated-multilingual speech recognition applications. Methods for model parameter learning (training) and for likelihood computation (recognition) are described based on statistical optimization principles integrated in neural network and dynamic system theories."
            },
            "slug": "Computational-Models-for-Speech-Production-Deng",
            "title": {
                "fragments": [],
                "text": "Computational Models for Speech Production"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A statistical task-dynamic model of speech production is described, motivated by the original deterministic version of the model and targeted for integrated-multilingual speech recognition applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "exploitation of neighboring frames by the DBN-DNN, and the s trong modeling power of deeper networks, as was discovered in the Bing voice search task [44], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Before this work, DNN-HMM acoustic models had only been trai ned with up to 48 hours of data [44] and hundreds of tied triphone states as targets, whereas this work used ov r 300 hours of training data and thousands of tied triphone states as targets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "Further work after that of [42] extended the DNN-HMM acousti c model from 24 hours of training data to 48 hours, and explored the respective roles of pre-training an d fine-tuning the DBN-DNN [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18141431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecd4bc32bb2717c96f76dd100fcd1255a07bd656",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, deep learning techniques have been successfully applied to automatic speech recognition tasks -first to phonetic recognition with context-independent deep belief network (DBN) hidden Markov models (HMMs) and later to large vocabulary continuous speech recognition using context-dependent (CD) DBN-HMMs. In this paper, we report our most recent experiments designed to understand the roles of the two main phases of the DBN learning -pre-training and fine tuning -in the recognition performance of a CD-DBN-HMM based large-vocabulary speech recognizer. As expected, we show that pre-training can initialize weights to a point in the space where fine-tuning can be effective and thus is crucial in training deep structured models. However, a moderate increase of the amount of unlabeled pre-training data has an insignificant effect on the final recognition results as long as the original training size is sufficiently large to initialize the DBN weights. On the other hand, with additional labeled training data, the fine-tuning phase of DBN training can significantly improve the recognition accuracy."
            },
            "slug": "Roles-of-Pre-Training-and-Fine-Tuning-in-DBN-HMMs-Yu-Deng",
            "title": {
                "fragments": [],
                "text": "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that pre-training can initialize weights to a point in the space where fine-tuning can be effective and thus is crucial in training deep structured models and in the recognition performance of a CD-DBN-HMM based large-vocabulary speech recognizer."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709878"
                        ],
                        "name": "S. M. Siniscalchi",
                        "slug": "S.-M.-Siniscalchi",
                        "structuredName": {
                            "firstName": "Sabato",
                            "lastName": "Siniscalchi",
                            "middleNames": [
                                "Marco"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Siniscalchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 245
                            }
                        ],
                        "text": "This level of accuracy for detecting sub-phonetic fundamental speech units may allow a new family of flexible speech recognition and understanding systems that make use of phonological features in the full detection-based framework discussed in [65]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The 22 attributes used in the recent work, as reported in [65], are a subset of the articulatory features explored in [66], [67]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "A recent study [65] demonstrated the effectiveness of DBN-DNNs for detecting sub-phonetic speech attributes (also known as phonological or articulatory features [66]) in the widely used Wall Street Journal speech database (5k-WSJ0)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3001257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0acc24337501da9a68f03e8a9a5b42d52ffa927",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Generation of high-precision sub-phonetic attribute (also known as phonological features) and phone lattices is a key frontend component for detection-based bottom-up speech recognition. In this paper we employ deep neural networks (DNNs) to improve detection accuracy over conventional shallow MLPs (multi-layer perceptrons) with one hidden layer. A range of DNN architectures with five to seven hidden layers and up to 2048 hidden units per layer have been explored. Training on the SI84 and testing on the Nov92 WSJ data, the proposed DNNs achieve significant improvements over the shallow MLPs, producing greater than 90% frame-level attribute estimation accuracies for all 21 attributes tested for the full system. On the phone detection task, we also obtain excellent frame-level accuracy of 86.6%. With this level of high-precision detection of basic speech units we have opened the door to a new family of flexible speech recognition system design for both top-down and bottom-up, lattice-based search strategies and knowledge integration."
            },
            "slug": "Boosting-attribute-and-phone-estimation-accuracies-Yu-Siniscalchi",
            "title": {
                "fragments": [],
                "text": "Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Deep neural networks are employed to improve detection accuracy over conventional shallow MLPs (multi-layer perceptrons) with one hidden layer to open the door to a new family of flexible speech recognition system design for both top-down and bottom-up, lattice-based search strategies and knowledge integration."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745715"
                        ],
                        "name": "F. Seide",
                        "slug": "F.-Seide",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Seide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Seide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143736623"
                        ],
                        "name": "Gang Li",
                        "slug": "Gang-Li",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "The DNN-HMM training recipe developed for the Bing voice search data was applied unaltered to the Switchboard speech recognition task [43] to confirm the suitability of DNN-HMM acoustic models for large vocabulary tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Detailed experiments [43] on the Switchboard task confirmed that the remarkable accuracy gains from the DNN-HMM acoustic model are due to the direct modeling of tied triphone states using the DBN-DNN, the effective exploitation of neighboring frames by the DBN-DNN, and the strong modeling power of deeper networks, as was discovered in the Bing voice search task [44], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "Using contextdependent HMM states, it is possible to outperform state-of-theart BMMI trained GMM-HMM systems with a two-hidden-layer neural network without using any pretraining [43], though using more hidden layers and pretraining works even better."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 398770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "473f0739666af2791ad6592822118240ed968b70",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, combine the classic artificial-neural-network HMMs with traditional context-dependent acoustic modeling and deep-belief-network pre-training. CD-DNN-HMMs greatly outperform conventional CD-GMM (Gaussian mixture model) HMMs: The word error rate is reduced by up to one third on the difficult benchmarking task of speaker-independent single-pass transcription of telephone conversations."
            },
            "slug": "Conversational-Speech-Transcription-Using-Deep-Seide-Li",
            "title": {
                "fragments": [],
                "text": "Conversational Speech Transcription Using Context-Dependent Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, combine the classic artificial-neural-network HMMs with traditional context-dependent acoustic modeling and deep-belief-network pre-training to greatly outperform conventional CD-GMM (Gaussian mixture model) HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691713"
                        ],
                        "name": "H. Zen",
                        "slug": "H.-Zen",
                        "structuredName": {
                            "firstName": "Heiga",
                            "lastName": "Zen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740397"
                        ],
                        "name": "M. Gales",
                        "slug": "M.-Gales",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gales",
                            "middleNames": [
                                "John",
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714830"
                        ],
                        "name": "Yoshihiko Nankaku",
                        "slug": "Yoshihiko-Nankaku",
                        "structuredName": {
                            "firstName": "Yoshihiko",
                            "lastName": "Nankaku",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiko Nankaku"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723069"
                        ],
                        "name": "K. Tokuda",
                        "slug": "K.-Tokuda",
                        "structuredName": {
                            "firstName": "Keiichi",
                            "lastName": "Tokuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tokuda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [41]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[41] H."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13134717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7591539ace4225adffcf8a893a3b00ee49741740",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiple acoustic models are often combined in statistical parametric speech synthesis. Both linear and non-linear functions of an observation sequence are used as features to be modeled. This paper shows that this combination of multiple acoustic models can be expressed as a product of experts (PoE); the likelihoods from the models are scaled, multiplied together, and then normalized. Normally these models are individually trained and only combined at the synthesis stage. This paper discusses a more consistent PoE framework where the models are jointly trained. A training algorithm for PoEs based on linear feature functions and Gaussian experts is derived by generalizing the training algorithm for trajectory HMMs. However for non-linear feature functions or non-Gaussian experts this is not possible, so a scheme based on contrastive divergence learning is described. Experimental results show that the PoE framework provides both a mathematically elegant way to train multiple acoustic models jointly and significant improvements in the quality of the synthesized speech."
            },
            "slug": "Product-of-Experts-for-Statistical-Parametric-Zen-Gales",
            "title": {
                "fragments": [],
                "text": "Product of Experts for Statistical Parametric Speech Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results show that the PoE framework provides both a mathematically elegant way to train multiple acoustic models jointly and significant improvements in the quality of the synthesized speech."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685224"
                        ],
                        "name": "Garimella S. V. S. Sivaram",
                        "slug": "Garimella-S.-V.-S.-Sivaram",
                        "structuredName": {
                            "firstName": "Garimella",
                            "lastName": "Sivaram",
                            "middleNames": [
                                "S.",
                                "V.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Garimella S. V. S. Sivaram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16217851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f98545b1510b03b49e034c53525a8d4986dd27f",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the sparse multilayer perceptron (SMLP) which jointly learns a sparse feature representation and nonlinear classifier boundaries to optimally discriminate multiple output classes. SMLP learns the transformation from the inputs to the targets as in multilayer perceptron (MLP) while the outputs of one of the internal hidden layers is forced to be sparse. This is achieved by adding a sparse regularization term to the cross-entropy cost and updating the parameters of the network to minimize the joint cost. On the TIMIT phoneme recognition task, SMLP-based systems trained on individual speech recognition feature streams perform significantly better than the corresponding MLP-based systems. Phoneme error rate of 19.6% is achieved using the combination of SMLP-based systems, a relative improvement of 3.0% over the combination of MLP-based systems."
            },
            "slug": "Sparse-Multilayer-Perceptron-for-Phoneme-Sivaram-Hermansky",
            "title": {
                "fragments": [],
                "text": "Sparse Multilayer Perceptron for Phoneme Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper introduces the sparse multilayer perceptron (SMLP) which jointly learns a sparse feature representation and nonlinear classifier boundaries to optimally discriminate multiple output classes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "For a summary of such methods, commonly known as the tandem approach, see [60], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3047120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e4e8c5149cfccf3cd3dc0259c35dec7a40d8aa3",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews a line of research carried out over the last decade in speech recognition assisted by discriminatively trained, feedforward networks. The particular focus is on the use of multiple layers of processing preceding the hidden Markov model based decoding of word sequences. Emphasis is placed on the use of multiple streams of highly dimensioned layers, which have proven useful for this purpose. This paper ultimately concludes that while the deep processing structures can provide improvements for this genre, choice of features and the structure with which they are incorporated, including layer width, can also be significant factors."
            },
            "slug": "Deep-and-Wide:-Multiple-Layers-in-Automatic-Speech-Morgan",
            "title": {
                "fragments": [],
                "text": "Deep and Wide: Multiple Layers in Automatic Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is concluded that while the deep processing structures can provide improvements for this genre, choice of features and the structure with which they are incorporated, including layer width, can also be significant factors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144156036"
                        ],
                        "name": "Brian Hutchinson",
                        "slug": "Brian-Hutchinson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Hutchinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Hutchinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Some recent attempts are described in [52] and [57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2731002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c6c4d884e3977203e798d3e46469663423762c",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop and describe a novel deep architecture, the Tensor Deep Stacking Network (T-DSN), where multiple blocks are stacked one on top of another and where a bilinear mapping from hidden representations to the output in each block is used to incorporate higher-order statistics of the input features. A learning algorithm for the T-DSN is presented, in which the main parameter estimation burden is shifted to a convex sub-problem with a closed-form solution. Using an efficient and scalable parallel implementation, we train a T-DSN to discriminate standard three-state monophones in the TIMIT database. The T-DSN outperforms an alternative pretrained Deep Neural Network (DNN) architecture in frame-level classification (both state and phone) and in the cross-entropy measure. For continuous phonetic recognition, T-DSN performs equivalently to a DNN but without the need for a hard-to-scale, sequential fine-tuning step."
            },
            "slug": "A-deep-architecture-with-bilinear-modeling-of-to-Hutchinson-Deng",
            "title": {
                "fragments": [],
                "text": "A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A novel deep architecture, the Tensor Deep Stacking Network (T-DSN), where multiple blocks are stacked one on top of another and where a bilinear mapping from hidden representations to the output in each block is used to incorporate higher-order statistics of the input features."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2512508"
                        ],
                        "name": "Andrew K. Halberstadt",
                        "slug": "Andrew-K.-Halberstadt",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Halberstadt",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew K. Halberstadt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7548084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "182c9ba291d97dc8d7482533044416869cb15f23",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of acoustic phonetic modeling. First, heterogeneous acoustic measurements are chosen in order to maximize the acoustic-phonetic information extracted from the speech signal in preprocessing. Second, classifier systems are presented for successfully utilizing high-dimensional acoustic measurement spaces. The techniques used for achieving these two goals can be broadly categorized as hierarchical, committeebased, or a hybrid of these two. This paper presents committeebased and hybrid approaches. In context-independent classification and context-dependent recognition on the TIMIT core test set using 39 classes, the system achieved error rates of 18.3% and 24.4%, respectively. These error rates are the lowest we have seen reported on these tasks. In addition, experiments with a telephone-based weather information word recognition task led to word error rate reductions of 10\u201316%."
            },
            "slug": "Heterogeneous-measurements-and-multiple-classifiers-Halberstadt-Glass",
            "title": {
                "fragments": [],
                "text": "Heterogeneous measurements and multiple classifiers for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In context-independent classification and context-dependent recognition on the TIMIT core test set using 39 classes, the system achieved error rates that are the lowest the authors have seen reported on these tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983154"
                        ],
                        "name": "Jiping Sun",
                        "slug": "Jiping-Sun",
                        "structuredName": {
                            "firstName": "Jiping",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiping Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "The 22 attributes used in the recent work, as reported in [65], are a subset of the articulatory feature s explored in [66], [67]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6948823,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7132561820763eb18fb94a50b0611cd6a82196c6",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling phonological units of speech is a critical issue in speech recognition. In this paper, our recent development of an overlapping-feature-based phonological model that represents long-span contextual dependency in speech acoustics is reported. In this model, high-level linguistic constraints are incorporated in automatic construction of the patterns of feature-overlapping and of the hidden Markov model (HMM) states induced by such patterns. The main linguistic information explored includes word and phrase boundaries, morpheme, syllable, syllable constituent categories, and word stress. A consistent computational framework developed for the construction of the feature-based model and the major components of the model are described. Experimental results on the use of the overlapping-feature model in an HMM-based system for speech recognition show improvements over the conventional triphone-based phonological model."
            },
            "slug": "An-overlapping-feature-based-phonological-model-to-Sun-Deng",
            "title": {
                "fragments": [],
                "text": "An overlapping-feature-based phonological model incorporating linguistic constraints: applications to speech recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The recent development of an overlapping-feature-based phonological model that represents long-span contextual dependency in speech acoustics is reported, and experimental results on the use of the overlapping- feature model in an HMM-based system for speech recognition show improvements over the conventional triphone-based pharmacological model."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061523260"
                        ],
                        "name": "Peter T. Pham",
                        "slug": "Peter-T.-Pham",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Pham",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter T. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918282"
                        ],
                        "name": "Yan Largman",
                        "slug": "Yan-Largman",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Largman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Largman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [39], convolutional DBN-DNNs were introduced and successfully applied to various audio tasks including phone classification on the TIMIT database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[39] H."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although convolutional models along the temporal dimension achieved good classification results [39], applying them to phone recognition is not straightforward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12219023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf38dfb13352449b965c08282b66d3ffc5a0539f",
            "isKey": true,
            "numCitedBy": 1086,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks."
            },
            "slug": "Unsupervised-feature-learning-for-audio-using-deep-Lee-Pham",
            "title": {
                "fragments": [],
                "text": "Unsupervised feature learning for audio classification using convolutional deep belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10228522,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "e7f56c88b49cd348cfcc5b34c9792ed2d968fb36",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The earlier version of the hidden trajectory model (HTM) for speech dynamics which predicts the \"static\" cepstra as the observed acoustic feature is generalized to one which predicts joint static cepstra and their temporal differentials (i.e., delta cepstra). The formulation of this generalized HTM is presented in the generative-modeling framework, enabling efficient computation of the joint likelihood for both static and delta cepstral sequences as the acoustic features given the model. The parameter estimation techniques for the new model are developed and presented, giving closed-form estimation formulas after the use of vector Taylor series approximation. We show principled generalization from the earlier static-cepstra HTM to the new static/delta-cepstra HTM not only in terms of model formulations but also in terms of their respective analytical forms in (monophone) parameter estimation. Experimental results on the standard TIMIT phonetic recognition task demonstrate recognition accuracy improvement over the earlier best HTM system, both significantly better than state-of-the-art triphone HMM systems."
            },
            "slug": "Use-of-Differential-Cepstra-as-Acoustic-Features-in-Deng-Yu",
            "title": {
                "fragments": [],
                "text": "Use of Differential Cepstra as Acoustic Features in Hidden Trajectory Modeling for Phonetic Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The earlier version of the hidden trajectory model (HTM) for speech dynamics which predicts the \"static\" cepstra as the observed acoustic feature is generalized to one which predicts joint Static/delta-cepstra HTM, enabling efficient computation of the joint likelihood for both static and delta cepstral sequences as the acoustic features given the model."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9353491"
                        ],
                        "name": "Qifeng Zhu",
                        "slug": "Qifeng-Zhu",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Qifeng Zhu",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145244442"
                        ],
                        "name": "K. Sonmez",
                        "slug": "K.-Sonmez",
                        "structuredName": {
                            "firstName": "Kemal",
                            "lastName": "Sonmez",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sonmez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739225"
                        ],
                        "name": "S. Sivadas",
                        "slug": "S.-Sivadas",
                        "structuredName": {
                            "firstName": "Sunil",
                            "lastName": "Sivadas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sivadas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732454"
                        ],
                        "name": "T. Shinozaki",
                        "slug": "T.-Shinozaki",
                        "structuredName": {
                            "firstName": "Takahiro",
                            "lastName": "Shinozaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinozaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066975404"
                        ],
                        "name": "P. Jain",
                        "slug": "P.-Jain",
                        "structuredName": {
                            "firstName": "Pratibha",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745455"
                        ],
                        "name": "D. Ellis",
                        "slug": "D.-Ellis",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ellis",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ellis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862682"
                        ],
                        "name": "G. Doddington",
                        "slug": "G.-Doddington",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Doddington",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Doddington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157307424"
                        ],
                        "name": "B. Chen",
                        "slug": "B.-Chen",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9376398"
                        ],
                        "name": "O. Cretin",
                        "slug": "O.-Cretin",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Cretin",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Cretin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705797"
                        ],
                        "name": "M. Athineos",
                        "slug": "M.-Athineos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Athineos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Athineos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Instead of replacing the coefficients usually modeled by GMM s, neural networks can also be used to provide additional features for the GMM to model [8], [9], [63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15156045,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d9d2ba2003d7324ae3d5ff7423a13f13efc79ca5",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite successes, there are still significant limitations to speech recognition performance, particularly for conversational speech and/or for speech with significant acoustic degradations from noise or reverberation. For this reason, authors have proposed methods that incorporate different (and larger) analysis windows, which are described in this article. Note in passing that we and many others have already taken advantage of processing techniques that incorporate information over long time ranges, for instance for normalization (by cepstral mean subtraction as stated in B. Atal (1974) or relative spectral analysis (RASTA) based in H. Hermansky and N. Morgan (1994)). They also have proposed features that are based on speech sound class posterior probabilities, which have good properties for both classification and stream combination."
            },
            "slug": "Pushing-the-envelope-aside-[speech-recognition]-Morgan-Zhu",
            "title": {
                "fragments": [],
                "text": "Pushing the envelope - aside [speech recognition]"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Authors have proposed methods that incorporate different analysis windows that incorporate features that are based on speech sound class posterior probabilities, which have good properties for both classification and stream combination."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938043"
                        ],
                        "name": "Suman V. Ravuri",
                        "slug": "Suman-V.-Ravuri",
                        "structuredName": {
                            "firstName": "Suman",
                            "lastName": "Ravuri",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suman V. Ravuri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7258004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4f7541a08c23fc308440ef2e86a00b57405dd25",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we extend the work done on integrating multilayer perceptron (MLP) networks with HMM systems via the Tandem approach. In particular, we explore whether the use of Deep Belief Networks (DBN) adds any substantial gain over MLPs on the Aurora2 speech recognition task under mismatched noise conditions. Our findings suggest that DBNs outperform single layer MLPs under the clean condition, but the gains diminish as the noise level is increased. Furthermore, using MFCCs in conjunction with the posteriors from DBNs outperforms merely using single DBNs in low to moderate noise conditions. MFCCs, however, do not help for the high noise settings."
            },
            "slug": "Comparing-multilayer-perceptron-to-Deep-Belief-for-Vinyals-Ravuri",
            "title": {
                "fragments": [],
                "text": "Comparing multilayer perceptron to Deep Belief Network Tandem features for robust ASR"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is suggested that DBNs outperform single layer MLPs under the clean condition, but the gains diminish as the noise level is increased, while using MFCCs in conjunction with the posteriors from DBN's outperforms merely using single DBNS in low to moderate noise conditions."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8258197"
                        ],
                        "name": "Dirk Van Compernolle",
                        "slug": "Dirk-Van-Compernolle",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Compernolle",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk Van Compernolle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719822"
                        ],
                        "name": "Kris Demuynck",
                        "slug": "Kris-Demuynck",
                        "structuredName": {
                            "firstName": "Kris",
                            "lastName": "Demuynck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kris Demuynck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705299"
                        ],
                        "name": "L. Atlas",
                        "slug": "L.-Atlas",
                        "structuredName": {
                            "firstName": "Les",
                            "lastName": "Atlas",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Atlas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38849495"
                        ],
                        "name": "P. Clark",
                        "slug": "P.-Clark",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143660166"
                        ],
                        "name": "Gregory Sell",
                        "slug": "Gregory-Sell",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Sell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Sell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118663449"
                        ],
                        "name": "Meihong Wang",
                        "slug": "Meihong-Wang",
                        "structuredName": {
                            "firstName": "Meihong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meihong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841922"
                        ],
                        "name": "D. Karakos",
                        "slug": "D.-Karakos",
                        "structuredName": {
                            "firstName": "Damianos",
                            "lastName": "Karakos",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Karakos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35996413"
                        ],
                        "name": "A. Jansen",
                        "slug": "A.-Jansen",
                        "structuredName": {
                            "firstName": "Aren",
                            "lastName": "Jansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152809214"
                        ],
                        "name": "Samuel Thomas",
                        "slug": "Samuel-Thomas",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409063839"
                        ],
                        "name": "S. SivaramG.S.V.",
                        "slug": "S.-SivaramG.S.V.",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "SivaramG.S.V.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. SivaramG.S.V."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055513699"
                        ],
                        "name": "Justine T. Kao",
                        "slug": "Justine-T.-Kao",
                        "structuredName": {
                            "firstName": "Justine",
                            "lastName": "Kao",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justine T. Kao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Model combination was then used to combine results from the GMM-HMM system with the DNN-HMM hybrid, using the SCARF framework [48]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1710846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26cf16673269bdb0979bc601a340083448e5ad44",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper summarizes the 2010 CLSP Summer Workshop on speech recognition at Johns Hopkins University. The key theme of the workshop was to improve on state-of-the-art speech recognition systems by using Segmental Conditional Random Fields (SCRFs) to integrate multiple types of information. This approach uses a state-of-the-art baseline as a springboard from which to add a suite of novel features including ones derived from acoustic templates, deep neural net phoneme detections, duration models, modulation features, and whole word point-process models. The SCRF framework is able to appropriately weight these different information sources to produce significant gains on both the Broadcast News and Wall Street Journal tasks."
            },
            "slug": "Speech-recognitionwith-segmental-conditional-random-Zweig-Nguyen",
            "title": {
                "fragments": [],
                "text": "Speech recognitionwith segmental conditional random fields: A summary of the JHU CLSP 2010 Summer Workshop"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This paper summarizes the 2010 CLSP Summer Workshop on speech recognition at Johns Hopkins University, to improve on state-of-the-art speech recognition systems by using Segmental Conditional Random Fields (SCRFs) to integrate multiple types of information."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145622870"
                        ],
                        "name": "Don X. Sun",
                        "slug": "Don-X.-Sun",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Sun",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don X. Sun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "A recent study [65] demonstrated the effectiveness of DBNDNNs for detecting subphonetic speech attributes (also known as phonological or articulatory features [66]) in the widely used Wall Street Journal speech database (5k-WSJ0)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The 22 attributes used in the recent work, as reported in [65], are a subset of the articulatory features explored in [66] and [67]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6352105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b588559fad19006dd4d10e6463179cbc04225eb7",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, the development of a feature\u2010based general statistical framework has been pursued for automatic speech recognition via novel designs of minimal or atomic units of speech, aiming at a parsimonious scheme to share the interword and interphone speech data and at a unified way to account for the context\u2010dependent behaviors in speech. The basic design philosophy has been motivated by the theory of distinctive features and by a new form of phonology which argues for use of multidimensional articulatory structures. In this paper, the most recently developed feature\u2010based recognizer is presented, which is capable of operating on all classes of English sounds. Detailed descriptions of the design considerations for the recognizer and of key aspects of the design process are provided. This process, which is called lexicon \u2018\u2018compilation,\u2019\u2019 consists of three elements (1) establishing a feature\u2010specification system; (2) constructing a probabilistic and fractional temporal overlapping pattern across the features; and (3) mapping from the feature\u2010overlap pattern to a state\u2010transition graph. A standard phonetic classification task from the TIMIT database is used as a test bed to evaluate the performance of the recognizer. The experimental results provide preliminary evidence for the effectiveness of the feature\u2010based approach to speech recognition."
            },
            "slug": "Speech-recognition-using-the-atomic-speech-units-Deng-Sun",
            "title": {
                "fragments": [],
                "text": "Speech recognition using the atomic speech units constructed from overlapping articulatory features"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents the most recently developed feature\u2010based recognizer, which is capable of operating on all classes of English sounds, and provides preliminary evidence for the effectiveness of the feature-based approach to speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [62] investigated a less direct way of producing feature vectors for the GMM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15954039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "375d7b8a70277d5d7b5e0cc999b03ba395c42901",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network (NN) bottleneck (BN) features are typically created by training a NN with a middle bottleneck layer. Recently, an alternative structure was proposed which trains a NN with a constant number of hidden units to predict output targets, and then reduces the dimensionality of these output probabilities through an auto-encoder, to create auto-encoder bottleneck (AE-BN) features. The benefit of placing the BN after the posterior estimation network is that it avoids the loss in frame classification accuracy incurred by networks that place the BN before the softmax. In this work, we investigate the use of pre-training when creating AE-BN features. Our experiments indicate that with the AE-BN architecture, pre-trained and deeper NNs produce better AE-BN features. On a 50-hour English Broadcast News task, the AE-BN features provide over a 1% absolute improvement compared to a state-of-the-art GMM/HMM with a WER of 18.8% and pre-trained NN hybrid system with a WER of 18.4%. In addition, on a larger 430-hour Broadcast News task, AE-BN features provide a 0.5% absolute improvement over a strong GMM/HMM baseline with a WER of 16.0%. Finally, system combination with the GMM/HMM baseline and AE-BN systems provides an additional 0.5% absolute on 430 hours over the AE-BN system alone, yielding a final WER of 15.0%."
            },
            "slug": "Auto-encoder-bottleneck-features-using-deep-belief-Sainath-Kingsbury",
            "title": {
                "fragments": [],
                "text": "Auto-encoder bottleneck features using deep belief networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The experiments indicate that with the AE-BN architecture, pre-trained and deeper NNs produce better AE-NP features, and system combination with the GMM/HMM baseline andAE-BN systems provides an additional 0.5% absolute improvement on a larger Broadcast News task."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Discriminative pre-training has also been found effective for the architectures called \u201cdeep convex network\u201d [51] and \u201cdeep stacking network\u201d [52], where pre-training is accomplished by convex optimization involving no generative models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93ba5b6a6d983c618d324e05086698ba43bf127a",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We recently developed context-dependent DNN-HMM (DeepNeural-Net/Hidden-Markov-Model) for large-vocabulary speech recognition. While achieving impressive recognition error rate reduction, we face the insurmountable problem of scalability in dealing with virtually unlimited amount of training data available nowadays. To overcome the scalability challenge, we have designed the deep convex network (DCN) architecture. The learning problem in DCN is convex within each module. Additional structure-exploited fine tuning further improves the quality of DCN. The full learning in DCN is batch-mode based instead of stochastic, naturally lending it amenable to parallel training that can be distributed over many machines. Experimental results on both MNIST and TIMIT tasks evaluated thus far demonstrate superior performance of DCN over the DBN (Deep Belief Network) counterpart that forms the basis of the DNN. The superiority is reflected not only in training scalability and CPU-only computation, but more importantly in classification accuracy in both tasks."
            },
            "slug": "Deep-Convex-Net:-A-Scalable-Architecture-for-Speech-Yu-Deng",
            "title": {
                "fragments": [],
                "text": "Deep Convex Net: A Scalable Architecture for Speech Pattern Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results on both MNIST and TIMIT tasks evaluated thus far demonstrate superior performance of DCN over the DBN (Deep Belief Network) counterpart that forms the basis of the DNN, reflected not only in training scalability and CPU-only computation, but more importantly in classification accuracy in both tasks."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2557391"
                        ],
                        "name": "Rohit Prabhavalkar",
                        "slug": "Rohit-Prabhavalkar",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Prabhavalkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohit Prabhavalkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398481836"
                        ],
                        "name": "E. Fosler-Lussier",
                        "slug": "E.-Fosler-Lussier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Fosler-Lussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fosler-Lussier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [8] NOvEMbER 2012 using such sequence classification criteria with shallow neural networks has already been shown by [36]\u2013[38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18366554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64595fd3db21e4e07252d8a2a5d640d2d7dd916d",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional random fields (CRFs) have recently found increased popularity in automatic speech recognition (ASR) applications. CRFs have previously been shown to be effective combiners of posterior estimates from multilayer perceptrons (MLPs) in phone and word recognition tasks. In this paper, we describe a novel hybrid Multilayer-CRF structure (ML-CRF), where a MLP-like hidden layer serves as input to the CRF; moreover, we propose a technique for directly training the ML-CRF to optimize a conditional log-likelihood based criterion, based on error backpropagation. The proposed technique thus allows for the implicit learning of suitable feature functions for the CRF. We present results for initial phone recognition experiments on the TIMIT database that indicate that our proposed method is a promising approach for training CRFs."
            },
            "slug": "Backpropagation-training-for-multilayer-conditional-Prabhavalkar-Fosler-Lussier",
            "title": {
                "fragments": [],
                "text": "Backpropagation training for multilayer conditional random field based phone recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel hybrid Multilayer-CRF structure is described, where a MLP-like hidden layer serves as input to the CRF, and a technique for directly training the ML- CRF to optimize a conditional log-likelihood based criterion, based on error backpropagation is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144620965"
                        ],
                        "name": "J. Ming",
                        "slug": "J.-Ming",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Ming",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ming"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145894070"
                        ],
                        "name": "F. Smith",
                        "slug": "F.-Smith",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Smith",
                            "middleNames": [
                                "Jack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "They were pretrained as DBNs and then discriminatively fine-tuned to predict target values for 384 HMM states that were obtained by clustering the context-dependent states in the baseline GMM-HMM system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17872406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6248fce7299e2b2871e78dcd506743ab86c7d27",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A crucial issue in triphone based continuous speech recognition is the large number of models to be estimated against the limited availability of training data. This problem can be relieved by composing a triphone model from less context-dependent models. This paper introduces a new statistical framework, derived from the Bayesian principle, to perform such a composition. The potential power of this new framework is explored, both algorithmically and experimentally, by an implementation with hidden Markov modeling techniques. This implementation is applied to the recognition of the 39-phone set on the TIMIT database. The new model achieves 74.4% and 75.6% accuracy, respectively, on the core and complete test sets."
            },
            "slug": "Improved-phone-recognition-using-Bayesian-triphone-Ming-Smith",
            "title": {
                "fragments": [],
                "text": "Improved phone recognition using Bayesian triphone models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new statistical framework, derived from the Bayesian principle, is introduced to perform a triphone model from less context-dependent models, and the potential power of this new framework is explored, both algorithmically and experimentally, by an implementation with hidden Markov modeling techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14787570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6629770cb6a00ad585918e71fe6dbad829ad0d1",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an application of recurrent networks for phone probability estimation in large vocabulary speech recognition. The need for efficient exploitation of context information is discussed; a role for which the recurrent net appears suitable. An overview of early developments of recurrent nets for phone recognition is given along with the more recent improvements that include their integration with Markov models. Recognition results are presented for the DARPA TIMIT and Resource Management tasks, and it is concluded that recurrent nets are competitive with traditional means for performing phone probability estimation."
            },
            "slug": "An-application-of-recurrent-nets-to-phone-Robinson",
            "title": {
                "fragments": [],
                "text": "An application of recurrent nets to phone probability estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Recognition results are presented for the DARPA TIMIT and Resource Management tasks, and it is concluded that recurrent nets are competitive with traditional means for performing phone probability estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738798"
                        ],
                        "name": "H. Hermansky",
                        "slug": "H.-Hermansky",
                        "structuredName": {
                            "firstName": "Hynek",
                            "lastName": "Hermansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hermansky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15052804,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b578f4faeb00b808e8786d897447f2493b12b4e9",
            "isKey": false,
            "numCitedBy": 2939,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "A new technique for the analysis of speech, the perceptual linear predictive (PLP) technique, is presented and examined. This technique uses three concepts from the psychophysics of hearing to derive an estimate of the auditory spectrum: (1) the critical-band spectral resolution, (2) the equal-loudness curve, and (3) the intensity-loudness power law. The auditory spectrum is then approximated by an autoregressive all-pole model. A 5th-order all-pole model is effective in suppressing speaker-dependent details of the auditory spectrum. In comparison with conventional linear predictive (LP) analysis, PLP analysis is more consistent with human hearing. The effective second formant F2' and the 3.5-Bark spectral-peak integration theories of vowel perception are well accounted for. PLP analysis is computationally efficient and yields a low-dimensional representation of speech. These properties are found to be useful in speaker-independent automatic-speech recognition."
            },
            "slug": "Perceptual-linear-predictive-(PLP)-analysis-of-Hermansky",
            "title": {
                "fragments": [],
                "text": "Perceptual linear predictive (PLP) analysis of speech."
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A new technique for the analysis of speech, the perceptual linear predictive (PLP) technique, which uses three concepts from the psychophysics of hearing to derive an estimate of the auditory spectrum, and yields a low-dimensional representation of speech."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Acoustical Society of America"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Viterbi decoding was done using the Google s ystem [49] with modifications to compute the scaled log likelihoods from the estimates of the posterior probabi lities and the state priors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 200
                            }
                        ],
                        "text": "6s to 210ms, without decreasing recognition accuracy, by quantizing the weights down to 8 bi ts and using the very fast SIMD primitives for fixedpoint computation that are provided by a modern x86 CPU[49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "[49] V. Vanhoucke, A. Senior, and M. Z. Mao, \u201cImproving the speed of neural networks on CPUs,\u201d in Proc."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15196840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbeaa499e10e98515f7e1c4ad89165e8c0677427",
            "isKey": false,
            "numCitedBy": 688,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3\u00d7 improvement over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware."
            },
            "slug": "Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior",
            "title": {
                "fragments": [],
                "text": "Improving the speed of neural networks on CPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses speech recognition as an example task, and shows that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speed up over an aggressively optimized floating-point baseline at no cost in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2182856"
                        ],
                        "name": "S. Ortmanns",
                        "slug": "S.-Ortmanns",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Ortmanns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ortmanns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122690"
                        ],
                        "name": "X. Aubert",
                        "slug": "X.-Aubert",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Aubert",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Aubert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 68236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd8bdcd6969670e8e43ff74cf458cfd5804c9e2f",
            "isKey": false,
            "numCitedBy": 476,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper describes a method for the construction of a word graph (or lattice) for large vocabulary, continuous speech recognition. The advantage of a word graph is that a fairly good degree of decoupling between acoustic recognition at the 10-ms level and the final search at the word level using a complicated language model can be achieved. The word graph algorithm is obtained as an extension of the one-pass beam search strategy using word dependent copies of the word models or lexical trees. The method has been tested successfully on the 20 000-word NAB'94 task (American English, continuous speech, 20 000 words, speaker independent) and compared with the integrated method. The experiments show that the word graph density can be reduced to an average number of about 10 word hypotheses, i.e. word edges in the graph, per spoken word with virtually no loss in recognition performance."
            },
            "slug": "A-word-graph-algorithm-for-large-vocabulary-speech-Ortmanns-Ney",
            "title": {
                "fragments": [],
                "text": "A word graph algorithm for large vocabulary continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A method for the construction of a word graph (or lattice) for large vocabulary, continuous speech recognition and it is shown that the word graph density can be reduced to an average number of about 10 word hypotheses, per spoken word with virtually no loss in recognition performance."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699205"
                        ],
                        "name": "S. Furui",
                        "slug": "S.-Furui",
                        "structuredName": {
                            "firstName": "Sadaoki",
                            "lastName": "Furui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Furui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "In these systems the acous ti input is typically represented by concatenating Mel Frequency Cepstral Coefficients (MFCCs) or Perceptual L inear Predictive coefficients (PLPs) [4] computed from the raw waveform, and their first- and second-order temp oral differences [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62219893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "374d9f4d9e8e1aae89033c9abe61e74e9bf2869b",
            "isKey": false,
            "numCitedBy": 1402,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes new techniques for automatic speaker verification using telephone speech. The operation of the system is based on a set of functions of time obtained from acoustic analysis of a fixed, sentence-long utterance. Cepstrum coefficients are extracted by means of LPC analysis successively throughout an utterance to form time functions, and frequency response distortions introduced by transmission systems are removed. The time functions are expanded by orthogonal polynomial representations and, after a feature selection procedure, brought into time registration with stored reference functions to calculate the overall distance. This is accomplished by a new time warping method using a dynamic programming technique. A decision is made to accept or reject an identity claim, based on the overall distance. Reference functions and decision thresholds are updated for each customer. Several sets of experimental utterances were used for the evaluation of the system, which include male and female utterances recorded over a conventional telephone connection. Male utterances processed by ADPCM and LPC coding systems were used together with unprocessed utterances. Results of the experiment indicate that verification error rate of one percent or less can be obtained even if the reference and test utterances are subjected to different transmission conditions."
            },
            "slug": "Cepstral-analysis-technique-for-automatic-speaker-Furui",
            "title": {
                "fragments": [],
                "text": "Cepstral analysis technique for automatic speaker verification"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "New techniques for automatic speaker verification using telephone speech based on a set of functions of time obtained from acoustic analysis of a fixed, sentence-long utterance using a new time warping method using a dynamic programming technique."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5575601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "isKey": false,
            "numCitedBy": 12422,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact)."
            },
            "slug": "Understanding-the-difficulty-of-training-deep-Glorot-Bengio",
            "title": {
                "fragments": [],
                "text": "Understanding the difficulty of training deep feedforward neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66648677"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749064"
                        ],
                        "name": "D. Kanevsky",
                        "slug": "D.-Kanevsky",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Kanevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kanevsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Using contextdependent HMM states, it is possible to outperform state-of-theart BMMI trained GMM-HMM systems with a two-hidden-layer\nneural network without using any pretraining [43], though using more hidden layers and pretraining works even better."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The acoustic models were further improved with BMMI."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "1% Triphone GMM-HMMs discriminatively trained w/ BMMI [32] 21."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "The baseline GMM-HMM system had a mixture of 40 Gaussians per (tied) HMM state that were first trained generatively to optimize a maximum likelihood (ML) criterion and then refined discriminatively to optimize a boosted maximummutual-information (BMMI) criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Semitied covariances (STCs) are used in the GMMs to model the LDA transformed features and BMMI [46] was used to train the model discriminatively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Using alignments from a baseline system, [32] trained a DBNDNN acoustic model on 50 hours of data from the 1996 and 1997 English Broadcast News Speech Corpora [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Using a DNN reduced the word error rate (WER) from the 27.4% of the baseline GMM-HMM (trained with BMMI) to 18.5%\u2014a 33% relative reduction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Further processing of LDA features is performed to create SAT features using VTLN followed by fMLLR. Finally, feature and model-space discriminative training is applied using the BMMI or MPE criterion."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14554298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "797f2dbebdcc17986305d4b340715224c74edd3b",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of exemplar-based methods, such as support vector machines (SVMs), k-nearest neighbors (kNNs) and sparse representations (SRs), in speech recognition has thus far been limited. Exemplar-based techniques utilize information about individual training examples and are computationally expensive, making it particularly difficult to investigate these methods on large-vocabulary continuous speech recognition (LVCSR) tasks. While research in LVCSR provides a good testbed to tackle real-world speech recognition problems, research in this area suffers from two main drawbacks. First, the overall complexity of an LVCSR system makes error analysis quite difficult. Second, exploring new research ideas on LVCSR tasks involves training and testing state-of-the-art LVCSR systems, which can render a large turnaround time. This makes a small vocabulary task such as TIMIT more appealing. TIMIT provides a phonetically rich and hand-labeled corpus that allows easy insight into new algorithms. However, research ideas explored for small vocabulary tasks do not always provide gains on LVCSR systems. In this paper, we combine the advantages of using both small and large vocabulary tasks by taking well-established techniques used in LVCSR systems and applying them on TIMIT to establish a new baseline. We then utilize these existing LVCSR techniques in creating a novel set of exemplar-based sparse representation (SR) features. Using these existing LVCSR techniques, we achieve a phonetic error rate (PER) of 19.4% on the TIMIT task. The additional use of SR features reduce the PER to 18.6%. We then explore applying the SR features to a large vocabulary Broadcast News task, where we achieve a 0.3% absolute reduction in word error rate (WER)."
            },
            "slug": "Exemplar-Based-Sparse-Representation-Features:-From-Sainath-Ramabhadran",
            "title": {
                "fragments": [],
                "text": "Exemplar-Based Sparse Representation Features: From TIMIT to LVCSR"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper combines the advantages of using both small and large vocabulary tasks by taking well-established techniques used in LVCSR systems and applying them on TIMIT to establish a new baseline, creating a novel set of exemplar-based sparse representation (SR) features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758476"
                        ],
                        "name": "Christian Plahl",
                        "slug": "Christian-Plahl",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Plahl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Plahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "For speech recognition, improved performance was achieved on both TIMIT and Broadcast News tasks by pretraining with a type of autoencoder that tries to find sparse codes [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6428154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0383b56a5275819c95cef5af11fdba72c5afacb",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann Machines (RBM) continue to be a popular methodology to pre-train weights of Deep Belief Networks (DBNs). However, the RBM objective function cannot be maximized directly. Therefore, it is not clear what function to monitor when deciding to stop the training, leading to a challenge in managing the computational costs. The Sparse Encoding Symmetric Machine (SESM) has been suggested as an alternative method for pre-training. By placing a sparseness term on the NN output codebook, SESM allows the objective function to be optimized directly and reliably be monitored as an indicator to stop the training. In this paper, we explore SESM to pre-train DBNs and apply this the first time to speech recognition. First, we provide a detailed analysis comparing the behavior of SESM and RBM. Second, we compare the performance of SESM pre-trained and RBM pre-trained DBNs on TIMIT and a 50 hour English Broadcast News task. Results indicate that pre-trained DBNs using SESM and RBMs achieve comparable performance and outperform randomly initialized DBNs with SESM providing a much easier stopping criterion relative to RBM."
            },
            "slug": "Improved-pre-training-of-Deep-Belief-Networks-using-Plahl-Sainath",
            "title": {
                "fragments": [],
                "text": "Improved pre-training of Deep Belief Networks using Sparse Encoding Symmetric Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results indicate that pre-trained DBNs using SESM and RBMs achieve comparable performance and outperform randomly initialized DBN\u2019s with SESm providing a much easier stopping criterion relative to RBM."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749064"
                        ],
                        "name": "D. Kanevsky",
                        "slug": "D.-Kanevsky",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Kanevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kanevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484153"
                        ],
                        "name": "K. Visweswariah",
                        "slug": "K.-Visweswariah",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Visweswariah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Visweswariah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Using contextdependent HMM states, it is possible to outperform state-of-theart BMMI trained GMM-HMM systems with a two-hidden-layer\nneural network without using any pretraining [43], though using more hidden layers and pretraining works even better."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The acoustic models were further improved with BMMI."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "The baseline GMM-HMM system had a mixture of 40 Gaussians per (tied) HMM state that were first trained generatively to optimize a maximum likelihood (ML) criterion and then refined discriminatively to optimize a boosted maximummutual-information (BMMI) criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Semitied covariances (STCs) are used in the GMMs to model the LDA transformed features and BMMI [46] was used to train the model discriminatively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Semi-Tied Covariances (STC) are used in the GMMs to model the LDA transformed features and BMMI[46] was used to train the model discriminatively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Using a DNN reduced the word error rate (WER) from the 27.4% of the baseline GMM-HMM (trained with BMMI) to 18.5%\u2014a 33% relative reduction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Further processing of LDA features is performed to create SAT features using VTLN followed by fMLLR. Finally, feature and model-space discriminative training is applied using the BMMI or MPE criterion."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14254768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8770b4a5ca7734c88e5755f9558f79e93229c023",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a modified form of the maximum mutual information (MMI) objective function which gives improved results for discriminative training. The modification consists of boosting the likelihoods of paths in the denominator lattice that have a higher phone error relative to the correct transcript, by using the same phone accuracy function that is used in Minimum Phone Error (MPE) training. We combine this with another improvement to our implementation of the Extended Baum-Welch update equations for MMI, namely the canceling of any shared part of the numerator and denominator statistics on each frame (a procedure that is already done in MPE). This change affects the Gaussian-specific learning rate. We also investigate another modification whereby we replace I-smoothing to the ML estimate with I-smoothing to the previous iteration's value. Boosted MMI gives better results than MPE in both model and feature-space discriminative training, although not consistently."
            },
            "slug": "Boosted-MMI-for-model-and-feature-space-training-Povey-Kanevsky",
            "title": {
                "fragments": [],
                "text": "Boosted MMI for model and feature-space discriminative training"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A modified form of the maximum mutual information (MMI) objective function which gives improved results for discriminative training by boosting the likelihoods of paths in the denominator lattice that have a higher phone error relative to the correct transcript."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13406,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2141386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "956b80fe8a56f90782fcb4fe1c536b24496094ba",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "While research in large vocabulary continuous speech recognition (LVCSR) has sparked the development of many state of the art research ideas, research in this domain suffers from two main drawbacks. First, because of the large number of parameters and poorly labeled transcriptions, gaining insight into further improvements based on error analysis is very difficult. Second, LVCSR systems often take a significantly longer time to train and test new research ideas compared to small vocabulary tasks. A small vocabulary task like TIMIT provides a phonetically rich and hand-labeled corpus and offers a good test bed to study algorithmic improvements. However, oftentimes research ideas explored for small vocabulary tasks do not always provide gains on LVCSR systems. In this paper, we address these issues by taking the standard \u021crecipe\u021d used in typical LVCSR systems and applying it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods. We find that at the speaker-independent (SI) level, our results offer comparable performance to other SI HMM systems. By taking advantage of speaker adaptation and discriminative training techniques commonly used in LVCSR systems, we achieve an error rate of 20%, the best results reported on the TIMIT task to date, moving us closer to the human reported phonetic recognition error rate of 15%. We propose the use of this system as the baseline for future research and believe that it will serve as a good framework to explore ideas that will carry over to LVCSR systems."
            },
            "slug": "An-exploration-of-large-vocabulary-tools-for-small-Sainath-Ramabhadran",
            "title": {
                "fragments": [],
                "text": "An exploration of large vocabulary tools for small vocabulary phonetic recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper takes the standard \u021crecipe\u021d used in typical LVCSR systems and applies it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods and finds that at the speaker-independent (SI) level, the results offer comparable performance to other SI HMM systems."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Some recent atte mpts are described in [52], [57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Discri minative pre-training has also been found effective for the architectures called \u201cdeep convex network\u201d [51] and \u201cdeep stacking network\u201d [52], where pre-training is accomplished by convex optimization involving no generati ve models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16171497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5352b7ca90cbe4938f8e71a25d49517e7f94670a",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Neural Networks (DNNs) have shown remarkable success in pattern recognition tasks. However, parallelizing DNN training across computers has been difficult. We present the Deep Stacking Network (DSN), which overcomes the problem of parallelizing learning algorithms for deep architectures. The DSN provides a method of stacking simple processing modules in buiding deep architectures, with a convex learning problem in each module. Additional fine tuning further improves the DSN, while introducing minor non-convexity. Full learning in the DSN is batch-mode, making it amenable to parallel training over many machines and thus be scalable over the potentially huge size of the training data. Experimental results on both the MNIST (image) and TIMIT (speech) classification tasks demonstrate that the DSN learning algorithm developed in this work is not only parallelizable in implementation but it also attains higher classification accuracy than the DNN."
            },
            "slug": "Scalable-stacking-and-learning-for-building-deep-Deng-Yu",
            "title": {
                "fragments": [],
                "text": "Scalable stacking and learning for building deep architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The Deep Stacking Network (DSN) is presented, which overcomes the problem of parallelizing learning algorithms for deep architectures and provides a method of stacking simple processing modules in buiding deep architectures, with a convex learning problem in each module."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453026"
                        ],
                        "name": "Isabelle Lajoie",
                        "slug": "Isabelle-Lajoie",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Lajoie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabelle Lajoie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "On vision tasks, performance similar to RBMs can be achieved by pre-training with \u201cdenoising\u201d autoencoders [54] that are regularized by setting a subset of the inputs to zero or \u201ccontractive\u201d autoencoders [55] that are regularized by penalizing the gradient of the activities of the hidden units with respect to the inputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17804904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "isKey": false,
            "numCitedBy": 5608,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
            },
            "slug": "Stacked-Denoising-Autoencoders:-Learning-Useful-in-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56128297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f09ce0dd760857e0d0e4879f6e2543f04c5d33",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation."
            },
            "slug": "Maximum-mutual-information-estimation-of-hidden-for-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method for estimating the parameters of hidden Markov models of speech is described and recognition results are presented comparing this method with maximum likelihood estimation."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 201
                            }
                        ],
                        "text": "DNN\u2019s can be discriminatively trained by backpropagating d erivatives of a cost function that measures the discrepancy between the target outputs and the actual outpu ts produced for each training case[14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20327,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107569404"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79770421"
                        ],
                        "name": "D. O'Shaughnessy",
                        "slug": "D.-O'Shaughnessy",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "O'Shaughnessy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. O'Shaughnessy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 170
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, but even with this constraint it was impossible to train all possible combinations of number of hidden layers [1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 174
                            }
                        ],
                        "text": "The biggest single advance occured nearly four decades ago with the introduction of the Expectation-Maximization (EM) algorithm for training Hidden Markov Models (HMMs) (see [1], [2] for informative historical reviews of the introduction of HMMs)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11371775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde885c9107d77d48ebb0a3e7cfcf58ced4270b7",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "T o advance research, it is important to identify promising future research directions , especially those that have not been adequately pursued or funded in the past. The working group producing this article was charged to elicit from the human language technology (HLT) community a set of well-considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition (ASR) and understanding. ASR has been an area of great interest and activity to the signal processing and HLT communities over the past several decades. As a first step, this group reviewed major developments in the field and the circumstances that led to their success and then focused on areas it deemed especially fertile for future research. Part 1 of this article will focus on historically significant developments in the ASR area, including several major research efforts that were guided by different funding agencies, and suggest general areas in which to focus research. Part 2 (to appear in the next issue) will explore in more detail several new avenues holding promise for substantial improvements in ASR performance. These entail cross-disciplinary research and specific approaches to address three-to-five-year grand challenges aimed at stimulating advanced research by dealing with realistic tasks of broad interest. SIGNIFICANT DEVELOPMENTS IN SPEECH RECOGNITION AND UNDERSTANDING The period since the mid-1970s has witnessed the multidisciplinary field of ASR proceed from its infancy to its coming of age and into a quickly growing number of practical applications and commercial markets. Despite its many achievements, however, ASR still remains far from being a solved problem. As in the past, we expect that further research and development will enable us to create increasingly powerful systems, deploy-able on a worldwide basis. This section briefly reviews highlights of major developments in ASR in five areas: infrastructure, knowledge representation, models and algorithms, search, and metadata. Broader and deeper discussions of these areas can INFRASTRUCTURE Moore's Law observes long-term progress in computer development and predicts doubling the amount of computation achievable for a given cost every 12 to 18 months, as well as a comparably shrinking cost of memory. These developments have been instrumental in enabling ASR researchers to run increasingly complex algorithms in sufficiently short time frames (e.g., meaningful experiments that can be done in less than a day) to make great progress since 1975. The availability of common speech corpora for speech training, \u2026"
            },
            "slug": "Developments-and-Directions-in-Speech-Recognition-,-Baker-Deng",
            "title": {
                "fragments": [],
                "text": "Developments and Directions in Speech Recognition and Understanding , Part 1 T"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The working group producing this article was charged to elicit from the human language technology community a set of well-considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition (ASR) and understanding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095625734"
                        ],
                        "name": "\u53e4\u4e95 \u8c9e\u7155",
                        "slug": "\u53e4\u4e95-\u8c9e\u7155",
                        "structuredName": {
                            "firstName": "\u53e4\u4e95",
                            "lastName": "\u8c9e\u7155",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u53e4\u4e95 \u8c9e\u7155"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61070897,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0bb1e2cc1a3b66e39d643d46e03c462a1105d454",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Digital-speech-processing,-synthesis,-and-\u53e4\u4e95",
            "title": {
                "fragments": [],
                "text": "Digital speech processing, synthesis, and recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107569404"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79770421"
                        ],
                        "name": "D. O'Shaughnessy",
                        "slug": "D.-O'Shaughnessy",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "O'Shaughnessy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. O'Shaughnessy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 357467,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b829f28c6329f66bb09750194ae36315ec7838ac",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "To advance research, it is important to identify promising future research directions, especially those that have not been adequately pursued or funded in the past. The working group producing this article was charged to elicit from the human language technology (HLT) community a set of well-considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition (ASR) and understanding. ASR has been an area of great interest and activity to the signal processing and HLT communities over the past several decades. As a first step, this group reviewed major developments in the field and the circumstances that led to their success and then focused on areas it deemed especially fertile for future research. Part 1 of this article will focus on historically significant developments in the ASR area, including several major research efforts that were guided by different funding agencies, and suggest general areas in which to focus research."
            },
            "slug": "Developments-and-directions-in-speech-recognition-1-Baker-Deng",
            "title": {
                "fragments": [],
                "text": "Developments and directions in speech recognition and understanding, Part 1 [DSP Education]"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The working group producing this article was charged to elicit from the human language technology community a set of well-considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition (ASR) and understanding."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 264
                            }
                        ],
                        "text": "His other contributions include Boltzmann machines, distributed representations, time-delay neural nets, mixtures of experts, variational learning, CD learning, and DBNs.\nLi Deng (deng@microsoft.com) received his Ph.D. degree from the University of Wisconsin\u2013Madison."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14805281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "isKey": false,
            "numCitedBy": 973,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."
            },
            "slug": "An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation of deep architectures on problems with many factors of variation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107569404"
                        ],
                        "name": "J. Baker",
                        "slug": "J.-Baker",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1418438552"
                        ],
                        "name": "Chin-Hul Lee",
                        "slug": "Chin-Hul-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hul",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hul Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1419536825"
                        ],
                        "name": "Douglas O'Shgughnessy",
                        "slug": "Douglas-O'Shgughnessy",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "O'Shgughnessy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas O'Shgughnessy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63630945,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "37c6843c66a0e18fbbc383a7cf344b7a7482746d",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "To advance research, it is important to identify promising future research directions, especially those that have not been adequately pursued or funded in the past. The working group producing this article was charged to elicit from the human language technology (HLT) community a set of well-considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition (ASR) and understanding. ASR has been an area of great interest and activity to the signal processing and HLT communities over the past several decades. As a first step, this group reviewed major developments in the field and the circumstances that led to their success and then focused on areas it deemed especially fertile for future research. Part 1 of this article will focus on historically significant developments in the ASR area, including several major research efforts that were guided by different funding agencies, and suggest general areas in which to focus research. Part 2 (to appear in the next issue) will explore in more detail several new avenues holding promise for substantial improvements in ASR performance. These entail cross-disciplinary research and specific approaches to address three-to-five-year grand challenges aimed at stimulating advanced research by dealing with realistic tasks of broad interest."
            },
            "slug": "Research-Developments-and-Directions-in-Speech-and-Baker-Deng",
            "title": {
                "fragments": [],
                "text": "Research Developments and Directions in Speech Recognition and Understanding, Part 1"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The working group producing this article was charged to elicit from the human language technology community a set of well-considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition (ASR) and understanding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "More sophisticated o ptimization methods that can be used on larger minibatches include non-linear conjugate-gradient [17], LBFG S [58] and \u201cHessian Free\u201d methods adapted to work for deep neural networks [59]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 305
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, b ut even with this constraint it was impossible to train all possible combinations of number of hidden layers [ 1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 261
                            }
                        ],
                        "text": "After this generative \u201cpre-training\u201d, the multiple layers of feature d tectors can be used as a much better starting point for a discriminative \u201cfine-tuning\u201d phase during which backprop agation through the DNN slightly adjusts the weights found in pre-training [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "If extra hidd en layers are added before and/or after the code layer, an aut oencoder can compress data much better than Principal Components Analysis[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14634,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47778994"
                        ],
                        "name": "A. Lahiri",
                        "slug": "A.-Lahiri",
                        "structuredName": {
                            "firstName": "Ahbik",
                            "lastName": "Lahiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41227297"
                        ],
                        "name": "B. Prochnow",
                        "slug": "B.-Prochnow",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Prochnow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Prochnow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 122
                            }
                        ],
                        "text": "More sophisticated optimization methods that can be used on larger minibatches include nonlinear conjugate-gradient [17], LBFGS [58] <AU: please spell out LBFGS>, and \u201cHessian-free\u201d methods adapted to work for DNNs [59]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "More sophisticated optimization methods that can be used on larger minibatches include non-linear conjugate-gradient [17], LBFGS [58] and \u201cHessian Free\u201d methods adapted to work for deep neural networks [59]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6076653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "053912e76e50c9f923a1fc1c173f1365776060cc",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining."
            },
            "slug": "On-optimization-methods-for-deep-learning-Le-Ngiam",
            "title": {
                "fragments": [],
                "text": "On optimization methods for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "With the EM algorithm, it became poss ible to develop speech recognition systems for real world tasks using the richness of Gaussian mixture mode ls (GMM) [3] to represent the relationship between HMM states and the acoustic input."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31558521,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5548693e214f9c7489086c25520f982c6b544be5",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "To use probabilistic functions of a Markov chain to model certain parameterizations of the speech signal, we extend an estimation technique of Liporace to the eases of multivariate mixtures, such as Gaussian sums, and products of mixtures. We also show how these problems relate to Liporace's original framework."
            },
            "slug": "Maximum-likelihood-estimation-for-multivariate-of-Juang-Levinson",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation for multivariate mixture observations of markov chains"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "To use probabilistic functions of a Markov chain to model certain parameterizations of the speech signal, an estimation technique of Liporace is extended to the eases of multivariate mixtures, such as Gaussian sums, and products of mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Very large training sets [16] can reduce overfitting whilst preserving modeling powe r, but only by making training very computationally expensive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1918673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
            "isKey": false,
            "numCitedBy": 875,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning."
            },
            "slug": "Deep,-Big,-Simple-Neural-Nets-for-Handwritten-Digit-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425018"
                        ],
                        "name": "S. Rifai",
                        "slug": "S.-Rifai",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Rifai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rifai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090922238"
                        ],
                        "name": "X. Muller",
                        "slug": "X.-Muller",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "On vision tasks, performance similar to RBMs can be achieved by pre-training with \u201cdenoising\u201d autoencoders [54] that are regularized by setting a subset of the inputs to zero or \u201ccontractive\u201d autoencoders [55] that are regularized by penalizing the gradient of the activities of the hidden units with respect to the inputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8141422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "195d0a8233a7a46329c742eaff56c276f847fadc",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining."
            },
            "slug": "Contractive-Auto-Encoders:-Explicit-Invariance-Rifai-Vincent",
            "title": {
                "fragments": [],
                "text": "Contractive Auto-Encoders: Explicit Invariance During Feature Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026A vAriety Of lvcsr tAsks.\nthe sUccesses Achieved Using pretrAining led tO A resUrgence Of\ninterest in dnns fOr AcOUstic MOdeling.\ncUrrently, the biggest disAdvAntAge Of dnns cOMpAred with gMMs is thAt it is MUch hArder tO MAke gOOd Use Of lArge clUster MAchines tO trAin theM On MAssive dAtA sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "He joined Microsoft Corporation in 1998 and MSR in 2002, where he is a researcher."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4567,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320076"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17976965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d63cdc1d1f023c63f8aa3b64cd5e853670680c3e",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we studied the objective functions of MMI, MCE, and MPE/MWE for discriminative learning in sequential pattern recognition. We presented an approach that unifies the objective functions of MMI, MCE, and MPE/MWE in a common rational-function form of (25). The exact structure of the rational-function form for each discriminative criterion was derived and studied. While the rational-function form of MMI has been known in the past, we provided the theoretical proof that the similar rational-function form exists for the objective functions of MCE and MPE/MWE. Moreover, we showed that the rational function forms for objective functions of MMI, MCE, and MPE/MWE differ in the constant weighting factors CDT (s1 . . . sR) and these weighting factors depend only on the labeled sequence s1 . . . sR, and are independent of the parameter set - to be optimized. The derived rational-function form for MMI, MCE, and MPE/MWE allows the GT/EBW-based parameter optimization framework to be applied directly in discriminative learning. In the past, lack of the appropriate rational-function form was a difficulty for MCE and MPE/MWE, because without this form, the GT/EBW-based parameter optimization framework cannot be directly applied. Based on the unified rational-function form, in a tutorial style, we derived the GT/EBW-based parameter optimization formulas for both discrete HMMs and CDHMMs in discriminative learning using MMI, MCE, and MPE/MWE criteria. The unifying review provided in this article has been based upon a large number of earlier contributions that have been cited and discussed throughout the article. Here we provide a brief summary of such background work. Extension to large-scale speech recognition tasks was accomplished in the work of [59] and [60]. The dissertation of [47] further improved the MMI criterion to that of MPE/MWE. In a parallel vein, the work of [20] provided an alternative approach to that of [41], with an attempt to more rigorously provide a CDHMM model re-estimation formula that gives positive growth of the MMI objective function. A crucial error of this attempt was corrected in [2] for establishing an existence proof of such positive growth. The main goal of this article is to provide an underlying foundation for MMI, MCE, and MPE/MWE at the objective function level to facilitate the development of new parameter optimization techniques and to incorporate other pattern recognition concepts, e.g., discriminative margins [66], into the current discriminative learning paradigm."
            },
            "slug": "Discriminative-learning-in-sequential-pattern-He-Deng",
            "title": {
                "fragments": [],
                "text": "Discriminative learning in sequential pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The main goal of this article is to provide an underlying foundation for MMI, MCE, and MPE/MWE at the objective function level to facilitate the development of new parameter optimization techniques and to incorporate other pattern recognition concepts, e.g., discriminative margins [66], into the current discrim inative learning paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21145246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e95d3934e51107da7610acd0b1bcb6551671f9f1",
            "isKey": false,
            "numCitedBy": 2744,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers."
            },
            "slug": "A-Practical-Guide-to-Training-Restricted-Boltzmann-Hinton",
            "title": {
                "fragments": [],
                "text": "A Practical Guide to Training Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This guide is an attempt to share expertise at training restricted Boltzmann machines with other machine learning researchers."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "More sophisticated o ptimization methods that can be used on larger minibatches include non-linear conjugate-gradient [17], LBFG S [58] and \u201cHessian Free\u201d methods adapted to work for deep neural networks [59]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11154521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "isKey": false,
            "numCitedBy": 844,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it."
            },
            "slug": "Deep-learning-via-Hessian-free-optimization-Martens",
            "title": {
                "fragments": [],
                "text": "Deep learning via Hessian-free optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A 2nd-order optimization method based on the \"Hessian-free\" approach is developed, and applied to training deep auto-encoders, and results superior to those reported by Hinton & Salakhutdinov (2006) are obtained."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000910233"
                        ],
                        "name": "VincentPascal",
                        "slug": "VincentPascal",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "VincentPascal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "VincentPascal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644091710"
                        ],
                        "name": "LarochelleHugo",
                        "slug": "LarochelleHugo",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "LarochelleHugo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "LarochelleHugo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000910158"
                        ],
                        "name": "LajoieIsabelle",
                        "slug": "LajoieIsabelle",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "LajoieIsabelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "LajoieIsabelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644010966"
                        ],
                        "name": "BengioYoshua",
                        "slug": "BengioYoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "BengioYoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "BengioYoshua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000909106"
                        ],
                        "name": "ManzagolPierre-Antoine",
                        "slug": "ManzagolPierre-Antoine",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "ManzagolPierre-Antoine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ManzagolPierre-Antoine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "On vision task s, performance similar to RBMs can be achieved by pre-training with \u201cdenoising\u201d autoencoders [54] that ar e regularized by setting a subset of the inputs to zero or \u201ccontractive\u201d autoencoders [55] that are regularized by penalizing the gradient of the activities of the hidden units with respect to the inputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 223716621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6093f35d7419f0ba34f2357f3ef9e50ee88cf33",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting..."
            },
            "slug": "Stacked-Denoising-Autoencoders:-Learning-Useful-in-VincentPascal-LarochelleHugo",
            "title": {
                "fragments": [],
                "text": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An original strategy for building deep networks is explored, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": false,
            "numCitedBy": 18213,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 305
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, b ut even with this constraint it was impossible to train all possible combinations of number of hidden layers [ 1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "Using contextdependent HMM states, it is possible to outperform state-of-theart BMMI trained GMM-HMM systems with a two-hidden-layer\nneural network without using any pretraining [43], though using more hidden layers and pretraining works even better."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The acoustic models were further improved with BMMI."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 243
                            }
                        ],
                        "text": "After weights are learned via cross entropy, these weights are used as a starting point for a second phase of fine-tuning using a sequence criterion [37] that utilizes the MPE objective function, a discriminative objective function similar to MMI [7] but which takes into account phoneme error rate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "In the more recent work reported in [31], one popular type of sequence classification criterion, maximum mutual information (MMI), proposed as early as 1986 [7], was successfully applied to learn DBN-DNN weights for the TIMIT phone recognition task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Using the joint optimization with careful scheduling, we observe that the sequential MMI training can outperform the frame-level training by about 5% relative within the same system in the same laboratory.\nconvolutional dnns for phone claSSification and recoGnition All the previously cited work reported phone recognition results on the TIMIT database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 248
                            }
                        ],
                        "text": "The baseline GMM-HMM system had a mixture of 40 Gaussians per (tied) HMM state that were first trained generatively to optimize a maximum likelihood (ML) criterion and then refined discriminatively to optimize a boosted maximummutual-information (BMMI) criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 347,
                                "start": 344
                            }
                        ],
                        "text": "The recognition accuracy of a GMM-HMM system can be further i mproved if it is discriminatively fine-tuned after it has been generatively trained to maximize its proba bility of generating the observed data, especially if the discriminative objective function used for training is closely related to the error rate on phones, words or sentences[7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Semitied covariances (STCs) are used in the GMMs to model the LDA transformed features and BMMI [46] was used to train the model discriminatively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 250
                            }
                        ],
                        "text": "After weights are learned via cro ss-entropy, these weights are used as a starting point for a second phase of fine-tuning using a sequence criterion [ 37] which utilizes the MPE objective function, a discriminative objective function similar to MMI [7] but wh ich takes into account phoneme error rate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "In addition to the DBN-DNN training, sequencelevel discriminative fine-tuning of the neural network was performed using MMI, similar to the method proposed in [37]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "[46] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah, \u201cBoosted MMI for model and feature-space discriminative training,\u201d in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "MMI optimizes the conditional probability ( )p l v1: 1:T T; of the whole sequence of labels, l1:T , with length T, given the whole visible feature utterance v1:T , or equivalently the hidden feature sequence h1:T extracted by the DNN\n( ) ( )\n( )\n( , )exp\np l v p l h\nZ h\nl l h ,\n: : : :\n:\n1 1\n, 11\nT T T T\nT\nij ij t tt T l d tdd D t T\n1 1 1 1\n1\nt\n;;\nc z m\n=\n= +-\n= == ` j/ // (17)\nwhere the transition feature ( , )l lij t t1z - takes on a value of one if l i1t =- and l jt = , and otherwise takes on a value of zero, where ijc is the parameter associated with this transition feature, htd is the dth dimension of the hidden unit value at the tth frame at the final layer of the DNN, and where D is the number of units in the final hidden layer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 336
                            }
                        ],
                        "text": "When GMMs were first used for acoustic modeling they were train ed as generative models using the EM algorithm and it was some time before researchers showed that significa nt gains could be achieved by a subsequent stage of discriminative training using an objective function more c losely related to the ultimate goal of an ASR system[7], [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "Using a DNN reduced the word error rate (WER) from the 27.4% of the baseline GMM-HMM (trained with BMMI) to 18.5%\u2014a 33% relative reduction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 180
                            }
                        ],
                        "text": "Further processing of LDA features is performed to create SAT features using VTLN followed by fMLLR. Finally, feature and model-space discriminative training is applied using the BMMI or MPE criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 204
                            }
                        ],
                        "text": "However, it has long been known that sequence classification criteria, whic h are more directly correlated with the overall word or phone error rate, can be very helpful in improving recogniti o accuracy [7], [35] and the benefit of using such sequence classification criteria with shallow neural netwo rks has already been shown by [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "In the more recent work reported in [31], one popular type of sequence cl assification criterion, maximum mutual information or MMI, proposed as early as 1986 [7], was successfully appli ed to learn DBN-DNN weights for the TIMIT phone recognition task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "MMI sequence discriminative training gave an error rate of 12.2% and model combination with the GMM system 11.8%.\nyoutuBe Speech recoGnition taSk In this task, the goal is to transcribe Youtube data."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum mutua  l information estimation of hidden Markov model parameters for s  peech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "inProceedings of the ICASSP  , 1986, pp. 49\u201352. April 27, 2012  DRAFT  23"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770190"
                        ],
                        "name": "K. Torkkola",
                        "slug": "K.-Torkkola",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Torkkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Torkkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 210
                            }
                        ],
                        "text": "The posterior probabilities that the DNN outputs can be converted into the scaled likelih ood by dividing them by the frequencies of the HMM-states in the forced alignment that is used for fine-tuni ng the DNN [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 154
                            }
                        ],
                        "text": "Instead of replacing the coefficients usually modeled by GMM s, neural networks can also be used to provide additional features for the GMM to model [8], [9], [63]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": ", MFCCs) with \u201ctandem\u201d or bottleneck features generated using neura l networks [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 221
                            }
                        ],
                        "text": "To reduce overfitting, large weights can be penalized in prop ortion to their squared magnitude, or the learning can simply be terminated at the point at which performance on a held-out validation set starts getting worse[9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "In fa ct two decades ago, researchers achieved some success using artificial neural networks with a single layer of non-l i ear hidden units to predict HMM states from windows of acoustic coefficients [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61934373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34e8204330ae81c62dbb00d4ed81b990cced6cb7",
            "isKey": true,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speech-recognition:-a-hybrid-approach-Torkkola-Kohonen",
            "title": {
                "fragments": [],
                "text": "Speech recognition: a hybrid approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "To make DBN-DNNs work really well on large vocabulary tasks it is important to replace the monophone HMMs used for TIMIT (and also for early neural network/HMM hybrid systems) with triphone HMMs that have many thousands of tied states [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Further work after that of [42] extended the DNN-HMM acoustic model from 24 h of training data to 48 h and explored the respective roles of pretraining and fine-tuning the DBNDNN [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 374,
                                "start": 370
                            }
                        ],
                        "text": "Detailed experiments [43] on the Switchboard task confirmed that the remarkable accuracy gains from the DNN-HMM acoustic model are due to the direct modeling of tied triphone states using the DBN-DNN, the effective exploitation of neighboring frames by the DBN-DNN, and the strong modeling power of deeper networks, as was discovered in the Bing voice search task [44], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "In addition to demonstrating that a DBN-DNN could provide gains on a large vocabulary task, several other important issues were explicitly investigated in [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Pretraining was performed following a recipe similar to [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The results reported in [42] demonstrated that the best DNNHMM acoustic model trained with context-dependent states as targets achieved a sentence accuracy of 69."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Audio Speech Lang. Processing"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "In this paper we review exploratory experiments on the TIMIT database [12], [13] that were used to demonstrate the power of this two-stage training procedure for acoustic modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Fortunately, the performance of the networks on the TIMIT core test set was fairly insensitive to the precise details of the architecture and the results in [13] suggest that any combin atio of the numbers in boldface probably has an error rate within about2% of the very best combination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Details of the le arning rates, stopping criteria, momentum, L2 weight penalties and mini-batch size for both the pre-training and fi e-tuning are given in [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "4% Monophone randomly initialized DNNs (6 layers)[13] 23."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "data and, on the TIMIT database, the work reported in [13] sho wed that the best performing DBN-DNNs trained with filter-bank features had a phone error rate 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "4% Monophone DBN-DNNs (6 layers) [13] 22."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "7% Monophone DBN-DNNs on fbank (8 layers) [13] 20."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acoustic modeling usi  ng deep belief networks,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Audio, Speech, and Language Processing,"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Further work after that of [42] extended the DNN-HMM acousti c model from 24 hours of training data to 48 hours, and explored the respective roles of pre-training an d fine-tuning the DBN-DNN [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "Pre-training was performed following a recipe similar to [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "exploitation of neighboring frames by the DBN-DNN, and the s trong modeling power of deeper networks, as was discovered in the Bing voice search task [44], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "To make DBN-DNNs work really well on large vocabulary tasks it is important to replace the monophone HMMs used for TIMIT (and also for early neural network/HMM hybrid systems) with triphone HMM s that have many thousands of tied states [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "In addition to demonstrating that a DBN-DNN could provide gains on a larg e vocabulary task, several other important issues were explicitly investigated in [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "The results r eported in [42] demonstrated that the best DNN-HMM acoustic model trained with context-dependent states as ta rgets achieved a sentence accuracy of 69."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Context-Dependen  t Pre-Trained Deep Neural Networks for Large-Vocabulary Sp  eech Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processin  g,, vol. 20, no. 1, pp. 30\u201342, jan 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Detailed experiments [43] on the Switchboard task confirmed that the remarkable accuracy gains from the DNNHMM acoustic model are due to the direct modeling of tied trip hone states using the DBN-DNN, the effective"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "The DNN-HMM training recipe developed for the Bing voice sea rch data was applied unaltered to the Switchboard speech recognition task [43] to confirm the suitability of DN N-HMM acoustic models for large vocabulary tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 183
                            }
                        ],
                        "text": "Using context-dependent HMM stat es, it is possible to outperform state-of-the-art BMMI trained GMM-HMM systems with a two-hidden-layer neural net work without using any pre-training [43], though using more hidden layers and pre-training works even better ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conversational speech trans  cription using context-dependent deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "i  n Proc. Interspeech  , 2011, pp. 437\u2013440."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699205"
                        ],
                        "name": "S. Furui",
                        "slug": "S.-Furui",
                        "structuredName": {
                            "firstName": "Sadaoki",
                            "lastName": "Furui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Furui"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 227300549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24ccfeccddf3651db2e0b24a94427e930dadcf7e",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Digital-Speech-Processing,-Synthesis,-and-Furui",
            "title": {
                "fragments": [],
                "text": "Digital Speech Processing, Synthesis, and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18116937,
            "fieldsOfStudy": [],
            "id": "909d882d73a9a7a151a0ae29143285999bb4c8ab",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Large-Vocabulary-Continuous-Speech-Recognition-:-a-Young",
            "title": {
                "fragments": [],
                "text": "Large Vocabulary Continuous Speech Recognition : a ReviewSteve"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64175775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d779f5c56a7121bdb62d73c1894a1ab0d182cbc2",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Application-of-Pretrained-Deep-Neural-Networks-to-Jaitly-Nguyen",
            "title": {
                "fragments": [],
                "text": "Application of Pretrained Deep Neural Networks to Large Vocabulary Conversational Speech Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improvements in using deep belief networks for large vocabulary continuous speech recognition Deep convex network : A scalable architecture for speech pattern classification"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . Interspeech"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improvements in using deep belief networks for large vocabulary continuous speech recognition Speech and Language Algorithm Group <AU: Kindly provide the location of the organization"
            },
            "venue": {
                "fragments": [],
                "text": "Improvements in using deep belief networks for large vocabulary continuous speech recognition Speech and Language Algorithm Group <AU: Kindly provide the location of the organization"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 310,
                                "start": 306
                            }
                        ],
                        "text": "Purely discriminative training of the whole DNN from random initial weights works much better than had been thought, provided the scales of the initial weights are set c arefully, a large amount of labeled training data is availab le, and mini-batch sizes over training epochs are set appropria tely [45], [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative pre-tr  aining of deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": " U.S. Patent Filing, Nov. 2011."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical approach to automatic speech recognition using the atomic speech units constructedfrom overlapping articulatory features"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Acoustical Society of America"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Some recent atte mpts are described in [52], [57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A deep architecture w  ith bilinear modeling of hidden representations: applicati  ons to phonetic recognition"
            },
            "venue": {
                "fragments": [],
                "text": "inProceedings of ICASSP  , 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "3% Augmented conditional Random Fields [26] 26."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech recognition using augmen  ted conditional random fields"
            },
            "venue": {
                "fragments": [],
                "text": " IEEE Transactions on Audio, Speech & Language Processing, vol. 17, no. 2, pp. 354\u2013365, 2009."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "He is currently the editor-in-chief of IEEE Transactions on Audio, Speech, and Language Processing (2012\u20132014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic Inference in Intelligent Systems: Networks of Plausible Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic Inference in Intelligent Systems: Networks of Plausible Inference"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Further study [45] suggests that feature-engineering tech niques such as HLDA and VTLN, commonly used in GMM-HMMs, are more helpful for shallow neural nets than for D BN-DNNs, presumably because DBN-DNNs are able to learn appropriate features in their lower layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 300
                            }
                        ],
                        "text": "Purely discriminative training of the whole DNN from random initial weights works much better than had been thought, provided the scales of the initial weights are set c arefully, a large amount of labeled training data is availab le, and mini-batch sizes over training epochs are set appropria tely [45], [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 278
                            }
                        ],
                        "text": "This type of discriminative pre-training works well in prac tice, approaching the accuracy achieved by generative DBN pre-training and further improvement can be achieved by stopping the discriminative pre-training after a single epoch instead of multiple epochs as reported in [45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature engineeringcontext-dependent deep neural networks for conversatio  nal speech transcription"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. IEEE ASRU, 2011, pp. 24\u201329."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 242
                            }
                        ],
                        "text": "If many different latent variables interact non-linearly t o generate each data vector, it is difficult to infer the state of the latent variables from the observed data in a directed m o el because of a phenomenon known as \u201cexplaining away\u201d [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inference in Intelligent Systems: Networks  of Plausible Inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 305
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, b ut even with this constraint it was impossible to train all possible combinations of number of hidden layers [ 1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Gradient descent from a random starting point near the origin is not the best way to find a good set of weights and unles s the initial scales of the weights are carefully chosen [15], the backpropagated gradients will have very di fferent magnitudes in different layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Understanding the difficulty o  f training deep feedforward neural networks"
            },
            "venue": {
                "fragments": [],
                "text": " Proceedings of AISTATS  , 2010, pp. 249\u2013256."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From 2004 he worked in the speech research group at RDI Company, Egypt. He then joined the ESAT-PSI speech group at the Katholieke Universiteit Leuven, Belgium"
            },
            "venue": {
                "fragments": [],
                "text": "September 2008 he started his PhD at the University of Toronto. His research focus is in developing machine learning techniques to advance human language technologies"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "For speech recognition, im proved performance was achieved on both TIMIT and Broadcast News tasks by pre-training with a type of autoenco der that tries to find sparse codes [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "I  mproved pre-training of deep belief networks using sparse en  codi g symmetric machines"
            },
            "venue": {
                "fragments": [],
                "text": "inProceedings of ICASSP  , 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 175
                            }
                        ],
                        "text": "In these systems the acous ti input is typically represented by concatenating Mel Frequency Cepstral Coefficients (MFCCs) or Perceptual L inear Predictive coefficients (PLPs) [4] computed from the raw waveform, and their first- and second-order temp oral differences [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Perceptual linear predictive (plp) analy  sis of speech,\u201dThe"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Acoustical Society of America ,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contracting autoencoders: Explicit invariance during feature extraction <AU: Kindly provide the complete page range"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 28th Int. Conf. Machine Learning"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 315
                            }
                        ],
                        "text": "The training set is small enough to make it feasible to try man y variations of a new method and many existing techniques have already been benchmarked on the core test se t so it is easy to see if a new approach is promising by comparing it with existing techniques that have been impl e ented by their proponents [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] achieved a similar improvement on TIMIT by applying state-of-the-art techniques developed for large vocabulary recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An explor  ati n of large vocabulary tools for small vocabulary phoneti c recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Automatic Speech Recognition and Understanding Works  h p, 2009."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Although convolutional models along the temporal dimensio achieved good classification results [39], applying them to phone recognition is not straightforward."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [39] convolutional DBN-DNNs were introduced and successfully applied to vario us audio tasks including phone classification on the TIMIT database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised featu  re learning for audio classification using convolutional de  ep belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 22  , Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A . Culotta, Eds., 2009, pp. 1096\u20131104."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised feature learning for audio classification using convolutional deep belief networks <AU: Kindly provide the place of publication and publisher name"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 22"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An application of pretrained deep neural networks to large vocabulary speech recognition <AU: Kindly provide the complete page range"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Interspeech"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 230
                            }
                        ],
                        "text": "A huge amount of research has gone into ways of con strai ing GMMs to increase their evaluation speed and to optimize the trade-off between their flexibility and t he amount of training data available to avoid serious overfitting [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large Vocabulary Continuous Speech Recognit  ion: A Review"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine  , vol. 13, no. 5, pp. 45\u201357, 1996."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "7% Monophone mcRBM-DBN-DNNs on fbank (5 layers) [33] 20."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phon  e recognition with the mean-covariance restricted Boltzmann  machine"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 23  , J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, a nd A. Culotta, Eds., 2010, pp. 469\u2013477."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Canada as an Assistant Professor, where he became a tenured Full Professor in 1996 WA as a Senior Researcher, where he is currently a Principal Researcher"
            },
            "venue": {
                "fragments": [],
                "text": "he joined Microsoft Research"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "4% Monophone DBN-DNNs with MMI training [31] 22."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "In the more recent work reported in [31], one popular type of sequence cl assification criterion, maximum mutual information or MMI, proposed as early as 1986 [7], was successfully appli ed to learn DBN-DNN weights for the TIMIT phone recognition task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Investigation of full-se  quence training of deep belief networks for speech recognit ion"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Interspeech  , 2010."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 310,
                                "start": 306
                            }
                        ],
                        "text": "Purely discriminative training of the whole DNN from random initial weights works much better than had been thought, provided the scales of the initial weights are set carefully, a large amount of labeled training data is available, and minibatch sizes over training epochs are set appropr ia te ly [45] , [53] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative pretraining of deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "U.S. Patent Filing, Nov. 2011. <AU: Is this a patent-type reference? If yes, provide the patent no.>"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "\u2026to be confused with a dynamic Bayesian net, which is a type of directed model of temporal data that unfortunately has the same acronym) <AU: please check that placement of footnote is OK as given> formed by the whole stack is a hybrid generative model whose top two layers are undirected (they\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational models for speech production, \" in Computational Models of Speech Pattern Processing <AU: Kindly provide the editor names"
            },
            "venue": {
                "fragments": [],
                "text": "Computational models for speech production, \" in Computational Models of Speech Pattern Processing <AU: Kindly provide the editor names"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structured speech modeling <AU: Kindly provide the issue number"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Audio Speech Lang. Processing"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine learning <AU: Kindly provide the complete page range"
            },
            "venue": {
                "fragments": [],
                "text": "Machine learning <AU: Kindly provide the complete page range"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 349,
                                "start": 345
                            }
                        ],
                        "text": "However, it has long been known that sequence classification criteria, whic h are more directly correlated with the overall word or phone error rate, can be very helpful in improving recogniti o accuracy [7], [35] and the benefit of using such sequence classification criteria with shallow neural netwo rks has already been shown by [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Backpropagat  ion training for multilayer conditional random field based ph  one recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ICASSP \u201910  , 2010, pp. 5534\u20135537."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [62] investigated a less direct way of producing f eature vectors for the GMM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-e  ncoder bottleneck features using deep belief networks"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. ICASSP 2012 , 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "A recent study [65] demonstrated the effectiveness of DBN-D NNs for detecting sub-phonetic speech attributes (also known as phonological or articulatory features [66]) in the widely used Wall Street Journal speech database (5k-WSJ0)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "The 22 attributes used in the recent work, as reported in [65], are a subset of the articulatory feature s explored in [66], [67]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical approach to automatic s  peech recognition using the atomic speech units constructed  from overlapping articulatory features,\u201dJournal of the Acoustical Society of America"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 85,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative pretraining of deep neural networks <AU: Is this a patent-type reference? If yes, provide the patent no"
            },
            "venue": {
                "fragments": [],
                "text": "U.S. Patent Filing"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "5% Monophone convolutional DNNs on fbank (3 layers) [34] 20."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The work reported in [34] applied local convolutional filter s with max-pooling to thefrequencyrather than time dimension of the spectrogram."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applyi  ng convolutional neural networks concepts to hybrid NN-HMM  model for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "inProceedings of ICASSP  , 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep convex network: A scalable architecture for speech pattern classification <AU: Kindly provide the complete page range"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Interspeech"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dahl received a B.A. in computer science, with highest honors, from Swarthmore College and an M.Sc. from the University of"
            },
            "venue": {
                "fragments": [],
                "text": "Dahl received a B.A. in computer science, with highest honors, from Swarthmore College and an M.Sc. from the University of"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improvements in using deep belief networks for large vocabulary continuous speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Improvements in using deep belief networks for large vocabulary continuous speech recognition"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "However, it has long been known that sequence classification criteria, whic h are more directly correlated with the overall word or phone error rate, can be very helpful in improving recogniti o accuracy [7], [35] and the benefit of using such sequence classification criteria with shallow neural netwo rks has already been shown by [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Finally, DNNs are learned using stochastic gradient descen t, while GMMs are learned using the EM algorithm or its extensions [35] which makes GMM learning much easier to p arallelize on a cluster machine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "(17) d erived from mutual information [35] is the same as the conditional likelihood associated with a specialized l inear-chain conditional random field."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative Learning in S  equential Pattern Recognition \u2014 A Unifying Review for Optimi  zation-Oriented Speech Recognition"
            },
            "venue": {
                "fragments": [],
                "text": " IEEE Signal Processing Magazine  , vol. 25, no. 5, pp. 14\u201336, 2008."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Roles of pretraining and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition<AU: Kindly provide the complete page range"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. NIPS Workshop Deep Learning and Unsupervised Feature Learning"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "A recent study [65] demonstrated the effectiveness of DBN-D NNs for detecting sub-phonetic speech attributes (also known as phonological or articulatory features [66]) in the widely used Wall Street Journal speech database (5k-WSJ0)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The 22 attributes used in the recent work, as reported in [65], are a subset of the articulatory feature s explored in [66], [67]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "This level of accuracy for detecting sub-phonetic fundamental speech units may allow a new famil y of flexible speech recognition and understanding systems that make use of phonological features in the full de tection-based framework discussed in [65]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting att  ribute and phone estimation accuracies with deep neural netw  orks for detectionbased speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": " Proceedings of ICASSP  , 2012."
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improve  ments in using deep belief networks for large vocabulary cont  inuous speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. UTML TR 2010-003, Technical Repor  t, Speech and Language Algorithm Group, IBM, February 2011."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 305
                            }
                        ],
                        "text": "For simplicity, all hidden layers always had the same size, b ut even with this constraint it was impossible to train all possible combinations of number of hidden layers [ 1, 2, 3, 4, 5, 6, 7, 8], number of units per layer [512, 1024, 2048, 3072] and number of frames of acoustic data in the input layer [7, 11, 15, 17, 27, 37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "6% Randomly initialized recurrent Neural Nets [27] 26."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An application to recurrent nets to phone  probability estimation,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Neural Networks  ,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 303
                            }
                        ],
                        "text": "Purely discriminative training of the whole DNN from random initial weights works much better than had been thought, provided the scales of the initial weights are set carefully, a large amount of labeled training data is available, and mini-batch sizes over training epochs are set appropriately [45], [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative pre-training of deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "U.S. Patent Filing, Nov. 2011."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Her research interests mainly focus in acoustic modeling, including sparse representations, deep belief networks, adaptation methods and noise robust speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Her research interests mainly focus in acoustic modeling, including sparse representations, deep belief networks, adaptation methods and noise robust speech recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "As shown in [22], there is a series of variational bounds on th e log probability of the training data, and furthermore, each time a new RBM is added to the stack, the variational boun d on the new and deeper DBN is better than the previous variational bound, provided the new RBM is initial zed and learned in the right way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "The RBMs in a stack can be combined in a surprising way to produce a single, multi-layer generative model calle d a deep belief net (DBN) [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast learning al  orithm for deep belief nets,\u201dNeural"
            },
            "venue": {
                "fragments": [],
                "text": "Computation ,"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Product of e  xp rts for statistical parametric speech synthesis"
            },
            "venue": {
                "fragments": [],
                "text": " IEEE Transactions on Audio, Speech, and Language Processing  , vol. 20, no. 3, pp. 794\u2013805, March 2012."
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 43,
            "methodology": 50
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 121,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Neural-Networks-for-Acoustic-Modeling-in-Hinton-Deng/e33cbb25a8c7390aec6a398e36381f4f7770c283?sort=total-citations"
}