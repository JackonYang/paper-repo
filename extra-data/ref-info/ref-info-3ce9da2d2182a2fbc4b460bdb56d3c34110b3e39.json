{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "As shown in (Gull 1989;MacKay 1992a), the maximum evidence = MP satis es the following implicitequation: 1= MP =Xi wMPi 2= (22)\n19where wMP is the parameter vector which minimizes the objective function M = ED + EW , and is the `number of well-determined parameters', given by = k Trace(A 1):\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "These correlated error ellipsoids have been demonstrated for a two output regressionnetwork in MacKay (1992b).5.2.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "For example, in (MacKay 1992b), the standard weight decay model with onlyone regularization constant was applied to a two layer regression network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "This matrix may be evaluated explicitly (MacKay 1992b;Thodberg 1993; Bishop 1992; Hassibi and Stork 1993), which does not take signi canttime when the number of parameters is small (a few hundred)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "Empirically, the correlation between the evidence andgeneralization error is often good (MacKay 1992b; Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "For a binary classi er, a numerical approximation to the integral over a Gaussianposterior distribution is given in (MacKay 1992c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 109
                            }
                        ],
                        "text": "It is now common practice for Bayesians to t models that have more parametersthan the number of data points (MacKay 1992a; Neal 1994; Weir 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "The Bayesian Occam's razor is demonstratedon model problems in (Gull 1988; MacKay 1992a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(t(N+1)jD;H) ' P (t(N+1)jD; MP;H):Now the error bars on log and log , found by di erentiating logP (Dj ; ;H) twice,are (MacKay 1992a): 2log jD ' 2= ; 2log jD ' 2=(N ): (25)Thus the error introduced by optimizing and is expected to be small for 1and N 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123141880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c528d4eb732bf4de66566ff7b502b3311560cb08",
            "isKey": true,
            "numCitedBy": 421,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. \nWhen applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance. \nComparisons of the inferences of the Bayesian framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models. \nThe relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness of candidate data measurements, in the context of both interpolation and classification problems. \nThe concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation."
            },
            "slug": "Bayesian-methods-for-adaptive-models-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for adaptive models"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models, and it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "As shown in (Gull 1989;MacKay 1992a), the maximum evidence = MP satis es the following implicitequation: 1= MP =Xi wMPi 2= (22)\n19where wMP is the parameter vector which minimizes the objective function M = ED + EW , and is the `number of well-determined parameters', given by = k Trace(A 1):\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "These correlated error ellipsoids have been demonstrated for a two output regressionnetwork in MacKay (1992b).5.2.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "For example, in (MacKay 1992b), the standard weight decay model with onlyone regularization constant was applied to a two layer regression network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "This matrix may be evaluated explicitly (MacKay 1992b;Thodberg 1993; Bishop 1992; Hassibi and Stork 1993), which does not take signi canttime when the number of parameters is small (a few hundred)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "Empirically, the correlation between the evidence andgeneralization error is often good (MacKay 1992b; Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "For a binary classi er, a numerical approximation to the integral over a Gaussianposterior distribution is given in (MacKay 1992c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 109
                            }
                        ],
                        "text": "It is now common practice for Bayesians to t models that have more parametersthan the number of data points (MacKay 1992a; Neal 1994; Weir 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "The Bayesian Occam's razor is demonstratedon model problems in (Gull 1988; MacKay 1992a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(t(N+1)jD;H) ' P (t(N+1)jD; MP;H):Now the error bars on log and log , found by di erentiating logP (Dj ; ;H) twice,are (MacKay 1992a): 2log jD ' 2= ; 2log jD ' 2=(N ): (25)Thus the error introduced by optimizing and is expected to be small for 1and N 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": true,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "This Gaussian approximation over and holds good for 1 and N 1 (MacKay 1995b).4.1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 147
                            }
                        ],
                        "text": "Several researchersare presently working on extending multilayer perceptrons to turn them into density\nREFERENCES 39models (Hinton and Zemel 1994; MacKay 1995a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "\u2026over the regularization constants and contributes an additional variance inonly one direction; to leading order in 1, P (t(N+1)jD;H) is normal, with variance(MacKay 1995b): 2t = gT A 1 + ( 2log jD + 2log jD)w0MPw0MPT g + 2 ;where w0MP @wMPj =@(log ) = A 1wMP, and 2log jD = 2 and 2log jD = 2N .5.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "\u2026this procedure (Buntine and Weigend 1991; Wolpert 1993), butit is counterproductive as far as practical manipulation is concerned (Gull 1988;MacKay 1995b): the resulting true posterior is a skew-peaked distribution, and apartfrom Monte Carlo methods there are currently no computational\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122200499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2861a5e59de4d61bcd8a9ae4785978ac11fc9c1",
            "isKey": true,
            "numCitedBy": 159,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-neural-networks-and-density-networks-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian neural networks and density networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "As shown in (Gull 1989;MacKay 1992a), the maximum evidence = MP satis es the following implicitequation: 1= MP =Xi wMPi 2= (22)\n19where wMP is the parameter vector which minimizes the objective function M = ED + EW , and is the `number of well-determined parameters', given by = k Trace(A 1):\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "These correlated error ellipsoids have been demonstrated for a two output regressionnetwork in MacKay (1992b).5.2.2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "For example, in (MacKay 1992b), the standard weight decay model with onlyone regularization constant was applied to a two layer regression network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "This matrix may be evaluated explicitly (MacKay 1992b;Thodberg 1993; Bishop 1992; Hassibi and Stork 1993), which does not take signi canttime when the number of parameters is small (a few hundred)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "Empirically, the correlation between the evidence andgeneralization error is often good (MacKay 1992b; Thodberg 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "For a binary classi er, a numerical approximation to the integral over a Gaussianposterior distribution is given in (MacKay 1992c)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 109
                            }
                        ],
                        "text": "It is now common practice for Bayesians to t models that have more parametersthan the number of data points (MacKay 1992a; Neal 1994; Weir 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 75
                            }
                        ],
                        "text": "The Bayesian Occam's razor is demonstratedon model problems in (Gull 1988; MacKay 1992a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 120
                            }
                        ],
                        "text": "(t(N+1)jD;H) ' P (t(N+1)jD; MP;H):Now the error bars on log and log , found by di erentiating logP (Dj ; ;H) twice,are (MacKay 1992a): 2log jD ' 2= ; 2log jD ' 2=(N ): (25)Thus the error introduced by optimizing and is expected to be small for 1and N 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1762283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e68c54f39e87daf3a8bdc0ee005aece3c652d11",
            "isKey": true,
            "numCitedBy": 3960,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. Occam's razor is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling."
            },
            "slug": "Bayesian-Interpolation-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data by examining the posterior probability distribution of regularizing constants and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7748868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d565fb42892f20c52b9fc615cc537835f30d094",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian \"evidence\" approximation has recently been employed to determine the noise and weight-penalty terms used in back-propagation. This paper shows that for neural nets it is far easier to use the exact result than it is to use the evidence approximation. Moreover, unlike the evidence approximation, the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code (the exact result is closed form). In addition, it turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error. Another advantage of the exact analysis is that it does not lead one to incorrect intuition, like the claim that using evidence one can \"evaluate different priors in light of the data\". This paper also discusses sufficiency conditions for the evidence approximation to hold, why it can sometimes give \"reasonable\" results, etc."
            },
            "slug": "On-the-Use-of-Evidence-in-Neural-Networks-Wolpert",
            "title": {
                "fragments": [],
                "text": "On the Use of Evidence in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error, and the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052393"
                        ],
                        "name": "H. H. Thodberg",
                        "slug": "H.-H.-Thodberg",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Thodberg",
                            "middleNames": [
                                "Henrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Thodberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15593225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3fb617767f9e500e84ed03fb48acdcf088f33dc",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "MacKay's Bayesian framework for backpropagation is a practical and powerful means of improving the generalisation ability of neural networks. The framework is reviewed and extended in a pedagogical way. The notation is simpliied using the ordinary weight decay parameter, and the noise parameter is shown to be nothing more than an overall scale. A detailed and explicit procedure for adjusting several weight decay parameters is given. Pruning is incorporated into the Bayesian framework. Appropriate symmetry factors on sparse architectures are deduced. Bayesian weight decay is demonstrated using artiicial data generated by a sparsely connected network. Pruning yields computational advantages: by removing unimportant weights the posterior weight distribution becomes Gaussian, and pruning removes zero-modes of the Hessian and redundant hidden units. In addition, pruning improves generalisation. The Bayesian evidence is used as a stop criterion for pruning. Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra. It outperforms \\early stopping\" as well as quadratic regression. The evidence of a committee of diierently trained networks is computed and the corresponding improved generalisation is veriied. The error bars on the predictions of the fat content are computed. There are three contributors: The random noise, the uncertainty in the weights, and the deviation among the committee members. Finally the Bayesian framework is compared to Moody's GPE."
            },
            "slug": "Ace-of-Bayes-:-Application-of-Neural-Thodberg",
            "title": {
                "fragments": [],
                "text": "Ace of Bayes : Application of Neural"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra and outperforms \\early stopping\" as well as quadratic regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 194
                            }
                        ],
                        "text": "\u2026P (wMPjHi) wjD| {z } :Evidence ' Best t likelihood Occam factor (7)Thus the evidence is found by taking the best t likelihood that the model can achieveand multiplying it by an `Occam factor' (Gull 1988), which is a term with magnitudeless than one that penalizes Hi for having the parameter w.1.7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 144
                            }
                        ],
                        "text": "\u2026recommended this procedure (Buntine and Weigend 1991; Wolpert 1993), butit is counterproductive as far as practical manipulation is concerned (Gull 1988;MacKay 1995b): the resulting true posterior is a skew-peaked distribution, and apartfrom Monte Carlo methods there are currently no\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "If a hyperparameter is well determinedby the data, integrating over it is very much like estimating the hyperparameter fromthe data and then using that estimate in our equations (Bretthorst 1988; Gull 1988;\n20MacKay 1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 131
                            }
                        ],
                        "text": "It is often possible to integrate over and early in the calculation, obtaininga true prior and a true likelihood (Bretthorst 1988; Gull 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 260
                            }
                        ],
                        "text": "Then P (wMPjHi) = 1= w,and Occam factor = wjD w ;i.e., the Occam factor is equal to the ratio of the posterior accessible volume of Hi'sparameter space to the prior accessible volume, or the factor by which Hi's hypothesisspace collapses when the data arrive (Gull 1988; Je reys 1939)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 88
                            }
                        ],
                        "text": "Bayesian methods are introduced and contrasted with orthodox statistics in(Jaynes 1983; Gull 1988; Loredo 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 64
                            }
                        ],
                        "text": "The Bayesian Occam's razor is demonstratedon model problems in (Gull 1988; MacKay 1992a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117915279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6272baf82e2e442edab4fb613ef2b7186bf5f1fb",
            "isKey": true,
            "numCitedBy": 288,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of Bayesian reasoning are reviewed and applied to problems of inference from data sampled from Poisson, Gaussian and Cauchy distributions. Probability distributions (priors and likelihoods) are assigned in appropriate hypothesis spaces using the Maximum Entropy Principle, and then manipulated via Bayes\u2019 Theorem. Bayesian hypothesis testing requires careful consideration of the prior ranges of any parameters involved, and this leads to a quantitive statement of Occam\u2019s Razor. As an example of this general principle we offer a solution to an important problem in regression analysis; determining the optimal number of parameters to use when fitting graphical data with a set of basis functions."
            },
            "slug": "Bayesian-Inductive-Inference-and-Maximum-Entropy-Gull",
            "title": {
                "fragments": [],
                "text": "Bayesian Inductive Inference and Maximum Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16302605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d275cf94e620bf5b3776bba8a88acccdcfcd9a19",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The attempt to find a single \"optimal\" weight vector in conventional network training can lead to overfitting and poor generalization. Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "slug": "Bayesian-Learning-via-Stochastic-Dynamics-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning via Stochastic Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Bayesian methods avoid overfitting and poor generalization by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data, by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47640603"
                        ],
                        "name": "P. Freeman",
                        "slug": "P.-Freeman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Freeman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 76
                            }
                        ],
                        "text": "In simple Gaussian cases it is possible tosolve for this optimal precision (Wallace and Freeman 1987), and it is closely relatedto the posterior error bars on the parameters, A 1, where A = rr logP (wjD;H)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118095811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04eb446825da7a4c2ab3fa6df7ebd377baa66ebe",
            "isKey": false,
            "numCitedBy": 598,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random. The encoded form has two parts. The first states the inferred estimates of the unknown parameters in the model, the second states the data using an optimal code based on the data probability distribution implied by those parameter estimates. Choosing the model and the estimates that give the most compact coding leads to an interesting general inference procedure. In its strict form it has great generality and several nice properties but is computationally infeasible. An approximate form is developed and its relation to other methods is explored."
            },
            "slug": "Estimation-and-Inference-by-Compact-Coding-Wallace-Freeman",
            "title": {
                "fragments": [],
                "text": "Estimation and Inference by Compact Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "Some authorshave recommended this procedure (Buntine and Weigend 1991; Wolpert 1993), butit is counterproductive as far as practical manipulation is concerned (Gull 1988;MacKay 1995b): the resulting true posterior is a skew-peaked distribution, and apartfrom Monte Carlo methods there are currently\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 95
                            }
                        ],
                        "text": "BerkeleyBretthorst G 1988 Bayesian spectrum analysis and parameter estimation SpringerBridle J S 1989 Probabilistic interpretation of feedforward classi cation networkoutputs, with relationships to statistical pattern recognition In Neuro-computing:algorithms, architectures and applications, ed F\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9346534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights."
            },
            "slug": "Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp",
            "title": {
                "fragments": [],
                "text": "Keeping the neural networks simple by minimizing the description length of the weights"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units without time-consuming Monte Carlo simulations is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65793277"
                        ],
                        "name": "T. Loredo",
                        "slug": "T.-Loredo",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Loredo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Loredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 99
                            }
                        ],
                        "text": "Bayesian methods are introduced and contrasted with orthodox statistics in(Jaynes 1983; Gull 1988; Loredo 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120668555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "065803f662547c47fc46bf7e649847f90e3e90e9",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 227,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, which does not offer clear, compelling criteria for the design of statistical methods. Bayesian probability theory offers unique and demonstrably optimal solutions to well-posed statistical problems, and is historically the original approach to statistics. The reasons for earlier rejection of Bayesian methods are discussed, and it is noted that the work of Cox, Jaynes, and others answers earlier objections, giving Bayesian inference a firm logical and mathematical foundation as the correct mathematical language for quantifying uncertainty. The Bayesian approaches to parameter estimation and model comparison are outlined and illustrated by application to a simple problem based on the gaussian distribution. As further illustrations of the Bayesian paradigm, Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background, and the analysis of the neutrinos detected from supernova SN 1987A. A brief bibliography of astrophysically interesting applications of Bayesian inference is provided."
            },
            "slug": "From-Laplace-to-Supernova-SN-1987A:-Bayesian-in-Loredo",
            "title": {
                "fragments": [],
                "text": "From Laplace to Supernova SN 1987A: Bayesian Inference in Astrophysics"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, and Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background and the analysis of the neutrinos detected from supernova SN 1987A."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067844876"
                        ],
                        "name": "Robin Hanson",
                        "slug": "Robin-Hanson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Hanson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 112
                            }
                        ],
                        "text": "The same method ofchopping up a complex model space is used in the unsupervised classi cation system,AutoClass (Hanson et al. 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14975902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4110a8aff2e92964187f888bbaa1312e0b39fd1d",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using Bayesian statistics. Within this framework, and using various mathematical and algorithmic approximations, the Auto Class system searches for the most probable classifications, automatically choosing the number of classes and complexity of class descriptions. Simpler versions of AutoClass have been applied to many large real data sets, have discovered new independently-verified phenomena, and have been released as a robust software package. Recent extensions allow attributes to be selectively correlated within particular classes, and allow classes to inherit, or share, model parameters though a class hierarchy."
            },
            "slug": "Bayesian-Classification-with-Correlation-and-Hanson-Stutz",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification with Correlation and Inheritance"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using Bayesian statistics, and using various mathematical and algorithmic approximations the Auto Class system searches for the most probable classifications."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20335,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 13
                            }
                        ],
                        "text": "As shown in (Gull 1989;MacKay 1992a), the maximum evidence = MP satis es the following implicitequation: 1= MP =Xi wMPi 2= (22)\n19where wMP is the parameter vector which minimizes the objective function M = ED + EW , and is the `number of well-determined parameters', given by = k Trace(A 1):\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 118754484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "82fa37d5be8e747131a5857992cc33bb95469ce3",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian derivation of \u201cClassic\u201d MaxEnt image processing (Skilling 1989a) shows that exp(\u03b1S(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of \u201cClassic\u201d MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter \u03b1, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of structure -2\u03b1S should be equal to the number of good singular values contained in the data. The performance of this new condition is discussed with reference to image deconvolution, but leads to a reconstruction that is visually disappointing. A deeper hypothesis space is proposed that overcomes these difficulties, by allowing for spatial correlations across the image."
            },
            "slug": "Developments-in-Maximum-Entropy-Data-Analysis-Gull",
            "title": {
                "fragments": [],
                "text": "Developments in Maximum Entropy Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 609306,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e0dab4fe4299bc2f8b4b18f82702af717cf3924",
            "isKey": false,
            "numCitedBy": 559,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an analysis of how the generalization performance (expected test set error) relates to the expected training set error for nonlinear learning systems, such as multilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and training set errors: \u2329etest(\u03bb)\u232a\u03be\u03be\u2032 \u2248 \u2329etrain(\u03bb)\u232a\u03be + 2\u03c3eff2 peff(\u03bb)/n (1) Here, n is the size of the training sample \u03be, \u03c3eff2 is the effective noise variance in the response variable(s), \u03bb, is a regularization or weight decay parameter, and Peff(\u03bb) is the effective number of parameters in the nonlinear model. The expectations \u2329 \u232a of training set and test set errors are taken over possible training sets \u03be and training and test sets \u03be\u2032 respectively. The effective number of parameters peff(\u03bb) usually differs from the true number of model parameters p for nonlinear or regularized models; this theoretical conclusion is supported by Monte Carlo experiments. In addition to the surprising result that peff(\u03bb) \u2260 p, we propose an estimate of (1) called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's F P E and AIC, Mallows Cp, and Barron's P S E to the nonlinear setting."
            },
            "slug": "The-Effective-Number-of-Parameters:-An-Analysis-of-Moody",
            "title": {
                "fragments": [],
                "text": "The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The surprising result that peff(\u03bb) \u2260 p is proposed, called the generalized prediction error (GPE) which generalizes well established estimates of prediction risk such as Akaike's F P E and AIC, Mallows Cp, and Barron's P S E to the nonlinear setting."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 106
                            }
                        ],
                        "text": "Multi-layer perceptrons havenot conventionally been used to create density models (though belief networks(Spiegelhalter and Lauritzen 1990) and other neural networks such as the Boltzmannmachine (Hinton and Sejnowski 1986) do de ne density models)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10739577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcce2a3564685657c23d1afa00155c03560e76ac",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A directed acyclic graph or influence diagram is frequently used as a representation for qualitative knowledge in some domains in which expert system techniques have been applied, and conditional probability tables on appropriate sets of variables form the quantitative part of the accumulated experience. It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates. By exploiting the graphical structure, the updating can be performed locally, either approximately or exactly, and the setup makes it possible to take advantage of a range of well-established statistical techniques. As examples we discuss discrete models, models based on Dirichlet distributions and models of the logistic regression type."
            },
            "slug": "Sequential-updating-of-conditional-probabilities-on-Spiegelhalter-Lauritzen",
            "title": {
                "fragments": [],
                "text": "Sequential updating of conditional probabilities on directed graphical structures"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates and how to take advantage of a range of well-established statistical techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 54
                            }
                        ],
                        "text": "Bayesian inference satis es the likelihood principle (Berger 1985): our inferencesdepend only on the probabilities assigned to the data that were received, not onproperties of other data sets which might have occurred but did not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 40
                            }
                        ],
                        "text": "Useful textbooks are (Box andTiao 1973; Berger 1985).1.1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16430409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a1e1da81b535e1bead3fc2ab6af8b07877823b9",
            "isKey": true,
            "numCitedBy": 163,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The elements of the Hessian matrix consist of the second derivatives of the error measure with respect to the weights and thresholds in the network. They are needed in Bayesian estimation of network regularization parameters, for estimation of error bars on the network outputs, for network pruning algorithms, and for fast retraining of the network following a small change in the training data. In this paper we present an extended backpropagation algorithm that allows all elements of the Hessian matrix to be evaluated exactly for a feedforward network of arbitrary topology. Software implementation of the algorithm is straightforward."
            },
            "slug": "Exact-Calculation-of-the-Hessian-Matrix-for-the-Bishop",
            "title": {
                "fragments": [],
                "text": "Exact Calculation of the Hessian Matrix for the Multilayer Perceptron"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents an extended backpropagation algorithm that allows all elements of the Hessian matrix to be evaluated exactly for a feedforward network of arbitrary topology."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736279"
                        ],
                        "name": "B. Hassibi",
                        "slug": "B.-Hassibi",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Hassibi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hassibi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7057040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516",
            "isKey": false,
            "numCitedBy": 1586,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization."
            },
            "slug": "Second-Order-Derivatives-for-Network-Pruning:-Brain-Hassibi-Stork",
            "title": {
                "fragments": [],
                "text": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case, and thus yields better generalization on test data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6725274"
                        ],
                        "name": "J. Copas",
                        "slug": "J.-Copas",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Copas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Copas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 110
                            }
                        ],
                        "text": "A non-Bayesian approach to this problem is to down-weight all predictionsby an empirically determined factor (Copas 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116430176,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c88d7b4f260ab4e66e274e3dda3780ef2148af37",
            "isKey": false,
            "numCitedBy": 566,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The fit of a regression predictor to new data is nearly always worse than its fit to the original data. Anticipating this shrinkage leads to Stein-type predictors which, under certain assumptions, give a uniformly lower prediction mean squared error than least squares. Shrinkage can be particularly marked when stepwise fitting is used: the shrinkage is then closer to that expected of the full regression rather than of the subset regression actually fitted. Preshrunk predictors for selected subsets are proposed and tested on a number of practical examples. Both multiple and binary (logistic) regression models are considered."
            },
            "slug": "Regression,-Prediction-and-Shrinkage-Copas",
            "title": {
                "fragments": [],
                "text": "Regression, Prediction and Shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": false,
            "numCitedBy": 3494,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17945746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f027c678076d7f2fd817f081079a334466449b1",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "When feasible, learning is a very attractive alternative to explicit programming. This is particularly true in areas where the problems do not lend themselves to systematic programming, such as pattern recognition in natural environments. The feasibility of learning an unknown function from examples depends on two questions: 1. Do the examples convey enough information to determine the function? 2. Is there a speedy way of constructing the function from the examples? These questions contrast the roles of information and complexity in learning. While the two roles share some ground, they are conceptually and technically different. In the common language of learning, the information question is that of generalization and the complexity question is that of scaling. The work of Vapnik and Chervonenkis (1971) provides the key tools for dealing with the information issue. In this review, we develop the main ideas of this framework and discuss how complexity fits in."
            },
            "slug": "The-Vapnik-Chervonenkis-Dimension:-Information-in-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The work of Vapnik and Chervonenkis (1971) provides the key tools for dealing with the information issue and the main ideas are developed and how complexity fits in are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152315379"
                        ],
                        "name": "Marvin H. J. Guber",
                        "slug": "Marvin-H.-J.-Guber",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Guber",
                            "middleNames": [
                                "H.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marvin H. J. Guber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121303978,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5a10b9657a5d0d11dcc89f668f21f8dac73b4694",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction.- 2 Single Stationary Sinusoid Plus Noise.- 3 The General Model Equation Plus Noise.- 4 Estimating the Parameters.- 5 Model Selection.- 6 Spectral Estimation.- 7 Applications.- 8 Summary and Conclusions.- A Choosing a Prior Probability.- B Improper Priors as Limits.- C Removing Nuisance Parameters.- D Uninformative Prior Probabilities.- E Computing the \"Student t-Distribution\"."
            },
            "slug": "Bayesian-Spectrum-Analysis-and-Parameter-Estimation-Guber",
            "title": {
                "fragments": [],
                "text": "Bayesian Spectrum Analysis and Parameter Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1365,
                                "start": 0
                            }
                        ],
                        "text": "Recalling that corresponds to the variance 2 w = 1= of the assumed distribution for fwig, equation (22) speci es an intuitive condition for matching the prior to the data: the variance is estimated by 2 w = hw2i, where the average is over the e ective well determined parameters, the other k e ective parameters having been set to zero by the prior. Similarly, in a regression problem with a Gaussian noise model, the maximum evidence value of satis es: 1= MP = 2ED=(N ): (24) Since 2ED is the sum of squared residuals, this expression can be recognized as a variance estimator with the number of degrees of freedom set to . Equations (22) and (24) can be used as re-estimation formulae for and . The computational overhead for these Bayesian calculations is not severe: it is only necessary to evaluate properties of the error bar matrix, A 1. One might argue that this matrix should be evaluated anyway, in order to compute the uncertainty in a model's predictions. This matrix may be evaluated explicitly (MacKay 1992b; Thodberg 1993; Bishop 1992; Hassibi and Stork 1993), which does not take signi cant time when the number of parameters is small (a few hundred). For large problems these calculations can be performed more e ciently using algorithms which evaluate products Av without explicitly evaluating A (Skilling 1993; Pearlmutter 1994). Thodberg (1993) combines equations (22) and (24) into a single re-estimation formula for the ratio = ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 629,
                                "start": 0
                            }
                        ],
                        "text": "Relationship to theories of generalization Bayesian model comparison assesses within a well de ned hypothesis space how probable a set of alternative models are. However, we often want to know how well each model is expected to generalize. Empirically, the correlation between the evidence and generalization error is often good (MacKay 1992b; Thodberg 1993). But a theoretical connection between the two is not yet established. Here, a brief discussion is given of similarities and di erences between the evidence and quantities arising in work on prediction of generalization error. 10.3.1. Relation to `G.P.E.' Moody's (1992) `Generalized Prediction Error' is a generalization of Akaike's `Final Prediction Error' to non-linear regularized models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1136,
                                "start": 5
                            }
                        ],
                        "text": "(iv) Repeat steps 2 and 3 twelve times, generating a new data set Dj from the original data set D each time to obtain a new wj . Save the list of vectors wj . (v) Separately, use each of w1, w2, : : :w12 to make predictions. For example, in the case of time series continuation, use each wj by itself to generate an entire continuation. These predictions can be viewed as samples from the model's predictive distribution. They might be summarized by measuring their mean and variance. In order to get a true sample from the posterior, we should also perturb the prior. For each weight, the mean to which each weight decays, ordinarily zero, should be randomized at step 2 by sampling from a Gaussian of variance 2 w = 1= . The above method should be particularly useful for obtaining error bars when neural nets are used to forecast a time-series by bootstrapping the network with its own predictions. A full Bayesian treatment of time-series modelling with neural nets has not yet been made. 10. Discussion 10.1. Applications The methods of sections 2 to 7 have been successfully applied to several practical problems. Thodberg (1993) has applied these methods to an industrial problem, the prediction of pork fat content from spectroscopic data."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2445072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f",
            "isKey": true,
            "numCitedBy": 958,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes."
            },
            "slug": "Autoencoders,-Minimum-Description-Length-and-Free-Hinton-Zemel",
            "title": {
                "fragments": [],
                "text": "Autoencoders, Minimum Description Length and Helmholtz Free Energy"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1251969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6867b6b564462d6b902f68e0bfa58f4717ca1cc",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Just storing the Hessian H (the matrix of second derivatives 2E/wiwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv{f(w)} = (/r)f(w rv)|r=0, note that Rv{w} = Hv and Rv{w} = v, and then apply Rv{} to the equations used to compute w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian."
            },
            "slug": "Fast-Exact-Multiplication-by-the-Hessian-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Fast Exact Multiplication by the Hessian"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work derives a technique that directly calculates Hv, where v is an arbitrary vector, and shows that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "The MDL principle (Wallace and Boulton 1968) states that one should prefermodels which can communicate the data in the smallest number of bits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61324799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d76ee9b6fb4d441e31abcd4dadc4e44c576017",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"
            },
            "slug": "An-Information-Measure-for-Classification-Wallace-Boulton",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is suggested that the best classification is that which results in the briefest recording of all the attribute information, and the measurements of each thing are regarded as being a message about that thing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "Recent work on `structural riskminimization' (Guyon et al. 1992) uses empirically tuned expressions of the form:Egen ' ED=N + c1 log(N= ) + c2N= (34)where is the `e ective V-C dimension' of the model, and is identical to the quantityin (23)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 70
                            }
                        ],
                        "text": "This procedure has been usedfruitfully in character recognition work (Guyon et al. 1992).10.3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14313855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04641b38cdab7b5b6c7d0d09193d3220ef40efc5",
            "isKey": true,
            "numCitedBy": 122,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of Structural Risk Minimization refers to tuning the capacity of the classifier to the available amount of training data. This capacity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) learning algorithm. Actions based on these three factors are combined here to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition."
            },
            "slug": "Structural-Risk-Minimization-for-Character-Guyon-Vapnik",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization for Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The method of Structural Risk Minimization is used to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7326,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748112"
                        ],
                        "name": "P. Howard",
                        "slug": "P.-Howard",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725956"
                        ],
                        "name": "J. Vitter",
                        "slug": "J.-Vitter",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Vitter",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vitter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 67368302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebde36056e35be82be3b2a27abf40191b02ab3d4",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Arithmetic coding provides an effective mechanism for removing redundancy in the encoding of data. We show how artihmetic coding works and describe an efficient implementation that uses table lookup as a fast alternative to arithmetic operations. The reduced-precision arithmetic has a provably negligible effect on the amount of compression achieved. We can speed up the implementation further by use of parallel processing"
            },
            "slug": "Arithmetic-coding-for-data-compression-:-Data-Howard-Vitter",
            "title": {
                "fragments": [],
                "text": "Arithmetic coding for data compression : Data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work shows how artihmetic coding works and describes an efficient implementation that uses table lookup as a fast alternative to arithmetic operations that has a provably negligible effect on the amount of compression achieved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3343393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd23c9168418324e81881365f297fb6a1caa3a07",
            "isKey": false,
            "numCitedBy": 3142,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "slug": "Arithmetic-coding-for-data-compression-Witten-Neal",
            "title": {
                "fragments": [],
                "text": "Arithmetic coding for data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method, which gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 46
                            }
                        ],
                        "text": "Another MDL thought experiment(Hinton and van Camp 1993) involves incorporating random bits into our message."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1538,
                                "start": 47
                            }
                        ],
                        "text": "Another MDL thought experiment (Hinton and van Camp 1993) involves incorporating random bits into our message. The data are communicated using a parameter block and a data block. The parameter vector sent is a random sample from the posterior distribution P (wjD;H) = P (Djw;H)P (wjH)=P (DjH). This sample w is sent to an arbitrary small granularity w using a message length L(wjH) = log[P (wjH) w]. The data are encoded relative to w with a message of length L(Djw;H) = log[P (Djw;H) D]. Once the data message has been received, the random bits used to generate the sample w from the posterior can be deduced by the receiver. The number of bits so recovered is log[P (wjD;H) w]. These recovered bits need not count towards the message length, since we might use some other optimally encoded message as a random bit string, thereby communicating that message at the same time. The net description cost is therefore: L(wjH) + L(Djw;H) `Bits back' = log P (wjH)P (Djw;H) D P (wjD;H) = logP (DjH) log D: Thus this thought experiment has yielded the optimal description length. 10.6. Ensemble Learning The posterior distribution P (wjD;H) may be a very complicated density. The methods described in this paper have assumed that, in local regions that contain signi cant probability mass, the posterior can be well approximated by a Gaussian found by making a quadratic expansion of logP (wjD;H) around a local maximum. (For brevity I here omit the parameters and .) An interesting idea that has been implemented by Hinton and van Camp (1993) is to try to improve the quality of this type of approximation by optimizing the entire posterior approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 64
                            }
                        ],
                        "text": "An interesting idea that has been implemented by Hinton and van Camp (1993)is to try to improve the quality of this type of approximation by optimizing the entireposterior approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 173
                            }
                        ],
                        "text": "This measure can be motivated by generalizing the MDL`bits back' thought experiment (section 10.5.2), with the random sample w drawnfrom Q instead of from P (Hinton and van Camp 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Keeping neural networks simple by minimizing the"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144850958"
                        ],
                        "name": "J. Patrick",
                        "slug": "J.-Patrick",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Patrick",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Patrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 89
                            }
                        ],
                        "text": "Althoughsome of the earliest work on complex model comparison involved the MDL framework(Patrick and Wallace 1982), MDL has no apparent advantages over the directprobabilistic approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 190222497,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0dced67b882a610063ce9de08af102978d1288ec",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Archaeoastronomy-in-the-Old-World:-STONE-CIRCLE-AN-Patrick-Wallace",
            "title": {
                "fragments": [],
                "text": "Archaeoastronomy in the Old World: STONE CIRCLE GEOMETRIES: AN INFORMATION THEORY APPROACH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145993114"
                        ],
                        "name": "R. William",
                        "slug": "R.-William",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "William",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. William"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 151380665,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "749ce8ccd9453d1b34901143cddf5f9bee2977cf",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-representations-by-back-propagation-nature-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagation errors, nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123740858,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bed94ff3851ea67c44da91968cf04acb6fe50f2a",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-Numerical-Analysis-Diaconis",
            "title": {
                "fragments": [],
                "text": "Bayesian Numerical Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222411842,
            "fieldsOfStudy": [],
            "id": "acb94e335c64a4bbda6a493d6386e1138bcd109c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1896
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "The scaling up of Gaussian approximation methods to larger neural networkproblems will be helped by the use of implicit second order methods (Skilling 1993;Pearlmutter 1994); these are algorithms that make use of properties of the HessianmatrixA, such as the value of Av for arbitrary vector v,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "For large problemsthese calculations can be performed more e ciently using algorithms which evaluateproducts Av without explicitly evaluating A (Skilling 1993; Pearlmutter 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119799812,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4aaaad8110174c4f283dd64932233f4d3584d76c",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Physics-and-Probability:-Bayesian-Numerical-Skilling",
            "title": {
                "fragments": [],
                "text": "Physics and Probability: Bayesian Numerical Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48133796"
                        ],
                        "name": "R. T. Cox",
                        "slug": "R.-T.-Cox",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Cox",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. T. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120613997,
            "fieldsOfStudy": [
                "Physics",
                "Mathematics"
            ],
            "id": "636b6040f970c0a1857039de2b3ea49cd4ac2101",
            "isKey": false,
            "numCitedBy": 1272,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-frequency-and-reasonable-expectation-Cox",
            "title": {
                "fragments": [],
                "text": "Probability, frequency and reasonable expectation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400920289"
                        ],
                        "name": "N. Weir",
                        "slug": "N.-Weir",
                        "structuredName": {
                            "firstName": "Nic",
                            "lastName": "Weir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Weir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 109
                            }
                        ],
                        "text": "It is now common practice for Bayesians to t models that have more parameters than the number of data points (MacKay 1992a; Neal 1994; Weir 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 134
                            }
                        ],
                        "text": "It is now common practice for Bayesians to t models that have more parametersthan the number of data points (MacKay 1992a; Neal 1994; Weir 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117181385,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "56917e0825718186fe42569d6820fafb6a075c58",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-Maximum-Entropy-Techniques-to-HST-Weir",
            "title": {
                "fragments": [],
                "text": "Applications of Maximum Entropy Techniques to HST Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 75
                            }
                        ],
                        "text": "Bayesian methods are introduced and contrasted with orthodox statistics in(Jaynes 1983; Gull 1988; Loredo 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian intervals versus confidence intervals E T Jaynes: Papers on Probability, Statistics and Stutisficd Physics"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian intervals versus confidence intervals E T Jaynes: Papers on Probability, Statistics and Stutisficd Physics"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The eeective number of parameters: An analysis of generalization and regularization in nonlinear learning systems Morgan Kaufmann || 1994 Priors for innnite networks Technical Report in preparation"
            },
            "venue": {
                "fragments": [],
                "text": "|| 1995b Hyperparameters: Optimize, or integrate out? In Maximum Entropy and Bayesian Methods Advances in Neural Information Processing Systems 4 Bayesian learning via stochastic dynamics In Advances in Neural Information Processing Systems 5 Stone circle geometries: an information theory approach I"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 196
                            }
                        ],
                        "text": "Multi-layer perceptrons havenot conventionally been used to create density models (though belief networks(Spiegelhalter and Lauritzen 1990) and other neural networks such as the Boltzmannmachine (Hinton and Sejnowski 1986) do de ne density models)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and releaming in Boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": "Learning and releaming in Boltzmann machines"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regression, prediction and shrinkage (with discussion"
            },
            "venue": {
                "fragments": [],
                "text": "J. R. Statist. Soc. B"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 110
                            }
                        ],
                        "text": "A non-Bayesian approach to this problem is to down-weight all predictionsby an empirically determined factor (Copas 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regression, prediction and shrinkage (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "J. R. Statist.Soc B"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Vapnik-Chervonenlds dimension: information versus complexity in leaming Neural Berger 1 1985 Stntistienl Decision Theory and Bayesian Analysis (Berlin: Springer"
            },
            "venue": {
                "fragments": [],
                "text": "Bishop C M"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "The scaling up of Gaussian approximation methods to larger neural networkproblems will be helped by the use of implicit second order methods (Skilling 1993;Pearlmutter 1994); these are algorithms that make use of properties of the HessianmatrixA, such as the value of Av for arbitrary vector v,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "For large problemsthese calculations can be performed more e ciently using algorithms which evaluateproducts Av without explicitly evaluating A (Skilling 1993; Pearlmutter 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 146
                            }
                        ],
                        "text": "For large problems these calculations can be performed more e ciently using algorithms which evaluate products Av without explicitly evaluating A (Skilling 1993; Pearlmutter 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian numerical analysis In Physics and Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "From Lapiace to s u p o v a SN 1987A Bayesian inference in astrophysics M u h u m Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for adaptive models PN) Thesis California Institute of Technology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Morgan Kaufmann) pp 471-9"
            },
            "venue": {
                "fragments": [],
                "text": "Hanson R, StUtz I and Cheeseman P"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sewnd order derivatives for network pruning: Optimal brain sukgeon Advances in Neural Information Pmcessing System 5 e d ~ C"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "This matrix may be evaluated explicitly (MacKay 1992b;Thodberg 1993; Bishop 1992; Hassibi and Stork 1993), which does not take signi canttime when the number of parameters is small (a few hundred)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exact calculation of the H e s s i a n m h i x for the multilayer percepvon Neural Compur"
            },
            "venue": {
                "fragments": [],
                "text": "Exact calculation of the H e s s i a n m h i x for the multilayer percepvon Neural Compur"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 24
                            }
                        ],
                        "text": "Morgan KaufmannJaynes E T 1983 Bayesian intervals versus con dence intervals In E.T. Jaynes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "Relation to \u2018GPE. Moody\u2019s (1992) \u2018Generalized Prediction Error\u2019 is a generalization of Akaike\u2019s \u2018Final Prediction Error\u2019 to nonlinear regularized models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian intervals versus confidence intervals E T Jaynes: Papers on Probability, Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prohabilistic displays Maximum Entmpy a d Bayesian Methods (Lanunie, 1990) ed"
            },
            "venue": {
                "fragments": [],
                "text": "W T Gmdy and L Schick (Dordrecht: Kluwer)"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Methods for Adaptive Models PhD thesis, California"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cox R 1946 Probability, frequency, and reasonable expectation Am"
            },
            "venue": {
                "fragments": [],
                "text": "J. Physics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Keeping neural networks simple by minimidng the description length of the weights Proc. 6thAnn. Worksbp on CompuferLearning Theoq"
            },
            "venue": {
                "fragments": [],
                "text": "Keeping neural networks simple by minimidng the description length of the weights Proc. 6thAnn. Worksbp on CompuferLearning Theoq"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning representations by"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 123
                            }
                        ],
                        "text": "The Gaussian noise model might also be modi ed to include the possibility ofoutliers, using a Bayesian robust noise model (Box and Tiao 1973)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 27
                            }
                        ],
                        "text": "Therecommended philosophy (Box and Tiao 1973) is to aim to incorporate everyimaginable possibility into the model space: for example, if it is conceivable that a verysimple model might be able to explain the data, one should include simple models inthe model space; if the noise might have a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Addisoc-Wesley) Breiman L 1992 Stacked regressions"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Inference in Statistied Analysis Technical Report"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "This matrix may be evaluated explicitly (MacKay 1992b;Thodberg 1993; Bishop 1992; Hassibi and Stork 1993), which does not take signi canttime when the number of parameters is small (a few hundred)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "This is sometimes used as the motivation for`pruning' a neural network (Hassibi and Stork 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 237
                            }
                        ],
                        "text": "We nd:wMP s = wMP ws 2sA 1es; MMP = w2s2 2s ;where the marginal error bars on parameter ws are 2s = eTsA 1es = A 1ss. Thequantity MMP is the saliency term which has been advocated as a guide for `optimalbrain damage' (LeCun et al. 1990; Hassibi and Stork 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sewnd order derivatives for network pruning: Optimal brain sukgeon Advances in Neural Information Pmcessing System 5 e"
            },
            "venue": {
                "fragments": [],
                "text": "Sewnd order derivatives for network pruning: Optimal brain sukgeon Advances in Neural Information Pmcessing System 5 e"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Morgan Kaufmann) pp"
            },
            "venue": {
                "fragments": [],
                "text": "PkD Thesis DepaNnent of Computer Science,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic displays In Maximum Entropy and Bayesian Methods Sequential updating of conditional probabilities on directed graphical structures Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic displays In Maximum Entropy and Bayesian Methods Sequential updating of conditional probabilities on directed graphical structures Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 109
                            }
                        ],
                        "text": "The magnitude of the Occam factor is thus a measure of complexity of the modelbut, unlike the V-C dimension (Abu-Mostafa 1990), it relates to the complexity of thepredictions that the model makes in data space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Vapnik-Chervonenlds dimension: information versus complexity in leaming Neural Berger 1 1985 Stntistienl Decision Theory and Bayesian Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "The Vapnik-Chervonenlds dimension: information versus complexity in leaming Neural Berger 1 1985 Stntistienl Decision Theory and Bayesian Analysis"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic displays In Maximum Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The e ective number of parameters: An analysis of generalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sequential updating of conditional probabilities on directed graphical smctuxs NerWorkr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "|| 1992c The evidence framework applied to classiication networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 194
                            }
                        ],
                        "text": "Cheap generation of predictive distributionsA simple way of obtaining random samples from the posterior probability distributionof the parameters has been used in Bayesian image reconstruction (Skilling et al.1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian classification with wrrelation and inheritance Pmc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "Some authorshave recommended this procedure (Buntine and Weigend 1991; Wolpert 1993), butit is counterproductive as far as practical manipulation is concerned (Gull 1988;MacKay 1995b): the resulting true posterior is a skew-peaked distribution, and apartfrom Monte Carlo methods there are currently\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian back-propagation CompkxSy3fems"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian back-propagation CompkxSy3fems"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "neural networks In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "neural networks In Advances in Neural Information Processing Systems"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "The MDL principle (Wallace and Boulton 1968) states that one should prefermodels which can communicate the data in the smallest number of bits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information measure for classiication Comput"
            },
            "venue": {
                "fragments": [],
                "text": "J"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning and releaming in Boltzmann machines Pamllel Disfribured Pmcessing ed D E Rumelhart and J E McClelland (Cambridge, M A MIT Press) pp"
            },
            "venue": {
                "fragments": [],
                "text": "Worksbp on CompuferLearning Theoq (New York ACM Press)"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sequential updating of conditional"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian learning via stochastic dynamics Advances in Neural Infomarion Processin8 S y m m"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 124
                            }
                        ],
                        "text": "Several researchersare presently working on extending multilayer perceptrons to turn them into density\nREFERENCES 39models (Hinton and Zemel 1994; MacKay 1995a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Autoenwders, minimum descdption length and Helmholtz free energy Advances in Neural Information Pmcessing System"
            },
            "venue": {
                "fragments": [],
                "text": "Autoenwders, minimum descdption length and Helmholtz free energy Advances in Neural Information Pmcessing System"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stone circle geometries: M i n f o d o n theory approach Arehoeoastmnomy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 112
                            }
                        ],
                        "text": "The same method ofchopping up a complex model space is used in the unsupervised classi cation system,AutoClass (Hanson et al. 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian classification with wrrelation and inheritance Pmc"
            },
            "venue": {
                "fragments": [],
                "text": "12th Int. Joint CO& on Amfiid Intelligence"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian learning via stochastic dynamics In Advances in Neural"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cox R 1946 Probability, frequency, and reasonable explation A m"
            },
            "venue": {
                "fragments": [],
                "text": "J. Phys"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian non-linear modelling for the prediction competition In ASHRAE Transactions"
            },
            "venue": {
                "fragments": [],
                "text": "Atlanta Georgia. ASHRAE || 1995a Bayesian neural networks and density networks Nuclear Instruments and Methods in Physics Research"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation and inference by compact coding J"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stone circle geometries: an information theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "R. Statist. Soc. B"
            },
            "venue": {
                "fragments": [],
                "text": "R. Statist. Soc. B"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 18
                            }
                        ],
                        "text": "The MDL principle (Wallace and Boulton 1968) states that one should prefer models which can communicate the data in the smallest number of bits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "The MDL principle (Wallace and Boulton 1968) states that one should prefermodels which can communicate the data in the smallest number of bits."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information measure for classi cation Comput. J"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 500,
                                "start": 122
                            }
                        ],
                        "text": "The results will refer to regression models; the corresponding results for classi cation models are obtained by replacing ED by G, and ZD( ) by 1. Bayesian inference for data modelling problems may be implemented by analytical methods, by Monte Carlo sampling, or by deterministic methods employing Gaussian approximations. For neural networks there are few analytic methods. Sophisticated Monte Carlo methods which make use of gradient information have been applied to model problems by Neal (1993). The methods reviewed here are based on Gaussian approximations to the posterior distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian intervals versus con dence intervals In E.T"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "The scaling up of Gaussian approximation methods to larger neural networkproblems will be helped by the use of implicit second order methods (Skilling 1993;Pearlmutter 1994); these are algorithms that make use of properties of the HessianmatrixA, such as the value of Av for arbitrary vector v,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "For large problemsthese calculations can be performed more e ciently using algorithms which evaluateproducts Av without explicitly evaluating A (Skilling 1993; Pearlmutter 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian numerical analysis Physics nnd Pmbabiiity ed"
            },
            "venue": {
                "fragments": [],
                "text": "W T Gmdy. Jr and P Milonni"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 24
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 89,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Probable-networks-and-plausible-predictions-a-of-Mackay/3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39?sort=total-citations"
}