{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566295"
                        ],
                        "name": "Raphael Hoffmann",
                        "slug": "Raphael-Hoffmann",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Hoffmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raphael Hoffmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799338"
                        ],
                        "name": "Congle Zhang",
                        "slug": "Congle-Zhang",
                        "structuredName": {
                            "firstName": "Congle",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Congle Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145787377"
                        ],
                        "name": "Xiao Ling",
                        "slug": "Xiao-Ling",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "Finally, for unstructured data, the strong distant supervision assumption has been relaxed to accommodate cases when a pair of entities may not hold for any relation in the KB [33], or may have multiple overlapping relations [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16483125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d48edf9e81653f4c3da716b037b0b50d54c5b034",
            "isKey": false,
            "numCitedBy": 870,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). \n \nThis paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level."
            },
            "slug": "Knowledge-Based-Weak-Supervision-for-Information-of-Hoffmann-Zhang",
            "title": {
                "fragments": [],
                "text": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36181176"
                        ],
                        "name": "Mike D. Mintz",
                        "slug": "Mike-D.-Mintz",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Mintz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike D. Mintz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87299088"
                        ],
                        "name": "Steven Bills",
                        "slug": "Steven-Bills",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bills",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bills"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144621026"
                        ],
                        "name": "R. Snow",
                        "slug": "R.-Snow",
                        "structuredName": {
                            "firstName": "Rion",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Snow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "In order to create an automated process requiring no human annotation, distant supervision has been proposed for text extraction [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 296
                            }
                        ],
                        "text": "Distant supervision was initially proposed for relation extraction from natural language text and in that context relies on the distant supervision assumption: if two entities are known (via the seed KB) to be involved in a relation, any sentence containing both entities expresses that relation [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Distant supervision based extraction: More recently, research methods (including this work) have employed the distant supervision paradigm [25] for leveraging information in a KB as a source of supervision for creating potentially noisy annotations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10910955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d84b57362e2010f6f65357267df7e0157af30684",
            "isKey": true,
            "numCitedBy": 2479,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression."
            },
            "slug": "Distant-supervision-for-relation-extraction-without-Mintz-Bills",
            "title": {
                "fragments": [],
                "text": "Distant supervision for relation extraction without labeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work investigates an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083532"
                        ],
                        "name": "M. Mironczuk",
                        "slug": "M.-Mironczuk",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Mironczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mironczuk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "Among those\nsystems, Hao et al. [19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al. [4] applied unsupervised learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "[19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11455344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97d169e837c9a9f0f10a09a0239d51792d9efeb8",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this study is to propose an information extraction system, called BigGrams, which is able to retrieve relevant and structural information (relevant phrases, keywords) from semi-structural web pages, i.e. HTML documents. For this purpose, a novel semi-supervised wrappers induction algorithm has been developed and embedded in the BigGrams system. The wrappers induction algorithm utilizes a formal concept analysis to induce information extraction patterns. Also, in this article, the author (1) presents the impact of the configuration of the information extraction system components on information extraction results and (2) tests the boosting mode of this system. Based on empirical research, the author established that the proposed taxonomy of seeds and the HTML tags level analysis, with appropriate pre-processing, improve information extraction results. Also, the boosting mode works well when certain requirements are met, i.e. when well-diversified input data are ensured."
            },
            "slug": "The-BigGrams:-the-semi-supervised-information-from-Mironczuk",
            "title": {
                "fragments": [],
                "text": "The BigGrams: the semi-supervised information extraction system from HTML: an improvement in the wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is established that the proposed taxonomy of seeds and the HTML tags level analysis, with appropriate pre-processing, improve information extraction results and the boosting mode of this system works well when certain requirements are met."
            },
            "venue": {
                "fragments": [],
                "text": "Knowledge and Information Systems"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46458310"
                        ],
                        "name": "Liyuan Liu",
                        "slug": "Liyuan-Liu",
                        "structuredName": {
                            "firstName": "Liyuan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liyuan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145201124"
                        ],
                        "name": "Xiang Ren",
                        "slug": "Xiang-Ren",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152206948"
                        ],
                        "name": "Qi Zhu",
                        "slug": "Qi-Zhu",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145521137"
                        ],
                        "name": "Shi Zhi",
                        "slug": "Shi-Zhi",
                        "structuredName": {
                            "firstName": "Shi",
                            "lastName": "Zhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shi Zhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2286096"
                        ],
                        "name": "Huan Gui",
                        "slug": "Huan-Gui",
                        "structuredName": {
                            "firstName": "Huan",
                            "lastName": "Gui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huan Gui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016781"
                        ],
                        "name": "Heng Ji",
                        "slug": "Heng-Ji",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145325584"
                        ],
                        "name": "Jiawei Han",
                        "slug": "Jiawei-Han",
                        "structuredName": {
                            "firstName": "Jiawei",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiawei Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Most recently, data programming [32] and heterogeneous supervision [24] are proposed to unite diverse, possibly conflicting sources of supervision such as annotations by humans, supervision from a KB, or any labeling function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19226723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17ff6fb495872abe1b09d3330d5c4676bd190568",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art."
            },
            "slug": "Heterogeneous-Supervision-for-Relation-Extraction:-Liu-Ren",
            "title": {
                "fragments": [],
                "text": "Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics, and adopts embedding techniques to learn the distributed representations of context."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145867172"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728179"
                        ],
                        "name": "G. Heitz",
                        "slug": "G.-Heitz",
                        "structuredName": {
                            "firstName": "Geremy",
                            "lastName": "Heitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40428294"
                        ],
                        "name": "Wilko Horn",
                        "slug": "Wilko-Horn",
                        "structuredName": {
                            "firstName": "Wilko",
                            "lastName": "Horn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wilko Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1914797"
                        ],
                        "name": "N. Lao",
                        "slug": "N.-Lao",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Lao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2931575"
                        ],
                        "name": "Thomas Strohmann",
                        "slug": "Thomas-Strohmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Strohmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Strohmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109375570"
                        ],
                        "name": "Shaohua Sun",
                        "slug": "Shaohua-Sun",
                        "structuredName": {
                            "firstName": "Shaohua",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohua Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "The Knowledge Vault project [10] reported that after applying automatic knowledge extraction on DOM trees of semi-structured websites, texts, web-tables, and semantic web annotations, 75% of the extracted facts and 94% of the highconfidence facts were covered by DOM trees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction based on partial tree alignment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with Vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 20
                            }
                        ],
                        "text": "In addition, unlike Knowledge Vault, we allow extracting facts where the subjects and objects are not present in the seed database."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web pages may contain thousands of text fields, and many real-world use cases will mention hundreds or thousands of entities, which poses challenges both at annotation time and at extraction time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "We leave for future work to investigate how many of these aforementioned mistakes can be solved by applying knowledge fusion [10, 11] on the extraction results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Here we focus on techniques applicable to the semi-structured Web:\nWrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webpage filtering by informativeness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "In Proceedings of the 14th international conference on World Wide Web, pages 76\u201385."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0.7 is 0.63 and 0.64 respectively [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "5.1.1 SWDE To evaluate the performance of our approach across a range of\nverticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": ", domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "World Wide Web, 10(2):113\u2013132, 2007."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "It is critical to continuously grow knowledge bases to cover long tail information from different verticals (i.e., domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Knowledge Vault (KV) project [10] is an exception however, which takes advantage of the DOM tree structure to predict relations for cooccurring entities on a page."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 119
                            }
                        ],
                        "text": "Indeed, a recent attempt to apply distantly supervised extraction on semi-structured data obtained quite low accuracy: Knowledge Vault trained two distantly supervised DOM extractors but their accuracy is only around 63% [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Among various types of Web sources, we argue that semi-structured websites (e.g., IMDb, as shown in Figure 1) are one of the most\n\u2217All work performed while at Amazon.\npromising knowledge sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "The CommonCrawl corpus consists of monthly snapshots of pages from millions of websites [1] on the Web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4557963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d",
            "isKey": true,
            "numCitedBy": 1393,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods."
            },
            "slug": "Knowledge-vault:-a-web-scale-approach-to-knowledge-Dong-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Knowledge vault: a web-scale approach to probabilistic knowledge fusion"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Knowledge Vault is a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories that computes calibrated probabilities of fact correctness."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40061310"
                        ],
                        "name": "Joseph Paul Cohen",
                        "slug": "Joseph-Paul-Cohen",
                        "structuredName": {
                            "firstName": "Joseph Paul",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Paul Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113832157"
                        ],
                        "name": "Wei Ding",
                        "slug": "Wei-Ding",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2666918"
                        ],
                        "name": "A. Bagherjeiran",
                        "slug": "A.-Bagherjeiran",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Bagherjeiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bagherjeiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "Among those\nsystems, Hao et al. [19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al. [4] applied unsupervised learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "[19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12424896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "752d9dd5beed497270244e382e8ca2619b113839",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous data extraction pipelines using wrappers have become common and integral parts of businesses dealing with stock, flight, or product information. Extracting data from websites that use HTML templates is difficult because available wrapper methods are not designed to deal with websites that change over time (the inclusion or removal of HTML elements). We are the first to perform large scale empirical analyses of the causes of shift and propose the concept of domain entropy to quantify it. We draw from this analysis to propose a new semi-supervised search approach called XTPath. XTPath combines the existing XPath with carefully designed annotation extraction and informed search strategies. XTPath is the first method to store contextual node information from the training DOM and utilize it in a supervised manner. We utilize this data with our proposed recursive tree matching method which locates nodes most similar in context. The search is based on a heuristic function that takes into account the similarity of a tree compared to the structure that was present in the training data. We systematically evaluate XTPath using 117,422 pages from 75 diverse websites in 8 vertical markets that covers vastly different topics. Our XTPath method consistently outperforms XPath and a current commercial system in terms of successful extractions in a blackbox test. We are the first supervised wrapper extraction method to make our code and datasets available (online here: this http URL)."
            },
            "slug": "Semi-Supervised-Web-Wrapper-Repair-via-Recursive-Cohen-Ding",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Web Wrapper Repair via Recursive Tree Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The XTPath method consistently outperforms XPath and a current commercial system in terms of successful extractions in a blackbox test and is the first supervised wrapper extraction method to make its code and datasets available."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052599266"
                        ],
                        "name": "Qiang Hao",
                        "slug": "Qiang-Hao",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625690"
                        ],
                        "name": "Rui Cai",
                        "slug": "Rui-Cai",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134722"
                        ],
                        "name": "Yanwei Pang",
                        "slug": "Yanwei-Pang",
                        "structuredName": {
                            "firstName": "Yanwei",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanwei Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "However, we believe CERES obtained significantly higher precision because (1) we obtained an average precision of 0.94 on mainstream websites in SWDE, even with quite limited training data on some of the domains, and (2) on long-tail multi-lingual websites in CommonCrawl, which present significant challenges for extraction, we obtained an average precision of 0.83."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "As with SWDE, half the pages were used for annotation and training and the other half for evaluation, with a 0.5 confidence threshold."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "5.1.3 CommonCrawl movie websites The SWDE dataset validated our ability to obtain information\nfrom major websites well-aligned to our seed KB."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "We used the same knowledge base created for the SWDE Movie vertical as the seed KB."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] promises to extract data for all sites in a vertical by only requiring that annotations be made for all pages of one site in that vertical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 212
                            }
                        ],
                        "text": "Multi-valued predicates are a challenging and little-explored topic in semi-structured extraction, with most prior unsupervised and semi-supervised extractors restricting the problem to single-valued extractions [4, 19, 15, 18, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "SWDE comprises ground truth annotations of 4\u20135 predicates for 8 verticals with 10 websites\nin each vertical and 200\u20132000 pages for each site."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "To compare with prior work, metrics in Table 3 follows the methodology of Hao et al. [19] in evaluating precision and recall on the SWDE dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Experiments on the SWDE dataset demonstrate the state-of-the-art results on our system in multiple verticals, and a large-scale extraction project on hundreds of thousands of webpages shows the real-world usefulness of our approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "However, we note that many of the predicates in SWDE, such as University."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "5.1.1 SWDE To evaluate the performance of our approach across a range of\nverticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "To evaluate the performance of our approach across a range of verticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "The seed KB is the same as used for the SWDE Movie vertical described in Section 5.1.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Table 3 compares CERES-FULL with our baselines, and the state-of-the-art results on SWDE in the literature."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] in evaluating precision and recall on the SWDE dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "On the SWDE dataset [19], which has been used as a standard testbed for DOM extraction, we are able to obtain an average accuracy of over 90% in various verticals, even higher than many annotation-based wrapper induction methods in the literature."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17002481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5da5952da2fceac1823393ee2e0bdde0e0a02d2b",
            "isKey": true,
            "numCitedBy": 77,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution."
            },
            "slug": "From-one-tree-to-a-forest:-a-unified-solution-for-Hao-Cai",
            "title": {
                "fragments": [],
                "text": "From one tree to a forest: a unified solution for structured web data extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships) are designed that can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695582"
                        ],
                        "name": "Stefano Ortona",
                        "slug": "Stefano-Ortona",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Ortona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Ortona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35203250"
                        ],
                        "name": "G. Orsi",
                        "slug": "G.-Orsi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Orsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orsi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834754"
                        ],
                        "name": "Marcello Buoncristiano",
                        "slug": "Marcello-Buoncristiano",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Buoncristiano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Buoncristiano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17790608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c73edd2643994924163ba757ca95c54c3378059",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated web scraping is a popular means for acquiring data from the web. Scrapers (or wrappers) are derived from either manually or automatically annotated examples, often resulting in under/over segmented data, together with missing or spurious content. Automatic repair and maintenance of the extracted data is thus a necessary complement to automatic wrapper generation. Moreover, the extracted data is often the result of a long-term data acquisition effort and thus jointly repairing wrappers together with the generated data reduces future needs for data cleaning. We study the problem of computing joint repairs for XPath-based wrappers and their extracted data. We show that the problem is NP-complete in general but becomes tractable under a few natural assumptions. Even tractable solutions to the problem are still impractical on very large datasets, but we propose an optimal approximation that proves effective across a wide variety of domains and sources. Our approach relies on encoded domain knowledge, but require no per-source supervision. An evaluation spanning more than 100k web pages from 100 different sites of a wide variety of application domains, shows that joint repairs are able to increase the quality of wrappers between 15% and 60% independently of the wrapper generation system, eliminating all errors in more than 50% of the cases."
            },
            "slug": "Joint-repairs-for-web-wrappers-Ortona-Orsi",
            "title": {
                "fragments": [],
                "text": "Joint repairs for web wrappers"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that joint repairs are able to increase the quality of wrappers between 15% and 60% independently of the wrapper generation system, eliminating all errors in more than 50% of the cases."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE 32nd International Conference on Data Engineering (ICDE)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145941665"
                        ],
                        "name": "S. Riedel",
                        "slug": "S.-Riedel",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Riedel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riedel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786422"
                        ],
                        "name": "Limin Yao",
                        "slug": "Limin-Yao",
                        "structuredName": {
                            "firstName": "Limin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Limin Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Finally, for unstructured data, the strong distant supervision assumption has been relaxed to accommodate cases when a pair of entities may not hold for any relation in the KB [33], or may have multiple overlapping relations [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2386383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7e7b9a731678bf0494fe29cbebb42a822224cc6",
            "isKey": false,
            "numCitedBy": 1034,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision. Crucially, these approaches are trained based on the assumption that each sentence which mentions the two related entities is an expression of the given relation. Here we argue that this leads to noisy patterns that hurt precision, in particular if the knowledge base is not directly related to the text we are working with. We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB. We apply our approach to extract relations from the New York Times corpus and use Freebase as knowledge base. When compared to a state-of-the-art approach for relation extraction under distant supervision, we achieve 31% error reduction."
            },
            "slug": "Modeling-Relations-and-Their-Mentions-without-Text-Riedel-Yao",
            "title": {
                "fragments": [],
                "text": "Modeling Relations and Their Mentions without Labeled Text"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel approach to distant supervision that can alleviate the problem of noisy patterns that hurt precision by using a factor graph and applying constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in the authors' training KB."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24604135"
                        ],
                        "name": "G. Grasso",
                        "slug": "G.-Grasso",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Grasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146371097"
                        ],
                        "name": "Xiaonan Guo",
                        "slug": "Xiaonan-Guo",
                        "structuredName": {
                            "firstName": "Xiaonan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaonan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35203250"
                        ],
                        "name": "G. Orsi",
                        "slug": "G.-Orsi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Orsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orsi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766300"
                        ],
                        "name": "C. Schallhart",
                        "slug": "C.-Schallhart",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Schallhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schallhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119128164"
                        ],
                        "name": "Cheng Wang",
                        "slug": "Cheng-Wang",
                        "structuredName": {
                            "firstName": "Cheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "An alternate approach is that used by DIADEM [14], which, rather than assuming a KB, identifies extractable fields using a well-defined ontology and a set of recognizers corresponding to each entity type as well as predicate labels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction based on partial tree alignment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with Vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web pages may contain thousands of text fields, and many real-world use cases will mention hundreds or thousands of entities, which poses challenges both at annotation time and at extraction time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Here we focus on techniques applicable to the semi-structured Web:\nWrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webpage filtering by informativeness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "In Proceedings of the 14th international conference on World Wide Web, pages 76\u201385."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0.7 is 0.63 and 0.64 respectively [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "5.1.1 SWDE To evaluate the performance of our approach across a range of\nverticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": ", domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "World Wide Web, 10(2):113\u2013132, 2007."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "It is critical to continuously grow knowledge bases to cover long tail information from different verticals (i.e., domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Among various types of Web sources, we argue that semi-structured websites (e.g., IMDb, as shown in Figure 1) are one of the most\n\u2217All work performed while at Amazon.\npromising knowledge sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "The CommonCrawl corpus consists of monthly snapshots of pages from millions of websites [1] on the Web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18942290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee6196f097453aa897b0712e3a31c315b8ae9894",
            "isKey": true,
            "numCitedBy": 72,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The web is overflowing with implicitly structured data, spread over hundreds of thousands of sites, hidden deep behind search forms, or siloed in marketplaces, only accessible as HTML. Automatic extraction of structured data at the scale of thousands of websites has long proven elusive, despite its central role in the \"web of data\". \n \nThrough an extensive evaluation spanning over 10000 web sites from multiple application domains, we show that automatic, yet accurate full-site extraction is no longer a distant dream. diadem is the first automatic full-site extraction system that is able to extract structured data from different domains at very high accuracy. It combines automated exploration of websites, identification of relevant data, and induction of exhaustive wrappers. Automating these components is the first challenge. diadem overcomes this challenge by combining phenomenological and ontological knowledge. Integrating these components is the second challenge. diadem overcomes this challenge through a self-adaptive network of relational transducers that produces effective wrappers for a wide variety of websites. \n \nOur extensive and publicly available evaluation shows that, for more than 90% of sites from three domains, diadem obtains an effective wrapper that extracts all relevant data with 97% average precision. diadem also tolerates noisy entity recognisers, and its components individually outperform comparable approaches."
            },
            "slug": "DIADEM:-Thousands-of-Websites-to-a-Single-Database-Furche-Gottlob",
            "title": {
                "fragments": [],
                "text": "DIADEM: Thousands of Websites to a Single Database"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "Wrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3514097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cc263c84b85027164bd39db169f5d5959ef6822",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques."
            },
            "slug": "A-hierarchical-approach-to-wrapper-induction-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "A hierarchical approach to wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can handle information sources that could not be wrapped by existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39756936"
                        ],
                        "name": "Tak-Lam Wong",
                        "slug": "Tak-Lam-Wong",
                        "structuredName": {
                            "firstName": "Tak-Lam",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tak-Lam Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144594306"
                        ],
                        "name": "Wai Lam",
                        "slug": "Wai-Lam",
                        "structuredName": {
                            "firstName": "Wai",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wai Lam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Moreover, most wrappers were designed to work at the site level (except for [34]) which requires manual labor to create annotations for each site to be processed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15287551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64ad134fa13221ed251d3d651298c636144201db",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework."
            },
            "slug": "Learning-to-Adapt-Web-Information-Extraction-and-a-Wong-Lam",
            "title": {
                "fragments": [],
                "text": "Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1953113"
                        ],
                        "name": "Anna Lisa Gentile",
                        "slug": "Anna-Lisa-Gentile",
                        "structuredName": {
                            "firstName": "Anna Lisa",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Lisa Gentile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046811422"
                        ],
                        "name": "Ziqi Zhang",
                        "slug": "Ziqi-Zhang",
                        "structuredName": {
                            "firstName": "Ziqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziqi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758555"
                        ],
                        "name": "F. Ciravegna",
                        "slug": "F.-Ciravegna",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Ciravegna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Ciravegna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction based on partial tree alignment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 212
                            }
                        ],
                        "text": "Multi-valued predicates are a challenging and little-explored topic in semi-structured extraction, with most prior unsupervised and semi-supervised extractors restricting the problem to single-valued extractions [4, 19, 15, 18, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with Vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web pages may contain thousands of text fields, and many real-world use cases will mention hundreds or thousands of entities, which poses challenges both at annotation time and at extraction time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Here we focus on techniques applicable to the semi-structured Web:\nWrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webpage filtering by informativeness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "The LODIE project [15] is another example where annotations are generated based on matching attribute values on a page to dictionaries of attribute values pre-assembled from across the Linked Open Data cloud."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "In Proceedings of the 14th international conference on World Wide Web, pages 76\u201385."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0.7 is 0.63 and 0.64 respectively [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "5.1.1 SWDE To evaluate the performance of our approach across a range of\nverticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "[19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 131
                            }
                        ],
                        "text": "Among those\nsystems, Hao et al. [19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al. [4] applied unsupervised learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": ", domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "Approaches in this category [2, 8, 35, 15] aim to either discover templates from a set of pages or find recurring patterns from a page to extract unseen relation instances from other pages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "World Wide Web, 10(2):113\u2013132, 2007."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "It is critical to continuously grow knowledge bases to cover long tail information from different verticals (i.e., domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "LODIE-LOD did not include University."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Among various types of Web sources, we argue that semi-structured websites (e.g., IMDb, as shown in Figure 1) are one of the most\n\u2217All work performed while at Amazon.\npromising knowledge sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "The CommonCrawl corpus consists of monthly snapshots of pages from millions of websites [1] on the Web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9118693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f355eac84c7aa4823eb55015257bf85bec3c80ae",
            "isKey": true,
            "numCitedBy": 8,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction (IE) is the technique for transforming unstructured textual data into structured representation that can be understood by machines. The exponential growth of the Web generates an exceptional quantity of data for which automatic knowledge capture is essential. This work describes the methodology for web scale information extraction in the LODIE project (linked open data information extraction) and highlights results from the early experiments carried out in the initial phase of the project. LODIE aims to develop information extraction techniques able to scale at web level and adapt to user information needs. The core idea behind LODIE is the usage of linked open data, a very large-scale information resource, as a ground-breaking solution for IE, which provides invaluable annotated data on a growing number of domains. This article has two objectives. First, describing the LODIE project as a whole and depicting its general challenges and directions. Second, describing some initial steps taken towards the general solution, focusing on a specific IE subtask, wrapper induction."
            },
            "slug": "Early-Steps-Towards-Web-Scale-Information-with-Gentile-Zhang",
            "title": {
                "fragments": [],
                "text": "Early Steps Towards Web Scale Information Extraction with LODIE"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The methodology for web scale information extraction in the LODIE project (linked open data information extraction) is described and results from the early experiments carried out in the initial phase of the project are highlighted."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40343421"
                        ],
                        "name": "Yanhong Zhai",
                        "slug": "Yanhong-Zhai",
                        "structuredName": {
                            "firstName": "Yanhong",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanhong Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "Approaches in this category [2, 8, 35, 15] aim to either discover templates from a set of pages or find recurring patterns from a page to extract unseen relation instances from other pages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12750207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f49e35b6a85b8d81d2c9d9e26e8bf19dd94fad3a",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately."
            },
            "slug": "Web-data-extraction-based-on-partial-tree-alignment-Zhai-Liu",
            "title": {
                "fragments": [],
                "text": "Web data extraction based on partial tree alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683442"
                        ],
                        "name": "Ravi Kumar",
                        "slug": "Ravi-Kumar",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ravi Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018773"
                        ],
                        "name": "Mohamed A. Soliman",
                        "slug": "Mohamed-A.-Soliman",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Soliman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed A. Soliman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] proposed a generic framework to enable existing wrapper techniques be resilient to noisy annotations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction based on partial tree alignment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with Vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web pages may contain thousands of text fields, and many real-world use cases will mention hundreds or thousands of entities, which poses challenges both at annotation time and at extraction time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Here we focus on techniques applicable to the semi-structured Web:\nWrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webpage filtering by informativeness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "In Proceedings of the 14th international conference on World Wide Web, pages 76\u201385."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0.7 is 0.63 and 0.64 respectively [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "5.1.1 SWDE To evaluate the performance of our approach across a range of\nverticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": ", domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "World Wide Web, 10(2):113\u2013132, 2007."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "It is critical to continuously grow knowledge bases to cover long tail information from different verticals (i.e., domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Among various types of Web sources, we argue that semi-structured websites (e.g., IMDb, as shown in Figure 1) are one of the most\n\u2217All work performed while at Amazon.\npromising knowledge sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "The CommonCrawl corpus consists of monthly snapshots of pages from millions of websites [1] on the Web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 42464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65ec5824f6a997df0322827285ee691510b4527a",
            "isKey": true,
            "numCitedBy": 139,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications."
            },
            "slug": "Automatic-Wrappers-for-Large-Scale-Web-Extraction-Dalvi-Kumar",
            "title": {
                "fragments": [],
                "text": "Automatic Wrappers for Large Scale Web Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By removing the site-level supervision that wrapper-based techniques require, this work is able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766422"
                        ],
                        "name": "A. Arasu",
                        "slug": "A.-Arasu",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Arasu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Arasu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "Approaches in this category [2, 8, 35, 15] aim to either discover templates from a set of pages or find recurring patterns from a page to extract unseen relation instances from other pages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207628158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "856867d7caeabdd5bba9d13574dd786aa8ee8b30",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases."
            },
            "slug": "Extracting-structured-data-from-Web-pages-Arasu-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Extracting structured data from Web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111220343"
                        ],
                        "name": "D. Wang",
                        "slug": "D.-Wang",
                        "structuredName": {
                            "firstName": "Daisy",
                            "lastName": "Wang",
                            "middleNames": [
                                "Zhe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48144872"
                        ],
                        "name": "Eugene Wu",
                        "slug": "Eugene-Wu",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145955157"
                        ],
                        "name": "Yang Zhang",
                        "slug": "Yang-Zhang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Unlike webtables [5], semistructured data lack the table structure (rows and columns) that helps identify entities and relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15642206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b5077161a6f55a0d18cdfa3abbb612663d08d69",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The World-Wide Web consists of a huge number of unstructured documents, but it also contains structured data in the form of HTML tables. We extracted 14.1 billion HTML tables from Google's general-purpose web crawl, and used statistical classification techniques to find the estimated 154M that contain high-quality relational data. Because each relational table has its own \"schema\" of labeled and typed columns, each such table can be considered a small structured database. The resulting corpus of databases is larger than any other corpus we are aware of, by at least five orders of magnitude. \n \nWe describe the WEBTABLES system to explore two fundamental questions about this collection of databases. First, what are effective techniques for searching for structured data at search-engine scales? Second, what additional power can be derived by analyzing such a huge corpus? \n \nFirst, we develop new techniques for keyword search over a corpus of tables, and show that they can achieve substantially higher relevance than solutions based on a traditional search engine. Second, we introduce a new object derived from the database corpus: the attribute correlation statistics database (AcsDB) that records corpus-wide statistics on co-occurrences of schema elements. In addition to improving search relevance, the AcsDB makes possible several novel applications: schema auto-complete, which helps a database designer to choose schema elements; attribute synonym finding, which automatically computes attribute synonym pairs for schema matching; and join-graph traversal, which allows a user to navigate between extracted schemas using automatically-generated join links."
            },
            "slug": "WebTables:-exploring-the-power-of-tables-on-the-web-Cafarella-Halevy",
            "title": {
                "fragments": [],
                "text": "WebTables: exploring the power of tables on the web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The WEBTABLES system develops new techniques for keyword search over a corpus of tables, and shows that they can achieve substantially higher relevance than solutions based on a traditional search engine."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2627799"
                        ],
                        "name": "P. Gulhane",
                        "slug": "P.-Gulhane",
                        "structuredName": {
                            "firstName": "Pankaj",
                            "lastName": "Gulhane",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gulhane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136102"
                        ],
                        "name": "Amit Madaan",
                        "slug": "Amit-Madaan",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Madaan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Madaan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259494"
                        ],
                        "name": "Rupesh R. Mehta",
                        "slug": "Rupesh-R.-Mehta",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Mehta",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rupesh R. Mehta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311735"
                        ],
                        "name": "J. Ramamirtham",
                        "slug": "J.-Ramamirtham",
                        "structuredName": {
                            "firstName": "Jeyashankher",
                            "lastName": "Ramamirtham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ramamirtham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696519"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837802"
                        ],
                        "name": "Sandeepkumar Satpal",
                        "slug": "Sandeepkumar-Satpal",
                        "structuredName": {
                            "firstName": "Sandeepkumar",
                            "lastName": "Satpal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeepkumar Satpal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757518"
                        ],
                        "name": "Srinivasan H. Sengamedu",
                        "slug": "Srinivasan-H.-Sengamedu",
                        "structuredName": {
                            "firstName": "Srinivasan",
                            "lastName": "Sengamedu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinivasan H. Sengamedu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990683"
                        ],
                        "name": "Ashwin Tengli",
                        "slug": "Ashwin-Tengli",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Tengli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashwin Tengli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081450365"
                        ],
                        "name": "Charu Tiwari",
                        "slug": "Charu-Tiwari",
                        "structuredName": {
                            "firstName": "Charu",
                            "lastName": "Tiwari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charu Tiwari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "1 that we apply the clustering algorithm described in [17] in an attempt to recover the inherent groups of templates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We first apply the clustering algorithm in [17] to cluster the webpages such that each cluster roughly corresponds to a template, and then apply our extraction algorithm to each cluster."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 212
                            }
                        ],
                        "text": "Multi-valued predicates are a challenging and little-explored topic in semi-structured extraction, with most prior unsupervised and semi-supervised extractors restricting the problem to single-valued extractions [4, 19, 15, 18, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "Wrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Unfortunately, a strict implementation of Vertex clustering algorithm [17] sometimes does not obtain ideal results, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "Although wrapper induction has been quite mature in terms of extraction quality, obtaining precision over 95% [17], it requires annotations on every website, an expensive and timeconsuming step if one wishes to extract from many websites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "VERTEX++: We implemented the Vertex wrapper learning algorithm [17], which uses manual annotations to learn extraction patterns, expressed by XPaths."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13091007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a12502ba5b9686e37b0ec9d86a2dc7f4b7022ac",
            "isKey": true,
            "numCitedBy": 83,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale."
            },
            "slug": "Web-scale-information-extraction-with-vertex-Gulhane-Madaan",
            "title": {
                "fragments": [],
                "text": "Web-scale information extraction with vertex"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages that is the first system to do high-precision information extraction at Web scale."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE 27th International Conference on Data Engineering"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760944"
                        ],
                        "name": "Mirko Bronzi",
                        "slug": "Mirko-Bronzi",
                        "structuredName": {
                            "firstName": "Mirko",
                            "lastName": "Bronzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirko Bronzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802817"
                        ],
                        "name": "Paolo Papotti",
                        "slug": "Paolo-Papotti",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Papotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paolo Papotti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 212
                            }
                        ],
                        "text": "Multi-valued predicates are a challenging and little-explored topic in semi-structured extraction, with most prior unsupervised and semi-supervised extractors restricting the problem to single-valued extractions [4, 19, 15, 18, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5774632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "131383aa1f91eb0e9578dcae80f4dfcfb0f11e3e",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised approach for harvesting the data exposed by a set of structured and partially overlapping data-intensive web sources. Our proposal comes within a formal framework tackling two problems: the data extraction problem, to generate extraction rules based on the input websites, and the data integration problem, to integrate the extracted data in a unified schema. We introduce an original algorithm, WEIR, to solve the stated problems and formally prove its correctness. WEIR leverages the overlapping data among sources to make better decisions both in the data extraction (by pruning rules that do not lead to redundant information) and in the data integration (by reflecting local properties of a source over the mediated schema). Along the way, we characterize the amount of redundancy needed by our algorithm to produce a solution, and present experimental results to show the benefits of our approach with respect to existing solutions."
            },
            "slug": "Extraction-and-Integration-of-Partially-Overlapping-Bronzi-Crescenzi",
            "title": {
                "fragments": [],
                "text": "Extraction and Integration of Partially Overlapping Web Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An original algorithm, WEIR, is introduced to solve the stated problems and formally prove its correctness and the amount of redundancy needed by the algorithm to produce a solution is characterized."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145867172"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728179"
                        ],
                        "name": "G. Heitz",
                        "slug": "G.-Heitz",
                        "structuredName": {
                            "firstName": "Geremy",
                            "lastName": "Heitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40428294"
                        ],
                        "name": "Wilko Horn",
                        "slug": "Wilko-Horn",
                        "structuredName": {
                            "firstName": "Wilko",
                            "lastName": "Horn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wilko Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109375570"
                        ],
                        "name": "Shaohua Sun",
                        "slug": "Shaohua-Sun",
                        "structuredName": {
                            "firstName": "Shaohua",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohua Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "We leave for future work to investigate how many of these aforementioned mistakes can be solved by applying knowledge fusion [10, 11] on the extraction results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 221
                            }
                        ],
                        "text": "Indeed, a recent attempt to apply distantly supervised extraction on semi-structured data obtained quite low accuracy: Knowledge Vault trained two distantly supervised DOM extractors but their accuracy is only around 63% [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6749594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da789add85784a9e75b566b9958d3267e7139e0",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of data fusion is to identify the true values of data items (e.g., the true date of birth for Tom Cruise) among multiple observed values drawn from different sources (e.g., Web sites) of varying (and unknown) reliability. A recent survey [20] has provided a detailed comparison of various fusion methods on Deep Web data. In this paper, we study the applicability and limitations of different fusion techniques on a more challenging problem: knowledge fusion. Knowledge fusion identifies true subject-predicate-object triples extracted by multiple information extractors from multiple information sources. These extractors perform the tasks of entity linkage and schema alignment, thus introducing an additional source of noise that is quite different from that traditionally considered in the data fusion literature, which only focuses on factual errors in the original sources. We adapt state-of-the-art data fusion techniques and apply them to a knowledge base with 1.6B unique knowledge triples extracted by 12 extractors from over 1B Web pages, which is three orders of magnitude larger than the data sets used in previous data fusion papers. We show great promise of the data fusion approaches in solving the knowledge fusion problem, and suggest interesting research directions through a detailed error analysis of the methods."
            },
            "slug": "From-Data-Fusion-to-Knowledge-Fusion-Dong-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "From Data Fusion to Knowledge Fusion"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper adapt state-of-the-art data fusion techniques and apply them to a knowledge base with 1.6B unique knowledge triples extracted by 12 extractors from over 1B Web pages, which is three orders of magnitude larger than the data sets used in previous data fusion papers."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "Wrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "given a website, wrapper induction asks for manual annotations, often on only a handful of pages, and derives the extraction patterns, usually presented as XPaths, that can be applied to the whole website [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145867172"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095406372"
                        ],
                        "name": "Van Dang",
                        "slug": "Van-Dang",
                        "structuredName": {
                            "firstName": "Van",
                            "lastName": "Dang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van Dang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40428294"
                        ],
                        "name": "Wilko Horn",
                        "slug": "Wilko-Horn",
                        "structuredName": {
                            "firstName": "Wilko",
                            "lastName": "Horn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wilko Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47757859"
                        ],
                        "name": "C. Lugaresi",
                        "slug": "C.-Lugaresi",
                        "structuredName": {
                            "firstName": "Camillo",
                            "lastName": "Lugaresi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lugaresi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109375570"
                        ],
                        "name": "Shaohua Sun",
                        "slug": "Shaohua-Sun",
                        "structuredName": {
                            "firstName": "Shaohua",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohua Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Note that we resort to existing work to verify the accuracy of the website\u2019s claims [12] and to tackle the problem of entity linkage between the extracted data and existing entities in the knowledge base [13]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 739365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "117da44f01ef45ef8223bec8f9c2346b131321f4",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "\n The quality of web sources has been traditionally evaluated using\n exogenous\n signals such as the hyperlink structure of the graph. We propose a new approach that relies on\n endogenous\n signals, namely, the correctness of factual information provided by the source. A source that has few false facts is considered to be trustworthy.\n \n The facts are automatically extracted from each source by information extraction methods commonly used to construct knowledge bases. We propose a way to distinguish errors made in the extraction process from factual errors in the web source per se, by using joint inference in a novel multi-layer probabilistic model.\n \n We call the trustworthiness score we computed\n Knowledge-Based Trust (KBT)\n . On synthetic data, we show that our method can reliably compute the true trustworthiness levels of the sources. We then apply it to a database of 2.8B facts extracted from the web, and thereby estimate the trustworthiness of 119M webpages. Manual evaluation of a subset of the results confirms the effectiveness of the method.\n"
            },
            "slug": "Knowledge-Based-Trust:-Estimating-the-of-Web-Dong-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A way to distinguish errors made in the extraction process from factual errors in the web source per se, by using joint inference in a novel multi-layer probabilistic model is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115362953"
                        ],
                        "name": "Furong Li",
                        "slug": "Furong-Li",
                        "structuredName": {
                            "firstName": "Furong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143917898"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016202"
                        ],
                        "name": "A. Langen",
                        "slug": "A.-Langen",
                        "structuredName": {
                            "firstName": "Anno",
                            "lastName": "Langen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Langen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48513301"
                        ],
                        "name": "Y. Li",
                        "slug": "Y.-Li",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": ", domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29857774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81f687b63a6a0627448279750c1020a0182308ab",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Collecting structured knowledge for real-world entities has become a critical task for many applications. A big gap between the knowledge in existing knowledge repositories and the knowledge in the real world is the knowledge on tail verticals (i.e., less popular domains). Such knowledge, though not necessarily globally popular, can be personal hobbies to many people and thus collectively impactful. This paper studies the problem of knowledge verification for tail verticals; that is, deciding the correctness of a given triple. \n \nThrough comprehensive experimental study we answer the following questions. 1) Can we find evidence for tail knowledge from an extensive set of sources, including knowledge bases, the web, and query logs? 2) Can we judge correctness of the triples based on the collected evidence? 3) How can we further improve knowledge verification on tail verticals? Our empirical study suggests a new knowledge-verification framework, which we call Facty, that applies various kinds of evidence collection techniques followed by knowledge fusion. Facty can verify 50% of the (correct) tail knowledge with a precision of 84%, and it significantly outperforms state-of-the-art methods. Detailed error analysis on the obtained results suggests future research directions."
            },
            "slug": "Knowledge-Verification-for-LongTail-Verticals-Li-Dong",
            "title": {
                "fragments": [],
                "text": "Knowledge Verification for LongTail Verticals"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The empirical study suggests a new knowledge-verification framework, which is called Facty, that applies various kinds of evidence collection techniques followed by knowledge fusion and significantly outperforms state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695582"
                        ],
                        "name": "Stefano Ortona",
                        "slug": "Stefano-Ortona",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Ortona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Ortona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35203250"
                        ],
                        "name": "G. Orsi",
                        "slug": "G.-Orsi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Orsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orsi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834754"
                        ],
                        "name": "Marcello Buoncristiano",
                        "slug": "Marcello-Buoncristiano",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Buoncristiano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Buoncristiano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 256
                            }
                        ],
                        "text": "[19], XTPATH [7], BIGGRAMS [26], and VERTEX++ use manual annotations for training; LODIE-IDEAL and LODIE-LOD [15] conduct automatic annotation (LODIE-IDEAL compares between all web sources in a vertical and LODIE-LOD compares with Wikipedia); RR+WADAR (2) [29] and Bronzi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cae0d8261360b86ca11aeb5d042316ed56cd8352",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Web scraping (or wrapping) is a popular means for acquiring data from the web. Recent advancements have made scalable wrapper-generation possible and enabled data acquisition processes involving thousands of sources. This makes wrapper analysis and maintenance both needed and challenging as no scalable tools exists that support these tasks. \n \nWe demonstrate WADaR, a scalable and highly automated tool for joint wrapper and data repair. WADaR uses off-the-shelf entity recognisers to locate target entities in wrapper-generated data. Markov chains are used to determine structural repairs, that are then encoded into suitable repairs for both the data and corresponding wrappers. \n \nWe show that WADaR is able to increase the quality of wrapper-generated relations between 15% and 60%, and to fully repair the corresponding wrapper without any knowledge of the original website in more than 50% of the cases."
            },
            "slug": "WADaR:-Joint-Wrapper-and-Data-Repair-Ortona-Orsi",
            "title": {
                "fragments": [],
                "text": "WADaR: Joint Wrapper and Data Repair"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "WADaR is shown to be a scalable and highly automated tool for joint wrapper and data repair, able to increase the quality of wrapper-generated relations between 15% and 60%, and to fully repair the corresponding wrapper without any knowledge of the original website in more than half of the cases."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 8
                            }
                        ],
                        "text": "[16] R. Grishman."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "A recent overview of this area can be found in a survey by Grishman [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2323268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4544fac5136f972fa4f3b19493d47f34ffe6236",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of the world's knowledge is recorded in natural language text, but making effective use of it in this form poses a major challenge. Information extraction converts this knowledge to a structured form suitable for computer manipulation, opening up many possibilities for using it. In this review, the author describes the processing pipeline of information extraction, how the pipeline components are trained, and how this training can be made more efficient. He also describes some of the challenges that must be addressed for information extraction to become a more widely used technology."
            },
            "slug": "Information-Extraction-Grishman",
            "title": {
                "fragments": [],
                "text": "Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The author describes the processing pipeline of information extraction, how the pipeline components are trained, and how this training can be made more efficient."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intelligent Systems"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143711421"
                        ],
                        "name": "Alexander J. Ratner",
                        "slug": "Alexander-J.-Ratner",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Ratner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander J. Ratner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801197"
                        ],
                        "name": "Christopher De Sa",
                        "slug": "Christopher-De-Sa",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Sa",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher De Sa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144766615"
                        ],
                        "name": "Sen Wu",
                        "slug": "Sen-Wu",
                        "structuredName": {
                            "firstName": "Sen",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sen Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196579"
                        ],
                        "name": "Daniel Selsam",
                        "slug": "Daniel-Selsam",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Selsam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Selsam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114485554"
                        ],
                        "name": "C. R\u00e9",
                        "slug": "C.-R\u00e9",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "R\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Most recently, data programming [32] and heterogeneous supervision [24] are proposed to unite diverse, possibly conflicting sources of supervision such as annotations by humans, supervision from a KB, or any labeling function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14141965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37acbbbcfe9d8eb89e5b01da28dac6d44c3903ee",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable."
            },
            "slug": "Data-Programming:-Creating-Large-Training-Sets,-Ratner-Sa",
            "title": {
                "fragments": [],
                "text": "Data Programming: Creating Large Training Sets, Quickly"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A paradigm for the programmatic creation of training sets called data programming is proposed in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10229533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b37a9abb13cecc8ef8ef961b06078a0a53b6e857",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A precondition for extracting information from large text corpora is discovering the information structures underlying the text. Progress in this direction is being made in the form of unsupervised information extraction (IE). We describe recent work in unsupervised relation extraction and compare its goals to those of grammar discovery for science sublanguages. We consider what this work on grammar discovery suggests for future directions in unsupervised IE."
            },
            "slug": "Structural-Linguistics-and-Unsupervised-Information-Grishman",
            "title": {
                "fragments": [],
                "text": "Structural Linguistics and Unsupervised Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Recent work in unsupervised relation extraction is described and its goals are compared to those of grammar discovery for science sublanguages and what this work on grammar discovery suggests for future directions in un supervised IE is considered."
            },
            "venue": {
                "fragments": [],
                "text": "AKBC-WEKEX@NAACL-HLT"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "Approaches in this category [2, 8, 35, 15] aim to either discover templates from a set of pages or find recurring patterns from a page to extract unseen relation instances from other pages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15075203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd1f9f7795b31493d98d9f260d37aad07550f6e",
            "isKey": false,
            "numCitedBy": 1157,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach."
            },
            "slug": "RoadRunner:-Towards-Automatic-Data-Extraction-from-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "RoadRunner: Towards Automatic Data Extraction from Large Web Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences is developed, which confirms the feasibility of the approach on real-life data-intensive Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742448"
                        ],
                        "name": "K. Bollacker",
                        "slug": "K.-Bollacker",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Bollacker",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bollacker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065123479"
                        ],
                        "name": "Colin Evans",
                        "slug": "Colin-Evans",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Evans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Evans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990264"
                        ],
                        "name": "Praveen K. Paritosh",
                        "slug": "Praveen-K.-Paritosh",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Paritosh",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Praveen K. Paritosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399112633"
                        ],
                        "name": "Tim Sturge",
                        "slug": "Tim-Sturge",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Sturge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Sturge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110748390"
                        ],
                        "name": "Jamie Taylor",
                        "slug": "Jamie-Taylor",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "For the Movie vertical, we use a seed knowledge base derived from a download of the IMDb database that powers the IMDb website, with an ontology based on Freebase [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Freebase: a collaboratively created graph database for structuring human knowledge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 121
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0.7 is 0.63 and 0.64 respectively [11]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207167677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1976c9eeccc7115d18a04f1e7fb5145db6b96002",
            "isKey": true,
            "numCitedBy": 3825,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications."
            },
            "slug": "Freebase:-a-collaboratively-created-graph-database-Bollacker-Evans",
            "title": {
                "fragments": [],
                "text": "Freebase: a collaboratively created graph database for structuring human knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD Conference"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143818235"
                        ],
                        "name": "Andrew Carlson",
                        "slug": "Andrew-Carlson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Carlson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Carlson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31779043"
                        ],
                        "name": "J. Betteridge",
                        "slug": "J.-Betteridge",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Betteridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Betteridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16411658"
                        ],
                        "name": "B. Kisiel",
                        "slug": "B.-Kisiel",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Kisiel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kisiel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717452"
                        ],
                        "name": "Burr Settles",
                        "slug": "Burr-Settles",
                        "structuredName": {
                            "firstName": "Burr",
                            "lastName": "Settles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Burr Settles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1842532"
                        ],
                        "name": "Estevam Hruschka",
                        "slug": "Estevam-Hruschka",
                        "structuredName": {
                            "firstName": "Estevam",
                            "lastName": "Hruschka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Estevam Hruschka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction based on partial tree alignment."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with Vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web pages may contain thousands of text fields, and many real-world use cases will mention hundreds or thousands of entities, which poses challenges both at annotation time and at extraction time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Here we focus on techniques applicable to the semi-structured Web:\nWrapper induction: Early work for extracting semi-structured data was based on a supervised learning scheme called wrapper induction [21, 36, 17, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webpage filtering by informativeness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "In Proceedings of the 14th international conference on World Wide Web, pages 76\u201385."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Comparison with Knowledge Vault [10]: Knowledge Vault trained two DOM extractors to extract knowledge from the Web using Freebase as the seed KB, and the precision when applying a threshold of 0.7 is 0.63 and 0.64 respectively [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "5.1.1 SWDE To evaluate the performance of our approach across a range of\nverticals, we employed a subset of the Structured Web Data Extraction dataset (SWDE) [19] as our testbed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": ", domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "World Wide Web, 10(2):113\u2013132, 2007."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "It is critical to continuously grow knowledge bases to cover long tail information from different verticals (i.e., domains) and different languages [23], and as such, there has been a lot of work on automatic knowledge extraction from the Web [10, 9, 15, 14, 6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Among various types of Web sources, we argue that semi-structured websites (e.g., IMDb, as shown in Figure 1) are one of the most\n\u2217All work performed while at Amazon.\npromising knowledge sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "The CommonCrawl corpus consists of monthly snapshots of pages from millions of websites [1] on the Web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8423494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7312b8568d63bbbb239583ed282f46cdc40978d",
            "isKey": true,
            "numCitedBy": 1739,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent."
            },
            "slug": "Toward-an-Architecture-for-Never-Ending-Language-Carlson-Betteridge",
            "title": {
                "fragments": [],
                "text": "Toward an Architecture for Never-Ending Language Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work proposes an approach and a set of design principles for an intelligent computer agent that runs forever and describes a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570016"
                        ],
                        "name": "Fabian Pedregosa",
                        "slug": "Fabian-Pedregosa",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Pedregosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian Pedregosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025780"
                        ],
                        "name": "G. Varoquaux",
                        "slug": "G.-Varoquaux",
                        "structuredName": {
                            "firstName": "Ga\u00ebl",
                            "lastName": "Varoquaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Varoquaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797840"
                        ],
                        "name": "Alexandre Gramfort",
                        "slug": "Alexandre-Gramfort",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Gramfort",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Gramfort"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52200573"
                        ],
                        "name": "V. Michel",
                        "slug": "V.-Michel",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Michel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Michel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8493461"
                        ],
                        "name": "B. Thirion",
                        "slug": "B.-Thirion",
                        "structuredName": {
                            "firstName": "Bertrand",
                            "lastName": "Thirion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Thirion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958756"
                        ],
                        "name": "O. Grisel",
                        "slug": "O.-Grisel",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Grisel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Grisel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27257992"
                        ],
                        "name": "Mathieu Blondel",
                        "slug": "Mathieu-Blondel",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Blondel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathieu Blondel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881041"
                        ],
                        "name": "Gilles Louppe",
                        "slug": "Gilles-Louppe",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Louppe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gilles Louppe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780213"
                        ],
                        "name": "P. Prettenhofer",
                        "slug": "P.-Prettenhofer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Prettenhofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Prettenhofer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067827437"
                        ],
                        "name": "Ron Weiss",
                        "slug": "Ron-Weiss",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39571582"
                        ],
                        "name": "Ron J. Weiss",
                        "slug": "Ron-J.-Weiss",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Weiss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron J. Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081469"
                        ],
                        "name": "J. Vanderplas",
                        "slug": "J.-Vanderplas",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Vanderplas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vanderplas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144720379"
                        ],
                        "name": "Alexandre Passos",
                        "slug": "Alexandre-Passos",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Passos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Passos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3084321"
                        ],
                        "name": "D. Cournapeau",
                        "slug": "D.-Cournapeau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cournapeau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cournapeau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423884"
                        ],
                        "name": "M. Brucher",
                        "slug": "M.-Brucher",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Brucher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brucher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35243423"
                        ],
                        "name": "M. Perrot",
                        "slug": "M.-Perrot",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Perrot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710398"
                        ],
                        "name": "E. Duchesnay",
                        "slug": "E.-Duchesnay",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Duchesnay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Duchesnay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "We used the logistic regression implementation provided by Scikit-learn using the LBFGS optimizer and L2 regularization with C=1 [31], and used Scikit-learn\u2019s agglomerative clustering implementation for the clustering step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10659969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74",
            "isKey": false,
            "numCitedBy": 43288,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            },
            "slug": "Scikit-learn:-Machine-Learning-in-Python-Pedregosa-Varoquaux",
            "title": {
                "fragments": [],
                "text": "Scikit-learn: Machine Learning in Python"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems, focusing on bringing machine learning to non-specialists using a general-purpose high-level language."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756801"
                        ],
                        "name": "D. Olteanu",
                        "slug": "D.-Olteanu",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Olteanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Olteanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2207525"
                        ],
                        "name": "H. Meuss",
                        "slug": "H.-Meuss",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Meuss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Meuss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767012"
                        ],
                        "name": "Fran\u00e7ois Bry",
                        "slug": "Fran\u00e7ois-Bry",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Bry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Bry"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 217
                            }
                        ],
                        "text": "Traditional DOM extraction typically uses wrapper induction: given a website, wrapper induction asks for manual annotations, often on only a handful of pages, and derives the extraction patterns, usually presented as XPaths, that can be applied to the whole website [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "Notably, the upper XPath from McKellon\u2019s page does exist on Winfrey\u2019s page as well, but it represents her \u201cproducer of\u201d relationship with the film Selma."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 238
                            }
                        ],
                        "text": "The consistency observation applies to relations as well; however, since some multi-valued predicates are often represented in long lists and tend to occur in parts of the page with a higher variety of formatting than the page topic, the XPaths of relation candidates may be sparser than they are for topic candidates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 200
                            }
                        ],
                        "text": "On the other hand, for predicates such as sound editor and camera operator, which do not exist in the seed KB, we do not necessarily make annotation mistakes but can make extraction mistakes when the XPaths are very similar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "XPaths representing the same predicate on a semi-structured website tend to be similar but may have some differences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 100
                            }
                        ],
                        "text": "We represent each webpage as a DOM Tree4; a node in the tree can be uniquely defined by an absolute XPath, which for brevity we simply refer to as an XPath [28]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "For each page wi, the XPaths to all mentions of topic candidates are collected (note that the candidate topic may appear in multiple text fields on the page)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 130
                            }
                        ],
                        "text": "3.2.2 Global evidence via clustering\nRecall that in the topic identification step, we follow the consistency observation and rank XPaths by how frequently they contain candidate topics, and prefer those with higher count."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 113
                            }
                        ],
                        "text": "If we have labeled multiple positive examples of a predicate on a page, and they differ only in indices of their XPaths, we consider it likely that they belong to a list of values, such as the list of cast members for a film."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "We use the (possibly noisy) annotations to train a machine learning model that classifies a DOM node, represented by an XPath, on a webpage, and outputs a relation between the topic entity and the entity represented by the node (either a predicate present in the training data or an \u201cOTHER\u201d label indicating that there is not a relation present in our ontology)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "Finding the dominant XPath: Our consistency observation tells us that the text field containing the name or identifier of the topic entity should be in roughly the same location from page to page."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 116
                            }
                        ],
                        "text": "The distance function between two DOM nodes is defined as the Levenshtein distance [22] between their corresponding XPaths."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "Figure 2 shows an example of XPaths representing the \u201cActed In\u201d predicate on IMDb."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 143
                            }
                        ],
                        "text": "We\nAlgorithm 2 Annotating a full page for a predicate Input: A semi-structured webpage w, a set of predicate objects O, a set C\nof clusters of XPaths for all predicate mentions across the website, and a list of predicates frequently duplicated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "VERTEX++: We implemented the Vertex wrapper learning algorithm [17], which uses manual annotations to learn extraction patterns, expressed by XPaths."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 370
                            }
                        ],
                        "text": "1: procedure PAGETOPICIDENTIFICATION(W, K) 2: procedure SCOREENTITIESFORPAGE(w) 3: p\u2190 {} /* initialize dictionary of entity score */ 4: pageSet\u2190 all entities on w via string matching 5: for all e \u2208 PageSet do 6: entitySet\u2190 all objects of triples in K 7: where e is the subject 8: p[e]\u2190 J(entitySet, PageSet) 9: return p\n10: 11: /* Score entities on a page and track counts of XPaths leading to\nhighest scoring entities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 79
                            }
                        ],
                        "text": "*/ 12: P \u2190 {} /* track entity scores for each page */ 13: pathCounts\u2190 {} /* track XPath counts across W */ 14: for all wi \u2208W do 15: P [wi]\u2190 ScoreEntitiesForPage(wi) 16: ci \u2190 argmax(P [wi]) 17: candidateTreePaths\u2190 XPaths of all mentions 18: of ci on wi 19: for all path \u2208 candidateTreePaths do 20: pathCounts[path] + + 21: 22: /* Match text at the most common XPath across W for the highest\nscoring entity ci for each page */ 23: T \u2190 {} initialize dictionary of topic entities 24: for all wi \u2208W do 25: maxCountTreePath\u2190 XPath extant on wi with 26: max count in pathCounts 27: topicTextF ield\u2190 text field found at 28: maxCountTreePath on wi 29: T [wi]\u2190 K\u2019s entity string matching topicTextF ield 30: with max score in P (wi) 31: return T\nWe admit that in this way we will not be able to extract relations between non-topic-entities, such as the directed by relation between the Crooklyn movie and its director in Figure 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "For this reason, we instead cluster the XPaths of all potential object mentions of a predicate across pages, and prefer those XPaths that appear in larger clusters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 635,
                                "start": 630
                            }
                        ],
                        "text": "Output: An annotation for each oi in O. 1: procedure BESTLOCALMENTION(mentions) 2: bestCount\u2190 0 3: bestMentions\u2190 [] 4: for mention \u2208 mentions do 5: ancestorNode\u2190 highest level node containing 6: mention and no other element in mentions 7: neighborCount\u2190 count of all objects for 8: predicate under ancestorNode 9: if neighborCount > bestCount then 10: bestCount\u2190 neighborCount 11: bestMentions\u2190 [mention] 12: else if neighborCount == bestCount then 13: bestMentions.append(mentions) 14: return bestMentions 15: 16: /* Main procedure for annotating oi in O. */ 17: A\u2190 \u2205 /* initialize set of annotations */ 18: for all oi in O do 19: mentions\u2190 XPaths corresponding to mentions 20: of o on w 21: bestLocMen\u2190 bestLocalMention(mentions) 22: if count(bestLocMen) == 1 then 23: mention\u2190 bestLocMen[0] 24: else 25: if predicate is frequently duplicated then 26: mention\u2190 mention in bestLocMen 27: corresponding to largest cluster in C 28: else 29: mention\u2190 Null 30: A.append(mention) 31: return A\ntry to harness local and global information to derive the correct annotation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "We represent each webpage as a DOM Tree(4); a node in the tree can be uniquely defined by an absolute XPath, which for brevity we simply refer to as an XPath [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15178370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b31f2e45ff25a6951f34c0f621a9992aaee615c",
            "isKey": true,
            "numCitedBy": 266,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The location path language XPath is of particular importance for XML applications since it is a core component of many XML processing standards such as XSLT or XQuery. In this paper, based on axis symmetry of XPath, equivalences of XPath 1.0 location paths involving reverse axes, such as anc and prec, are established. These equivalences are used as rewriting rules in an algorithm for transforming location paths with reverse axes into equivalent reverse-axis-free ones. Location paths without reverse axes, as generated by the presented rewriting algorithm, enable efficient SAX-like streamed data processing of XPath."
            },
            "slug": "XPath:-Looking-Forward-Olteanu-Meuss",
            "title": {
                "fragments": [],
                "text": "XPath: Looking Forward"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Equivalences of XPath 1.0 location paths involving reverse axes, such as anc and prec, are established and used as rewriting rules in an algorithm for transforming location paths with reverse axes into equivalent reverse-axis-free ones."
            },
            "venue": {
                "fragments": [],
                "text": "EDBT Workshops"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154179"
                        ],
                        "name": "V. Levenshtein",
                        "slug": "V.-Levenshtein",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Levenshtein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Levenshtein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60827152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd",
            "isKey": false,
            "numCitedBy": 10968,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Binary-codes-capable-of-correcting-deletions,-and-Levenshtein",
            "title": {
                "fragments": [],
                "text": "Binary codes capable of correcting deletions, insertions, and reversals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2627799"
                        ],
                        "name": "P. Gulhane",
                        "slug": "P.-Gulhane",
                        "structuredName": {
                            "firstName": "Pankaj",
                            "lastName": "Gulhane",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gulhane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696519"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757518"
                        ],
                        "name": "Srinivasan H. Sengamedu",
                        "slug": "Srinivasan-H.-Sengamedu",
                        "structuredName": {
                            "firstName": "Srinivasan",
                            "lastName": "Sengamedu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinivasan H. Sengamedu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990683"
                        ],
                        "name": "Ashwin Tengli",
                        "slug": "Ashwin-Tengli",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Tengli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashwin Tengli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Each text field on the webpage is matched against the KB using fuzzy string matching process presented in [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 212
                            }
                        ],
                        "text": "Multi-valued predicates are a challenging and little-explored topic in semi-structured extraction, with most prior unsupervised and semi-supervised extractors restricting the problem to single-valued extractions [4, 19, 15, 18, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[18] exploit the redundancy in the attribute value content across websites in a vertical to make extractions from a new site in the vertical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53223798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b95fc81dd41f5e1dc11d9ebdcbbd98387813adf0",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites. We start by populating a seed database with records extracted from a few initial sites. We then identify values within the pages of each new site that match attribute values contained in the seed set of records. To filter out noisy attribute value matches, we exploit the fact that attribute values occur at fixed positions within template-based sites. We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages. Finally, we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach."
            },
            "slug": "Exploiting-content-redundancy-for-web-information-Gulhane-Rastogi",
            "title": {
                "fragments": [],
                "text": "Exploiting content redundancy for web information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work proposes a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites by developing an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '10"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 23,
            "methodology": 15
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 34,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/CERES:-Distantly-Supervised-Relation-Extraction-the-Lockard-Dong/3ea6c256ab2a7d3fd32f29bfe8183367b50897a6?sort=total-citations"
}