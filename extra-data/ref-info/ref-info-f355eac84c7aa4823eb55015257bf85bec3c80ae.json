{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 54
                            }
                        ],
                        "text": "Two well-known studies in this stream are RoadRunner (Crescenzi and Mecca 2004) and EXALG (Arasu and Garcia-Molina 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6572841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a47888c0243cac0b173c2748d8ed1b0a2a15fdd8",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction from websites is nowadays a relevant problem, usually performed by software modules called wrappers. A key requirement is that the wrapper generation process should be automated to the largest extent, in order to allow for large-scale extraction tasks even in presence of changes in the underlying sites. So far, however, only semi-automatic proposals have appeared in the literature.We present a novel approach to information extraction from websites, which reconciles recent proposals for supervised wrapper induction with the more traditional field of grammar inference. Grammar inference provides a promising theoretical framework for the study of unsupervised---that is, fully automatic---wrapper generation algorithms. However, due to some unrealistic assumptions on the input, these algorithms are not practically applicable to Web information extraction tasks.The main contributions of the article stand in the definition of a class of regular languages, called the prefix mark-up languages, that abstract the structures usually found in HTML pages, and in the definition of a polynomial-time unsupervised learning algorithm for this class. The article shows that, differently from other known classes, prefix mark-up languages and the associated algorithm can be practically used for information extraction purposes.A system based on the techniques described in the article has been implemented in a working prototype. We present some experimental results on known Websites, and discuss opportunities and limitations of the proposed approach."
            },
            "slug": "Automatic-information-extraction-from-large-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "Automatic information extraction from large websites"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A novel approach to information extraction from websites is presented, which reconciles recent proposals for supervised wrapper induction with the more traditional field of grammar inference, and shows that, differently from other known classes, prefix mark-up languages and the associated algorithm can be practically used for information extraction purposes."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1953113"
                        ],
                        "name": "Anna Lisa Gentile",
                        "slug": "Anna-Lisa-Gentile",
                        "structuredName": {
                            "firstName": "Anna Lisa",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Lisa Gentile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046811422"
                        ],
                        "name": "Ziqi Zhang",
                        "slug": "Ziqi-Zhang",
                        "structuredName": {
                            "firstName": "Ziqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziqi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736067"
                        ],
                        "name": "Isabelle Augenstein",
                        "slug": "Isabelle-Augenstein",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Augenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabelle Augenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758555"
                        ],
                        "name": "F. Ciravegna",
                        "slug": "F.-Ciravegna",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Ciravegna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Ciravegna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 31
                            }
                        ],
                        "text": "This experiment is reported in Gentile et al. (2013), and the induced patterns produced good extraction results, with overall F-measure of 80 percent on a publicly available data set (Hao et al. 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "The MS and MT models have not been yet implemented in LODIE, while we did implement a prototype for MW (Gentile et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "Some learning tasks won\u2019t be robust to noise in the training data, and therefore filtering will be needed; other tasks will be able to handle noise (Gentile et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 179
                            }
                        ],
                        "text": "(2011) refer to figures reported in the paper From One Tree to a Forest; Ideal Dictionaries refers to our method applied using ad hoc dictionaries, reported as topline experiment (Gentile et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 191
                            }
                        ],
                        "text": "Hao et al. (2011) refer to figures reported in the paper From One Tree to a Forest; Ideal Dictionaries refers to our method applied using ad hoc dictionaries, reported as topline experiment (Gentile et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5797695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29b8164adfae1cdd5fd4af4ab43d0bed1764e9db",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This work explores the usage of Linked Data for Web scale Information Extraction and shows encouraging results on the task of Wrapper Induction. We propose a simple knowledge based method which is (i) highly flexible with respect to different domains and (ii) does not require any training material, but exploits Linked Data as background knowledge source to build essential learning resources. The major contribution of this work is a study of how Linked Data - an imprecise, redundant and large-scale knowledge resource - can be used to support Web scale Information Extraction in an effective and efficient way and identify the challenges involved. We show that, for domains that are covered, Linked Data serve as a powerful knowledge resource for Information Extraction. Experiments on a publicly available dataset demonstrate that, under certain conditions, this simple unsupervised approach can achieve competitive results against some complex state of the art that always depends on training data."
            },
            "slug": "Unsupervised-wrapper-induction-using-linked-data-Gentile-Zhang",
            "title": {
                "fragments": [],
                "text": "Unsupervised wrapper induction using linked data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a simple knowledge based method which is highly flexible with respect to different domains and does not require any training material, but exploits Linked Data as background knowledge source to build essential learning resources."
            },
            "venue": {
                "fragments": [],
                "text": "K-CAP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145612610"
                        ],
                        "name": "Doug Downey",
                        "slug": "Doug-Downey",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Downey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Downey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3447955"
                        ],
                        "name": "Stanley Kok",
                        "slug": "Stanley-Kok",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Kok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley Kok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36445704"
                        ],
                        "name": "Ana-Maria Popescu",
                        "slug": "Ana-Maria-Popescu",
                        "structuredName": {
                            "firstName": "Ana-Maria",
                            "lastName": "Popescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana-Maria Popescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296031"
                        ],
                        "name": "Tal Shaked",
                        "slug": "Tal-Shaked",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Shaked",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Shaked"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321874"
                        ],
                        "name": "A. Yates",
                        "slug": "A.-Yates",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yates"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 36
                            }
                        ],
                        "text": "Web-Scale Information Extraction in KnowItAll (Preliminary Results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Two well-known earlier systems in this area are Snowball (Agichtein et al. 2001) and KnowItAll (Etzioni et al. 2004, Banko et al. 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 110
                            }
                        ],
                        "text": "Snowball iteratively learns new instances of a given type of relation from a large document collection, while KnowItAll learns new entities of predefined classes from the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 151
                            }
                        ],
                        "text": "\u2026information from a gigantic data source suchas the web has been considered a major research chal-lenge, and over the years many different approaches (Etzioni et al. 2004; Banko et al. 2007; Carlson et al. 2010; Freedman and Ramshaw 2011; Nakashole, Theobald, and Weikum 2011) have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6755965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ef07373873cc0f0b940512dcdde4e7b54b0cfb0",
            "isKey": true,
            "numCitedBy": 892,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous,domain-independent, and scalable manner.The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems."
            },
            "slug": "Web-scale-information-extraction-in-knowitall:-Etzioni-Cafarella",
            "title": {
                "fragments": [],
                "text": "Web-scale information extraction in knowitall: (preliminary results)"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "KnowItAll, a system that aims to automate the tedious process of extracting large collections of facts from the web in an autonomous, domain-independent, and scalable manner, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50452701"
                        ],
                        "name": "M. Broadhead",
                        "slug": "M.-Broadhead",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Broadhead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Broadhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 118
                            }
                        ],
                        "text": "While these systems learn to extract predefined types of information based on (limited) training data, the TextRunner (Banko et al. 2007) system proposes an \u201copen information-extraction\u201d paradigm, which exploits generic patterns to extract generic facts from the web for unlimited domains without predefined interests."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 117
                            }
                        ],
                        "text": "Two well-known earlier systems in this area are Snowball (Agichtein et al. 2001) and KnowItAll (Etzioni et al. 2004, Banko et al. 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 172
                            }
                        ],
                        "text": "\u2026information from a gigantic data source suchas the web has been considered a major research chal-lenge, and over the years many different approaches (Etzioni et al. 2004; Banko et al. 2007; Carlson et al. 2010; Freedman and Ramshaw 2011; Nakashole, Theobald, and Weikum 2011) have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 119
                            }
                        ],
                        "text": "While these systems learn to extract predefined types of information based on (limited) training data, the TextRunner (Banko et al. 2007) system proposes an \u201copen information-extraction\u201d paradigm, which exploits generic patterns to extract generic facts from the web for unlimited domains without\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207169186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "498bb0efad6ec15dd09d941fb309aa18d6df9f5f",
            "isKey": true,
            "numCitedBy": 2290,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER\u2019s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000more abstract assertions."
            },
            "slug": "Open-Information-Extraction-from-the-Web-Banko-Cafarella",
            "title": {
                "fragments": [],
                "text": "Open Information Extraction from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143818235"
                        ],
                        "name": "Andrew Carlson",
                        "slug": "Andrew-Carlson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Carlson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Carlson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143796414"
                        ],
                        "name": "C. Schafer",
                        "slug": "C.-Schafer",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Schafer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schafer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 62
                            }
                        ],
                        "text": "It generally addresses extracting data from detail web pages (Carlson and Schafer 2008), which are pages corresponding to a single data record (or entity) of a certain type or concept (also called vertical in the literature), and renders various attributes of that record in a human-readable form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9040930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "725bd456f1439a71cb3c59ab5efa574545cb0153",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of extracting structured records from semi-structured web pages with no human supervision required for each target web site. Previous work on this problem has either required significant human effort for each target site or used brittle heuristics to identify semantic data types. Our method only requires annotation for a few pages from a few sites in the target domain. Thus, after a tiny investment of human effort, our method allows automatic extraction from potentially thousands of other sites within the same domain. Our approach extends previous methods for detecting data fields in semi-structured web pages by matching those fields to domain schema columns using robust models of data values and contexts. Annotating 2---5 pages for 4---6 web sites yields an extraction accuracy of 83.8% on job offer sites and 91.1% on vacation rental sites. These results significantly outperform a baseline approach."
            },
            "slug": "Bootstrapping-Information-Extraction-from-Web-Pages-Carlson-Schafer",
            "title": {
                "fragments": [],
                "text": "Bootstrapping Information Extraction from Semi-structured Web Pages"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work considers the problem of extracting structured records from semi-structured web pages with no human supervision required for each target web site, and extends previous methods for detecting data fields in semi- structural web pages by matching those fields to domain schema columns using robust models of data values and contexts."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052599266"
                        ],
                        "name": "Qiang Hao",
                        "slug": "Qiang-Hao",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625690"
                        ],
                        "name": "Rui Cai",
                        "slug": "Rui-Cai",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134722"
                        ],
                        "name": "Yanwei Pang",
                        "slug": "Yanwei-Pang",
                        "structuredName": {
                            "firstName": "Yanwei",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanwei Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "To calculate the performance metrics we followed the evaluation methodology proposed by Hao et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 184
                            }
                        ],
                        "text": "This experiment is reported in Gentile et al. (2013), and the induced patterns produced good extraction results, with overall F-measure of 80 percent on a publicly available data set (Hao et al. 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 138
                            }
                        ],
                        "text": "Creating such annotations requires significant human effort and remains a bottleneck in the wrapper induction process (Wong and Lam 2010; Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 129
                            }
                        ],
                        "text": "Although recent studies (Carlson and\nSPRING 2015 57\nSchafer 2008; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Hao et al. 2011) have focused on addressing these two issues, these methods still depend on manually labelled examples to train a wrapper, while in some cases (Dalvi,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 37
                            }
                        ],
                        "text": "Our method does not outperform Hao\u2019s (Hao et al. 2011), but has the advantage of being totally unsupervised."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 137
                            }
                        ],
                        "text": "(2013), and the induced patterns produced good extraction results, with overall F-measure of 80 percent on a publicly available data set (Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 50
                            }
                        ],
                        "text": "A groundtruth for the WI task is also provided by Hao et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 50
                            }
                        ],
                        "text": "Statistics of the Selected Gold Standard Data Set (Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 210
                            }
                        ],
                        "text": "Table 3 compares the average of results of our method (LOD dictionaries) against the same method applied starting from Ideal Dictionaries (figures obtained from Gentile et al. [2013]) and against the method by Hao et al. (2011), from which we also obtained the data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 265
                            }
                        ],
                        "text": "Thus, research in recent years has focused on developing robust wrapper induction approaches (Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011) and methods that are general across attributes, verticals, and websites (Muslea, Minton, and Knoblock 2003; Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 90
                            }
                        ],
                        "text": "The data extracted by our wrappers is compared against the groundtruth values provided by Hao et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Hao et al. (2011) refer to figures reported in the paper From One Tree to a Forest; Ideal Dictionaries refers to our method applied using ad hoc dictionaries, reported as topline experiment (Gentile et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17002481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5da5952da2fceac1823393ee2e0bdde0e0a02d2b",
            "isKey": true,
            "numCitedBy": 77,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution."
            },
            "slug": "From-one-tree-to-a-forest:-a-unified-solution-for-Hao-Cai",
            "title": {
                "fragments": [],
                "text": "From one tree to a forest: a unified solution for structured web data extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships) are designed that can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046811422"
                        ],
                        "name": "Ziqi Zhang",
                        "slug": "Ziqi-Zhang",
                        "structuredName": {
                            "firstName": "Ziqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziqi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1953113"
                        ],
                        "name": "Anna Lisa Gentile",
                        "slug": "Anna-Lisa-Gentile",
                        "structuredName": {
                            "firstName": "Anna Lisa",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Lisa Gentile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2456849"
                        ],
                        "name": "E. Blomqvist",
                        "slug": "E.-Blomqvist",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Blomqvist",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Blomqvist"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736067"
                        ],
                        "name": "Isabelle Augenstein",
                        "slug": "Isabelle-Augenstein",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Augenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabelle Augenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758555"
                        ],
                        "name": "F. Ciravegna",
                        "slug": "F.-Ciravegna",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Ciravegna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Ciravegna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "An SKP is an ontological view over a class (defined in a reference ontology in the LOD), and captures and summarises the usage of that class in data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "At this stage we do not propose a systematic methodology to define T, and we still require the user to manually define T, but we aim to ease the task by providing SKPs, which we consider as building blocks to add additional information to the underneath ontologies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "Although so far we only addressed synonymity of relations (Zhang et al. 2013), synonymity of concepts in different data sets can be included (Parundekar, Knoblock, and Ambite 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 76
                            }
                        ],
                        "text": "The solution we propose is to exploit statistical knowledge patterns (SKP) (Zhang et al. 2013) as a gate to the LOD ontologies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "While we explored initial methods to formalize user needs (Zhang et al. 2013), in this article we summarise initial results on ideas (2) and (3), presenting experiments on the usage of LOD as training data for the task of wrapper induction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Each SKP contains properties and axioms involving the main class derived from a reference ontology and properties and axioms involving the main class that are not expressed in the reference ontology but that can be induced from statistical measures on statements published as linked data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "An SKP is represented and stored as an OWL ontology."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18246151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab4694d8b3fb70cba98d81106105f1aa8abee25b",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web of Data is a rich common resource with billions of triples available in thousands of datasets and individual Web documents created by both expert and non-expert ontologists. A common problem is the imprecision in the use of vocabularies: annotators can misunderstand the semantics of a class or property or may not be able to find the right objects to annotate with. This decreases the quality of data and may eventually hamper its usability over large scale. This paper describes Statistical Knowledge Patterns (SKP) as a means to address this issue. SKPs encapsulate key information about ontology classes, including synonymous properties in (and across) datasets, and are automatically generated based on statistical data analysis. SKPs can be effectively used to automatically normalise data, and hence increase recall in querying. Both pattern extraction and pattern usage are completely automated. The main benefits of SKPs are that: (1) their structure allows for both accurate query expansion and restriction; (2) they are context dependent, hence they describe the usage and meaning of properties in the context of a particular class; and (3) they can be generated offline, hence the equivalence among relations can be used efficiently at run time."
            },
            "slug": "Statistical-Knowledge-Patterns:-Identifying-in-Zhang-Gentile",
            "title": {
                "fragments": [],
                "text": "Statistical Knowledge Patterns: Identifying Synonymous Relations in Large Linked Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Statistical Knowledge Patterns (SKP) encapsulate key information about ontology classes, including synonymous properties in (and across) datasets, and are automatically generated based on statistical data analysis and can be effectively used to automatically normalise data, and hence increase recall in querying."
            },
            "venue": {
                "fragments": [],
                "text": "SEMWEB"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061877173"
                        ],
                        "name": "V. L\u00f3pez",
                        "slug": "V.-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Vanessa",
                            "lastName": "L\u00f3pez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729747"
                        ],
                        "name": "A. Nikolov",
                        "slug": "A.-Nikolov",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Nikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3122192"
                        ],
                        "name": "M. Sabou",
                        "slug": "M.-Sabou",
                        "structuredName": {
                            "firstName": "Marta",
                            "lastName": "Sabou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sabou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2326420"
                        ],
                        "name": "V. Uren",
                        "slug": "V.-Uren",
                        "structuredName": {
                            "firstName": "Victoria",
                            "lastName": "Uren",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Uren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721851"
                        ],
                        "name": "E. Motta",
                        "slug": "E.-Motta",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Motta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Motta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393699478"
                        ],
                        "name": "M. d\u2019Aquin",
                        "slug": "M.-d\u2019Aquin",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "d\u2019Aquin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. d\u2019Aquin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 231
                            }
                        ],
                        "text": "This aspect has not yet been tackled in LODIE, but we will explore methods with minimum requirements in computational terms such as simple feature-overlapping-based methods (Banerjee and Pedersen 2002) and string distance metrics (Lopez et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 89
                            }
                        ],
                        "text": "Similarly, community-provided resources and annotations can contain errors, imprecision (Lopez et al. 2010), spam, or even deviations from standards (Halpin, Hayes, and McCusker 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 780727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "474c9976fc220b4dfc32b28fdf5432c75ec1a7c6",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Linked Data semantic sources, in particular DBpedia, can be used to answer many user queries. PowerAqua is an open multi-ontology Question Answering (QA) system for the Semantic Web (SW). However, the emergence of Linked Data, characterized by its openness, heterogeneity and scale, introduces a new dimension to the Semantic Web scenario, in which exploiting the relevant information to extract answers for Natural Language (NL) user queries is a major challenge. In this paper we discuss the issues and lessons learned from our experience of integrating PowerAqua as a front-end for DBpedia and a subset of Linked Data sources. As such, we go one step beyond the state of the art on end-users interfaces for Linked Data by introducing mapping and fusion techniques needed to translate a user query by means of multiple sources. Our first informal experiments probe whether, in fact, it is feasible to obtain answers to user queries by composing information across semantic sources and Linked Data, even in its current form, where the strength of Linked Data is more a by-product of its size than its quality. We believe our experiences can be extrapolated to a variety of end-user applications that wish to scale, open up, exploit and re-use what possibly is the greatest wealth of data about everything in the history of Artificial Intelligence."
            },
            "slug": "Scaling-Up-Question-Answering-to-Linked-Data-L\u00f3pez-Nikolov",
            "title": {
                "fragments": [],
                "text": "Scaling Up Question-Answering to Linked Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The issues and lessons learned from the experience of integrating PowerAqua as a front-end for DBpedia and a subset of Linked Data sources are discussed and extrapolated to a variety of end-user applications that wish to scale, open up, exploit and re-use what possibly is the greatest wealth of data about everything in the history of Artificial Intelligence."
            },
            "venue": {
                "fragments": [],
                "text": "EKAW"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684012"
                        ],
                        "name": "L. Gravano",
                        "slug": "L.-Gravano",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Gravano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gravano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20772364"
                        ],
                        "name": "Jeff Pavel",
                        "slug": "Jeff-Pavel",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Pavel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Pavel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073824059"
                        ],
                        "name": "Viktoriya Sokolova",
                        "slug": "Viktoriya-Sokolova",
                        "structuredName": {
                            "firstName": "Viktoriya",
                            "lastName": "Sokolova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Viktoriya Sokolova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677089"
                        ],
                        "name": "Aleksandr Voskoboynik",
                        "slug": "Aleksandr-Voskoboynik",
                        "structuredName": {
                            "firstName": "Aleksandr",
                            "lastName": "Voskoboynik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleksandr Voskoboynik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 58
                            }
                        ],
                        "text": "Two well-known earlier systems in this area are Snowball (Agichtein et al. 2001) and KnowItAll (Etzioni et al. 2004, Banko et al. 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Snowball: A Prototype System for Extracting Relations from Large Text Collections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 57
                            }
                        ],
                        "text": "Two well-known earlier systems in this area are Snowball (Agichtein et al. 2001) and KnowItAll (Etzioni et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "StatSnowball: A Statistical Approach to Extracting Entity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 66
                            }
                        ],
                        "text": "Both have inspired a number of more recent studies, including StatSnowball (Zhu 2009), ExtremeExtraction (Freedman and Ramshaw 2011), NELL (Carlson et al. 2010), and PROSPERA (Nakashole, Theobald, and Weikum 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Snowball iteratively learns new instances of a given type of relation from a large document collection, while KnowItAll learns new entities of predefined classes from the web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16581515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d26d446c7e809bd4e1ee6e696a17764521618c7",
            "isKey": true,
            "numCitedBy": 67,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Text documents often hide valuable structured data. For example, a collection of newspaper articles might contain information on the location of the headquarters of a number of organizations. If we need to nd the location of the headquarters of, say, Microsoft, we could try and use traditional information-retrieval techniques for nding documents that contain the answer to our query. Alternatively, we could answer such a query more precisely if we somehow had available a table listing all the organization-location pairs that are mentioned in our document collection. One could view the extraction process as automatically building a materialized view over the unstructured text data. In this demo we present an interactive prototype of our Snowball system for extracting relations from collections of plain-text documents with minimal human participation. Our method builds on the DIPRE idea introduced by Brin [3]. Our system and techniques were presented in detail in [2] and [1]."
            },
            "slug": "Snowball:-a-prototype-system-for-extracting-from-Agichtein-Gravano",
            "title": {
                "fragments": [],
                "text": "Snowball: a prototype system for extracting relations from large text collections"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This demo presents an interactive prototype of the Snowball system for extracting relations from collections of plain-text documents with minimal human participation, which builds on the DIPRE idea introduced by Brin."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680484"
                        ],
                        "name": "K. Balog",
                        "slug": "K.-Balog",
                        "structuredName": {
                            "firstName": "Krisztian",
                            "lastName": "Balog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Balog"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708801"
                        ],
                        "name": "P. Serdyukov",
                        "slug": "P.-Serdyukov",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Serdyukov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Serdyukov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509504"
                        ],
                        "name": "A. D. Vries",
                        "slug": "A.-D.-Vries",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Vries",
                            "middleNames": [
                                "P.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Vries"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 60
                            }
                        ],
                        "text": "The TREC2011 evaluation on the related-entity finding task (Balog and Serdyukov 2011) has proposed using LOD to support answering generic queries in large corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 60
                            }
                        ],
                        "text": "2010) and extraction of specific answers from large corpora (Balog and Serdyukov 2011), but a generalized approach to the use of LOD for training largescale IE is still missing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 168
                            }
                        ],
                        "text": "Current approaches to using LOD for web-scale IE are limited in scope to recognizing tables (Mulwad et al. 2010) and extraction of specific answers from large corpora (Balog and Serdyukov 2011), but a generalized approach to the use of LOD for training largescale IE is still missing."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14045841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3d4c267bbfd251cd32a61d60334f5d5f9db4f9a",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The issue of combining (noisy) textual material (the Web) with semi-structured data (like Wikipedia or slightly more structured data sources like IMDB) is however an interesting line of research. As many data sources, and in particular those being constructed as so-called Linked Open Data (LOD), are naturally organized around entities, it would be reasonable to examine this problem in the context of entity retrieval. To foster research in this direction, we introduced the new Entity List Completion (ELC) pilot task. ELC is motivated by the same user scenario as REF, but with the main difference that entities are represented by their URIs in a Semantic Web crawl (the Billion Triple Collection). In addition, a small number of example entities (defined by their URIs) are made available as part of the topic definition. Our goal is to turn this pilot task to an \"official\" task in 2011."
            },
            "slug": "Overview-of-the-TREC-2010-Entity-Track-Balog-Serdyukov",
            "title": {
                "fragments": [],
                "text": "Overview of the TREC 2010 Entity Track"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The new Entity List Completion (ELC) pilot task is introduced, motivated by the same user scenario as REF, but with the main difference that entities are represented by their URIs in a Semantic Web crawl (the Billion Triple Collection)."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144367841"
                        ],
                        "name": "Roi Blanco",
                        "slug": "Roi-Blanco",
                        "structuredName": {
                            "firstName": "Roi",
                            "lastName": "Blanco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roi Blanco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723984"
                        ],
                        "name": "H. Halpin",
                        "slug": "H.-Halpin",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Halpin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Halpin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40469496"
                        ],
                        "name": "Daniel M. Herzig",
                        "slug": "Daniel-M.-Herzig",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Herzig",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel M. Herzig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814916"
                        ],
                        "name": "P. Mika",
                        "slug": "P.-Mika",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32546616"
                        ],
                        "name": "Jeffrey Pound",
                        "slug": "Jeffrey-Pound",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pound",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pound"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921567"
                        ],
                        "name": "H. Thompson",
                        "slug": "H.-Thompson",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Thompson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "To alleviate human effort, some unsupervised methods are proposed to first cluster web pages that share similar structures (for example, Blanco et al. 2011), and then deduce a shared template for each cluster of web pages."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9926859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "142f21fec89ca30221c74ffa6624d8dbecaf5a0d",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The search for entities is the most common search type on the web beside navigational searches. Whereas most common search techniques are based on the textual descriptions of web pages, semantic search approaches exploit the increasing amount of structured data on the Web in the form of annotations to web-pages and Linked Data. In many technologies, this structured data can consist of factual assertions about entities in which URIs are used to identify entities and their properties. The hypothesis is that this kind of structured data can improve entity search on the web. In order to test this hypothesis and to consistently progress in this eld, a standardized evaluation is necessary. In this work, we discuss an evaluation campaign that specically targets entity search over Linked Data by the means of keyword queries, including both queries that directly mention the entity as well as those that only describe the entities. We also discuss how crowd-sourcing was used to obtain relevance assessments from non-expert web users, the participating systems and the factors that contributed to positive results, and how the competition generalizes results from a previous crowd-sourced entity search evaluation."
            },
            "slug": "Entity-Search-Evaluation-over-Structured-Web-Data-Blanco-Halpin",
            "title": {
                "fragments": [],
                "text": "Entity Search Evaluation over Structured Web Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work discusses an evaluation campaign that specically targets entity search over Linked Data by the means of keyword queries, including both queries that directly mention the entity as well as those that only describe the entities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683498"
                        ],
                        "name": "M. Michelson",
                        "slug": "M.-Michelson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Michelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Michelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 104
                            }
                        ],
                        "text": "Obtaining dictionaries from the web is a topic that has been tackled in previous research, for example, Michelson and Knoblock (2008) and Zhang and Iria (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1408190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b0d38f4b824a523a882bf0e601e79602359a9d7",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In order for agents to act on behalf of users, they will have to retrieve and integrate vast amounts of textual data on the World Wide Web. However, much of the useful data on the Web is neither grammatical nor formally structured, making querying difficult. Examples of these types of data sources are online classifieds like Craigslist and auction item listings like eBay. We call this unstructured, ungrammatical data \"posts.\" The unstructured nature of posts makes query and integration difficult because the attributes are embedded within the text. Also, these attributes do not conform to standardized values, which prevents queries based on a common attribute value. The schema is unknown and the values may vary dramatically making accurate search difficult. Creating relational data for easy querying requires that we define a schema for the embedded attributes and extract values from the posts while standardizing these values. Traditional information extraction (IE) is inadequate to perform this task because it relies on clues from the data, such as structure or natural language, neither of which are found in posts. Furthermore, traditional information extraction does not incorporate data cleaning, which is necessary to accurately query and integrate the source. The two-step approach described in this paper creates relational data sets from unstructured and ungrammatical text by addressing both issues. To do this, we require a set of known entities called a \"reference set.\" The first step aligns each post to each member of each reference set. This allows our algorithm to define a schema over the post and include standard values for the attributes defined by this schema. The second step performs information extraction for the attributes, including attributes not easily represented by reference sets, such as a price. In this manner we create a relational structure over previously unstructured data, supporting deep and accurate queries over the data as well as standard values for integration. Our experimental results show that our technique matches the posts to the reference set accurately and efficiently and outperforms state-of-the-art extraction systems on the extraction task from posts."
            },
            "slug": "Creating-Relational-Data-from-Unstructured-and-Data-Michelson-Knoblock",
            "title": {
                "fragments": [],
                "text": "Creating Relational Data from Unstructured and Ungrammatical Data Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The two-step approach described in this paper creates relational data sets from unstructured and ungrammatical text by addressing both issues, supporting deep and accurate queries over the data as well as standard values for integration."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39756936"
                        ],
                        "name": "Tak-Lam Wong",
                        "slug": "Tak-Lam-Wong",
                        "structuredName": {
                            "firstName": "Tak-Lam",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tak-Lam Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144594306"
                        ],
                        "name": "Wai Lam",
                        "slug": "Wai-Lam",
                        "structuredName": {
                            "firstName": "Wai",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wai Lam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 155
                            }
                        ],
                        "text": "Wrapper induction (Kushmerick, Weld, and Doorenbos 1997; Muslea, Minton, and Knoblock 2003; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Wong and Lam 2010) is the task of automatically learning wrappers using a collection of manually annotated web pages as training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 60
                            }
                        ],
                        "text": "Porting wrappers across websites often requires relearning (Wong and Lam 2010), and even very slight change in structures can cause wrappers to break."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 119
                            }
                        ],
                        "text": "Creating such annotations requires significant human effort and remains a bottleneck in the wrapper induction process (Wong and Lam 2010; Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15287551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64ad134fa13221ed251d3d651298c636144201db",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework."
            },
            "slug": "Learning-to-Adapt-Web-Information-Extraction-and-a-Wong-Lam",
            "title": {
                "fragments": [],
                "text": "Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051816"
                        ],
                        "name": "Georgi Kobilarov",
                        "slug": "Georgi-Kobilarov",
                        "structuredName": {
                            "firstName": "Georgi",
                            "lastName": "Kobilarov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgi Kobilarov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729154"
                        ],
                        "name": "C. Bizer",
                        "slug": "C.-Bizer",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bizer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bizer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145044578"
                        ],
                        "name": "S. Auer",
                        "slug": "S.-Auer",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Auer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568027"
                        ],
                        "name": "Jens Lehmann",
                        "slug": "Jens-Lehmann",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Lehmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jens Lehmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 77
                            }
                        ],
                        "text": "One limitation of our experiment is the usage of a single resource from LOD (DBpedia) rather than exploiting the LOD as a whole."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "DBpedia \u2014"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 69
                            }
                        ],
                        "text": "We show results on the WI task, exploiting linked data obtained from DBpedia as learning material."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 263
                            }
                        ],
                        "text": "If querying for the property dbpedia.org/property/ genre of the concept schema.org/Movie, returned values mostly refer to the genre of the movie soundtrack than the genre of the movie, which is a result of the triple extraction process from Wikipedia to DBpedia (Kobilarov et al. 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 183
                            }
                        ],
                        "text": "org/Movie, returned values mostly refer to the genre of the movie soundtrack than the genre of the movie, which is a result of the triple extraction process from Wikipedia to DBpedia (Kobilarov et al. 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Following the running example, DBpedia SPQRQL endpoint7 can be queried for all titles (dbpedia.org/property/name) and directors (dbpedia.org/ ontology/director) of resources of type schema.org/ Movie."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 151
                            }
                        ],
                        "text": "His current research focuses on mining and exploiting background knowledge from (mostly web-based) data resources (for example, Wikipedia, Wiktionary, DBpedia, and linked data in general) to support various NLP tasks, such as semantic relatedness, disambiguation, and information extraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "For each mapped concept-attribute pair, we queried DBpedia SPARQL endpoint to retrieve all unique objects of triples with subject of type ci and with ai,k as property."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15987260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "321d84b27405caf3a7f9ecae5a30371708076e64",
            "isKey": true,
            "numCitedBy": 18,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The DBpedia project has extracted a rich knowledge base from Wikipedia and serves this knowledge base as Linked Data on the Web. DBpedia\u2019s knowledge base currently provides 274 million pieces of information about 2.6 million concepts. As DBpedia covers a wide range of domains and has a high degree of conceptual overlap with various openlicense datasets that are already available on the Web, an increasing number of data publishers has started to set data links from their data sources to DBpedia, making DBpedia one of the central interlinking hubs of the emerging Web of Data. This paper gives an overview about the DBpedia project and describes how application developers can make use of DBpedia knowledge within their applications."
            },
            "slug": "DBpedia-A-Linked-Data-Hub-and-Data-Source-for-Web-Kobilarov-Bizer",
            "title": {
                "fragments": [],
                "text": "DBpedia - A Linked Data Hub and Data Source for Web and Enterprise Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An overview about the DBpedia project is given and how application developers can make use of DBpedia knowledge within their applications are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805399"
                        ],
                        "name": "Varish Mulwad",
                        "slug": "Varish-Mulwad",
                        "structuredName": {
                            "firstName": "Varish",
                            "lastName": "Mulwad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varish Mulwad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144121212"
                        ],
                        "name": "Timothy W. Finin",
                        "slug": "Timothy-W.-Finin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Finin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy W. Finin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39955233"
                        ],
                        "name": "Zareen Syed",
                        "slug": "Zareen-Syed",
                        "structuredName": {
                            "firstName": "Zareen",
                            "lastName": "Syed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zareen Syed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145029954"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Anupam",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 93
                            }
                        ],
                        "text": "Current approaches to using LOD for web-scale IE are limited in scope to recognizing tables (Mulwad et al. 2010) and extraction of specific answers from large corpora (Balog and Serdyukov 2011), but a generalized approach to the use of LOD for training largescale IE is still missing."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17370195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d3c9b806c3d4fe0eb43ebaae8864910a7ddd4a9",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Vast amounts of information is available in structured forms like spreadsheets, database relations, and tables found in documents and on the Web. We describe an approach that uses linked data to interpret such tables and associate their components with nodes in a reference linked data collection. Our proposed framework assigns a class (i.e. type) to table columns, links table cells to entities, and inferred relations between columns to properties. The resulting interpretation can be used to annotate tables, confirm existing facts in the linked data collection, and propose new facts to be added. Our implemented prototype uses DBpedia as the linked data collection and Wikitology for background knowledge. We evaluated its performance using a collection of tables from Google Squared, Wikipedia and the Web."
            },
            "slug": "Using-Linked-Data-to-Interpret-Tables-Mulwad-Finin",
            "title": {
                "fragments": [],
                "text": "Using Linked Data to Interpret Tables"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach that uses linked data to interpret such tables and associate their components with nodes in a reference linked data collection, which assigns a class to table columns, links table cells to entities, and inferred relations between columns to properties is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLD"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030274"
                        ],
                        "name": "A. Doan",
                        "slug": "A.-Doan",
                        "structuredName": {
                            "firstName": "AnHai",
                            "lastName": "Doan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804315"
                        ],
                        "name": "Z. Ives",
                        "slug": "Z.-Ives",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Ives",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Ives"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 141
                            }
                        ],
                        "text": "An extensive range of work has been carried out to study wrapper induction in the past, and an extensive survey can be found in the paper by Doan, Halevy, and Ives (2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20197700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5982babd2c39cd0013c98801a76e3b8cbbf6fec2",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-Data-Integration-Doan-Halevy",
            "title": {
                "fragments": [],
                "text": "Principles of Data Integration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145044578"
                        ],
                        "name": "S. Auer",
                        "slug": "S.-Auer",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Auer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574714"
                        ],
                        "name": "Sebastian Tramp",
                        "slug": "Sebastian-Tramp",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Tramp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Tramp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568027"
                        ],
                        "name": "Jens Lehmann",
                        "slug": "Jens-Lehmann",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Lehmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jens Lehmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024066"
                        ],
                        "name": "Sebastian Hellmann",
                        "slug": "Sebastian-Hellmann",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Hellmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Hellmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1866677"
                        ],
                        "name": "D. Aum\u00fcller",
                        "slug": "D.-Aum\u00fcller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Aum\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Aum\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 152
                            }
                        ],
                        "text": "On LOD the majority of resources are created automatically by converting legacy databases with limited or no human validation; thus errors are present (Auer et al. 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7988851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04d582fa264e0fb18d21031b395c6afe4171659b",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present Triplify - a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project."
            },
            "slug": "Triplify:-light-weight-linked-data-publication-from-Auer-Tramp",
            "title": {
                "fragments": [],
                "text": "Triplify: light-weight linked data publication from relational databases"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Triplify is implemented as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications and is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2627799"
                        ],
                        "name": "P. Gulhane",
                        "slug": "P.-Gulhane",
                        "structuredName": {
                            "firstName": "Pankaj",
                            "lastName": "Gulhane",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gulhane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136102"
                        ],
                        "name": "Amit Madaan",
                        "slug": "Amit-Madaan",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Madaan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Madaan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259494"
                        ],
                        "name": "Rupesh R. Mehta",
                        "slug": "Rupesh-R.-Mehta",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Mehta",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rupesh R. Mehta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311735"
                        ],
                        "name": "J. Ramamirtham",
                        "slug": "J.-Ramamirtham",
                        "structuredName": {
                            "firstName": "Jeyashankher",
                            "lastName": "Ramamirtham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ramamirtham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696519"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837802"
                        ],
                        "name": "Sandeepkumar Satpal",
                        "slug": "Sandeepkumar-Satpal",
                        "structuredName": {
                            "firstName": "Sandeepkumar",
                            "lastName": "Satpal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeepkumar Satpal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757518"
                        ],
                        "name": "Srinivasan H. Sengamedu",
                        "slug": "Srinivasan-H.-Sengamedu",
                        "structuredName": {
                            "firstName": "Srinivasan",
                            "lastName": "Sengamedu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinivasan H. Sengamedu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990683"
                        ],
                        "name": "Ashwin Tengli",
                        "slug": "Ashwin-Tengli",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Tengli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashwin Tengli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081450365"
                        ],
                        "name": "Charu Tiwari",
                        "slug": "Charu-Tiwari",
                        "structuredName": {
                            "firstName": "Charu",
                            "lastName": "Tiwari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charu Tiwari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13091007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a12502ba5b9686e37b0ec9d86a2dc7f4b7022ac",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale."
            },
            "slug": "Web-scale-information-extraction-with-vertex-Gulhane-Madaan",
            "title": {
                "fragments": [],
                "text": "Web-scale information extraction with vertex"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages that is the first system to do high-precision information extraction at Web scale."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE 27th International Conference on Data Engineering"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 19
                            }
                        ],
                        "text": "Wrapper induction (Kushmerick, Weld, and Doorenbos 1997; Muslea, Minton, and Knoblock 2003; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Wong and Lam 2010) is the task of automatically learning wrappers using a collection of manually annotated web pages as training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766422"
                        ],
                        "name": "A. Arasu",
                        "slug": "A.-Arasu",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Arasu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Arasu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 91
                            }
                        ],
                        "text": "Two well-known studies in this stream are RoadRunner (Crescenzi and Mecca 2004) and EXALG (Arasu and Garcia-Molina 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207628158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "856867d7caeabdd5bba9d13574dd786aa8ee8b30",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases."
            },
            "slug": "Extracting-structured-data-from-Web-pages-Arasu-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Extracting structured data from Web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115592"
                        ],
                        "name": "Ndapandula Nakashole",
                        "slug": "Ndapandula-Nakashole",
                        "structuredName": {
                            "firstName": "Ndapandula",
                            "lastName": "Nakashole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ndapandula Nakashole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144530424"
                        ],
                        "name": "M. Theobald",
                        "slug": "M.-Theobald",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Theobald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Theobald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751591"
                        ],
                        "name": "G. Weikum",
                        "slug": "G.-Weikum",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Weikum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weikum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 239
                            }
                        ],
                        "text": "\u2026information from a gigantic data source suchas the web has been considered a major research chal-lenge, and over the years many different approaches (Etzioni et al. 2004; Banko et al. 2007; Carlson et al. 2010; Freedman and Ramshaw 2011; Nakashole, Theobald, and Weikum 2011) have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 176
                            }
                        ],
                        "text": "Both have inspired a number of more recent studies, including StatSnowball (Zhu 2009), ExtremeExtraction (Freedman and Ramshaw 2011), NELL (Carlson et al. 2010), and PROSPERA (Nakashole, Theobald, and Weikum 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1064204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af7191977af0b69cde3f17dd698a7f1e7a805f61",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Harvesting relational facts from Web sources has received great attention for automatically constructing large knowledge bases. Stateof-the-art approaches combine pattern-based gathering of fact candidates with constraint-based reasoning. However, they still face major challenges regarding the trade-offs between precision, recall, and scalability. Techniques that scale well are susceptible to noisy patterns that degrade precision, while techniques that employ deep reasoning for high precision cannot cope with Web-scale data.\n This paper presents a scalable system, called PROSPERA, for high-quality knowledge harvesting. We propose a new notion of ngram-itemsets for richer patterns, and use MaxSat-based constraint reasoning on both the quality of patterns and the validity of fact candidates.We compute pattern-occurrence statistics for two benefits: they serve to prune the hypotheses space and to derive informative weights of clauses for the reasoner. The paper shows how to incorporate these building blocks into a scalable architecture that can parallelize all phases on a Hadoop-based distributed platform. Our experiments with the ClueWeb09 corpus include comparisons to the recent ReadTheWeb experiment. We substantially outperform these prior results in terms of recall, with the same precision, while having low run-times."
            },
            "slug": "Scalable-knowledge-harvesting-with-high-precision-Nakashole-Theobald",
            "title": {
                "fragments": [],
                "text": "Scalable knowledge harvesting with high precision and high recall"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new notion of ngram-itemsets for richer patterns is proposed, and MaxSat-based constraint reasoning is used on both the quality of patterns and the validity of fact candidates, to use in a scalable system for high-quality knowledge harvesting."
            },
            "venue": {
                "fragments": [],
                "text": "WSDM '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683442"
                        ],
                        "name": "Ravi Kumar",
                        "slug": "Ravi-Kumar",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ravi Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018773"
                        ],
                        "name": "Mohamed A. Soliman",
                        "slug": "Mohamed-A.-Soliman",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Soliman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed A. Soliman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 97
                            }
                        ],
                        "text": "Although recent studies (Carlson and\nSPRING 2015 57\nSchafer 2008; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Hao et al. 2011) have focused on addressing these two issues, these methods still depend on manually labelled examples to train a wrapper, while in some cases (Dalvi,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 123
                            }
                        ],
                        "text": "Wrapper induction (Kushmerick, Weld, and Doorenbos 1997; Muslea, Minton, and Knoblock 2003; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Wong and Lam 2010) is the task of automatically learning wrappers using a collection of manually annotated web pages as training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 125
                            }
                        ],
                        "text": "Thus, research in recent years has focused on developing robust wrapper induction approaches (Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011) and methods that are general across attributes, verticals, and websites (Muslea, Minton, and Knoblock 2003; Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65ec5824f6a997df0322827285ee691510b4527a",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications."
            },
            "slug": "Automatic-Wrappers-for-Large-Scale-Web-Extraction-Dalvi-Kumar",
            "title": {
                "fragments": [],
                "text": "Automatic Wrappers for Large Scale Web Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By removing the site-level supervision that wrapper-based techniques require, this work is able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730124"
                        ],
                        "name": "P. Bohannon",
                        "slug": "P.-Bohannon",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bohannon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bohannon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 199
                            }
                        ],
                        "text": "\u20262009; Dalvi, Kumar, and Soliman 2011; Hao et al. 2011) have focused on addressing these two issues, these methods still depend on manually labelled examples to train a wrapper, while in some cases (Dalvi, Bohannon, and Sha 2009), even more training data is required to enhance wrapper robustness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 59
                            }
                        ],
                        "text": "This is often referred to as the \u201cwrapper breakage\u201d problem (Dalvi, Bohannon, and Sha 2009; Parameswaran et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 92
                            }
                        ],
                        "text": "Wrapper induction (Kushmerick, Weld, and Doorenbos 1997; Muslea, Minton, and Knoblock 2003; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Wong and Lam 2010) is the task of automatically learning wrappers using a collection of manually annotated web pages as training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 66
                            }
                        ],
                        "text": "Although recent studies (Carlson and\nSPRING 2015 57\nSchafer 2008; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Hao et al. 2011) have focused on addressing these two issues, these methods still depend on manually labelled examples to train a wrapper, while in some cases (Dalvi,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 94
                            }
                        ],
                        "text": "Thus, research in recent years has focused on developing robust wrapper induction approaches (Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011) and methods that are general across attributes, verticals, and websites (Muslea, Minton, and Knoblock 2003; Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10711472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b47e5640376b1a3858e1f8119b8588d1e7517f6c",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "On script-generated web sites, many documents share common HTML tree structure, allowing wrappers to effectively extract information of interest. Of course, the scripts and thus the tree structure evolve over time, causing wrappers to break repeatedly, and resulting in a high cost of maintaining wrappers. In this paper, we explore a novel approach: we use temporal snapshots of web pages to develop a tree-edit model of HTML, and use this model to improve wrapper construction. We view the changes to the tree structure as suppositions of a series of edit operations: deleting nodes, inserting nodes and substituting labels of nodes. The tree structures evolve by choosing these edit operations stochastically. Our model is attractive in that the probability that a source tree has evolved into a target tree can be estimated efficiently--in quadratic time in the size of the trees--making it a potentially useful tool for a variety of tree-evolution problems. We give an algorithm to learn the probabilistic model from training examples consisting of pairs of trees, and apply this algorithm to collections of web-page snapshots to derive HTML-specific tree edit models. Finally, we describe a novel wrapper-construction framework that takes the tree-edit model into account, and compare the quality of resulting wrappers to that of traditional wrappers on synthetic and real HTML document examples."
            },
            "slug": "Robust-web-extraction:-an-approach-based-on-a-model-Dalvi-Bohannon",
            "title": {
                "fragments": [],
                "text": "Robust web extraction: an approach based on a probabilistic tree-edit model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses temporal snapshots of web pages to develop a tree-edit model of HTML, and uses this model to improve wrapper construction, and gives an algorithm to learn the probabilistic model from training examples consisting of pairs of trees."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD Conference"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2959971"
                        ],
                        "name": "Rahul Parundekar",
                        "slug": "Rahul-Parundekar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Parundekar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Parundekar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887330"
                        ],
                        "name": "J. Ambite",
                        "slug": "J.-Ambite",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Ambite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ambite"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 142
                            }
                        ],
                        "text": "Although so far we only addressed synonymity of relations (Zhang et al. 2013), synonymity of concepts in different data sets can be included (Parundekar, Knoblock, and Ambite 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 995898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3259ffe9b38877ba33d596dab5ffe64bde546a1d",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 279,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the increase in the number of linked instances in the Linked Data Cloud in recent times, the absence of links at the concept level has resulted in heterogenous schemas, challenging the interoperability goal of the Semantic Web. In this paper, we address this problem by finding alignments between concepts from multiple Linked Data sources. Instead of only considering the existing concepts present in each ontology, we hypothesize new composite concepts defined as disjunctions of conjunctions of (RDF) types and value restrictions, which we call restriction classes, and generate alignments between these composite concepts. This extended concept language enables us to find more complete definitions and to even align sources that have rudimentary ontologies, such as those that are simple renderings of relational databases. Our concept alignment approach is based on analyzing the extensions of these concepts and their linked instances. Having explored the alignment of conjunctive concepts in our previous work, in this paper, we focus on concept coverings (disjunctions of restriction classes). We present an evaluation of this new algorithm to Geospatial, Biological Classification, and Genetics domains. The resulting alignments are useful for refining existing ontologies and determining the alignments between concepts in the ontologies, thus increasing the interoperability in the Linked Open Data Cloud."
            },
            "slug": "Discovering-Concept-Coverings-in-Ontologies-of-Data-Parundekar-Knoblock",
            "title": {
                "fragments": [],
                "text": "Discovering Concept Coverings in Ontologies of Linked Data Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper hypothesizes new composite concepts defined as disjunctions of conjunctions of (RDF) types and value restrictions, which are called restriction classes, and generates alignments between these composite concepts, and presents an evaluation of this new algorithm to Geospatial, Biological Classification, and Genetics domains."
            },
            "venue": {
                "fragments": [],
                "text": "SEMWEB"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1953113"
                        ],
                        "name": "Anna Lisa Gentile",
                        "slug": "Anna-Lisa-Gentile",
                        "structuredName": {
                            "firstName": "Anna Lisa",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Lisa Gentile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046811422"
                        ],
                        "name": "Ziqi Zhang",
                        "slug": "Ziqi-Zhang",
                        "structuredName": {
                            "firstName": "Ziqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziqi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117704217"
                        ],
                        "name": "Lei Xia",
                        "slug": "Lei-Xia",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145893247"
                        ],
                        "name": "J. Iria",
                        "slug": "J.-Iria",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Iria",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Iria"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": "We call this step disambiguation (Gentile et al. 2010)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11749058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69030896c1909a844be5c656d3116f97809bd345",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural Language is a mean to express and discuss about concepts, objects, events, i.e., it carries semantic contents. One of the ultimate aims of Natural Language Processing techniques is to identify the meaning of the text, providing effective ways to make a proper linkage between textual references and their referents, that is, real world objects. This work addresses the problem of giving a sense to proper names in a text, that is, automatically associating words representing Named Entities with their referents. The proposed methodology for Named Entity Disambiguation is based on Semantic Relatedness Scores obtained with a graph based model over Wikipedia. We show that, without building a Bag of Words representation of the text, but only considering named entities within the text, the proposed paradigm achieves results competitive with the state of the art on two different datasets."
            },
            "slug": "Semantic-Relatedness-Approach-for-Named-Entity-Gentile-Zhang",
            "title": {
                "fragments": [],
                "text": "Semantic Relatedness Approach for Named Entity Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work addresses the problem of giving a sense to proper names in a text, that is, automatically associating words representing Named Entities with their referents, based on Semantic Relatedness Scores obtained with a graph based model over Wikipedia."
            },
            "venue": {
                "fragments": [],
                "text": "IRCDL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052513135"
                        ],
                        "name": "Marjorie Freedman",
                        "slug": "Marjorie-Freedman",
                        "structuredName": {
                            "firstName": "Marjorie",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marjorie Freedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3256207"
                        ],
                        "name": "Elizabeth Boschee",
                        "slug": "Elizabeth-Boschee",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Boschee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Boschee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50543673"
                        ],
                        "name": "Ryan Gabbard",
                        "slug": "Ryan-Gabbard",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Gabbard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Gabbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3217960"
                        ],
                        "name": "G. Kratkiewicz",
                        "slug": "G.-Kratkiewicz",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kratkiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kratkiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052966583"
                        ],
                        "name": "Nicolas Ward",
                        "slug": "Nicolas-Ward",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 105
                            }
                        ],
                        "text": "Both have inspired a number of more recent studies, including StatSnowball (Zhu 2009), ExtremeExtraction (Freedman and Ramshaw 2011), NELL (Carlson et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 212
                            }
                        ],
                        "text": "\u2026information from a gigantic data source suchas the web has been considered a major research chal-lenge, and over the years many different approaches (Etzioni et al. 2004; Banko et al. 2007; Carlson et al. 2010; Freedman and Ramshaw 2011; Nakashole, Theobald, and Weikum 2011) have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 106
                            }
                        ],
                        "text": "Both have inspired a number of more recent studies, including StatSnowball (Zhu 2009), ExtremeExtraction (Freedman and Ramshaw 2011), NELL (Carlson et al. 2010), and PROSPERA (Nakashole, Theobald, and Weikum 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1453540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04002fc78051e803da09c4deb71e4add5c600c9a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on empirical results in extreme extraction. It is extreme in that (1) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that (2) relatively little training data is assumed. We are able to surpass human recall and achieve an F1 of 0.51 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited (5 hours) manual rule writing. We compare the performance of three systems: extraction with handwritten rules, bootstrapped extraction, and a combination. We show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and F1."
            },
            "slug": "Extreme-Extraction-\u2013-Machine-Reading-in-a-Week-Freedman-Ramshaw",
            "title": {
                "fragments": [],
                "text": "Extreme Extraction \u2013 Machine Reading in a Week"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that while the recall of the handwritten rules surpasses that of the learning system, the learned system is able to improve the overall recall and F1."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254043"
                        ],
                        "name": "Jun Zhu",
                        "slug": "Jun-Zhu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38301933"
                        ],
                        "name": "Zaiqing Nie",
                        "slug": "Zaiqing-Nie",
                        "structuredName": {
                            "firstName": "Zaiqing",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zaiqing Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3028405"
                        ],
                        "name": "Xiaojiang Liu",
                        "slug": "Xiaojiang-Liu",
                        "structuredName": {
                            "firstName": "Xiaojiang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojiang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49846744"
                        ],
                        "name": "Bo Zhang",
                        "slug": "Bo-Zhang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1788263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "604e9253336d1ee44b3c3c9b59ff4cb72b3f991b",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional relation extraction methods require pre-specified relations and relation-specific human-tagged examples. Bootstrapping systems significantly reduce the number of training examples, but they usually apply heuristic-based methods to combine a set of strict hard rules, which limit the ability to generalize and thus generate a low recall. Furthermore, existing bootstrapping methods do not perform open information extraction (Open IE), which can identify various types of relations without requiring pre-specifications. In this paper, we propose a statistical extraction framework called Statistical Snowball (StatSnowball), which is a bootstrapping system and can perform both traditional relation extraction and Open IE.\n StatSnowball uses the discriminative Markov logic networks (MLNs) and softens hard rules by learning their weights in a maximum likelihood estimate sense. MLN is a general model, and can be configured to perform different levels of relation extraction. In StatSnwoball, pattern selection is performed by solving an l1-norm penalized maximum likelihood estimation, which enjoys well-founded theories and efficient solvers. We extensively evaluate the performance of StatSnowball in different configurations on both a small but fully labeled data set and large-scale Web data. Empirical results show that StatSnowball can achieve a significantly higher recall without sacrificing the high precision during iterations with a small number of seeds, and the joint inference of MLN can improve the performance. Finally, StatSnowball is efficient and we have developed a working entity relation search engine called Renlifang based on it."
            },
            "slug": "StatSnowball:-a-statistical-approach-to-extracting-Zhu-Nie",
            "title": {
                "fragments": [],
                "text": "StatSnowball: a statistical approach to extracting entity relationships"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A statistical extraction framework called Statistical Snowball (StatSnowball), which is a bootstrapping system and can perform both traditional relation extraction and Open IE, is proposed and a working entity relation search engine called Renlifang is developed based on it."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255957"
                        ],
                        "name": "Cynthia A. Thompson",
                        "slug": "Cynthia-A.-Thompson",
                        "structuredName": {
                            "firstName": "Cynthia",
                            "lastName": "Thompson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cynthia A. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 149
                            }
                        ],
                        "text": "Research has shown\nthat the relation between the quantity of training data and learning accuracy follows a nonlinear curve with diminishing returns (Thompson, Califf, and Mooney 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1371723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37c9408d511cbc1122b5b570694eed52b04e9636",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In natural language acquisition, it is dicult to gather the annotated data needed for supervised learning; however, unannotated data is fairly plentiful. Active learning methods attempt to select for annotation and training only the most informative examples, and therefore are potentially very useful in natural language applications. However, existing results for active learning have only considered standard classication tasks. To reduce annotation eort while maintaining accuracy, we apply active learning to two non-classication tasks in natural language processing: semantic parsing and information extraction. We show that active learning can signicantly reduce the number of annotated examples required to achieve a given level of performance for these complex tasks."
            },
            "slug": "Active-Learning-for-Natural-Language-Parsing-and-Thompson-Califf",
            "title": {
                "fragments": [],
                "text": "Active Learning for Natural Language Parsing and Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that active learning can signicantly reduce the number of annotated examples required to achieve a given level of performance for these complex tasks: semantic parsing and information extraction."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046811422"
                        ],
                        "name": "Ziqi Zhang",
                        "slug": "Ziqi-Zhang",
                        "structuredName": {
                            "firstName": "Ziqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziqi Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145893247"
                        ],
                        "name": "J. Iria",
                        "slug": "J.-Iria",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Iria",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Iria"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "Obtaining dictionaries from the web is a topic that has been tackled in previous research, for example, Michelson and Knoblock (2008) and Zhang and Iria (2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14467084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2b52e94c7bc6b75c2cfa01940fd59c605628dd3",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Gazetteers or entity dictionaries are important knowledge resources for solving a wide range of NLP problems, such as entity extraction. We introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia. Unlike previous methods, our method exploits the rich content and various structural elements of Wikipedia, and does not rely on language- or domain-specific knowledge. Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers."
            },
            "slug": "A-Novel-Approach-to-Automatic-Gazetteer-Generation-Zhang-Iria",
            "title": {
                "fragments": [],
                "text": "A Novel Approach to Automatic Gazetteer Generation using Wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work introduces a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia, that exploits the rich content and various structural elements of Wikipedia, and does not rely on language- or domain-specific knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "PWNLP@IJCNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723984"
                        ],
                        "name": "H. Halpin",
                        "slug": "H.-Halpin",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Halpin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Halpin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145222158"
                        ],
                        "name": "P. Hayes",
                        "slug": "P.-Hayes",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hayes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hayes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2902113"
                        ],
                        "name": "Jamie P McCusker",
                        "slug": "Jamie-P-McCusker",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "McCusker",
                            "middleNames": [
                                "P"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie P McCusker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679913"
                        ],
                        "name": "D. McGuinness",
                        "slug": "D.-McGuinness",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "McGuinness",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McGuinness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921567"
                        ],
                        "name": "H. Thompson",
                        "slug": "H.-Thompson",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Thompson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 150
                            }
                        ],
                        "text": "Similarly, community-provided resources and annotations can contain errors, imprecision (Lopez et al. 2010), spam, or even deviations from standards (Halpin, Hayes, and McCusker 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1057253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4359bd61412e6c9fd667cb451354eb8c15341def",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In Linked Data, the use of owl:sameAs is ubiquitous in interlinking data-sets. There is however, ongoing discussion about its use, and potential misuse, particularly with regards to interactions with inference. In fact, owl:sameAs can be viewed as encoding only one point on a scale of similarity, one that is often too strong for many of its current uses. We describe how referentially opaque contexts that do not allow inference exist, and then outline some varieties of referentially-opaque alternatives to owl:sameAs. Finally, we report on an empirical experiment over randomly selected owl:sameAs statements from the Web of data. This theoretical apparatus and experiment shed light upon how owl:sameAs is being used (and misused) on the Web of data."
            },
            "slug": "When-owl:-sameAs-Isn't-the-Same:-An-Analysis-of-in-Halpin-Hayes",
            "title": {
                "fragments": [],
                "text": "When owl: sameAs Isn't the Same: An Analysis of Identity in Linked Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is described how referentially opaque contexts that do not allow inference exist, and some varieties of referential-opaque alternatives to owl:sameAs are outlined, to shed light upon how owl: sameAs is being used (and misused) on the Web of data."
            },
            "venue": {
                "fragments": [],
                "text": "SEMWEB"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145592539"
                        ],
                        "name": "Aditya G. Parameswaran",
                        "slug": "Aditya-G.-Parameswaran",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Parameswaran",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aditya G. Parameswaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696519"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 90
                            }
                        ],
                        "text": "This is often referred to as the \u201cwrapper breakage\u201d problem (Dalvi, Bohannon, and Sha 2009; Parameswaran et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3246806,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "746d1171ccc733e96676d739fe9e59ee1b3825ea",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of constructing wrappers for web information extraction that are robust to changes in websites. We consider two models to study robustness formally: the adversarial model, where we look at the worst-case robustness of wrappers, and probabilistic model, where we look at the expected robustness of wrappers, as web-pages evolve. Under both models, we present efficient algorithms for constructing the provably most robust wrapper. By evaluating on real websites, we demonstrate that in practice, our algorithms are highly effective in coping up with changes in websites, and reduce the wrapper breakage by up to 500% over existing techniques."
            },
            "slug": "Optimal-schemes-for-robust-web-extraction-Parameswaran-Dalvi",
            "title": {
                "fragments": [],
                "text": "Optimal schemes for robust web extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Under both models, efficient algorithms for constructing the provably most robust wrapper are presented, which are highly effective in coping up with changes in websites, and reduce the wrapper breakage by up to 500% over existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143818235"
                        ],
                        "name": "Andrew Carlson",
                        "slug": "Andrew-Carlson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Carlson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Carlson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31779043"
                        ],
                        "name": "J. Betteridge",
                        "slug": "J.-Betteridge",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Betteridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Betteridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16411658"
                        ],
                        "name": "B. Kisiel",
                        "slug": "B.-Kisiel",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Kisiel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kisiel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717452"
                        ],
                        "name": "Burr Settles",
                        "slug": "Burr-Settles",
                        "structuredName": {
                            "firstName": "Burr",
                            "lastName": "Settles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Burr Settles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1842532"
                        ],
                        "name": "Estevam Hruschka",
                        "slug": "Estevam-Hruschka",
                        "structuredName": {
                            "firstName": "Estevam",
                            "lastName": "Hruschka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Estevam Hruschka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 191
                            }
                        ],
                        "text": "\u2026information from a gigantic data source suchas the web has been considered a major research chal-lenge, and over the years many different approaches (Etzioni et al. 2004; Banko et al. 2007; Carlson et al. 2010; Freedman and Ramshaw 2011; Nakashole, Theobald, and Weikum 2011) have been proposed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 140
                            }
                        ],
                        "text": "Both have inspired a number of more recent studies, including StatSnowball (Zhu 2009), ExtremeExtraction (Freedman and Ramshaw 2011), NELL (Carlson et al. 2010), and PROSPERA (Nakashole, Theobald, and Weikum 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8423494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7312b8568d63bbbb239583ed282f46cdc40978d",
            "isKey": false,
            "numCitedBy": 1739,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent."
            },
            "slug": "Toward-an-Architecture-for-Never-Ending-Language-Carlson-Betteridge",
            "title": {
                "fragments": [],
                "text": "Toward an Architecture for Never-Ending Language Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work proposes an approach and a set of design principles for an intelligent computer agent that runs forever and describes a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 174
                            }
                        ],
                        "text": "This aspect has not yet been tackled in LODIE, but we will explore methods with minimum requirements in computational terms such as simple feature-overlapping-based methods (Banerjee and Pedersen 2002) and string distance metrics (Lopez et al. 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 173
                            }
                        ],
                        "text": "This aspect has not yet been tackled in LODIE, but we will explore methods with minimum requirements in computational terms such as simple feature-overlapping-based methods (Banerjee and Pedersen 2002) and string distance metrics (Lopez et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21336774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fbfb2d55c6609235ae4f102dfbbc2a44c793c13",
            "isKey": false,
            "numCitedBy": 1003,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an adaptation of Lesk's dictionary-based word sense disambiguation algorithm. Rather than using a standard dictionary as the source of glosses for our approach, the lexical database WordNet is employed. This provides a rich hierarchy of semantic relations that our algorithm can exploit. This method is evaluated using the English lexical sample data from the SENSEVAL-2 word sense disambiguation exercise, and attains an overall accuracy of 32%. This represents a significant improvement over the 16% and 23% accuracy attained by variations of the Lesk algorithm used as benchmarks during the Senseval-2 comparative exercise among word sense disambiguation systems."
            },
            "slug": "An-Adapted-Lesk-Algorithm-for-Word-Sense-Using-Banerjee-Pedersen",
            "title": {
                "fragments": [],
                "text": "An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents an adaptation of Lesk's dictionary-based word sense disambiguation algorithm that uses the lexical database WordNet as the source of glosses for this approach, and attains an overall accuracy of 32%."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 57
                            }
                        ],
                        "text": "Wrapper induction (Kushmerick, Weld, and Doorenbos 1997; Muslea, Minton, and Knoblock 2003; Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011; Wong and Lam 2010) is the task of automatically learning wrappers using a collection of manually annotated web pages as training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 230
                            }
                        ],
                        "text": "Thus, research in recent years has focused on developing robust wrapper induction approaches (Dalvi, Bohannon, and Sha 2009; Dalvi, Kumar, and Soliman 2011) and methods that are general across attributes, verticals, and websites (Muslea, Minton, and Knoblock 2003; Hao et al. 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1732348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ea0a88c70db7873ede4524ca7f967acf44fb7b4",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-view learners reduce the need for labeled data by exploiting disjoint sub-sets of features (views), each of which is sufficient for learning. Such algorithms assume that each view is a strong view (i.e., perfect learning is possible in each view). We extend the multi-view framework by introducing a novel algorithm, Aggressive Co-Testing, that exploits both strong and weak views; in a weak view, one can learn a concept that is strictly more general or specific than the target concept. Aggressive Co-Testing uses the weak views both for detecting the most informative examples in the domain and for improving the accuracy of the predictions. In a case study on 33 wrapper induction tasks, our algorithm requires significantly fewer labeled examples than existing state-of-the-art approaches."
            },
            "slug": "Active-Learning-with-Strong-and-Weak-Views:-A-Case-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "Active Learning with Strong and Weak Views: A Case Study on Wrapper Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work extends the multi-view framework by introducing a novel algorithm, Aggressive Co-Testing, that exploits both strong and weak views; in a weak view, one can learn a concept that is strictly more general or specific than the target concept."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145893247"
                        ],
                        "name": "J. Iria",
                        "slug": "J.-Iria",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Iria",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Iria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117704217"
                        ],
                        "name": "Lei Xia",
                        "slug": "Lei-Xia",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046811422"
                        ],
                        "name": "Ziqi Zhang",
                        "slug": "Ziqi-Zhang",
                        "structuredName": {
                            "firstName": "Ziqi",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziqi Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 98
                            }
                        ],
                        "text": "As opposed to approaches based on complex\nmachine-learning algorithms, for example, random walks (Iria, Xia, and Zhang 2007), we will focus on lexical-syntactic shallow pattern-generalization algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 654690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "305847f415423a517d4f16ea2372548e160ef31e",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe our work on a random walks-based approach to disambiguating people in web search results, and the implementation of a system that supports such approach, which we used to participate at Semeval'07 Web People Search task."
            },
            "slug": "WIT:-Web-People-Search-Disambiguation-using-Random-Iria-Xia",
            "title": {
                "fragments": [],
                "text": "WIT: Web People Search Disambiguation using Random Walks"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This work on a random walks-based approach to disambiguating people in web search results, and the implementation of a system that supports such approach, which was used to participate at Semeval'07 Web People Search task."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909923"
                        ],
                        "name": "Hamed Valizadegan",
                        "slug": "Hamed-Valizadegan",
                        "structuredName": {
                            "firstName": "Hamed",
                            "lastName": "Valizadegan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hamed Valizadegan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39900740"
                        ],
                        "name": "P. Tan",
                        "slug": "P.-Tan",
                        "structuredName": {
                            "firstName": "Pang-Ning",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 94
                            }
                        ],
                        "text": "We will cast filtering as a problem of detecting noise in training data (Jiang and Zhou 2004, Valizadegan and Tan 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14213136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a82186c9ccf97a1bfd61851d18a532578568597",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of identifying mislabeled training examples has been examined in several studies, with a variety of approaches developed for editing the training data to obtain better classifiers. Many of these approaches involve applying an individual or an ensemble of classifiers to the training set and filtering the mislabeled examples based on their consistency with respect to the classifier\u2019s outputs. In this study, we formulate mislabeled detection as an optimization problem and introduce a kernel-based approach for filtering the mislabeled examples. Experimental results using a variety of data sets from the UCI data repository demonstrate the effectiveness of our proposed method, compared to existing nearest-neighbor and ensemble-based filtering schemes."
            },
            "slug": "Kernel-Based-Detection-of-Mislabeled-Training-Valizadegan-Tan",
            "title": {
                "fragments": [],
                "text": "Kernel Based Detection of Mislabeled Training Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This study forms mislabeled detection as an optimization problem and introduces a kernel-based approach for filtering the mislabeling examples and demonstrates the effectiveness of the proposed method, compared to existing nearest-neighbor and ensemble-based filtering schemes."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192443"
                        ],
                        "name": "Yuan Jiang",
                        "slug": "Yuan-Jiang",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2329323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f71e4b90ce0a5fa15b7a148e97485c0b92d816a9",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Since kNN classifiers are sensitive to outliers and noise contained in the training data set, many approaches have been proposed to edit the training data so that the performance of the classifiers can be improved. In this paper, through detaching the two schemes adopted by the Depuration algorithm, two new editing approaches are derived. Moreover, this paper proposes to use neural network ensemble to edit the training data for kNN classifiers. Experiments show that such an approach is better than the approaches derived from Depuration, while these approaches are better than or comparable to Depuration."
            },
            "slug": "Editing-Training-Data-for-kNN-Classifiers-with-Jiang-Zhou",
            "title": {
                "fragments": [],
                "text": "Editing Training Data for kNN Classifiers with Neural Network Ensemble"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Through detaching the two schemes adopted by the Depuration algorithm, two new editing approaches are derived and it is proposed to use neural network ensemble to edit the training data for kNN classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ISNN"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66988009"
                        ],
                        "name": "Matem\u00e1tica",
                        "slug": "Matem\u00e1tica",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Matem\u00e1tica",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matem\u00e1tica"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 132444334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe88603a338c69f37931facd17f22d6c4b5d5fe1",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Return by mail or fax to: SIAM Connie Young, Conference Director 3600 Market Street \u2013 6 Floor Philadelphia, PA 19104-2688 Fax: 215-386-7999 Personal Information Name: _______________________________________________________________ Affiliation: _______________________________________________________________ Conference Name: _______________________________________________________________ Conference Location: _______________________________________________________________"
            },
            "slug": "Society-for-Industrial-and-Applied-Mathematics-Matem\u00e1tica",
            "title": {
                "fragments": [],
                "text": "Society for Industrial and Applied Mathematics"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Return by mail or fax to: SIAM Connie Young, Conference Director 3600 Market Street \u2013 6 Floor Philadelphia, PA 19104-2688."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A view is a smaller ontology only including concepts and relations that can describe the user needs"
            },
            "venue": {
                "fragments": [],
                "text": "A view is a smaller ontology only including concepts and relations that can describe the user needs"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 98
                            }
                        ],
                        "text": "As opposed to approaches based on complex\nmachine-learning algorithms, for example, random walks (Iria, Xia, and Zhang 2007), we will focus on lexical-syntactic shallow pattern-generalization algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WIT: Web People Search Articles SPRING"
            },
            "venue": {
                "fragments": [],
                "text": "WIT: Web People Search Articles SPRING"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "7. for example, dbpedia.org/sparql"
            },
            "venue": {
                "fragments": [],
                "text": "7. for example, dbpedia.org/sparql"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "4. schema.org/Movie"
            },
            "venue": {
                "fragments": [],
                "text": "4. schema.org/Movie"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Disambiguation Using Random Walks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval'07"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5. dbpedia.org/property/name"
            },
            "venue": {
                "fragments": [],
                "text": "5. dbpedia.org/property/name"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "6. dbpedia.org/ontology/director"
            },
            "venue": {
                "fragments": [],
                "text": "6. dbpedia.org/ontology/director"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 18
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 45,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Early-Steps-Towards-Web-Scale-Information-with-Gentile-Zhang/f355eac84c7aa4823eb55015257bf85bec3c80ae?sort=total-citations"
}