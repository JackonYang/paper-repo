{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "This permits the replacement of the di cult minimization problem (9) by least{squares minimization (11)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14796036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15a2c58b29c5a84a134d1504faff528101321f21",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting (Freund & Schapire 1996, Schapire & Singer 1998) is one of the most important recent developments in classiication methodology. The performance of many classiication algorithms can often be dramatically improved by sequentially applying them to reweighted versions of the input data, and taking a weighted majority vote of the sequence of classiiers thereby produced. We show that this seemingly mysterious phenomenon can be understood in terms of well known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multi-class generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multi-class generalizations of boosting in most situations, and far superior in some. We suggest a minor modiication to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-rst truncated tree induction , often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster com-putationally making it more suitable to large scale data mining applications ."
            },
            "slug": "Additive-Logistic-Regression-:-a-Statistical-Hastie",
            "title": {
                "fragments": [],
                "text": "Additive Logistic Regression : a Statistical"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work develops more direct approximations of boosting that exhibit performance comparable to other recently proposed multi-class generalizations of boosting, and suggests a minor modiication to boosting that can reduce computation, often by factors of 10 to 50."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Speci cally, the line search (8) is performed"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 3
                            }
                        ],
                        "text": "Du y and Helmbold 1998 elegantly exploit this analogy to motivate their GeoLev and GeoArc procedures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The multiplier m in (6) is given by the line search m = argmin Ey;x (y; Fm 1(x) gm(x)) : (8)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10301405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee6139e7f174fff4ae6b22299e80bf0151b0a64e",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-geometric-approach-to-leveraging-weak-learners-Duffy-Helmbold",
            "title": {
                "fragments": [],
                "text": "A geometric approach to leveraging weak learners"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "and Fm(x) = Fm 1(x) + lm1(x 2 Rlm): There is no closed form solution to (20)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 153
                            }
                        ],
                        "text": "In machine learning, (9) (10) is called \\boosting\" where y 2 f 1; 1g and (y; F ) is either an exponential loss criterion e yF (Freund and Schapire 1996, Schapire and Singer 1998) or negative binomial log{likelihood (Friedman, Hastie, and\nTibshirani 1998 { FHT98)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "xi2Rlm log (1 + exp( 2yi(Fm 1(xi) + ))) (20)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "MH (Schapire and Singer 1998) over the 100 randomly generated targets (Section 6.1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2329907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d",
            "isKey": true,
            "numCitedBy": 1918,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
            },
            "slug": "Improved-Boosting-Algorithms-Using-Confidence-rated-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms Using Confidence-rated Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several improvements to Freund and Schapire's AdaBoost boosting algorithm are described, particularly in a setting in which hypotheses may assign confidences to each of their predictions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6725274"
                        ],
                        "name": "J. Copas",
                        "slug": "J.-Copas",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Copas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Copas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "In most applications of numerical optimization a single line search (5) is performed along the descent direction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "The step is taken to be pm = mgm where m = argmin (Pm 1 gm) : (5)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "The negative gradient gm is said to de ne the \\steepest{descent\" direction and the last step (5) is called the \\line search\" along that direction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 202
                            }
                        ],
                        "text": "When the goal is prediction (as opposed to compression) it has often been found that regularization through shrinkage provides superior results to that obtained by restricting the number of components (Copas 1983)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 116430176,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c88d7b4f260ab4e66e274e3dda3780ef2148af37",
            "isKey": true,
            "numCitedBy": 566,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The fit of a regression predictor to new data is nearly always worse than its fit to the original data. Anticipating this shrinkage leads to Stein-type predictors which, under certain assumptions, give a uniformly lower prediction mean squared error than least squares. Shrinkage can be particularly marked when stepwise fitting is used: the shrinkage is then closer to that expected of the full regression rather than of the subset regression actually fitted. Preshrunk predictors for selected subsets are proposed and tested on a number of practical examples. Both multiple and binary (logistic) regression models are considered."
            },
            "slug": "Regression,-Prediction-and-Shrinkage-Copas",
            "title": {
                "fragments": [],
                "text": "Regression, Prediction and Shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Suppose that for a particular loss (y; F ) and/or base learner h(x; a) the solution to (9) is di cult to obtain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In machine learning, (9) (10) is called \\boosting\" where y 2 f 1; 1g and (y; F ) is either an exponential loss criterion e yF (Freund and Schapire 1996, Schapire and Singer 1998) or negative binomial log{likelihood (Friedman, Hastie, and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 134
                            }
                        ],
                        "text": "and the approximation updated Fm(x) = Fm 1(x) + mh(x; am): Basically, instead of obtaining the solution under a smoothness constraint (9), the constraint is applied to the unconstrained (rough) solution f gm(xi)g N i=1 (7)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Given the current approximation Fm 1(x) at the mth iteration, the function mh(x; am) (9) (10) is the best greedy step towards the minimizing solution F (x) (1), under the constraint that the step \\direction\" h(x; am) be a member of the parameterized class of functions h(x; a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "In the special case where y 2 f 1; 1g and the loss function (y; F ) depends on y and F only through their product (y; F ) = (yF ), the analogy of boosting (9) (10) to steepest{descent minimization has been noted in the machine learning literature (Breiman 1997a, Ratsch, Onoda, and Muller 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "In the context of additive models (2) constructed in a forward stage{wise manner (9) (10), proportional shrinkage is implemented by replacing line 6 of the generic algorithm (Algorithm 1) with Fm(x) = Fm 1(x) + mh(x; am); 1; (33) and making the corresponding equivalent changes in all of the speci c algorithms (Algorithms 2 - 6)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 127
                            }
                        ],
                        "text": "In machine learning, (9) (10) is called \\boosting\" where y 2 f 1; 1g and (y; F ) is either an exponential loss criterion e yF (Freund and Schapire 1996, Schapire and Singer 1998) or negative binomial log{likelihood (Friedman, Hastie, and\nTibshirani 1998 { FHT98)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "This permits the replacement of the di cult minimization problem (9) by least{squares minimization (11)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 141
                            }
                        ],
                        "text": "Drucker 1997 employs a di erent strategy of casting regression into the framework of classi cation in the context of the AdaBoost algorithm (Freund and Schapire 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": true,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "For additive expansions (2) a natural regularization parameter is the number of components M: This is analogous to \\stepwise\" regression where the fh(x; am)g M 1 are considered explanatory variables that are sequentially entered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Such expansions (2) are at the heart of many function approximation methods such as neural networks (Rumelhart, Hinton, and Williams 1986), radial basis functions (Powell"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "In the context of additive models (2) constructed in a forward stage{wise manner (9) (10), proportional shrinkage is implemented by replacing line 6 of the generic algorithm (Algorithm 1) with Fm(x) = Fm 1(x) + mh(x; am); 1; (33) and making the corresponding equivalent changes in all of the speci c algorithms (Algorithms 2 - 6)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "One way to do this is to assume a parameterized form such as (2) and do parameter optimization as discussed in Section 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "The functions h(x; a) in (2) are usually chosen to be simple functions of x with parameters a = fa1; a2; g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 246
                            }
                        ],
                        "text": "\u2026the special case where y 2 f 1; 1g and the loss function (y; F ) depends on y and F only through their product (y; F ) = (yF ), the analogy of boosting (9) (10) to steepest{descent minimization has been noted in the machine learning literature (Breiman 1997a, Ratsch, Onoda, and Muller 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14488820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a92684c164b0c46020a371ae5116df74bb37a412",
            "isKey": true,
            "numCitedBy": 551,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood. By formulating prediction as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. The minimax theorem is an essential ingredient of the convergence proofs. An arcing algorithm is described that converges to the optimal strategy. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Schapire, Freund, Bartlett, and Lee (1997) offered an explanation of why Adaboost works in terms of its ability to produce generally high margins. The empirical comparison of Adaboost to the optimal arcing algorithm shows that their explanation is not complete."
            },
            "slug": "Prediction-Games-and-Arcing-Algorithms-Breiman",
            "title": {
                "fragments": [],
                "text": "Prediction Games and Arcing Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost and others in reducing generalization error has not been well understood, and an explanation of whyAdaboost works in terms of its ability to produce generally high margins is offered."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122951478,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "55df65a0a36f8327cd181a42ab096036d518c9c9",
            "isKey": false,
            "numCitedBy": 984,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Radial basis function methods are modern ways to approximate multivariate functions, especially in the absence of grid data. They have been known, tested and analysed for several years now and many positive properties have been identified. This paper gives a selective but up-to-date survey of several recent developments that explains their usefulness from the theoretical point of view and contributes useful new classes of radial basis function. We consider particularly the new results on convergence rates of interpolation with radial basis functions, as well as some of the various achievements on approximation on spheres, and the efficient numerical computation of interpolants for very large sets of data. Several examples of useful applications are stated at the end of the paper."
            },
            "slug": "Radial-basis-functions-Buhmann-Buhmann",
            "title": {
                "fragments": [],
                "text": "Radial basis functions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper gives a selective but up-to-date survey of several recent developments that explains their usefulness from the theoretical point of view and contributes useful new classes of radial basis function."
            },
            "venue": {
                "fragments": [],
                "text": "Acta Numerica"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "~ yi = @ (yi; F (xi)) @F (xi) F (x)=Fm 1(x) = 2yi (1 + exp(2yiFm 1(xi)): (19)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Here the second derivatives at the mth iteration are j~ yij (2 j~ yij) with ~ yi given by (19)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 101
                            }
                        ],
                        "text": "Such expansions (2) are at the heart of many function approximation methods such as neural networks (Rumelhart, Hinton, and Williams 1986), radial basis functions (Powell\nCSIRO CMIS, Locked Bag 17, North Ryde NSW 1670; jhf@stat.stanford.edu\n1987), MARS (Friedman 1991), wavelets (Donoho 1993), and\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": true,
            "numCitedBy": 20330,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "By construction, the unconstrained negative gradient (7) gives the best steepest{descent step direction at Fm 1(x)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "gm(x) = Ey @ (y; F (x)) @F (x) jx F (x)=Fm 1(x) : (7)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 219
                            }
                        ],
                        "text": "and the approximation updated Fm(x) = Fm 1(x) + mh(x; am): Basically, instead of obtaining the solution under a smoothness constraint (9), the constraint is applied to the unconstrained (rough) solution f gm(xi)g N i=1 (7)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "This constrained negative gradient h(x; am) is used in place of the unconstrained one gm(x) (7) in the steepest{descent strategy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Drucker 1997 employs a di erent strategy of casting regression into the framework of classi cation in the context of the AdaBoost algorithm (Freund and Schapire 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16242966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d8226a52ebc70c8d97ccae10a74e1b0a3908ec1",
            "isKey": true,
            "numCitedBy": 542,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In the regression context, boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor. We use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines. Performance is analyzed on three non-linear functions and the Boston housing database. In all cases, boosting is at least equivalent, and in most cases better than bagging in terms of prediction error."
            },
            "slug": "Improving-Regressors-using-Boosting-Techniques-Drucker",
            "title": {
                "fragments": [],
                "text": "Improving Regressors using Boosting Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work uses regression trees as fundamental building blocks in bagging committee machines and boosting committee machines to build a committee of regressors that may be superior to a single regressor."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 188
                            }
                        ],
                        "text": "Of special interest here is the case where these functions are characterized by small decision trees, such as those produced by CARTTM (Breiman, Friedman, Olshen, and Stone 1983) or C4.5 (Quinlan 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21897,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026;a NX i=1 (yi; Fm 1(xi) + h(xi; a)) (9)\nand then\nFm(x) = Fm 1(x) + mh(x; am): (10)\nIn signal processing this strategy is called \\matching pursuit\" (Mallat and Zhang 1993) where (y; F ) is squared{error loss and the fh(x; am)g M 1 are called basis functions, usually taken from an over{complete\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "(y; F ) = 1 2 (y F ) 2 jy F j (jy F j =2) jy F j > : (15)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The solution to (15) (16) can be obtained by standard iterative methods (see Huber 1964)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8851,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "For steepest{descent fm(x) = mgm(x) (6) with"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 244
                            }
                        ],
                        "text": "\u2026of many function approximation methods such as neural networks (Rumelhart, Hinton, and Williams 1986), radial basis functions (Powell\nCSIRO CMIS, Locked Bag 17, North Ryde NSW 1670; jhf@stat.stanford.edu\n1987), MARS (Friedman 1991), wavelets (Donoho 1993), and support vector machines (Vapnik 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "It can thus can be viewed as a steepest{descent step (6) under that constraint."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "The multiplier m in (6) is given by the line search m = argmin Ey;x (y; Fm 1(x) gm(x)) : (8)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15691796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed422c3353edd494899163782ac411374156f33b",
            "isKey": true,
            "numCitedBy": 362,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Noise reduction by constrained reconstructions in the wavelet-transform domain. Department of Mathematics, Dart-NONLINEAR WAVELET METHODS 203 Andrew Bruce and Carl Taswell for many discussions about wavelet software. The NMR datasets were provided by Chris Raphael (Figure 1) and Jee Hoch (Figure 11), the ESCA dataset by Jean-Paul Bib erian, the image dataset by Ingrid Daubechies, the seismic dataset by Paul Donoho. Many thanks to Tina Sharp for intense last-minute editorial work. Consider now a simple recursive nonlinear multiresolution scheme based on decimating by factors of 3. The ne-to-coarse mapping is obtained by grouping the signal in triplets of successive points, and replacing each group of three by a single number { the median of the group of 3. (This is a sort of nonlinear Haar analysis, since dyadic Haar wavelets correspond to grouping data in pairs and keeping only the mean of each pair.) This triadic nonlinear coarsening operator gives rise in an obvious way to a telescoping nonlinear multiresolution decomposition. Figure 28 shows the result of setting to zero the ne scale coeecients of this nonlinear triadic transform applied to the noisy data in Figure 26(b). This clearly performs much better than the linear recovery in Figure 27. Theoretical work to date on nonlinear multiresolution analysis has been done by Ron DeVore (S. Carolina) and Bradley Lucier (Purdue). Interesting applied work with mammograms has been done by Rich Richardson (Univ. of Texas at along with the author, have developed a variety of algorithms based on ideas from robust statistics. Acknowledgements. It is a pleasure to thank Iain Johnstone, G erard Kerkyacharian, and Dominique Picard, with whom much of the theory described here has been developed , and to thank Yves Meyer, Ronald Coifman, and Ingrid Daubechies for encouragement at key moments. Speciic inspirations provided by the work of Ronald DeVore and Bjj orn Jawerth are also gratefully acknowledged, as well as stimulating conversations with Albert Cohen and Bradley Lucier. Thanks to NONLINEAR WAVELET METHODS 201 Of course, in Figure 25 we are showing what happens when the wavelet transform is segmented exactly at the point of discontinuity. How are we to obtain, in analyzing noisy data, information about the proper segmentation point? Viewing the collection of segmented wavelet transforms with diierent values of t as a collection of bases B (t) , this is really a problem of selecting a best basis. Therefore we propose a best-basis segmentation SURE(y; \u2026"
            },
            "slug": "Nonlinear-Wavelet-Methods-for-Recovery-of-Signals,-Donoho",
            "title": {
                "fragments": [],
                "text": "Nonlinear Wavelet Methods for Recovery of Signals, Densities, and Spectra from Indirect and Noisy Da"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work focuses on noise reduction by constrained reconstructions in the wavelet-transform domain and proposes a best-basis segmentation SURE(y; \u2026), a telescoping nonlinear multiresolution decomposition based on decimating by factors of 3."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 216
                            }
                        ],
                        "text": "1 Numerical optimization In general, choosing a parameterized model F (x;P) changes the function optimization problem to one of parameter optimization P = argmin P (P) = argmin P Ey;x (y; F (x;P)); F (x) = F (x;P ): (3)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 246
                            }
                        ],
                        "text": "\u2026the special case where y 2 f 1; 1g and the loss function (y; F ) depends on y and F only through their product (y; F ) = (yF ), the analogy of boosting (9) (10) to steepest{descent minimization has been noted in the machine learning literature (Breiman 1997a, Ratsch, Onoda, and Muller 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "For most F (x;P) and , numerical optimization methods must be applied to solve (3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117039944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c326a656ae9ca3a9cd5d7030e6bad0de2b394bb",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The size of many data bases have grown to the point where they cannot fit into the fast memory of even large memory machines, to say nothing of current workstations. If what we want to do is to use these data bases to construct predictions of various characteristics, then since the usual methods require that all data be held in fast memory, various work-arounds have to be used. This paper studies one such class of methods which give accuracy comparable to that which could have been obtained if all data could have been held in core and which are computationally fast. The procedure takes small bites of the data, grows a predictor on each small bite and then pastes these predictors together. The methods are also applicable to on-line learning."
            },
            "slug": "Pasting-Bites-Together-For-Prediction-In-Large-Data-Breiman",
            "title": {
                "fragments": [],
                "text": "Pasting Bites Together For Prediction In Large Data Sets And On-Line"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper studies one class of methods which give accuracy comparable to that which could have be obtained if all data could have been held in core and which are computationally fast."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117118"
                        ],
                        "name": "S. Sperlich",
                        "slug": "S.-Sperlich",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Sperlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sperlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69932841"
                        ],
                        "name": "J. Zelinka",
                        "slug": "J.-Zelinka",
                        "structuredName": {
                            "firstName": "Ji\u00e9r\u00ed",
                            "lastName": "Zelinka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelinka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Separate line searches in each such \\subset\" corresponds to applying (13) separately to the data in each terminal node of the decision tree lm = medianW yi Fm 1(xi) h(xi; am) xi 2 Rlm ; wi = jh(xi; am)j; (14)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 83
                            }
                        ],
                        "text": "In fact, considerable success is often achieved with the additive component alone (Hastie and Tibshirani 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Inserting these results (12) (13) into Algorithm 1 yields an algorithm for least-absolute-deviation boosting, based on any base learner h(x; a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61047344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87083d0fd9fb1af9c7171b26c1653f835b2bb9a3",
            "isKey": false,
            "numCitedBy": 375,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In Chapter 8 we discussed additive models (AM) of the form \n \n$$ E(Y|X) = c + \\sum\\limits_{\\alpha = 1}^d {g_\\alpha (x_\\alpha )} . $$ \n \n(1) \n \nNote that we put EY = c and E(g \u03b1 (X \u03b1 ) = 0 for identification."
            },
            "slug": "Generalized-Additive-Models-Sperlich-Zelinka",
            "title": {
                "fragments": [],
                "text": "Generalized Additive Models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In Chapter 8 the authors discussed additive models (AM) of the form E(Y|X) = c + \\sum\\limits_{alpha = 1}^d {g_alpha (x_\\alpha )} ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054683374"
                        ],
                        "name": "R. Becker",
                        "slug": "R.-Becker",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Becker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35157864"
                        ],
                        "name": "W. Cleveland",
                        "slug": "W.-Cleveland",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cleveland",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Cleveland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71268515"
                        ],
                        "name": "Ming-Jen Shyu",
                        "slug": "Ming-Jen-Shyu",
                        "structuredName": {
                            "firstName": "Ming-Jen",
                            "lastName": "Shyu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Jen Shyu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62176590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "319c6098f7f94cf24c5c637b718e721449fd4a70",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Trellis display is a framework for the visualization of data. Its most prominent aspect is an overall visual design, reminiscent of a garden trelliswork, in which panels are laid out into rows, columns, and pages. On each panel of the trellis, a subset of the data is graphed by a display method such as a scatterplot, curve plot, boxplot, 3-D wireframe, normal quantile plot, or dot plot. Each panel shows the relationship of certain variables conditional on the values of other variables. A number of display methods employed in the visual design of Trellis display enable it to succeed in uncovering the structure of data even when the structure is quite complicated. For example, Trellis display provides a powerful mechanism for understanding interactions in studies of how a response depends on explanatory variables. Three examples demonstrate this; in each case, we make important discoveries not appreciated in the original analyses. Several control methods are also essential to Trellis display. A con..."
            },
            "slug": "The-Visual-Design-and-Control-of-Trellis-Display-Becker-Cleveland",
            "title": {
                "fragments": [],
                "text": "The Visual Design and Control of Trellis Display"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Trellis display provides a powerful mechanism for understanding interactions in studies of how a response depends on explanatory variables, and makes important discoveries not appreciated in the original analyses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15090870"
                        ],
                        "name": "W. Griffin",
                        "slug": "W.-Griffin",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Griffin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152936168"
                        ],
                        "name": "N. Fisher",
                        "slug": "N.-Fisher",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Fisher",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79286965"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152424041"
                        ],
                        "name": "S. O\u2019Reilly",
                        "slug": "S.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. O\u2019Reilly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144336295"
                        ],
                        "name": "C. Ryan",
                        "slug": "C.-Ryan",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Ryan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ryan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 93
                            }
                        ],
                        "text": "This data set consists of a sample of N = 13317 garnets collected from around the world (Gri n et al 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "The trees use only order information on the input variables x, and the pseudo{responses ~ yi (12) have only two values, ~ yi 2 f 1; 1g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Inserting these results (12) (13) into Algorithm 1 yields an algorithm for least-absolute-deviation boosting, based on any base learner h(x; a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "~ yi = @ (yi; F (xi)) @F (xi) F (x)=Fm 1(x) = sign(yi Fm 1(xi)): (12)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 58924490,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "261593b742c727448885718690c6990f3e07dd6d",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Three novel statistical approaches (Cluster Analysis by Regressive Partitioning [CARP], Patient Rule Induction Method [PRIM], and ModeMap) have been used to define compositional populations within a large database (n > 13,000) of Cr\u2010pyrope garnets from the subcontinental lithospheric mantle (SCLM). The variables used are the major oxides and proton\u2010microprobe data for Zn, Ga, Sr, Y, and Zr. Because the rules defining these populations (classes) are expressed in simple compositional variables, they are easily applied to new samples and other databases. The classes defined by the three methods show strong similarities and correlations, suggesting that they are statistically meaningful. The geological significance of the classes has been tested by classifying garnets from 184 mantle\u2010derived peridotite xenoliths and from a smaller database (n > 5400) of garnets analyzed for >20 trace elements by laser ablation microprobe\u2013inductively coupled plasma\u2010mass spectrometry (LAM\u2013ICPMS). The relative abundances of these classes in the lithospheric mantle vary widely across different tectonic settings, and some classes are absent or very rare in either Archean or Phanerozoic SCLM. Their distribution with depth also varies widely within individual lithospheric sections and between different sections of similar tectonothermal age. These garnet classes therefore are a useful tool for mapping the geology of the SCLM. Archean SCLM sections show high degrees of depletion and varying degrees of metasomatism, and they are commonly strongly layered. Several Proterozoic SCLM sections show a concentration of more depleted material near their base, grading upward into more fertile lherzolites. The distribution of garnet classes reflecting low\u2010T phlogopite\u2010related metasomatism and high\u2010T melt\u2010related metasomatism suggests that many of these Proterozoic SCLM sections consist of strongly metasomatized Archean SCLM. The garnet\u2010facies SCLM beneath Phanerozoic terrains is only mildly depleted relative to Primitive Upper Mantle (PUM) compositions. These data emphasize the secular evolution of SCLM composition defined earlier [ Griffin et al., 1998 , 1999a ] and suggest that at least part of this evolutionary trend reflects reworking and refertilization of SCLM formed in the Archean time."
            },
            "slug": "Cr\u2010pyrope-garnets-in-the-lithospheric-mantle-2.-and-Griffin-Fisher",
            "title": {
                "fragments": [],
                "text": "Cr\u2010pyrope garnets in the lithospheric mantle 2. Compositional populations and their distribution in time and space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1849084"
                        ],
                        "name": "H. Warner",
                        "slug": "H.-Warner",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Warner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Warner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11769320"
                        ],
                        "name": "A. Toronto",
                        "slug": "A.-Toronto",
                        "structuredName": {
                            "firstName": "Alan F.",
                            "lastName": "Toronto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Toronto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83126137"
                        ],
                        "name": "L. Veasey",
                        "slug": "L.-Veasey",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Veasey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Veasey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068764054"
                        ],
                        "name": "R. Stephenson",
                        "slug": "R.-Stephenson",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Stephenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stephenson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Note that in uence trimming based on (22) (24) is identical to the \\weight trimming\" strategy employed with Real AdaBoost, whereas (23) (24) is equivalent to that used with LogitBoost, in FHT98."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Thus, another measure of the in uence or \\weight\" of the ith observation on the estimate mh(x; am) at the mth iteration is wi = j~ yij (2 j~ yij): (23) In uence trimming deletes all observations with wi{values less that wl( ), where l( ) is the solution to"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34658272,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "3e67f926cafcf029b3583ff44acf3f6dccdd0fae",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An equation of conditional probability is derived to express the logical process used by a clinician in making a diagnosis from clinical data. Solutions of this equation take the form of a differential diagnosis. The probability that each disease represents the correct diagnosis in any particular patient may be calculated. Sufficient statistical data regarding the incidence of clinical signs, symptoms, and electrocardiographic findings in patients with congenital heart disease have been assembled to allow application of this approach to differential diagnosis in this field. This approach provides a means by which electronic computing equipment may be used to advantage in clinical medicine."
            },
            "slug": "A-mathematical-approach-to-medical-diagnosis.-to-Warner-Toronto",
            "title": {
                "fragments": [],
                "text": "A mathematical approach to medical diagnosis. Application to congenital heart disease."
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An equation of conditional probability is derived to express the logical process used by a clinician in making a diagnosis from clinical data, and provides a means by which electronic computing equipment may be used to advantage in clinical medicine."
            },
            "venue": {
                "fragments": [],
                "text": "JAMA"
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 418,
                                "start": 414
                            }
                        ],
                        "text": "This gives the following algorithm for logit gradient boosting with decision trees: Algorithm 5: L2 TreeBoost F0(x) = 1 2 log 1+ y 1 y For m = 1 to M do: ~ yi = 2yi (1 + exp(2yiFm 1(xi)); i = 1; N fRlmg L 1 = L{terminal node tree(f~ yi;xig N 1 ) lm = P xi2Rlm ~ yi P xi2Rlm j~ yij (2 j~ yij); l = 1; L Fm(x) = Fm 1(x) + lm1(x 2 Rlm) endFor end Algorithm The nal approximation FM (x) is related to log{odds through (18)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 195
                            }
                        ],
                        "text": "4 Two{class logistic regression Here the loss function is negative binomial log{likelihood (FHT98) (y; F ) = log (1 + exp( 2yF )) ; y 2 f 1; 1g; where F (x) = 1 2 log Pr(y = 1 jx) Pr(y = 1 jx) : (18)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4042,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 287
                            }
                        ],
                        "text": "\u2026of many function approximation methods such as neural networks (Rumelhart, Hinton, and Williams 1986), radial basis functions (Powell\nCSIRO CMIS, Locked Bag 17, North Ryde NSW 1670; jhf@stat.stanford.edu\n1987), MARS (Friedman 1991), wavelets (Donoho 1993), and support vector machines (Vapnik 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1927644"
                        ],
                        "name": "F. R. Forst",
                        "slug": "F.-R.-Forst",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Forst",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. R. Forst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Separate line searches in each such \\subset\" corresponds to applying (13) separately to the data in each terminal node of the decision tree lm = medianW yi Fm 1(xi) h(xi; am) xi 2 Rlm ; wi = jh(xi; am)j; (14)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 142
                            }
                        ],
                        "text": "\u2026the update is\nFm(x) = Fm 1(x) + lm1(x 2 Rlm):\nThe solution to (17) can be approximated by a single step of the standard iterative procedure (Huber 1964) starting at the median\n~rlm = medianxi2Rlmfrm 1(xi)g:\nwhere frm 1(xi)g N 1 are the current residuals\nrm 1(xi) = yi Fm 1(xi):\nThe\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "However, the values of fh(xi; am) jxi 2 Rlmg are all equal to the same value h(xi; am) = hlm1(xi 2 Rlm), so that (14) reduces to lm = 1 hlm median fyi Fm 1(xi)j xi 2 Rlmg"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 37
                            }
                        ],
                        "text": "We consider the Huber loss function (Huber 1964)\n(y; F ) =\n1 2 (y F ) 2 jy F j (jy F j =2) jy F j >\n: (15)\nHere the pseudo{response is\n~yi =\n@ (yi; F (xi))\n@F (xi)\nF (x)=Fm 1(x)\n=\nyi Fm 1(xi) jyi Fm 1(xi)j\nsign(yi Fm 1(xi)) jyi Fm 1(xi)j > :\nand the line search becomes\nm = argmin NX i=1 (yi; Fm\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 77
                            }
                        ],
                        "text": "The solution to (15) (16) can be obtained by standard iterative methods (see Huber 1964)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61846277,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c87d57da3b1f2b467ef4995d30df832ee2281107",
            "isKey": true,
            "numCitedBy": 3976,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-robust-estimation-of-the-location-parameter-Forst",
            "title": {
                "fragments": [],
                "text": "On robust estimation of the location parameter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "and Fm(x) = Fm 1(x) + lm1(x 2 Rlm): There is no closed form solution to (20)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 153
                            }
                        ],
                        "text": "In machine learning, (9) (10) is called \\boosting\" where y 2 f 1; 1g and (y; F ) is either an exponential loss criterion e yF (Freund and Schapire 1996, Schapire and Singer 1998) or negative binomial log{likelihood (Friedman, Hastie, and\nTibshirani 1998 { FHT98)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "xi2Rlm log (1 + exp( 2yi(Fm 1(xi) + ))) (20)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "MH (Schapire and Singer 1998) over the 100 randomly generated targets (Section 6.1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12646365,
            "fieldsOfStudy": [],
            "id": "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms using Confidence-Rated Predictions"
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 376,
                                "start": 1
                            }
                        ],
                        "text": "The (generic) function h(x; a) in (2) is usually a simple parameterized function of the input variables x, characterized by parameters a = {a1, a2, . . .}. The individual terms differ in the joint values am chosen for these parameters. Such expansions (2) are at the heart of many function approximation methods such as neural networks [Rumelhart, Hinton, and Williams (1986)], radial basis functions [Powell (1987)], MARS [Friedman (1991)], wavelets [Donoho (1993)] and support vector machines [Vapnik (1995)]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 216
                            }
                        ],
                        "text": "Such expansions (2) are at the heart of many function approximation methods such as neural networks [Rumelhart, Hinton, and Williams (1986)], radial basis functions [Powell (1987)], MARS [Friedman (1991)], wavelets [Donoho (1993)] and support vector machines [Vapnik (1995)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 456,
                                "start": 216
                            }
                        ],
                        "text": "Such expansions (2) are at the heart of many function approximation methods such as neural networks [Rumelhart, Hinton, and Williams (1986)], radial basis functions [Powell (1987)], MARS [Friedman (1991)], wavelets [Donoho (1993)] and support vector machines [Vapnik (1995)]. Of special interest here is the case where each of the functions h(x; am) is a small regression tree, such as those produced by CART TM [Breiman, Friedman, Olshen and Stone (1983)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Duffy and Helmbold (1999) elegantly exploit this analogy to motivate their GeoLev and GeoArc procedures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 388,
                                "start": 0
                            }
                        ],
                        "text": "Duffy and Helmbold (1999) elegantly exploit this analogy to motivate their GeoLev and GeoArc procedures. The quantity yF is called the \"margin\" and the steepest-descent is performed in the space of margin values, rather than the space of function values F. The latter approach permits application to more general loss functions where the notion of margins is not apparent. Drucker (1997) employs a different strategy of casting regression into the framework of classification in the context of the AdaBoost algorithm [Freund and Schapire (1996)]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 53
                            }
                        ],
                        "text": "can be easily interpreted, but due to instability such interpretations should be treated with caution. The interpretability of larger trees is questionable [Ripley (1996)]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear wavelete methods for recovery"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 327,
                                "start": 324
                            }
                        ],
                        "text": "Given a \\training\" sample fyi;xig N 1 of known (y;x){values, the goal is to nd a function F (x) that maps x to y, such that over the joint distribution of all (y;x){values, the expected value of some speci ed loss function (y; F (x)) is minimized F (x) = argmin F (x) Ey;x (y; F (x)) = argmin F (x) Ex [Ey( (y; F (x)) jx] : (1)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 309,
                                "start": 306
                            }
                        ],
                        "text": "Although the number of such pieces is generally much larger than that produced by a single tree, this aspect of the approximating function F\u0302M (x) might be expected to represent a disadvantage with respect to methods that provide continuous approximations, especially when the true underlying target F (x) (1) is continuous and fairly smooth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "To the extent that F\u0302 (x) at least qualitatively re ects the nature of the target function F (x) (1), such tools can provide information concerning the underlying relationship between the inputs x and the output variable y."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "Given the current approximation Fm 1(x) at the mth iteration, the function mh(x; am) (9) (10) is the best greedy step towards the minimizing solution F (x) (1), under the constraint that the step \\direction\" h(x; am) be a member of the parameterized class of functions h(x; a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "1 Random function generator One of the most important characteristics of any problem a ecting performance is the true underlying target function F (x) (1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 134
                            }
                        ],
                        "text": "Important characteristics of problems that a ect performance include training sample size N , true underlying \\target\" function F (x) (1), and the distribution of the departures, \", of y jx from F (x)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Note that this error measure (51) includes the irreducible error associated with the (unknown) underlying target function F (x) (1), so that it will typically be substantially larger than the error (34) in approximating the target itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 205
                            }
                        ],
                        "text": "This can be inverted to yield probability estimates p+(x) = c Pr(y = 1 jx) = 1/ (1 + e 2FM ) p (x) = c Pr(y = 1 jx) = 1/ (1 + eM ): These in turn can be used for classi cation \u0177(x) = 2 1[c( 1; 1) p+(x) > c(1; 1) p (x)] 1 where c(\u0177; y) is the cost associated with predicting \u0177 when the truth is y."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The design and control of Trellis display"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. & Statist. Graphics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "\u2026of many function approximation methods such as neural networks (Rumelhart, Hinton, and Williams 1986), radial basis functions (Powell\nCSIRO CMIS, Locked Bag 17, North Ryde NSW 1670; jhf@stat.stanford.edu\n1987), MARS (Friedman 1991), wavelets (Donoho 1993), and support vector machines (Vapnik 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "In machine learning, (9) (10) is called \\boosting\" where y 2 f 1; 1g and (y; F ) is either an exponential loss criterion e yF (Freund and Schapire 1996, Schapire and Singer 1998) or negative binomial log{likelihood (Friedman, Hastie, and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 173
                            }
                        ],
                        "text": "It is interesting to note that here the performance of MARS was considerably enhanced by using the 2500 test set for model selection, rather than its default GCV criterion (Friedman 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Given the current approximation Fm 1(x) at the mth iteration, the function mh(x; am) (9) (10) is the best greedy step towards the minimizing solution F (x) (1), under the constraint that the step \\direction\" h(x; am) be a member of the parameterized class of functions h(x; a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "In this section we investigate the extent of the piecewise{ constant disadvantage by comparing the accuracy of Gradient TreeBoost with that of MARS (Friedman 1991) over these 100 targets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "In the special case where y 2 f 1; 1g and the loss function (y; F ) depends on y and F only through their product (y; F ) = (yF ), the analogy of boosting (9) (10) to steepest{descent minimization has been noted in the machine learning literature (Breiman 1997a, Ratsch, Onoda, and Muller 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "In the context of additive models (2) constructed in a forward stage{wise manner (9) (10), proportional shrinkage is implemented by replacing line 6 of the generic algorithm (Algorithm 1) with Fm(x) = Fm 1(x) + mh(x; am); 1; (33) and making the corresponding equivalent changes in all of the speci c algorithms (Algorithms 2 - 6)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "and then Fm(x) = Fm 1(x) + mh(x; am): (10) In signal processing this strategy is called \\matching pursuit\" (Mallat and Zhang 1993) where (y; F ) is squared{error loss and the fh(x; am)g M 1 are called basis functions, usually taken from an over{complete wavelet{like dictionary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 3
                            }
                        ],
                        "text": "In Breiman et al 1983, the in uence measure (40) is augmented by a strategy involving surrogate splits intended to uncover the masking of in uential variables by others highly associated with them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Breiman et al 1983 proposed\nJ\u03022j (T ) = L 1X t=1 I\u03022t 1(vt = j) (40)\nwhere the summation is over the non{terminal nodes t of the L{terminal node tree T , vt is the splitting variable associated with node t, and I\u03022t is the corresponding empirical improvement in squared{error (32) as a result of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Breiman et al 1983 used (40) directly as a measure of in uence, rather than squared{in uence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classi cation and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 153
                            }
                        ],
                        "text": "In machine learning, (9) (10) is called \\boosting\" where y 2 f 1; 1g and (y; F ) is either an exponential loss criterion e yF (Freund and Schapire 1996, Schapire and Singer 1998) or negative binomial log{likelihood (Friedman, Hastie, and\nTibshirani 1998 { FHT98)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "MH (Schapire and Singer 1998) over the 100 randomly generated targets (Section 6.1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved boosting algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118224933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c71ca26b183025b9f39f940f5e730f2c9a64e414",
            "isKey": false,
            "numCitedBy": 1425,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-functions-for-multivariable-a-review-Powell",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariable interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Note that in uence trimming based on (22) (24) is identical to the \\weight trimming\" strategy employed with Real AdaBoost, whereas (23) (24) is equivalent to that used with LogitBoost, in FHT98."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Thus, wi = exp( 2yiFm 1(xi)) (22) can be viewed as a measure of the \\in uence\" or weight of the ith observation on the estimate mh(x; am)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 287
                            }
                        ],
                        "text": "\u2026of many function approximation methods such as neural networks (Rumelhart, Hinton, and Williams 1986), radial basis functions (Powell\nCSIRO CMIS, Locked Bag 17, North Ryde NSW 1670; jhf@stat.stanford.edu\n1987), MARS (Friedman 1991), wavelets (Donoho 1993), and support vector machines (Vapnik 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": false,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The design and control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft margins for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist. 19 1-141."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive logistic regression: a statistical view of boosting (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist. 28 337-407."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The solution to (15) (16) can be obtained by standard iterative methods (see Huber 1964)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariate interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": "In Algorithms for Approximation,"
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 16,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Greedy-function-approximation:-A-gradient-boosting-Friedman/1679beddda3a183714d380e944fe6bf586c083cd?sort=total-citations"
}